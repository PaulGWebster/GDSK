mm0,%xmm5
	vaesenc	%xmm15,%xmm11,%xmm11
	movbeq	64(%r14),%r12
	vpclmulqdq	$0x11,%xmm1,%xmm0,%xmm1
	vmovdqu	96+8(%rsp),%xmm0
	vaesenc	%xmm15,%xmm12,%xmm12
	movq	%r13,48+8(%rsp)
	vaesenc	%xmm15,%xmm13,%xmm13
	movq	%r12,56+8(%rsp)
	vpxor	%xmm2,%xmm4,%xmm4
	vmovdqu	96-32(%r9),%xmm2
	vaesenc	%xmm15,%xmm14,%xmm14

	vmovups	80-128(%rcx),%xmm15
	vpxor	%xmm3,%xmm6,%xmm6
	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm3
	vaesenc	%xmm15,%xmm9,%xmm9
	vpxor	%xmm5,%xmm6,%xmm6
	vpclmulqdq	$0x10,%xmm2,%xmm0,%xmm5
	vaesenc	%xmm15,%xmm10,%xmm10
	movbeq	56(%r14),%r13
	vpxor	%xmm1,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm2,%xmm0,%xmm1
	vpxor	112+8(%rsp),%xmm8,%xmm8
	vaesenc	%xmm15,%xmm11,%xmm11
	movbeq	48(%r14),%r12
	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm2
	vaesenc	%xmm15,%xmm12,%xmm12
	movq	%r13,64+8(%rsp)
	vaesenc	%xmm15,%xmm13,%xmm13
	movq	%r12,72+8(%rsp)
	vpxor	%xmm3,%xmm4,%xmm4
	vmovdqu	112-32(%r9),%xmm3
	vaesenc	%xmm15,%xmm14,%xmm14

	vmovups	96-128(%rcx),%xmm15
	vpxor	%xmm5,%xmm6,%xmm6
	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm5
	vaesenc	%xmm15,%xmm9,%xmm9
	vpxor	%xmm1,%xmm6,%xmm6
	vpclmulqdq	$0x01,%xmm3,%xmm8,%xmm1
	vaesenc	%xmm15,%xmm10,%xmm10
	movbeq	40(%r14),%r13
	vpxor	%xmm2,%xmm7,%xmm7
	vpclmulqdq	$0x00,%xmm3,%xmm8,%xmm2
	vaesenc	%xmm15,%xmm11,%xmm11
	movbeq	32(%r14),%r12
	vpclmulqdq	$0x11,%xmm3,%xmm8,%xmm8
	vaesenc	%xmm15,%xmm12,%xmm12
	movq	%r13,80+8(%rsp)
	vaesenc	%xmm15,%xmm13,%xmm13
	movq	%r12,88+8(%rsp)
	vpxor	%xmm5,%xmm6,%xmm6
	vaesenc	%xmm15,%xmm14,%xmm14
	vpxor	%xmm1,%xmm6,%xmm6

	vmovups	112-128(%rcx),%xmm15
	vpslldq	$8,%xmm6,%xmm5
	vpxor	%xmm2,%xmm4,%xmm4
	vmovdqu	16(%r11),%xmm3

	vaesenc	%xmm15,%xmm9,%xmm9
	vpxor	%xmm8,%xmm7,%xmm7
	vaesenc	%xmm15,%xmm10,%xmm10
	vpxor	%xmm5,%xmm4,%xmm4
	movbeq	24(%r14),%r13
	vaesenc	%xmm15,%xmm11,%xmm11
	movbeq	16(%r14),%r12
	vpalignr	$8,%xmm4,%xmm4,%xmm0
	vpclmulqdq	$0x10,%xmm3,%xmm4,%xmm4
	movq	%r13,96+8(%rsp)
	vaesenc	%xmm15,%xmm12,%xmm12
	movq	%r12,104+8(%rsp)
	vaesenc	%xmm15,%xmm13,%xmm13
	vmovups	128-128(%rcx),%xmm1
	vaesenc	%xmm15,%xmm14,%xmm14

	vaesenc	%xmm1,%xmm9,%xmm9
	vmovups	144-128(%rcx),%xmm15
	vaesenc	%xmm1,%xmm10,%xmm10
	vpsrldq	$8,%xmm6,%xmm6
	vaesenc	%xmm1,%xmm11,%xmm11
	vpxor	%xmm6,%xmm7,%xmm7
	vaesenc	%xmm1,%xmm12,%xmm12
	vpxor	%xmm0,%xmm4,%xmm4
	movbeq	8(%r14),%r13
	vaesenc	%xmm1,%xmm13,%xmm13
	movbeq	0(%r14),%r12
	vaesenc	%xmm1,%xmm14,%xmm14
	vmovups	160-128(%rcx),%xmm1
	cmpl	$11,%ebp
	jb	.Lenc_tail

	vaesenc	%xmm15,%xmm9,%xmm9
	vaesenc	%xmm15,%xmm10,%xmm10
	vaesenc	%xmm15,%xmm11,%xmm11
	vaesenc	%xmm15,%xmm12,%xmm12
	vaesenc	%xmm15,%xmm13,%xmm13
	vaesenc	%xmm15,%xmm14,%xmm14

	vaesenc	%xmm1,%xmm9,%xmm9
	vaesenc	%xmm1,%xmm10,%xmm10
	vaesenc	%xmm1,%xmm11,%xmm11
	vaesenc	%xmm1,%xmm12,%xmm12
	vaesenc	%xmm1,%xmm13,%xmm13
	vmovups	176-128(%rcx),%xmm15
	vaesenc	%xmm1,%xmm14,%xmm14
	vmovups	192-128(%rcx),%xmm1
	je	.Lenc_tail

	vaesenc	%xmm15,%xmm9,%xmm9
	vaesenc	%xmm15,%xmm10,%xmm10
	vaesenc	%xmm15,%xmm11,%xmm11
	vaesenc	%xmm15,%xmm12,%xmm12
	vaesenc	%xmm15,%xmm13,%xmm13
	vaesenc	%xmm15,%xmm14,%xmm14

	vaesenc	%xmm1,%xmm9,%xmm9
	vaesenc	%xmm1,%xmm10,%xmm10
	vaesenc	%xmm1,%xmm11,%xmm11
	vaesenc	%xmm1,%xmm12,%xmm12
	vaesenc	%xmm1,%xmm13,%xmm13
	vmovups	208-128(%rcx),%xmm15
	vaesenc	%xmm1,%xmm14,%xmm14
	vmovups	224-128(%rcx),%xmm1
	jmp	.Lenc_tail

.align	32
.Lhandle_ctr32:
	vmovdqu	(%r11),%xmm0
	vpshufb	%xmm0,%xmm1,%xmm6
	vmovdqu	48(%r11),%xmm5
	vpaddd	64(%r11),%xmm6,%xmm10
	vpaddd	%xmm5,%xmm6,%xmm11
	vmovdqu	0-32(%r9),%xmm3
	vpaddd	%xmm5,%xmm10,%xmm12
	vpshufb	%xmm0,%xmm10,%xmm10
	vpaddd	%xmm5,%xmm11,%xmm13
	vpshufb	%xmm0,%xmm11,%xmm11
	vpxor	%xmm15,%xmm10,%xmm10
	vpaddd	%xmm5,%xmm12,%xmm14
	vpshufb	%xmm0,%xmm12,%xmm12
	vpxor	%xmm15,%xmm11,%xmm11
	vpaddd	%xmm5,%xmm13,%xmm1
	vpshufb	%xmm0,%xmm13,%xmm13
	vpshufb	%xmm0,%xmm14,%xmm14
	vpshufb	%xmm0,%xmm1,%xmm1
	jmp	.Lresume_ctr32

.align	32
.Lenc_tail:
	vaesenc	%xmm15,%xmm9,%xmm9
	vmovdqu	%xmm7,16+8(%rsp)
	vpalignr	$8,%xmm4,%xmm4,%xmm8
	vaesenc	%xmm15,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm3,%xmm4,%xmm4
	vpxor	0(%rdi),%xmm1,%xmm2
	vaesenc	%xmm15,%xmm11,%xmm11
	vpxor	16(%rdi),%xmm1,%xmm0
	vaesenc	%xmm15,%xmm12,%xmm12
	vpxor	32(%rdi),%xmm1,%xmm5
	vaesenc	%xmm15,%xmm13,%xmm13
	vpxor	48(%rdi),%xmm1,%xmm6
	vaesenc	%xmm15,%xmm14,%xmm14
	vpxor	64(%rdi),%xmm1,%xmm7
	vpxor	80(%rdi),%xmm1,%xmm3
	vmovdqu	(%r8),%xmm1

	vaesenclast	%xmm2,%xmm9,%xmm9
	vmovdqu	32(%r11),%xmm2
	vaesenclast	%xmm0,%xmm10,%xmm10
	vpaddb	%xmm2,%xmm1,%xmm0
	movq	%r13,112+8(%rsp)
	leaq	96(%rdi),%rdi
	vaesenclast	%xmm5,%xmm11,%xmm11
	vpaddb	%xmm2,%xmm0,%xmm5
	movq	%r12,120+8(%rsp)
	leaq	96(%rsi),%rsi
	vmovdqu	0-128(%rcx),%xmm15
	vaesenclast	%xmm6,%xmm12,%xmm12
	vpaddb	%xmm2,%xmm5,%xmm6
	vaesenclast	%xmm7,%xmm13,%xmm13
	vpaddb	%xmm2,%xmm6,%xmm7
	vaesenclast	%xmm3,%xmm14,%xmm14
	vpaddb	%xmm2,%xmm7,%xmm3

	addq	$0x60,%r10
	subq	$0x6,%rdx
	jc	.L6x_done

	vmovups	%xmm9,-96(%rsi)
	vpxor	%xmm15,%xmm1,%xmm9
	vmovups	%xmm10,-80(%rsi)
	vmovdqa	%xmm0,%xmm10
	vmovups	%xmm11,-64(%rsi)
	vmovdqa	%xmm5,%xmm11
	vmovups	%xmm12,-48(%rsi)
	vmovdqa	%xmm6,%xmm12
	vmovups	%xmm13,-32(%rsi)
	vmovdqa	%xmm7,%xmm13
	vmovups	%xmm14,-16(%rsi)
	vmovdqa	%xmm3,%xmm14
	vmovdqu	32+8(%rsp),%xmm7
	jmp	.Loop6x

.L6x_done:
	vpxor	16+8(%rsp),%xmm8,%xmm8
	vpxor	%xmm4,%xmm8,%xmm8

	.byte	0xf3,0xc3
.cfi_endproc	
.size	_aesni_ctr32_ghash_6x,.-_aesni_ctr32_ghash_6x
.globl	aesni_gcm_decrypt
.type	aesni_gcm_decrypt,@function
.align	32
aesni_gcm_decrypt:
.cfi_startproc	
	xorq	%r10,%r10
	cmpq	$0x60,%rdx
	jb	.Lgcm_dec_abort

	leaq	(%rsp),%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	vzeroupper

	vmovdqu	(%r8),%xmm1
	addq	$-128,%rsp
	movl	12(%r8),%ebx
	leaq	.Lbswap_mask(%rip),%r11
	leaq	-128(%rcx),%r14
	movq	$0xf80,%r15
	vmovdqu	(%r9),%xmm8
	andq	$-128,%rsp
	vmovdqu	(%r11),%xmm0
	leaq	128(%rcx),%rcx
	leaq	32+32(%r9),%r9
	movl	240-128(%rcx),%ebp
	vpshufb	%xmm0,%xmm8,%xmm8

	andq	%r15,%r14
	andq	%rsp,%r15
	subq	%r14,%r15
	jc	.Ldec_no_key_aliasing
	cmpq	$768,%r15
	jnc	.Ldec_no_key_aliasing
	subq	%r15,%rsp
.Ldec_no_key_aliasing:

	vmovdqu	80(%rdi),%xmm7
	leaq	(%rdi),%r14
	vmovdqu	64(%rdi),%xmm4
	leaq	-192(%rdi,%rdx,1),%r15
	vmovdqu	48(%rdi),%xmm5
	shrq	$4,%rdx
	xorq	%r10,%r10
	vmovdqu	32(%rdi),%xmm6
	vpshufb	%xmm0,%xmm7,%xmm7
	vmovdqu	16(%rdi),%xmm2
	vpshufb	%xmm0,%xmm4,%xmm4
	vmovdqu	(%rdi),%xmm3
	vpshufb	%xmm0,%xmm5,%xmm5
	vmovdqu	%xmm4,48(%rsp)
	vpshufb	%xmm0,%xmm6,%xmm6
	vmovdqu	%xmm5,64(%rsp)
	vpshufb	%xmm0,%xmm2,%xmm2
	vmovdqu	%xmm6,80(%rsp)
	vpshufb	%xmm0,%xmm3,%xmm3
	vmovdqu	%xmm2,96(%rsp)
	vmovdqu	%xmm3,112(%rsp)

	call	_aesni_ctr32_ghash_6x

	vmovups	%xmm9,-96(%rsi)
	vmovups	%xmm10,-80(%rsi)
	vmovups	%xmm11,-64(%rsi)
	vmovups	%xmm12,-48(%rsi)
	vmovups	%xmm13,-32(%rsi)
	vmovups	%xmm14,-16(%rsi)

	vpshufb	(%r11),%xmm8,%xmm8
	vmovdqu	%xmm8,-64(%r9)

	vzeroupper
	movq	-48(%rax),%r15
.cfi_restore	%r15
	movq	-40(%rax),%r14
.cfi_restore	%r14
	movq	-32(%rax),%r13
.cfi_restore	%r13
	movq	-24(%rax),%r12
.cfi_restore	%r12
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lgcm_dec_abort:
	movq	%r10,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
.type	_aesni_ctr32_6x,@function
.align	32
_aesni_ctr32_6x:
.cfi_startproc	
	vmovdqu	0-128(%rcx),%xmm4
	vmovdqu	32(%r11),%xmm2
	leaq	-1(%rbp),%r13
	vmovups	16-128(%rcx),%xmm15
	leaq	32-128(%rcx),%r12
	vpxor	%xmm4,%xmm1,%xmm9
	addl	$100663296,%ebx
	jc	.Lhandle_ctr32_2
	vpaddb	%xmm2,%xmm1,%xmm10
	vpaddb	%xmm2,%xmm10,%xmm11
	vpxor	%xmm4,%xmm10,%xmm10
	vpaddb	%xmm2,%xmm11,%xmm12
	vpxor	%xmm4,%xmm11,%xmm11
	vpaddb	%xmm2,%xmm12,%xmm13
	vpxor	%xmm4,%xmm12,%xmm12
	vpaddb	%xmm2,%xmm13,%xmm14
	vpxor	%xmm4,%xmm13,%xmm13
	vpaddb	%xmm2,%xmm14,%xmm1
	vpxor	%xmm4,%xmm14,%xmm14
	jmp	.Loop_ctr32

.align	16
.Loop_ctr32:
	vaesenc	%xmm15,%xmm9,%xmm9
	vaesenc	%xmm15,%xmm10,%xmm10
	vaesenc	%xmm15,%xmm11,%xmm11
	vaesenc	%xmm15,%xmm12,%xmm12
	vaesenc	%xmm15,%xmm13,%xmm13
	vaesenc	%xmm15,%xmm14,%xmm14
	vmovups	(%r12),%xmm15
	leaq	16(%r12),%r12
	decl	%r13d
	jnz	.Loop_ctr32

	vmovdqu	(%r12),%xmm3
	vaesenc	%xmm15,%xmm9,%xmm9
	vpxor	0(%rdi),%xmm3,%xmm4
	vaesenc	%xmm15,%xmm10,%xmm10
	vpxor	16(%rdi),%xmm3,%xmm5
	vaesenc	%xmm15,%xmm11,%xmm11
	vpxor	32(%rdi),%xmm3,%xmm6
	vaesenc	%xmm15,%xmm12,%xmm12
	vpxor	48(%rdi),%xmm3,%xmm8
	vaesenc	%xmm15,%xmm13,%xmm13
	vpxor	64(%rdi),%xmm3,%xmm2
	vaesenc	%xmm15,%xmm14,%xmm14
	vpxor	80(%rdi),%xmm3,%xmm3
	leaq	96(%rdi),%rdi

	vaesenclast	%xmm4,%xmm9,%xmm9
	vaesenclast	%xmm5,%xmm10,%xmm10
	vaesenclast	%xmm6,%xmm11,%xmm11
	vaesenclast	%xmm8,%xmm12,%xmm12
	vaesenclast	%xmm2,%xmm13,%xmm13
	vaesenclast	%xmm3,%xmm14,%xmm14
	vmovups	%xmm9,0(%rsi)
	vmovups	%xmm10,16(%rsi)
	vmovups	%xmm11,32(%rsi)
	vmovups	%xmm12,48(%rsi)
	vmovups	%xmm13,64(%rsi)
	vmovups	%xmm14,80(%rsi)
	leaq	96(%rsi),%rsi

	.byte	0xf3,0xc3
.align	32
.Lhandle_ctr32_2:
	vpshufb	%xmm0,%xmm1,%xmm6
	vmovdqu	48(%r11),%xmm5
	vpaddd	64(%r11),%xmm6,%xmm10
	vpaddd	%xmm5,%xmm6,%xmm11
	vpaddd	%xmm5,%xmm10,%xmm12
	vpshufb	%xmm0,%xmm10,%xmm10
	vpaddd	%xmm5,%xmm11,%xmm13
	vpshufb	%xmm0,%xmm11,%xmm11
	vpxor	%xmm4,%xmm10,%xmm10
	vpaddd	%xmm5,%xmm12,%xmm14
	vpshufb	%xmm0,%xmm12,%xmm12
	vpxor	%xmm4,%xmm11,%xmm11
	vpaddd	%xmm5,%xmm13,%xmm1
	vpshufb	%xmm0,%xmm13,%xmm13
	vpxor	%xmm4,%xmm12,%xmm12
	vpshufb	%xmm0,%xmm14,%xmm14
	vpxor	%xmm4,%xmm13,%xmm13
	vpshufb	%xmm0,%xmm1,%xmm1
	vpxor	%xmm4,%xmm14,%xmm14
	jmp	.Loop_ctr32
.cfi_endproc	
.size	_aesni_ctr32_6x,.-_aesni_ctr32_6x

.globl	aesni_gcm_encrypt
.type	aesni_gcm_encrypt,@function
.align	32
aesni_gcm_encrypt:
.cfi_startproc	
	xorq	%r10,%r10
	cmpq	$288,%rdx
	jb	.Lgcm_enc_abort

	leaq	(%rsp),%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	vzeroupper

	vmovdqu	(%r8),%xmm1
	addq	$-128,%rsp
	movl	12(%r8),%ebx
	leaq	.Lbswap_mask(%rip),%r11
	leaq	-128(%rcx),%r14
	movq	$0xf80,%r15
	leaq	128(%rcx),%rcx
	vmovdqu	(%r11),%xmm0
	andq	$-128,%rsp
	movl	240-128(%rcx),%ebp

	andq	%r15,%r14
	andq	%rsp,%r15
	subq	%r14,%r15
	jc	.Lenc_no_key_aliasing
	cmpq	$768,%r15
	jnc	.Lenc_no_key_aliasing
	subq	%r15,%rsp
.Lenc_no_key_aliasing:

	leaq	(%rsi),%r14
	leaq	-192(%rsi,%rdx,1),%r15
	shrq	$4,%rdx

	call	_aesni_ctr32_6x
	vpshufb	%xmm0,%xmm9,%xmm8
	vpshufb	%xmm0,%xmm10,%xmm2
	vmovdqu	%xmm8,112(%rsp)
	vpshufb	%xmm0,%xmm11,%xmm4
	vmovdqu	%xmm2,96(%rsp)
	vpshufb	%xmm0,%xmm12,%xmm5
	vmovdqu	%xmm4,80(%rsp)
	vpshufb	%xmm0,%xmm13,%xmm6
	vmovdqu	%xmm5,64(%rsp)
	vpshufb	%xmm0,%xmm14,%xmm7
	vmovdqu	%xmm6,48(%rsp)

	call	_aesni_ctr32_6x

	vmovdqu	(%r9),%xmm8
	leaq	32+32(%r9),%r9
	subq	$12,%rdx
	movq	$192,%r10
	vpshufb	%xmm0,%xmm8,%xmm8

	call	_aesni_ctr32_ghash_6x
	vmovdqu	32(%rsp),%xmm7
	vmovdqu	(%r11),%xmm0
	vmovdqu	0-32(%r9),%xmm3
	vpunpckhqdq	%xmm7,%xmm7,%xmm1
	vmovdqu	32-32(%r9),%xmm15
	vmovups	%xmm9,-96(%rsi)
	vpshufb	%xmm0,%xmm9,%xmm9
	vpxor	%xmm7,%xmm1,%xmm1
	vmovups	%xmm10,-80(%rsi)
	vpshufb	%xmm0,%xmm10,%xmm10
	vmovups	%xmm11,-64(%rsi)
	vpshufb	%xmm0,%xmm11,%xmm11
	vmovups	%xmm12,-48(%rsi)
	vpshufb	%xmm0,%xmm12,%xmm12
	vmovups	%xmm13,-32(%rsi)
	vpshufb	%xmm0,%xmm13,%xmm13
	vmovups	%xmm14,-16(%rsi)
	vpshufb	%xmm0,%xmm14,%xmm14
	vmovdqu	%xmm9,16(%rsp)
	vmovdqu	48(%rsp),%xmm6
	vmovdqu	16-32(%r9),%xmm0
	vpunpckhqdq	%xmm6,%xmm6,%xmm2
	vpclmulqdq	$0x00,%xmm3,%xmm7,%xmm5
	vpxor	%xmm6,%xmm2,%xmm2
	vpclmulqdq	$0x11,%xmm3,%xmm7,%xmm7
	vpclmulqdq	$0x00,%xmm15,%xmm1,%xmm1

	vmovdqu	64(%rsp),%xmm9
	vpclmulqdq	$0x00,%xmm0,%xmm6,%xmm4
	vmovdqu	48-32(%r9),%xmm3
	vpxor	%xmm5,%xmm4,%xmm4
	vpunpckhqdq	%xmm9,%xmm9,%xmm5
	vpclmulqdq	$0x11,%xmm0,%xmm6,%xmm6
	vpxor	%xmm9,%xmm5,%xmm5
	vpxor	%xmm7,%xmm6,%xmm6
	vpclmulqdq	$0x10,%xmm15,%xmm2,%xmm2
	vmovdqu	80-32(%r9),%xmm15
	vpxor	%xmm1,%xmm2,%xmm2

	vmovdqu	80(%rsp),%xmm1
	vpclmulqdq	$0x00,%xmm3,%xmm9,%xmm7
	vmovdqu	64-32(%r9),%xmm0
	vpxor	%xmm4,%xmm7,%xmm7
	vpunpckhqdq	%xmm1,%xmm1,%xmm4
	vpclmulqdq	$0x11,%xmm3,%xmm9,%xmm9
	vpxor	%xmm1,%xmm4,%xmm4
	vpxor	%xmm6,%xmm9,%xmm9
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm5
	vpxor	%xmm2,%xmm5,%xmm5

	vmovdqu	96(%rsp),%xmm2
	vpclmulqdq	$0x00,%xmm0,%xmm1,%xmm6
	vmovdqu	96-32(%r9),%xmm3
	vpxor	%xmm7,%xmm6,%xmm6
	vpunpckhqdq	%xmm2,%xmm2,%xmm7
	vpclmulqdq	$0x11,%xmm0,%xmm1,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpxor	%xmm9,%xmm1,%xmm1
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm4
	vmovdqu	128-32(%r9),%xmm15
	vpxor	%xmm5,%xmm4,%xmm4

	vpxor	112(%rsp),%xmm8,%xmm8
	vpclmulqdq	$0x00,%xmm3,%xmm2,%xmm5
	vmovdqu	112-32(%r9),%xmm0
	vpunpckhqdq	%xmm8,%xmm8,%xmm9
	vpxor	%xmm6,%xmm5,%xmm5
	vpclmulqdq	$0x11,%xmm3,%xmm2,%xmm2
	vpxor	%xmm8,%xmm9,%xmm9
	vpxor	%xmm1,%xmm2,%xmm2
	vpclmulqdq	$0x00,%xmm15,%xmm7,%xmm7
	vpxor	%xmm4,%xmm7,%xmm4

	vpclmulqdq	$0x00,%xmm0,%xmm8,%xmm6
	vmovdqu	0-32(%r9),%xmm3
	vpunpckhqdq	%xmm14,%xmm14,%xmm1
	vpclmulqdq	$0x11,%xmm0,%xmm8,%xmm8
	vpxor	%xmm14,%xmm1,%xmm1
	vpxor	%xmm5,%xmm6,%xmm5
	vpclmulqdq	$0x10,%xmm15,%xmm9,%xmm9
	vmovdqu	32-32(%r9),%xmm15
	vpxor	%xmm2,%xmm8,%xmm7
	vpxor	%xmm4,%xmm9,%xmm6

	vmovdqu	16-32(%r9),%xmm0
	vpxor	%xmm5,%xmm7,%xmm9
	vpclmulqdq	$0x00,%xmm3,%xmm14,%xmm4
	vpxor	%xmm9,%xmm6,%xmm6
	vpunpckhqdq	%xmm13,%xmm13,%xmm2
	vpclmulqdq	$0x11,%xmm3,%xmm14,%xmm14
	vpxor	%xmm13,%xmm2,%xmm2
	vpslldq	$8,%xmm6,%xmm9
	vpclmulqdq	$0x00,%xmm15,%xmm1,%xmm1
	vpxor	%xmm9,%xmm5,%xmm8
	vpsrldq	$8,%xmm6,%xmm6
	vpxor	%xmm6,%xmm7,%xmm7

	vpclmulqdq	$0x00,%xmm0,%xmm13,%xmm5
	vmovdqu	48-32(%r9),%xmm3
	vpxor	%xmm4,%xmm5,%xmm5
	vpunpckhqdq	%xmm12,%xmm12,%xmm9
	vpclmulqdq	$0x11,%xmm0,%xmm13,%xmm13
	vpxor	%xmm12,%xmm9,%xmm9
	vpxor	%xmm14,%xmm13,%xmm13
	vpalignr	$8,%xmm8,%xmm8,%xmm14
	vpclmulqdq	$0x10,%xmm15,%xmm2,%xmm2
	vmovdqu	80-32(%r9),%xmm15
	vpxor	%xmm1,%xmm2,%xmm2

	vpclmulqdq	$0x00,%xmm3,%xmm12,%xmm4
	vmovdqu	64-32(%r9),%xmm0
	vpxor	%xmm5,%xmm4,%xmm4
	vpunpckhqdq	%xmm11,%xmm11,%xmm1
	vpclmulqdq	$0x11,%xmm3,%xmm12,%xmm12
	vpxor	%xmm11,%xmm1,%xmm1
	vpxor	%xmm13,%xmm12,%xmm12
	vxorps	16(%rsp),%xmm7,%xmm7
	vpclmulqdq	$0x00,%xmm15,%xmm9,%xmm9
	vpxor	%xmm2,%xmm9,%xmm9

	vpclmulqdq	$0x10,16(%r11),%xmm8,%xmm8
	vxorps	%xmm14,%xmm8,%xmm8

	vpclmulqdq	$0x00,%xmm0,%xmm11,%xmm5
	vmovdqu	96-32(%r9),%xmm3
	vpxor	%xmm4,%xmm5,%xmm5
	vpunpckhqdq	%xmm10,%xmm10,%xmm2
	vpclmulqdq	$0x11,%xmm0,%xmm11,%xmm11
	vpxor	%xmm10,%xmm2,%xmm2
	vpalignr	$8,%xmm8,%xmm8,%xmm14
	vpxor	%xmm12,%xmm11,%xmm11
	vpclmulqdq	$0x10,%xmm15,%xmm1,%xmm1
	vmovdqu	128-32(%r9),%xmm15
	vpxor	%xmm9,%xmm1,%xmm1

	vxorps	%xmm7,%xmm14,%xmm14
	vpclmulqdq	$0x10,16(%r11),%xmm8,%xmm8
	vxorps	%xmm14,%xmm8,%xmm8

	vpclmulqdq	$0x00,%xmm3,%xmm10,%xmm4
	vmovdqu	112-32(%r9),%xmm0
	vpxor	%xmm5,%xmm4,%xmm4
	vpunpckhqdq	%xmm8,%xmm8,%xmm9
	vpclmulqdq	$0x11,%xmm3,%xmm10,%xmm10
	vpxor	%xmm8,%xmm9,%xmm9
	vpxor	%xmm11,%xmm10,%xmm10
	vpclmulqdq	$0x00,%xmm15,%xmm2,%xmm2
	vpxor	%xmm1,%xmm2,%xmm2

	vpclmulqdq	$0x00,%xmm0,%xmm8,%xmm5
	vpclmulqdq	$0x11,%xmm0,%xmm8,%xmm7
	vpxor	%xmm4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm15,%xmm9,%xmm6
	vpxor	%xmm10,%xmm7,%xmm7
	vpxor	%xmm2,%xmm6,%xmm6

	vpxor	%xmm5,%xmm7,%xmm4
	vpxor	%xmm4,%xmm6,%xmm6
	vpslldq	$8,%xmm6,%xmm1
	vmovdqu	16(%r11),%xmm3
	vpsrldq	$8,%xmm6,%xmm6
	vpxor	%xmm1,%xmm5,%xmm8
	vpxor	%xmm6,%xmm7,%xmm7

	vpalignr	$8,%xmm8,%xmm8,%xmm2
	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
	vpxor	%xmm2,%xmm8,%xmm8

	vpalignr	$8,%xmm8,%xmm8,%xmm2
	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
	vpxor	%xmm7,%xmm2,%xmm2
	vpxor	%xmm2,%xmm8,%xmm8
	vpshufb	(%r11),%xmm8,%xmm8
	vmovdqu	%xmm8,-64(%r9)

	vzeroupper
	movq	-48(%rax),%r15
.cfi_restore	%r15
	movq	-40(%rax),%r14
.cfi_restore	%r14
	movq	-32(%rax),%r13
.cfi_restore	%r13
	movq	-24(%rax),%r12
.cfi_restore	%r12
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lgcm_enc_abort:
	movq	%r10,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	aesni_gcm_encrypt,.-aesni_gcm_encrypt
.align	64
.Lbswap_mask:
.byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
.Lpoly:
.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2
.Lone_msb:
.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1
.Ltwo_lsb:
.byte	2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
.Lone_lsb:
.byte	1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
.byte	65,69,83,45,78,73,32,71,67,77,32,109,111,100,117,108,101,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	64
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                           node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/modes/ghash-x86_64.s          0000664 0000000 0000000 00000112247 14746647661 0030504 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	


.globl	gcm_gmult_4bit
.type	gcm_gmult_4bit,@function
.align	16
gcm_gmult_4bit:
.cfi_startproc	
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
	subq	$280,%rsp
.cfi_adjust_cfa_offset	280
.Lgmult_prologue:

	movzbq	15(%rdi),%r8
	leaq	.Lrem_4bit(%rip),%r11
	xorq	%rax,%rax
	xorq	%rbx,%rbx
	movb	%r8b,%al
	movb	%r8b,%bl
	shlb	$4,%al
	movq	$14,%rcx
	movq	8(%rsi,%rax,1),%r8
	movq	(%rsi,%rax,1),%r9
	andb	$0xf0,%bl
	movq	%r8,%rdx
	jmp	.Loop1

.align	16
.Loop1:
	shrq	$4,%r8
	andq	$0xf,%rdx
	movq	%r9,%r10
	movb	(%rdi,%rcx,1),%al
	shrq	$4,%r9
	xorq	8(%rsi,%rbx,1),%r8
	shlq	$60,%r10
	xorq	(%rsi,%rbx,1),%r9
	movb	%al,%bl
	xorq	(%r11,%rdx,8),%r9
	movq	%r8,%rdx
	shlb	$4,%al
	xorq	%r10,%r8
	decq	%rcx
	js	.Lbreak1

	shrq	$4,%r8
	andq	$0xf,%rdx
	movq	%r9,%r10
	shrq	$4,%r9
	xorq	8(%rsi,%rax,1),%r8
	shlq	$60,%r10
	xorq	(%rsi,%rax,1),%r9
	andb	$0xf0,%bl
	xorq	(%r11,%rdx,8),%r9
	movq	%r8,%rdx
	xorq	%r10,%r8
	jmp	.Loop1

.align	16
.Lbreak1:
	shrq	$4,%r8
	andq	$0xf,%rdx
	movq	%r9,%r10
	shrq	$4,%r9
	xorq	8(%rsi,%rax,1),%r8
	shlq	$60,%r10
	xorq	(%rsi,%rax,1),%r9
	andb	$0xf0,%bl
	xorq	(%r11,%rdx,8),%r9
	movq	%r8,%rdx
	xorq	%r10,%r8

	shrq	$4,%r8
	andq	$0xf,%rdx
	movq	%r9,%r10
	shrq	$4,%r9
	xorq	8(%rsi,%rbx,1),%r8
	shlq	$60,%r10
	xorq	(%rsi,%rbx,1),%r9
	xorq	%r10,%r8
	xorq	(%r11,%rdx,8),%r9

	bswapq	%r8
	bswapq	%r9
	movq	%r8,8(%rdi)
	movq	%r9,(%rdi)

	leaq	280+48(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lgmult_epilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_gmult_4bit,.-gcm_gmult_4bit
.globl	gcm_ghash_4bit
.type	gcm_ghash_4bit,@function
.align	16
gcm_ghash_4bit:
.cfi_startproc	
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
	subq	$280,%rsp
.cfi_adjust_cfa_offset	280
.Lghash_prologue:
	movq	%rdx,%r14
	movq	%rcx,%r15
	subq	$-128,%rsi
	leaq	16+128(%rsp),%rbp
	xorl	%edx,%edx
	movq	0+0-128(%rsi),%r8
	movq	0+8-128(%rsi),%rax
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	16+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	16+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,0(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,0(%rbp)
	movq	32+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,0-128(%rbp)
	movq	32+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,1(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,8(%rbp)
	movq	48+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,8-128(%rbp)
	movq	48+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,2(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,16(%rbp)
	movq	64+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,16-128(%rbp)
	movq	64+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,3(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,24(%rbp)
	movq	80+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,24-128(%rbp)
	movq	80+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,4(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,32(%rbp)
	movq	96+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,32-128(%rbp)
	movq	96+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,5(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,40(%rbp)
	movq	112+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,40-128(%rbp)
	movq	112+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,6(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,48(%rbp)
	movq	128+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,48-128(%rbp)
	movq	128+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,7(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,56(%rbp)
	movq	144+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,56-128(%rbp)
	movq	144+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,8(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,64(%rbp)
	movq	160+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,64-128(%rbp)
	movq	160+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,9(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,72(%rbp)
	movq	176+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,72-128(%rbp)
	movq	176+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,10(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,80(%rbp)
	movq	192+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,80-128(%rbp)
	movq	192+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,11(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,88(%rbp)
	movq	208+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,88-128(%rbp)
	movq	208+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,12(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,96(%rbp)
	movq	224+0-128(%rsi),%r8
	shlb	$4,%dl
	movq	%rax,96-128(%rbp)
	movq	224+8-128(%rsi),%rax
	shlq	$60,%r10
	movb	%dl,13(%rsp)
	orq	%r10,%rbx
	movb	%al,%dl
	shrq	$4,%rax
	movq	%r8,%r10
	shrq	$4,%r8
	movq	%r9,104(%rbp)
	movq	240+0-128(%rsi),%r9
	shlb	$4,%dl
	movq	%rbx,104-128(%rbp)
	movq	240+8-128(%rsi),%rbx
	shlq	$60,%r10
	movb	%dl,14(%rsp)
	orq	%r10,%rax
	movb	%bl,%dl
	shrq	$4,%rbx
	movq	%r9,%r10
	shrq	$4,%r9
	movq	%r8,112(%rbp)
	shlb	$4,%dl
	movq	%rax,112-128(%rbp)
	shlq	$60,%r10
	movb	%dl,15(%rsp)
	orq	%r10,%rbx
	movq	%r9,120(%rbp)
	movq	%rbx,120-128(%rbp)
	addq	$-128,%rsi
	movq	8(%rdi),%r8
	movq	0(%rdi),%r9
	addq	%r14,%r15
	leaq	.Lrem_8bit(%rip),%r11
	jmp	.Louter_loop
.align	16
.Louter_loop:
	xorq	(%r14),%r9
	movq	8(%r14),%rdx
	leaq	16(%r14),%r14
	xorq	%r8,%rdx
	movq	%r9,(%rdi)
	movq	%rdx,8(%rdi)
	shrq	$32,%rdx
	xorq	%rax,%rax
	roll	$8,%edx
	movb	%dl,%al
	movzbl	%dl,%ebx
	shlb	$4,%al
	shrl	$4,%ebx
	roll	$8,%edx
	movq	8(%rsi,%rax,1),%r8
	movq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	xorq	%r8,%r12
	movq	%r9,%r10
	shrq	$8,%r8
	movzbq	%r12b,%r12
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	movl	8(%rdi),%edx
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	movl	4(%rdi),%edx
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	movl	0(%rdi),%edx
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	shrl	$4,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r12,2),%r12
	movzbl	%dl,%ebx
	shlb	$4,%al
	movzbq	(%rsp,%rcx,1),%r13
	shrl	$4,%ebx
	shlq	$48,%r12
	xorq	%r8,%r13
	movq	%r9,%r10
	xorq	%r12,%r9
	shrq	$8,%r8
	movzbq	%r13b,%r13
	shrq	$8,%r9
	xorq	-128(%rbp,%rcx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rcx,8),%r9
	roll	$8,%edx
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	movb	%dl,%al
	xorq	%r10,%r8
	movzwq	(%r11,%r13,2),%r13
	movzbl	%dl,%ecx
	shlb	$4,%al
	movzbq	(%rsp,%rbx,1),%r12
	andl	$240,%ecx
	shlq	$48,%r13
	xorq	%r8,%r12
	movq	%r9,%r10
	xorq	%r13,%r9
	shrq	$8,%r8
	movzbq	%r12b,%r12
	movl	-4(%rdi),%edx
	shrq	$8,%r9
	xorq	-128(%rbp,%rbx,8),%r8
	shlq	$56,%r10
	xorq	(%rbp,%rbx,8),%r9
	movzwq	(%r11,%r12,2),%r12
	xorq	8(%rsi,%rax,1),%r8
	xorq	(%rsi,%rax,1),%r9
	shlq	$48,%r12
	xorq	%r10,%r8
	xorq	%r12,%r9
	movzbq	%r8b,%r13
	shrq	$4,%r8
	movq	%r9,%r10
	shlb	$4,%r13b
	shrq	$4,%r9
	xorq	8(%rsi,%rcx,1),%r8
	movzwq	(%r11,%r13,2),%r13
	shlq	$60,%r10
	xorq	(%rsi,%rcx,1),%r9
	xorq	%r10,%r8
	shlq	$48,%r13
	bswapq	%r8
	xorq	%r13,%r9
	bswapq	%r9
	cmpq	%r15,%r14
	jb	.Louter_loop
	movq	%r8,8(%rdi)
	movq	%r9,(%rdi)

	leaq	280+48(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	0(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lghash_epilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_ghash_4bit,.-gcm_ghash_4bit
.globl	gcm_init_clmul
.type	gcm_init_clmul,@function
.align	16
gcm_init_clmul:
.cfi_startproc	
.L_init_clmul:
	movdqu	(%rsi),%xmm2
	pshufd	$78,%xmm2,%xmm2


	pshufd	$255,%xmm2,%xmm4
	movdqa	%xmm2,%xmm3
	psllq	$1,%xmm2
	pxor	%xmm5,%xmm5
	psrlq	$63,%xmm3
	pcmpgtd	%xmm4,%xmm5
	pslldq	$8,%xmm3
	por	%xmm3,%xmm2


	pand	.L0x1c2_polynomial(%rip),%xmm5
	pxor	%xmm5,%xmm2


	pshufd	$78,%xmm2,%xmm6
	movdqa	%xmm2,%xmm0
	pxor	%xmm2,%xmm6
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm3
	pxor	%xmm0,%xmm3
.byte	102,15,58,68,194,0
.byte	102,15,58,68,202,17
.byte	102,15,58,68,222,0
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3

	movdqa	%xmm3,%xmm4
	psrldq	$8,%xmm3
	pslldq	$8,%xmm4
	pxor	%xmm3,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	pshufd	$78,%xmm2,%xmm3
	pshufd	$78,%xmm0,%xmm4
	pxor	%xmm2,%xmm3
	movdqu	%xmm2,0(%rdi)
	pxor	%xmm0,%xmm4
	movdqu	%xmm0,16(%rdi)
.byte	102,15,58,15,227,8
	movdqu	%xmm4,32(%rdi)
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm3
	pxor	%xmm0,%xmm3
.byte	102,15,58,68,194,0
.byte	102,15,58,68,202,17
.byte	102,15,58,68,222,0
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3

	movdqa	%xmm3,%xmm4
	psrldq	$8,%xmm3
	pslldq	$8,%xmm4
	pxor	%xmm3,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	movdqa	%xmm0,%xmm5
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm3
	pxor	%xmm0,%xmm3
.byte	102,15,58,68,194,0
.byte	102,15,58,68,202,17
.byte	102,15,58,68,222,0
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3

	movdqa	%xmm3,%xmm4
	psrldq	$8,%xmm3
	pslldq	$8,%xmm4
	pxor	%xmm3,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	pshufd	$78,%xmm5,%xmm3
	pshufd	$78,%xmm0,%xmm4
	pxor	%xmm5,%xmm3
	movdqu	%xmm5,48(%rdi)
	pxor	%xmm0,%xmm4
	movdqu	%xmm0,64(%rdi)
.byte	102,15,58,15,227,8
	movdqu	%xmm4,80(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_init_clmul,.-gcm_init_clmul
.globl	gcm_gmult_clmul
.type	gcm_gmult_clmul,@function
.align	16
gcm_gmult_clmul:
.cfi_startproc	
.byte	243,15,30,250
.L_gmult_clmul:
	movdqu	(%rdi),%xmm0
	movdqa	.Lbswap_mask(%rip),%xmm5
	movdqu	(%rsi),%xmm2
	movdqu	32(%rsi),%xmm4
.byte	102,15,56,0,197
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm3
	pxor	%xmm0,%xmm3
.byte	102,15,58,68,194,0
.byte	102,15,58,68,202,17
.byte	102,15,58,68,220,0
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3

	movdqa	%xmm3,%xmm4
	psrldq	$8,%xmm3
	pslldq	$8,%xmm4
	pxor	%xmm3,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
.byte	102,15,56,0,197
	movdqu	%xmm0,(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_gmult_clmul,.-gcm_gmult_clmul
.globl	gcm_ghash_clmul
.type	gcm_ghash_clmul,@function
.align	32
gcm_ghash_clmul:
.cfi_startproc	
.byte	243,15,30,250
.L_ghash_clmul:
	movdqa	.Lbswap_mask(%rip),%xmm10

	movdqu	(%rdi),%xmm0
	movdqu	(%rsi),%xmm2
	movdqu	32(%rsi),%xmm7
.byte	102,65,15,56,0,194

	subq	$0x10,%rcx
	jz	.Lodd_tail

	movdqu	16(%rsi),%xmm6
	movl	OPENSSL_ia32cap_P+4(%rip),%eax
	cmpq	$0x30,%rcx
	jb	.Lskip4x

	andl	$71303168,%eax
	cmpl	$4194304,%eax
	je	.Lskip4x

	subq	$0x30,%rcx
	movq	$0xA040608020C0E000,%rax
	movdqu	48(%rsi),%xmm14
	movdqu	64(%rsi),%xmm15




	movdqu	48(%rdx),%xmm3
	movdqu	32(%rdx),%xmm11
.byte	102,65,15,56,0,218
.byte	102,69,15,56,0,218
	movdqa	%xmm3,%xmm5
	pshufd	$78,%xmm3,%xmm4
	pxor	%xmm3,%xmm4
.byte	102,15,58,68,218,0
.byte	102,15,58,68,234,17
.byte	102,15,58,68,231,0

	movdqa	%xmm11,%xmm13
	pshufd	$78,%xmm11,%xmm12
	pxor	%xmm11,%xmm12
.byte	102,68,15,58,68,222,0
.byte	102,68,15,58,68,238,17
.byte	102,68,15,58,68,231,16
	xorps	%xmm11,%xmm3
	xorps	%xmm13,%xmm5
	movups	80(%rsi),%xmm7
	xorps	%xmm12,%xmm4

	movdqu	16(%rdx),%xmm11
	movdqu	0(%rdx),%xmm8
.byte	102,69,15,56,0,218
.byte	102,69,15,56,0,194
	movdqa	%xmm11,%xmm13
	pshufd	$78,%xmm11,%xmm12
	pxor	%xmm8,%xmm0
	pxor	%xmm11,%xmm12
.byte	102,69,15,58,68,222,0
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm8
	pxor	%xmm0,%xmm8
.byte	102,69,15,58,68,238,17
.byte	102,68,15,58,68,231,0
	xorps	%xmm11,%xmm3
	xorps	%xmm13,%xmm5

	leaq	64(%rdx),%rdx
	subq	$0x40,%rcx
	jc	.Ltail4x

	jmp	.Lmod4_loop
.align	32
.Lmod4_loop:
.byte	102,65,15,58,68,199,0
	xorps	%xmm12,%xmm4
	movdqu	48(%rdx),%xmm11
.byte	102,69,15,56,0,218
.byte	102,65,15,58,68,207,17
	xorps	%xmm3,%xmm0
	movdqu	32(%rdx),%xmm3
	movdqa	%xmm11,%xmm13
.byte	102,68,15,58,68,199,16
	pshufd	$78,%xmm11,%xmm12
	xorps	%xmm5,%xmm1
	pxor	%xmm11,%xmm12
.byte	102,65,15,56,0,218
	movups	32(%rsi),%xmm7
	xorps	%xmm4,%xmm8
.byte	102,68,15,58,68,218,0
	pshufd	$78,%xmm3,%xmm4

	pxor	%xmm0,%xmm8
	movdqa	%xmm3,%xmm5
	pxor	%xmm1,%xmm8
	pxor	%xmm3,%xmm4
	movdqa	%xmm8,%xmm9
.byte	102,68,15,58,68,234,17
	pslldq	$8,%xmm8
	psrldq	$8,%xmm9
	pxor	%xmm8,%xmm0
	movdqa	.L7_mask(%rip),%xmm8
	pxor	%xmm9,%xmm1
.byte	102,76,15,110,200

	pand	%xmm0,%xmm8
.byte	102,69,15,56,0,200
	pxor	%xmm0,%xmm9
.byte	102,68,15,58,68,231,0
	psllq	$57,%xmm9
	movdqa	%xmm9,%xmm8
	pslldq	$8,%xmm9
.byte	102,15,58,68,222,0
	psrldq	$8,%xmm8
	pxor	%xmm9,%xmm0
	pxor	%xmm8,%xmm1
	movdqu	0(%rdx),%xmm8

	movdqa	%xmm0,%xmm9
	psrlq	$1,%xmm0
.byte	102,15,58,68,238,17
	xorps	%xmm11,%xmm3
	movdqu	16(%rdx),%xmm11
.byte	102,69,15,56,0,218
.byte	102,15,58,68,231,16
	xorps	%xmm13,%xmm5
	movups	80(%rsi),%xmm7
.byte	102,69,15,56,0,194
	pxor	%xmm9,%xmm1
	pxor	%xmm0,%xmm9
	psrlq	$5,%xmm0

	movdqa	%xmm11,%xmm13
	pxor	%xmm12,%xmm4
	pshufd	$78,%xmm11,%xmm12
	pxor	%xmm9,%xmm0
	pxor	%xmm8,%xmm1
	pxor	%xmm11,%xmm12
.byte	102,69,15,58,68,222,0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	movdqa	%xmm0,%xmm1
.byte	102,69,15,58,68,238,17
	xorps	%xmm11,%xmm3
	pshufd	$78,%xmm0,%xmm8
	pxor	%xmm0,%xmm8

.byte	102,68,15,58,68,231,0
	xorps	%xmm13,%xmm5

	leaq	64(%rdx),%rdx
	subq	$0x40,%rcx
	jnc	.Lmod4_loop

.Ltail4x:
.byte	102,65,15,58,68,199,0
.byte	102,65,15,58,68,207,17
.byte	102,68,15,58,68,199,16
	xorps	%xmm12,%xmm4
	xorps	%xmm3,%xmm0
	xorps	%xmm5,%xmm1
	pxor	%xmm0,%xmm1
	pxor	%xmm4,%xmm8

	pxor	%xmm1,%xmm8
	pxor	%xmm0,%xmm1

	movdqa	%xmm8,%xmm9
	psrldq	$8,%xmm8
	pslldq	$8,%xmm9
	pxor	%xmm8,%xmm1
	pxor	%xmm9,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	addq	$0x40,%rcx
	jz	.Ldone
	movdqu	32(%rsi),%xmm7
	subq	$0x10,%rcx
	jz	.Lodd_tail
.Lskip4x:





	movdqu	(%rdx),%xmm8
	movdqu	16(%rdx),%xmm3
.byte	102,69,15,56,0,194
.byte	102,65,15,56,0,218
	pxor	%xmm8,%xmm0

	movdqa	%xmm3,%xmm5
	pshufd	$78,%xmm3,%xmm4
	pxor	%xmm3,%xmm4
.byte	102,15,58,68,218,0
.byte	102,15,58,68,234,17
.byte	102,15,58,68,231,0

	leaq	32(%rdx),%rdx
	nop
	subq	$0x20,%rcx
	jbe	.Leven_tail
	nop
	jmp	.Lmod_loop

.align	32
.Lmod_loop:
	movdqa	%xmm0,%xmm1
	movdqa	%xmm4,%xmm8
	pshufd	$78,%xmm0,%xmm4
	pxor	%xmm0,%xmm4

.byte	102,15,58,68,198,0
.byte	102,15,58,68,206,17
.byte	102,15,58,68,231,16

	pxor	%xmm3,%xmm0
	pxor	%xmm5,%xmm1
	movdqu	(%rdx),%xmm9
	pxor	%xmm0,%xmm8
.byte	102,69,15,56,0,202
	movdqu	16(%rdx),%xmm3

	pxor	%xmm1,%xmm8
	pxor	%xmm9,%xmm1
	pxor	%xmm8,%xmm4
.byte	102,65,15,56,0,218
	movdqa	%xmm4,%xmm8
	psrldq	$8,%xmm8
	pslldq	$8,%xmm4
	pxor	%xmm8,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm3,%xmm5

	movdqa	%xmm0,%xmm9
	movdqa	%xmm0,%xmm8
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm8
.byte	102,15,58,68,218,0
	psllq	$1,%xmm0
	pxor	%xmm8,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm8
	pslldq	$8,%xmm0
	psrldq	$8,%xmm8
	pxor	%xmm9,%xmm0
	pshufd	$78,%xmm5,%xmm4
	pxor	%xmm8,%xmm1
	pxor	%xmm5,%xmm4

	movdqa	%xmm0,%xmm9
	psrlq	$1,%xmm0
.byte	102,15,58,68,234,17
	pxor	%xmm9,%xmm1
	pxor	%xmm0,%xmm9
	psrlq	$5,%xmm0
	pxor	%xmm9,%xmm0
	leaq	32(%rdx),%rdx
	psrlq	$1,%xmm0
.byte	102,15,58,68,231,0
	pxor	%xmm1,%xmm0

	subq	$0x20,%rcx
	ja	.Lmod_loop

.Leven_tail:
	movdqa	%xmm0,%xmm1
	movdqa	%xmm4,%xmm8
	pshufd	$78,%xmm0,%xmm4
	pxor	%xmm0,%xmm4

.byte	102,15,58,68,198,0
.byte	102,15,58,68,206,17
.byte	102,15,58,68,231,16

	pxor	%xmm3,%xmm0
	pxor	%xmm5,%xmm1
	pxor	%xmm0,%xmm8
	pxor	%xmm1,%xmm8
	pxor	%xmm8,%xmm4
	movdqa	%xmm4,%xmm8
	psrldq	$8,%xmm8
	pslldq	$8,%xmm4
	pxor	%xmm8,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
	testq	%rcx,%rcx
	jnz	.Ldone

.Lodd_tail:
	movdqu	(%rdx),%xmm8
.byte	102,69,15,56,0,194
	pxor	%xmm8,%xmm0
	movdqa	%xmm0,%xmm1
	pshufd	$78,%xmm0,%xmm3
	pxor	%xmm0,%xmm3
.byte	102,15,58,68,194,0
.byte	102,15,58,68,202,17
.byte	102,15,58,68,223,0
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3

	movdqa	%xmm3,%xmm4
	psrldq	$8,%xmm3
	pslldq	$8,%xmm4
	pxor	%xmm3,%xmm1
	pxor	%xmm4,%xmm0

	movdqa	%xmm0,%xmm4
	movdqa	%xmm0,%xmm3
	psllq	$5,%xmm0
	pxor	%xmm0,%xmm3
	psllq	$1,%xmm0
	pxor	%xmm3,%xmm0
	psllq	$57,%xmm0
	movdqa	%xmm0,%xmm3
	pslldq	$8,%xmm0
	psrldq	$8,%xmm3
	pxor	%xmm4,%xmm0
	pxor	%xmm3,%xmm1


	movdqa	%xmm0,%xmm4
	psrlq	$1,%xmm0
	pxor	%xmm4,%xmm1
	pxor	%xmm0,%xmm4
	psrlq	$5,%xmm0
	pxor	%xmm4,%xmm0
	psrlq	$1,%xmm0
	pxor	%xmm1,%xmm0
.Ldone:
.byte	102,65,15,56,0,194
	movdqu	%xmm0,(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_ghash_clmul,.-gcm_ghash_clmul
.globl	gcm_init_avx
.type	gcm_init_avx,@function
.align	32
gcm_init_avx:
.cfi_startproc	
	vzeroupper

	vmovdqu	(%rsi),%xmm2
	vpshufd	$78,%xmm2,%xmm2


	vpshufd	$255,%xmm2,%xmm4
	vpsrlq	$63,%xmm2,%xmm3
	vpsllq	$1,%xmm2,%xmm2
	vpxor	%xmm5,%xmm5,%xmm5
	vpcmpgtd	%xmm4,%xmm5,%xmm5
	vpslldq	$8,%xmm3,%xmm3
	vpor	%xmm3,%xmm2,%xmm2


	vpand	.L0x1c2_polynomial(%rip),%xmm5,%xmm5
	vpxor	%xmm5,%xmm2,%xmm2

	vpunpckhqdq	%xmm2,%xmm2,%xmm6
	vmovdqa	%xmm2,%xmm0
	vpxor	%xmm2,%xmm6,%xmm6
	movq	$4,%r10
	jmp	.Linit_start_avx
.align	32
.Linit_loop_avx:
	vpalignr	$8,%xmm3,%xmm4,%xmm5
	vmovdqu	%xmm5,-16(%rdi)
	vpunpckhqdq	%xmm0,%xmm0,%xmm3
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm1
	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm6,%xmm3,%xmm3
	vpxor	%xmm0,%xmm1,%xmm4
	vpxor	%xmm4,%xmm3,%xmm3

	vpslldq	$8,%xmm3,%xmm4
	vpsrldq	$8,%xmm3,%xmm3
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm3,%xmm1,%xmm1
	vpsllq	$57,%xmm0,%xmm3
	vpsllq	$62,%xmm0,%xmm4
	vpxor	%xmm3,%xmm4,%xmm4
	vpsllq	$63,%xmm0,%xmm3
	vpxor	%xmm3,%xmm4,%xmm4
	vpslldq	$8,%xmm4,%xmm3
	vpsrldq	$8,%xmm4,%xmm4
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm4,%xmm1,%xmm1

	vpsrlq	$1,%xmm0,%xmm4
	vpxor	%xmm0,%xmm1,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpsrlq	$5,%xmm4,%xmm4
	vpxor	%xmm4,%xmm0,%xmm0
	vpsrlq	$1,%xmm0,%xmm0
	vpxor	%xmm1,%xmm0,%xmm0
.Linit_start_avx:
	vmovdqa	%xmm0,%xmm5
	vpunpckhqdq	%xmm0,%xmm0,%xmm3
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x11,%xmm2,%xmm0,%xmm1
	vpclmulqdq	$0x00,%xmm2,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm6,%xmm3,%xmm3
	vpxor	%xmm0,%xmm1,%xmm4
	vpxor	%xmm4,%xmm3,%xmm3

	vpslldq	$8,%xmm3,%xmm4
	vpsrldq	$8,%xmm3,%xmm3
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm3,%xmm1,%xmm1
	vpsllq	$57,%xmm0,%xmm3
	vpsllq	$62,%xmm0,%xmm4
	vpxor	%xmm3,%xmm4,%xmm4
	vpsllq	$63,%xmm0,%xmm3
	vpxor	%xmm3,%xmm4,%xmm4
	vpslldq	$8,%xmm4,%xmm3
	vpsrldq	$8,%xmm4,%xmm4
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm4,%xmm1,%xmm1

	vpsrlq	$1,%xmm0,%xmm4
	vpxor	%xmm0,%xmm1,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpsrlq	$5,%xmm4,%xmm4
	vpxor	%xmm4,%xmm0,%xmm0
	vpsrlq	$1,%xmm0,%xmm0
	vpxor	%xmm1,%xmm0,%xmm0
	vpshufd	$78,%xmm5,%xmm3
	vpshufd	$78,%xmm0,%xmm4
	vpxor	%xmm5,%xmm3,%xmm3
	vmovdqu	%xmm5,0(%rdi)
	vpxor	%xmm0,%xmm4,%xmm4
	vmovdqu	%xmm0,16(%rdi)
	leaq	48(%rdi),%rdi
	subq	$1,%r10
	jnz	.Linit_loop_avx

	vpalignr	$8,%xmm4,%xmm3,%xmm5
	vmovdqu	%xmm5,-16(%rdi)

	vzeroupper
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_init_avx,.-gcm_init_avx
.globl	gcm_gmult_avx
.type	gcm_gmult_avx,@function
.align	32
gcm_gmult_avx:
.cfi_startproc	
.byte	243,15,30,250
	jmp	.L_gmult_clmul
.cfi_endproc	
.size	gcm_gmult_avx,.-gcm_gmult_avx
.globl	gcm_ghash_avx
.type	gcm_ghash_avx,@function
.align	32
gcm_ghash_avx:
.cfi_startproc	
.byte	243,15,30,250
	vzeroupper

	vmovdqu	(%rdi),%xmm10
	leaq	.L0x1c2_polynomial(%rip),%r10
	leaq	64(%rsi),%rsi
	vmovdqu	.Lbswap_mask(%rip),%xmm13
	vpshufb	%xmm13,%xmm10,%xmm10
	cmpq	$0x80,%rcx
	jb	.Lshort_avx
	subq	$0x80,%rcx

	vmovdqu	112(%rdx),%xmm14
	vmovdqu	0-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm14
	vmovdqu	32-64(%rsi),%xmm7

	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vmovdqu	96(%rdx),%xmm15
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpxor	%xmm14,%xmm9,%xmm9
	vpshufb	%xmm13,%xmm15,%xmm15
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	16-64(%rsi),%xmm6
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vmovdqu	80(%rdx),%xmm14
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vpxor	%xmm15,%xmm8,%xmm8

	vpshufb	%xmm13,%xmm14,%xmm14
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vmovdqu	48-64(%rsi),%xmm6
	vpxor	%xmm14,%xmm9,%xmm9
	vmovdqu	64(%rdx),%xmm15
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	80-64(%rsi),%xmm7

	vpshufb	%xmm13,%xmm15,%xmm15
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpxor	%xmm1,%xmm4,%xmm4
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	64-64(%rsi),%xmm6
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vpxor	%xmm15,%xmm8,%xmm8

	vmovdqu	48(%rdx),%xmm14
	vpxor	%xmm3,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpxor	%xmm4,%xmm1,%xmm1
	vpshufb	%xmm13,%xmm14,%xmm14
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vmovdqu	96-64(%rsi),%xmm6
	vpxor	%xmm5,%xmm2,%xmm2
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	128-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9

	vmovdqu	32(%rdx),%xmm15
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpxor	%xmm1,%xmm4,%xmm4
	vpshufb	%xmm13,%xmm15,%xmm15
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	112-64(%rsi),%xmm6
	vpxor	%xmm2,%xmm5,%xmm5
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vpxor	%xmm15,%xmm8,%xmm8

	vmovdqu	16(%rdx),%xmm14
	vpxor	%xmm3,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpxor	%xmm4,%xmm1,%xmm1
	vpshufb	%xmm13,%xmm14,%xmm14
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vmovdqu	144-64(%rsi),%xmm6
	vpxor	%xmm5,%xmm2,%xmm2
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	176-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9

	vmovdqu	(%rdx),%xmm15
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpxor	%xmm1,%xmm4,%xmm4
	vpshufb	%xmm13,%xmm15,%xmm15
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	160-64(%rsi),%xmm6
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm7,%xmm9,%xmm2

	leaq	128(%rdx),%rdx
	cmpq	$0x80,%rcx
	jb	.Ltail_avx

	vpxor	%xmm10,%xmm15,%xmm15
	subq	$0x80,%rcx
	jmp	.Loop8x_avx

.align	32
.Loop8x_avx:
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vmovdqu	112(%rdx),%xmm14
	vpxor	%xmm0,%xmm3,%xmm3
	vpxor	%xmm15,%xmm8,%xmm8
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm10
	vpshufb	%xmm13,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm11
	vmovdqu	0-64(%rsi),%xmm6
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm12
	vmovdqu	32-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9

	vmovdqu	96(%rdx),%xmm15
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpxor	%xmm3,%xmm10,%xmm10
	vpshufb	%xmm13,%xmm15,%xmm15
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vxorps	%xmm4,%xmm11,%xmm11
	vmovdqu	16-64(%rsi),%xmm6
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vpxor	%xmm5,%xmm12,%xmm12
	vxorps	%xmm15,%xmm8,%xmm8

	vmovdqu	80(%rdx),%xmm14
	vpxor	%xmm10,%xmm12,%xmm12
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpxor	%xmm11,%xmm12,%xmm12
	vpslldq	$8,%xmm12,%xmm9
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vpsrldq	$8,%xmm12,%xmm12
	vpxor	%xmm9,%xmm10,%xmm10
	vmovdqu	48-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm14
	vxorps	%xmm12,%xmm11,%xmm11
	vpxor	%xmm1,%xmm4,%xmm4
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	80-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9
	vpxor	%xmm2,%xmm5,%xmm5

	vmovdqu	64(%rdx),%xmm15
	vpalignr	$8,%xmm10,%xmm10,%xmm12
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpshufb	%xmm13,%xmm15,%xmm15
	vpxor	%xmm3,%xmm0,%xmm0
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	64-64(%rsi),%xmm6
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm4,%xmm1,%xmm1
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vxorps	%xmm15,%xmm8,%xmm8
	vpxor	%xmm5,%xmm2,%xmm2

	vmovdqu	48(%rdx),%xmm14
	vpclmulqdq	$0x10,(%r10),%xmm10,%xmm10
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpshufb	%xmm13,%xmm14,%xmm14
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vmovdqu	96-64(%rsi),%xmm6
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	128-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9
	vpxor	%xmm2,%xmm5,%xmm5

	vmovdqu	32(%rdx),%xmm15
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpshufb	%xmm13,%xmm15,%xmm15
	vpxor	%xmm3,%xmm0,%xmm0
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	112-64(%rsi),%xmm6
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm4,%xmm1,%xmm1
	vpclmulqdq	$0x00,%xmm7,%xmm9,%xmm2
	vpxor	%xmm15,%xmm8,%xmm8
	vpxor	%xmm5,%xmm2,%xmm2
	vxorps	%xmm12,%xmm10,%xmm10

	vmovdqu	16(%rdx),%xmm14
	vpalignr	$8,%xmm10,%xmm10,%xmm12
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm3
	vpshufb	%xmm13,%xmm14,%xmm14
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm4
	vmovdqu	144-64(%rsi),%xmm6
	vpclmulqdq	$0x10,(%r10),%xmm10,%xmm10
	vxorps	%xmm11,%xmm12,%xmm12
	vpunpckhqdq	%xmm14,%xmm14,%xmm9
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm7,%xmm8,%xmm5
	vmovdqu	176-64(%rsi),%xmm7
	vpxor	%xmm14,%xmm9,%xmm9
	vpxor	%xmm2,%xmm5,%xmm5

	vmovdqu	(%rdx),%xmm15
	vpclmulqdq	$0x00,%xmm6,%xmm14,%xmm0
	vpshufb	%xmm13,%xmm15,%xmm15
	vpclmulqdq	$0x11,%xmm6,%xmm14,%xmm1
	vmovdqu	160-64(%rsi),%xmm6
	vpxor	%xmm12,%xmm15,%xmm15
	vpclmulqdq	$0x10,%xmm7,%xmm9,%xmm2
	vpxor	%xmm10,%xmm15,%xmm15

	leaq	128(%rdx),%rdx
	subq	$0x80,%rcx
	jnc	.Loop8x_avx

	addq	$0x80,%rcx
	jmp	.Ltail_no_xor_avx

.align	32
.Lshort_avx:
	vmovdqu	-16(%rdx,%rcx,1),%xmm14
	leaq	(%rdx,%rcx,1),%rdx
	vmovdqu	0-64(%rsi),%xmm6
	vmovdqu	32-64(%rsi),%xmm7
	vpshufb	%xmm13,%xmm14,%xmm15

	vmovdqa	%xmm0,%xmm3
	vmovdqa	%xmm1,%xmm4
	vmovdqa	%xmm2,%xmm5
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-32(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	16-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vpsrldq	$8,%xmm7,%xmm7
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-48(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	48-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vmovdqu	80-64(%rsi),%xmm7
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-64(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	64-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vpsrldq	$8,%xmm7,%xmm7
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-80(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	96-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vmovdqu	128-64(%rsi),%xmm7
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-96(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	112-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vpsrldq	$8,%xmm7,%xmm7
	subq	$0x10,%rcx
	jz	.Ltail_avx

	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vmovdqu	-112(%rdx),%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vmovdqu	144-64(%rsi),%xmm6
	vpshufb	%xmm13,%xmm14,%xmm15
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2
	vmovq	184-64(%rsi),%xmm7
	subq	$0x10,%rcx
	jmp	.Ltail_avx

.align	32
.Ltail_avx:
	vpxor	%xmm10,%xmm15,%xmm15
.Ltail_no_xor_avx:
	vpunpckhqdq	%xmm15,%xmm15,%xmm8
	vpxor	%xmm0,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm6,%xmm15,%xmm0
	vpxor	%xmm15,%xmm8,%xmm8
	vpxor	%xmm1,%xmm4,%xmm4
	vpclmulqdq	$0x11,%xmm6,%xmm15,%xmm1
	vpxor	%xmm2,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm7,%xmm8,%xmm2

	vmovdqu	(%r10),%xmm12

	vpxor	%xmm0,%xmm3,%xmm10
	vpxor	%xmm1,%xmm4,%xmm11
	vpxor	%xmm2,%xmm5,%xmm5

	vpxor	%xmm10,%xmm5,%xmm5
	vpxor	%xmm11,%xmm5,%xmm5
	vpslldq	$8,%xmm5,%xmm9
	vpsrldq	$8,%xmm5,%xmm5
	vpxor	%xmm9,%xmm10,%xmm10
	vpxor	%xmm5,%xmm11,%xmm11

	vpclmulqdq	$0x10,%xmm12,%xmm10,%xmm9
	vpalignr	$8,%xmm10,%xmm10,%xmm10
	vpxor	%xmm9,%xmm10,%xmm10

	vpclmulqdq	$0x10,%xmm12,%xmm10,%xmm9
	vpalignr	$8,%xmm10,%xmm10,%xmm10
	vpxor	%xmm11,%xmm10,%xmm10
	vpxor	%xmm9,%xmm10,%xmm10

	cmpq	$0,%rcx
	jne	.Lshort_avx

	vpshufb	%xmm13,%xmm10,%xmm10
	vmovdqu	%xmm10,(%rdi)
	vzeroupper
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_ghash_avx,.-gcm_ghash_avx
.align	64
.Lbswap_mask:
.byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
.L0x1c2_polynomial:
.byte	1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2
.L7_mask:
.long	7,0,7,0
.L7_mask_poly:
.long	7,0,450,0
.align	64
.type	.Lrem_4bit,@object
.Lrem_4bit:
.long	0,0,0,471859200,0,943718400,0,610271232
.long	0,1887436800,0,1822425088,0,1220542464,0,1423966208
.long	0,3774873600,0,4246732800,0,3644850176,0,3311403008
.long	0,2441084928,0,2376073216,0,2847932416,0,3051356160
.type	.Lrem_8bit,@object
.Lrem_8bit:
.value	0x0000,0x01C2,0x0384,0x0246,0x0708,0x06CA,0x048C,0x054E
.value	0x0E10,0x0FD2,0x0D94,0x0C56,0x0918,0x08DA,0x0A9C,0x0B5E
.value	0x1C20,0x1DE2,0x1FA4,0x1E66,0x1B28,0x1AEA,0x18AC,0x196E
.value	0x1230,0x13F2,0x11B4,0x1076,0x1538,0x14FA,0x16BC,0x177E
.value	0x3840,0x3982,0x3BC4,0x3A06,0x3F48,0x3E8A,0x3CCC,0x3D0E
.value	0x3650,0x3792,0x35D4,0x3416,0x3158,0x309A,0x32DC,0x331E
.value	0x2460,0x25A2,0x27E4,0x2626,0x2368,0x22AA,0x20EC,0x212E
.value	0x2A70,0x2BB2,0x29F4,0x2836,0x2D78,0x2CBA,0x2EFC,0x2F3E
.value	0x7080,0x7142,0x7304,0x72C6,0x7788,0x764A,0x740C,0x75CE
.value	0x7E90,0x7F52,0x7D14,0x7CD6,0x7998,0x785A,0x7A1C,0x7BDE
.value	0x6CA0,0x6D62,0x6F24,0x6EE6,0x6BA8,0x6A6A,0x682C,0x69EE
.value	0x62B0,0x6372,0x6134,0x60F6,0x65B8,0x647A,0x663C,0x67FE
.value	0x48C0,0x4902,0x4B44,0x4A86,0x4FC8,0x4E0A,0x4C4C,0x4D8E
.value	0x46D0,0x4712,0x4554,0x4496,0x41D8,0x401A,0x425C,0x439E
.value	0x54E0,0x5522,0x5764,0x56A6,0x53E8,0x522A,0x506C,0x51AE
.value	0x5AF0,0x5B32,0x5974,0x58B6,0x5DF8,0x5C3A,0x5E7C,0x5FBE
.value	0xE100,0xE0C2,0xE284,0xE346,0xE608,0xE7CA,0xE58C,0xE44E
.value	0xEF10,0xEED2,0xEC94,0xED56,0xE818,0xE9DA,0xEB9C,0xEA5E
.value	0xFD20,0xFCE2,0xFEA4,0xFF66,0xFA28,0xFBEA,0xF9AC,0xF86E
.value	0xF330,0xF2F2,0xF0B4,0xF176,0xF438,0xF5FA,0xF7BC,0xF67E
.value	0xD940,0xD882,0xDAC4,0xDB06,0xDE48,0xDF8A,0xDDCC,0xDC0E
.value	0xD750,0xD692,0xD4D4,0xD516,0xD058,0xD19A,0xD3DC,0xD21E
.value	0xC560,0xC4A2,0xC6E4,0xC726,0xC268,0xC3AA,0xC1EC,0xC02E
.value	0xCB70,0xCAB2,0xC8F4,0xC936,0xCC78,0xCDBA,0xCFFC,0xCE3E
.value	0x9180,0x9042,0x9204,0x93C6,0x9688,0x974A,0x950C,0x94CE
.value	0x9F90,0x9E52,0x9C14,0x9DD6,0x9898,0x995A,0x9B1C,0x9ADE
.value	0x8DA0,0x8C62,0x8E24,0x8FE6,0x8AA8,0x8B6A,0x892C,0x88EE
.value	0x83B0,0x8272,0x8034,0x81F6,0x84B8,0x857A,0x873C,0x86FE
.value	0xA9C0,0xA802,0xAA44,0xAB86,0xAEC8,0xAF0A,0xAD4C,0xAC8E
.value	0xA7D0,0xA612,0xA454,0xA596,0xA0D8,0xA11A,0xA35C,0xA29E
.value	0xB5E0,0xB422,0xB664,0xB7A6,0xB2E8,0xB32A,0xB16C,0xB0AE
.value	0xBBF0,0xBA32,0xB874,0xB9B6,0xBCF8,0xBD3A,0xBF7C,0xBEBE

.byte	71,72,65,83,72,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	64
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                         node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/poly1305/                     0000775 0000000 0000000 00000000000 14746647661 0026450 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/poly1305/poly1305-x86_64.s    0000664 0000000 0000000 00000207333 14746647661 0031074 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	



.globl	poly1305_init
.hidden	poly1305_init
.globl	poly1305_blocks
.hidden	poly1305_blocks
.globl	poly1305_emit
.hidden	poly1305_emit

.type	poly1305_init,@function
.align	32
poly1305_init:
.cfi_startproc	
	xorq	%rax,%rax
	movq	%rax,0(%rdi)
	movq	%rax,8(%rdi)
	movq	%rax,16(%rdi)

	cmpq	$0,%rsi
	je	.Lno_key

	leaq	poly1305_blocks(%rip),%r10
	leaq	poly1305_emit(%rip),%r11
	movq	OPENSSL_ia32cap_P+4(%rip),%r9
	leaq	poly1305_blocks_avx(%rip),%rax
	leaq	poly1305_emit_avx(%rip),%rcx
	btq	$28,%r9
	cmovcq	%rax,%r10
	cmovcq	%rcx,%r11
	leaq	poly1305_blocks_avx2(%rip),%rax
	btq	$37,%r9
	cmovcq	%rax,%r10
	movq	$2149646336,%rax
	shrq	$32,%r9
	andq	%rax,%r9
	cmpq	%rax,%r9
	je	.Linit_base2_44
	movq	$0x0ffffffc0fffffff,%rax
	movq	$0x0ffffffc0ffffffc,%rcx
	andq	0(%rsi),%rax
	andq	8(%rsi),%rcx
	movq	%rax,24(%rdi)
	movq	%rcx,32(%rdi)
	movq	%r10,0(%rdx)
	movq	%r11,8(%rdx)
	movl	$1,%eax
.Lno_key:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_init,.-poly1305_init

.type	poly1305_blocks,@function
.align	32
poly1305_blocks:
.cfi_startproc	
.Lblocks:
	shrq	$4,%rdx
	jz	.Lno_data

	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lblocks_body:

	movq	%rdx,%r15

	movq	24(%rdi),%r11
	movq	32(%rdi),%r13

	movq	0(%rdi),%r14
	movq	8(%rdi),%rbx
	movq	16(%rdi),%rbp

	movq	%r13,%r12
	shrq	$2,%r13
	movq	%r12,%rax
	addq	%r12,%r13
	jmp	.Loop

.align	32
.Loop:
	addq	0(%rsi),%r14
	adcq	8(%rsi),%rbx
	leaq	16(%rsi),%rsi
	adcq	%rcx,%rbp
	mulq	%r14
	movq	%rax,%r9
	movq	%r11,%rax
	movq	%rdx,%r10

	mulq	%r14
	movq	%rax,%r14
	movq	%r11,%rax
	movq	%rdx,%r8

	mulq	%rbx
	addq	%rax,%r9
	movq	%r13,%rax
	adcq	%rdx,%r10

	mulq	%rbx
	movq	%rbp,%rbx
	addq	%rax,%r14
	adcq	%rdx,%r8

	imulq	%r13,%rbx
	addq	%rbx,%r9
	movq	%r8,%rbx
	adcq	$0,%r10

	imulq	%r11,%rbp
	addq	%r9,%rbx
	movq	$-4,%rax
	adcq	%rbp,%r10

	andq	%r10,%rax
	movq	%r10,%rbp
	shrq	$2,%r10
	andq	$3,%rbp
	addq	%r10,%rax
	addq	%rax,%r14
	adcq	$0,%rbx
	adcq	$0,%rbp
	movq	%r12,%rax
	decq	%r15
	jnz	.Loop

	movq	%r14,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rbp,16(%rdi)

	movq	0(%rsp),%r15
.cfi_restore	%r15
	movq	8(%rsp),%r14
.cfi_restore	%r14
	movq	16(%rsp),%r13
.cfi_restore	%r13
	movq	24(%rsp),%r12
.cfi_restore	%r12
	movq	32(%rsp),%rbp
.cfi_restore	%rbp
	movq	40(%rsp),%rbx
.cfi_restore	%rbx
	leaq	48(%rsp),%rsp
.cfi_adjust_cfa_offset	-48
.Lno_data:
.Lblocks_epilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks,.-poly1305_blocks

.type	poly1305_emit,@function
.align	32
poly1305_emit:
.cfi_startproc	
.Lemit:
	movq	0(%rdi),%r8
	movq	8(%rdi),%r9
	movq	16(%rdi),%r10

	movq	%r8,%rax
	addq	$5,%r8
	movq	%r9,%rcx
	adcq	$0,%r9
	adcq	$0,%r10
	shrq	$2,%r10
	cmovnzq	%r8,%rax
	cmovnzq	%r9,%rcx

	addq	0(%rdx),%rax
	adcq	8(%rdx),%rcx
	movq	%rax,0(%rsi)
	movq	%rcx,8(%rsi)

	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_emit,.-poly1305_emit
.type	__poly1305_block,@function
.align	32
__poly1305_block:
.cfi_startproc	
	mulq	%r14
	movq	%rax,%r9
	movq	%r11,%rax
	movq	%rdx,%r10

	mulq	%r14
	movq	%rax,%r14
	movq	%r11,%rax
	movq	%rdx,%r8

	mulq	%rbx
	addq	%rax,%r9
	movq	%r13,%rax
	adcq	%rdx,%r10

	mulq	%rbx
	movq	%rbp,%rbx
	addq	%rax,%r14
	adcq	%rdx,%r8

	imulq	%r13,%rbx
	addq	%rbx,%r9
	movq	%r8,%rbx
	adcq	$0,%r10

	imulq	%r11,%rbp
	addq	%r9,%rbx
	movq	$-4,%rax
	adcq	%rbp,%r10

	andq	%r10,%rax
	movq	%r10,%rbp
	shrq	$2,%r10
	andq	$3,%rbp
	addq	%r10,%rax
	addq	%rax,%r14
	adcq	$0,%rbx
	adcq	$0,%rbp
	.byte	0xf3,0xc3
.cfi_endproc	
.size	__poly1305_block,.-__poly1305_block

.type	__poly1305_init_avx,@function
.align	32
__poly1305_init_avx:
.cfi_startproc	
	movq	%r11,%r14
	movq	%r12,%rbx
	xorq	%rbp,%rbp

	leaq	48+64(%rdi),%rdi

	movq	%r12,%rax
	call	__poly1305_block

	movl	$0x3ffffff,%eax
	movl	$0x3ffffff,%edx
	movq	%r14,%r8
	andl	%r14d,%eax
	movq	%r11,%r9
	andl	%r11d,%edx
	movl	%eax,-64(%rdi)
	shrq	$26,%r8
	movl	%edx,-60(%rdi)
	shrq	$26,%r9

	movl	$0x3ffffff,%eax
	movl	$0x3ffffff,%edx
	andl	%r8d,%eax
	andl	%r9d,%edx
	movl	%eax,-48(%rdi)
	leal	(%rax,%rax,4),%eax
	movl	%edx,-44(%rdi)
	leal	(%rdx,%rdx,4),%edx
	movl	%eax,-32(%rdi)
	shrq	$26,%r8
	movl	%edx,-28(%rdi)
	shrq	$26,%r9

	movq	%rbx,%rax
	movq	%r12,%rdx
	shlq	$12,%rax
	shlq	$12,%rdx
	orq	%r8,%rax
	orq	%r9,%rdx
	andl	$0x3ffffff,%eax
	andl	$0x3ffffff,%edx
	movl	%eax,-16(%rdi)
	leal	(%rax,%rax,4),%eax
	movl	%edx,-12(%rdi)
	leal	(%rdx,%rdx,4),%edx
	movl	%eax,0(%rdi)
	movq	%rbx,%r8
	movl	%edx,4(%rdi)
	movq	%r12,%r9

	movl	$0x3ffffff,%eax
	movl	$0x3ffffff,%edx
	shrq	$14,%r8
	shrq	$14,%r9
	andl	%r8d,%eax
	andl	%r9d,%edx
	movl	%eax,16(%rdi)
	leal	(%rax,%rax,4),%eax
	movl	%edx,20(%rdi)
	leal	(%rdx,%rdx,4),%edx
	movl	%eax,32(%rdi)
	shrq	$26,%r8
	movl	%edx,36(%rdi)
	shrq	$26,%r9

	movq	%rbp,%rax
	shlq	$24,%rax
	orq	%rax,%r8
	movl	%r8d,48(%rdi)
	leaq	(%r8,%r8,4),%r8
	movl	%r9d,52(%rdi)
	leaq	(%r9,%r9,4),%r9
	movl	%r8d,64(%rdi)
	movl	%r9d,68(%rdi)

	movq	%r12,%rax
	call	__poly1305_block

	movl	$0x3ffffff,%eax
	movq	%r14,%r8
	andl	%r14d,%eax
	shrq	$26,%r8
	movl	%eax,-52(%rdi)

	movl	$0x3ffffff,%edx
	andl	%r8d,%edx
	movl	%edx,-36(%rdi)
	leal	(%rdx,%rdx,4),%edx
	shrq	$26,%r8
	movl	%edx,-20(%rdi)

	movq	%rbx,%rax
	shlq	$12,%rax
	orq	%r8,%rax
	andl	$0x3ffffff,%eax
	movl	%eax,-4(%rdi)
	leal	(%rax,%rax,4),%eax
	movq	%rbx,%r8
	movl	%eax,12(%rdi)

	movl	$0x3ffffff,%edx
	shrq	$14,%r8
	andl	%r8d,%edx
	movl	%edx,28(%rdi)
	leal	(%rdx,%rdx,4),%edx
	shrq	$26,%r8
	movl	%edx,44(%rdi)

	movq	%rbp,%rax
	shlq	$24,%rax
	orq	%rax,%r8
	movl	%r8d,60(%rdi)
	leaq	(%r8,%r8,4),%r8
	movl	%r8d,76(%rdi)

	movq	%r12,%rax
	call	__poly1305_block

	movl	$0x3ffffff,%eax
	movq	%r14,%r8
	andl	%r14d,%eax
	shrq	$26,%r8
	movl	%eax,-56(%rdi)

	movl	$0x3ffffff,%edx
	andl	%r8d,%edx
	movl	%edx,-40(%rdi)
	leal	(%rdx,%rdx,4),%edx
	shrq	$26,%r8
	movl	%edx,-24(%rdi)

	movq	%rbx,%rax
	shlq	$12,%rax
	orq	%r8,%rax
	andl	$0x3ffffff,%eax
	movl	%eax,-8(%rdi)
	leal	(%rax,%rax,4),%eax
	movq	%rbx,%r8
	movl	%eax,8(%rdi)

	movl	$0x3ffffff,%edx
	shrq	$14,%r8
	andl	%r8d,%edx
	movl	%edx,24(%rdi)
	leal	(%rdx,%rdx,4),%edx
	shrq	$26,%r8
	movl	%edx,40(%rdi)

	movq	%rbp,%rax
	shlq	$24,%rax
	orq	%rax,%r8
	movl	%r8d,56(%rdi)
	leaq	(%r8,%r8,4),%r8
	movl	%r8d,72(%rdi)

	leaq	-48-64(%rdi),%rdi
	.byte	0xf3,0xc3
.cfi_endproc	
.size	__poly1305_init_avx,.-__poly1305_init_avx

.type	poly1305_blocks_avx,@function
.align	32
poly1305_blocks_avx:
.cfi_startproc	
	movl	20(%rdi),%r8d
	cmpq	$128,%rdx
	jae	.Lblocks_avx
	testl	%r8d,%r8d
	jz	.Lblocks

.Lblocks_avx:
	andq	$-16,%rdx
	jz	.Lno_data_avx

	vzeroupper

	testl	%r8d,%r8d
	jz	.Lbase2_64_avx

	testq	$31,%rdx
	jz	.Leven_avx

	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lblocks_avx_body:

	movq	%rdx,%r15

	movq	0(%rdi),%r8
	movq	8(%rdi),%r9
	movl	16(%rdi),%ebp

	movq	24(%rdi),%r11
	movq	32(%rdi),%r13


	movl	%r8d,%r14d
	andq	$-2147483648,%r8
	movq	%r9,%r12
	movl	%r9d,%ebx
	andq	$-2147483648,%r9

	shrq	$6,%r8
	shlq	$52,%r12
	addq	%r8,%r14
	shrq	$12,%rbx
	shrq	$18,%r9
	addq	%r12,%r14
	adcq	%r9,%rbx

	movq	%rbp,%r8
	shlq	$40,%r8
	shrq	$24,%rbp
	addq	%r8,%rbx
	adcq	$0,%rbp

	movq	$-4,%r9
	movq	%rbp,%r8
	andq	%rbp,%r9
	shrq	$2,%r8
	andq	$3,%rbp
	addq	%r9,%r8
	addq	%r8,%r14
	adcq	$0,%rbx
	adcq	$0,%rbp

	movq	%r13,%r12
	movq	%r13,%rax
	shrq	$2,%r13
	addq	%r12,%r13

	addq	0(%rsi),%r14
	adcq	8(%rsi),%rbx
	leaq	16(%rsi),%rsi
	adcq	%rcx,%rbp

	call	__poly1305_block

	testq	%rcx,%rcx
	jz	.Lstore_base2_64_avx


	movq	%r14,%rax
	movq	%r14,%rdx
	shrq	$52,%r14
	movq	%rbx,%r11
	movq	%rbx,%r12
	shrq	$26,%rdx
	andq	$0x3ffffff,%rax
	shlq	$12,%r11
	andq	$0x3ffffff,%rdx
	shrq	$14,%rbx
	orq	%r11,%r14
	shlq	$24,%rbp
	andq	$0x3ffffff,%r14
	shrq	$40,%r12
	andq	$0x3ffffff,%rbx
	orq	%r12,%rbp

	subq	$16,%r15
	jz	.Lstore_base2_26_avx

	vmovd	%eax,%xmm0
	vmovd	%edx,%xmm1
	vmovd	%r14d,%xmm2
	vmovd	%ebx,%xmm3
	vmovd	%ebp,%xmm4
	jmp	.Lproceed_avx

.align	32
.Lstore_base2_64_avx:
	movq	%r14,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rbp,16(%rdi)
	jmp	.Ldone_avx

.align	16
.Lstore_base2_26_avx:
	movl	%eax,0(%rdi)
	movl	%edx,4(%rdi)
	movl	%r14d,8(%rdi)
	movl	%ebx,12(%rdi)
	movl	%ebp,16(%rdi)
.align	16
.Ldone_avx:
	movq	0(%rsp),%r15
.cfi_restore	%r15
	movq	8(%rsp),%r14
.cfi_restore	%r14
	movq	16(%rsp),%r13
.cfi_restore	%r13
	movq	24(%rsp),%r12
.cfi_restore	%r12
	movq	32(%rsp),%rbp
.cfi_restore	%rbp
	movq	40(%rsp),%rbx
.cfi_restore	%rbx
	leaq	48(%rsp),%rsp
.cfi_adjust_cfa_offset	-48
.Lno_data_avx:
.Lblocks_avx_epilogue:
	.byte	0xf3,0xc3
.cfi_endproc	

.align	32
.Lbase2_64_avx:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lbase2_64_avx_body:

	movq	%rdx,%r15

	movq	24(%rdi),%r11
	movq	32(%rdi),%r13

	movq	0(%rdi),%r14
	movq	8(%rdi),%rbx
	movl	16(%rdi),%ebp

	movq	%r13,%r12
	movq	%r13,%rax
	shrq	$2,%r13
	addq	%r12,%r13

	testq	$31,%rdx
	jz	.Linit_avx

	addq	0(%rsi),%r14
	adcq	8(%rsi),%rbx
	leaq	16(%rsi),%rsi
	adcq	%rcx,%rbp
	subq	$16,%r15

	call	__poly1305_block

.Linit_avx:

	movq	%r14,%rax
	movq	%r14,%rdx
	shrq	$52,%r14
	movq	%rbx,%r8
	movq	%rbx,%r9
	shrq	$26,%rdx
	andq	$0x3ffffff,%rax
	shlq	$12,%r8
	andq	$0x3ffffff,%rdx
	shrq	$14,%rbx
	orq	%r8,%r14
	shlq	$24,%rbp
	andq	$0x3ffffff,%r14
	shrq	$40,%r9
	andq	$0x3ffffff,%rbx
	orq	%r9,%rbp

	vmovd	%eax,%xmm0
	vmovd	%edx,%xmm1
	vmovd	%r14d,%xmm2
	vmovd	%ebx,%xmm3
	vmovd	%ebp,%xmm4
	movl	$1,20(%rdi)

	call	__poly1305_init_avx

.Lproceed_avx:
	movq	%r15,%rdx

	movq	0(%rsp),%r15
.cfi_restore	%r15
	movq	8(%rsp),%r14
.cfi_restore	%r14
	movq	16(%rsp),%r13
.cfi_restore	%r13
	movq	24(%rsp),%r12
.cfi_restore	%r12
	movq	32(%rsp),%rbp
.cfi_restore	%rbp
	movq	40(%rsp),%rbx
.cfi_restore	%rbx
	leaq	48(%rsp),%rax
	leaq	48(%rsp),%rsp
.cfi_adjust_cfa_offset	-48
.Lbase2_64_avx_epilogue:
	jmp	.Ldo_avx
.cfi_endproc	

.align	32
.Leven_avx:
.cfi_startproc	
	vmovd	0(%rdi),%xmm0
	vmovd	4(%rdi),%xmm1
	vmovd	8(%rdi),%xmm2
	vmovd	12(%rdi),%xmm3
	vmovd	16(%rdi),%xmm4

.Ldo_avx:
	leaq	-88(%rsp),%r11
.cfi_def_cfa	%r11,0x60
	subq	$0x178,%rsp
	subq	$64,%rdx
	leaq	-32(%rsi),%rax
	cmovcq	%rax,%rsi

	vmovdqu	48(%rdi),%xmm14
	leaq	112(%rdi),%rdi
	leaq	.Lconst(%rip),%rcx



	vmovdqu	32(%rsi),%xmm5
	vmovdqu	48(%rsi),%xmm6
	vmovdqa	64(%rcx),%xmm15

	vpsrldq	$6,%xmm5,%xmm7
	vpsrldq	$6,%xmm6,%xmm8
	vpunpckhqdq	%xmm6,%xmm5,%xmm9
	vpunpcklqdq	%xmm6,%xmm5,%xmm5
	vpunpcklqdq	%xmm8,%xmm7,%xmm8

	vpsrlq	$40,%xmm9,%xmm9
	vpsrlq	$26,%xmm5,%xmm6
	vpand	%xmm15,%xmm5,%xmm5
	vpsrlq	$4,%xmm8,%xmm7
	vpand	%xmm15,%xmm6,%xmm6
	vpsrlq	$30,%xmm8,%xmm8
	vpand	%xmm15,%xmm7,%xmm7
	vpand	%xmm15,%xmm8,%xmm8
	vpor	32(%rcx),%xmm9,%xmm9

	jbe	.Lskip_loop_avx


	vmovdqu	-48(%rdi),%xmm11
	vmovdqu	-32(%rdi),%xmm12
	vpshufd	$0xEE,%xmm14,%xmm13
	vpshufd	$0x44,%xmm14,%xmm10
	vmovdqa	%xmm13,-144(%r11)
	vmovdqa	%xmm10,0(%rsp)
	vpshufd	$0xEE,%xmm11,%xmm14
	vmovdqu	-16(%rdi),%xmm10
	vpshufd	$0x44,%xmm11,%xmm11
	vmovdqa	%xmm14,-128(%r11)
	vmovdqa	%xmm11,16(%rsp)
	vpshufd	$0xEE,%xmm12,%xmm13
	vmovdqu	0(%rdi),%xmm11
	vpshufd	$0x44,%xmm12,%xmm12
	vmovdqa	%xmm13,-112(%r11)
	vmovdqa	%xmm12,32(%rsp)
	vpshufd	$0xEE,%xmm10,%xmm14
	vmovdqu	16(%rdi),%xmm12
	vpshufd	$0x44,%xmm10,%xmm10
	vmovdqa	%xmm14,-96(%r11)
	vmovdqa	%xmm10,48(%rsp)
	vpshufd	$0xEE,%xmm11,%xmm13
	vmovdqu	32(%rdi),%xmm10
	vpshufd	$0x44,%xmm11,%xmm11
	vmovdqa	%xmm13,-80(%r11)
	vmovdqa	%xmm11,64(%rsp)
	vpshufd	$0xEE,%xmm12,%xmm14
	vmovdqu	48(%rdi),%xmm11
	vpshufd	$0x44,%xmm12,%xmm12
	vmovdqa	%xmm14,-64(%r11)
	vmovdqa	%xmm12,80(%rsp)
	vpshufd	$0xEE,%xmm10,%xmm13
	vmovdqu	64(%rdi),%xmm12
	vpshufd	$0x44,%xmm10,%xmm10
	vmovdqa	%xmm13,-48(%r11)
	vmovdqa	%xmm10,96(%rsp)
	vpshufd	$0xEE,%xmm11,%xmm14
	vpshufd	$0x44,%xmm11,%xmm11
	vmovdqa	%xmm14,-32(%r11)
	vmovdqa	%xmm11,112(%rsp)
	vpshufd	$0xEE,%xmm12,%xmm13
	vmovdqa	0(%rsp),%xmm14
	vpshufd	$0x44,%xmm12,%xmm12
	vmovdqa	%xmm13,-16(%r11)
	vmovdqa	%xmm12,128(%rsp)

	jmp	.Loop_avx

.align	32
.Loop_avx:




















	vpmuludq	%xmm5,%xmm14,%xmm10
	vpmuludq	%xmm6,%xmm14,%xmm11
	vmovdqa	%xmm2,32(%r11)
	vpmuludq	%xmm7,%xmm14,%xmm12
	vmovdqa	16(%rsp),%xmm2
	vpmuludq	%xmm8,%xmm14,%xmm13
	vpmuludq	%xmm9,%xmm14,%xmm14

	vmovdqa	%xmm0,0(%r11)
	vpmuludq	32(%rsp),%xmm9,%xmm0
	vmovdqa	%xmm1,16(%r11)
	vpmuludq	%xmm8,%xmm2,%xmm1
	vpaddq	%xmm0,%xmm10,%xmm10
	vpaddq	%xmm1,%xmm14,%xmm14
	vmovdqa	%xmm3,48(%r11)
	vpmuludq	%xmm7,%xmm2,%xmm0
	vpmuludq	%xmm6,%xmm2,%xmm1
	vpaddq	%xmm0,%xmm13,%xmm13
	vmovdqa	48(%rsp),%xmm3
	vpaddq	%xmm1,%xmm12,%xmm12
	vmovdqa	%xmm4,64(%r11)
	vpmuludq	%xmm5,%xmm2,%xmm2
	vpmuludq	%xmm7,%xmm3,%xmm0
	vpaddq	%xmm2,%xmm11,%xmm11

	vmovdqa	64(%rsp),%xmm4
	vpaddq	%xmm0,%xmm14,%xmm14
	vpmuludq	%xmm6,%xmm3,%xmm1
	vpmuludq	%xmm5,%xmm3,%xmm3
	vpaddq	%xmm1,%xmm13,%xmm13
	vmovdqa	80(%rsp),%xmm2
	vpaddq	%xmm3,%xmm12,%xmm12
	vpmuludq	%xmm9,%xmm4,%xmm0
	vpmuludq	%xmm8,%xmm4,%xmm4
	vpaddq	%xmm0,%xmm11,%xmm11
	vmovdqa	96(%rsp),%xmm3
	vpaddq	%xmm4,%xmm10,%xmm10

	vmovdqa	128(%rsp),%xmm4
	vpmuludq	%xmm6,%xmm2,%xmm1
	vpmuludq	%xmm5,%xmm2,%xmm2
	vpaddq	%xmm1,%xmm14,%xmm14
	vpaddq	%xmm2,%xmm13,%xmm13
	vpmuludq	%xmm9,%xmm3,%xmm0
	vpmuludq	%xmm8,%xmm3,%xmm1
	vpaddq	%xmm0,%xmm12,%xmm12
	vmovdqu	0(%rsi),%xmm0
	vpaddq	%xmm1,%xmm11,%xmm11
	vpmuludq	%xmm7,%xmm3,%xmm3
	vpmuludq	%xmm7,%xmm4,%xmm7
	vpaddq	%xmm3,%xmm10,%xmm10

	vmovdqu	16(%rsi),%xmm1
	vpaddq	%xmm7,%xmm11,%xmm11
	vpmuludq	%xmm8,%xmm4,%xmm8
	vpmuludq	%xmm9,%xmm4,%xmm9
	vpsrldq	$6,%xmm0,%xmm2
	vpaddq	%xmm8,%xmm12,%xmm12
	vpaddq	%xmm9,%xmm13,%xmm13
	vpsrldq	$6,%xmm1,%xmm3
	vpmuludq	112(%rsp),%xmm5,%xmm9
	vpmuludq	%xmm6,%xmm4,%xmm5
	vpunpckhqdq	%xmm1,%xmm0,%xmm4
	vpaddq	%xmm9,%xmm14,%xmm14
	vmovdqa	-144(%r11),%xmm9
	vpaddq	%xmm5,%xmm10,%xmm10

	vpunpcklqdq	%xmm1,%xmm0,%xmm0
	vpunpcklqdq	%xmm3,%xmm2,%xmm3


	vpsrldq	$5,%xmm4,%xmm4
	vpsrlq	$26,%xmm0,%xmm1
	vpand	%xmm15,%xmm0,%xmm0
	vpsrlq	$4,%xmm3,%xmm2
	vpand	%xmm15,%xmm1,%xmm1
	vpand	0(%rcx),%xmm4,%xmm4
	vpsrlq	$30,%xmm3,%xmm3
	vpand	%xmm15,%xmm2,%xmm2
	vpand	%xmm15,%xmm3,%xmm3
	vpor	32(%rcx),%xmm4,%xmm4

	vpaddq	0(%r11),%xmm0,%xmm0
	vpaddq	16(%r11),%xmm1,%xmm1
	vpaddq	32(%r11),%xmm2,%xmm2
	vpaddq	48(%r11),%xmm3,%xmm3
	vpaddq	64(%r11),%xmm4,%xmm4

	leaq	32(%rsi),%rax
	leaq	64(%rsi),%rsi
	subq	$64,%rdx
	cmovcq	%rax,%rsi










	vpmuludq	%xmm0,%xmm9,%xmm5
	vpmuludq	%xmm1,%xmm9,%xmm6
	vpaddq	%xmm5,%xmm10,%xmm10
	vpaddq	%xmm6,%xmm11,%xmm11
	vmovdqa	-128(%r11),%xmm7
	vpmuludq	%xmm2,%xmm9,%xmm5
	vpmuludq	%xmm3,%xmm9,%xmm6
	vpaddq	%xmm5,%xmm12,%xmm12
	vpaddq	%xmm6,%xmm13,%xmm13
	vpmuludq	%xmm4,%xmm9,%xmm9
	vpmuludq	-112(%r11),%xmm4,%xmm5
	vpaddq	%xmm9,%xmm14,%xmm14

	vpaddq	%xmm5,%xmm10,%xmm10
	vpmuludq	%xmm2,%xmm7,%xmm6
	vpmuludq	%xmm3,%xmm7,%xmm5
	vpaddq	%xmm6,%xmm13,%xmm13
	vmovdqa	-96(%r11),%xmm8
	vpaddq	%xmm5,%xmm14,%xmm14
	vpmuludq	%xmm1,%xmm7,%xmm6
	vpmuludq	%xmm0,%xmm7,%xmm7
	vpaddq	%xmm6,%xmm12,%xmm12
	vpaddq	%xmm7,%xmm11,%xmm11

	vmovdqa	-80(%r11),%xmm9
	vpmuludq	%xmm2,%xmm8,%xmm5
	vpmuludq	%xmm1,%xmm8,%xmm6
	vpaddq	%xmm5,%xmm14,%xmm14
	vpaddq	%xmm6,%xmm13,%xmm13
	vmovdqa	-64(%r11),%xmm7
	vpmuludq	%xmm0,%xmm8,%xmm8
	vpmuludq	%xmm4,%xmm9,%xmm5
	vpaddq	%xmm8,%xmm12,%xmm12
	vpaddq	%xmm5,%xmm11,%xmm11
	vmovdqa	-48(%r11),%xmm8
	vpmuludq	%xmm3,%xmm9,%xmm9
	vpmuludq	%xmm1,%xmm7,%xmm6
	vpaddq	%xmm9,%xmm10,%xmm10

	vmovdqa	-16(%r11),%xmm9
	vpaddq	%xmm6,%xmm14,%xmm14
	vpmuludq	%xmm0,%xmm7,%xmm7
	vpmuludq	%xmm4,%xmm8,%xmm5
	vpaddq	%xmm7,%xmm13,%xmm13
	vpaddq	%xmm5,%xmm12,%xmm12
	vmovdqu	32(%rsi),%xmm5
	vpmuludq	%xmm3,%xmm8,%xmm7
	vpmuludq	%xmm2,%xmm8,%xmm8
	vpaddq	%xmm7,%xmm11,%xmm11
	vmovdqu	48(%rsi),%xmm6
	vpaddq	%xmm8,%xmm10,%xmm10

	vpmuludq	%xmm2,%xmm9,%xmm2
	vpmuludq	%xmm3,%xmm9,%xmm3
	vpsrldq	$6,%xmm5,%xmm7
	vpaddq	%xmm2,%xmm11,%xmm11
	vpmuludq	%xmm4,%xmm9,%xmm4
	vpsrldq	$6,%xmm6,%xmm8
	vpaddq	%xmm3,%xmm12,%xmm2
	vpaddq	%xmm4,%xmm13,%xmm3
	vpmuludq	-32(%r11),%xmm0,%xmm4
	vpmuludq	%xmm1,%xmm9,%xmm0
	vpunpckhqdq	%xmm6,%xmm5,%xmm9
	vpaddq	%xmm4,%xmm14,%xmm4
	vpaddq	%xmm0,%xmm10,%xmm0

	vpunpcklqdq	%xmm6,%xmm5,%xmm5
	vpunpcklqdq	%xmm8,%xmm7,%xmm8


	vpsrldq	$5,%xmm9,%xmm9
	vpsrlq	$26,%xmm5,%xmm6
	vmovdqa	0(%rsp),%xmm14
	vpand	%xmm15,%xmm5,%xmm5
	vpsrlq	$4,%xmm8,%xmm7
	vpand	%xmm15,%xmm6,%xmm6
	vpand	0(%rcx),%xmm9,%xmm9
	vpsrlq	$30,%xmm8,%xmm8
	vpand	%xmm15,%xmm7,%xmm7
	vpand	%xmm15,%xmm8,%xmm8
	vpor	32(%rcx),%xmm9,%xmm9





	vpsrlq	$26,%xmm3,%xmm13
	vpand	%xmm15,%xmm3,%xmm3
	vpaddq	%xmm13,%xmm4,%xmm4

	vpsrlq	$26,%xmm0,%xmm10
	vpand	%xmm15,%xmm0,%xmm0
	vpaddq	%xmm10,%xmm11,%xmm1

	vpsrlq	$26,%xmm4,%xmm10
	vpand	%xmm15,%xmm4,%xmm4

	vpsrlq	$26,%xmm1,%xmm11
	vpand	%xmm15,%xmm1,%xmm1
	vpaddq	%xmm11,%xmm2,%xmm2

	vpaddq	%xmm10,%xmm0,%xmm0
	vpsllq	$2,%xmm10,%xmm10
	vpaddq	%xmm10,%xmm0,%xmm0

	vpsrlq	$26,%xmm2,%xmm12
	vpand	%xmm15,%xmm2,%xmm2
	vpaddq	%xmm12,%xmm3,%xmm3

	vpsrlq	$26,%xmm0,%xmm10
	vpand	%xmm15,%xmm0,%xmm0
	vpaddq	%xmm10,%xmm1,%xmm1

	vpsrlq	$26,%xmm3,%xmm13
	vpand	%xmm15,%xmm3,%xmm3
	vpaddq	%xmm13,%xmm4,%xmm4

	ja	.Loop_avx

.Lskip_loop_avx:



	vpshufd	$0x10,%xmm14,%xmm14
	addq	$32,%rdx
	jnz	.Long_tail_avx

	vpaddq	%xmm2,%xmm7,%xmm7
	vpaddq	%xmm0,%xmm5,%xmm5
	vpaddq	%xmm1,%xmm6,%xmm6
	vpaddq	%xmm3,%xmm8,%xmm8
	vpaddq	%xmm4,%xmm9,%xmm9

.Long_tail_avx:
	vmovdqa	%xmm2,32(%r11)
	vmovdqa	%xmm0,0(%r11)
	vmovdqa	%xmm1,16(%r11)
	vmovdqa	%xmm3,48(%r11)
	vmovdqa	%xmm4,64(%r11)







	vpmuludq	%xmm7,%xmm14,%xmm12
	vpmuludq	%xmm5,%xmm14,%xmm10
	vpshufd	$0x10,-48(%rdi),%xmm2
	vpmuludq	%xmm6,%xmm14,%xmm11
	vpmuludq	%xmm8,%xmm14,%xmm13
	vpmuludq	%xmm9,%xmm14,%xmm14

	vpmuludq	%xmm8,%xmm2,%xmm0
	vpaddq	%xmm0,%xmm14,%xmm14
	vpshufd	$0x10,-32(%rdi),%xmm3
	vpmuludq	%xmm7,%xmm2,%xmm1
	vpaddq	%xmm1,%xmm13,%xmm13
	vpshufd	$0x10,-16(%rdi),%xmm4
	vpmuludq	%xmm6,%xmm2,%xmm0
	vpaddq	%xmm0,%xmm12,%xmm12
	vpmuludq	%xmm5,%xmm2,%xmm2
	vpaddq	%xmm2,%xmm11,%xmm11
	vpmuludq	%xmm9,%xmm3,%xmm3
	vpaddq	%xmm3,%xmm10,%xmm10

	vpshufd	$0x10,0(%rdi),%xmm2
	vpmuludq	%xmm7,%xmm4,%xmm1
	vpaddq	%xmm1,%xmm14,%xmm14
	vpmuludq	%xmm6,%xmm4,%xmm0
	vpaddq	%xmm0,%xmm13,%xmm13
	vpshufd	$0x10,16(%rdi),%xmm3
	vpmuludq	%xmm5,%xmm4,%xmm4
	vpaddq	%xmm4,%xmm12,%xmm12
	vpmuludq	%xmm9,%xmm2,%xmm1
	vpaddq	%xmm1,%xmm11,%xmm11
	vpshufd	$0x10,32(%rdi),%xmm4
	vpmuludq	%xmm8,%xmm2,%xmm2
	vpaddq	%xmm2,%xmm10,%xmm10

	vpmuludq	%xmm6,%xmm3,%xmm0
	vpaddq	%xmm0,%xmm14,%xmm14
	vpmuludq	%xmm5,%xmm3,%xmm3
	vpaddq	%xmm3,%xmm13,%xmm13
	vpshufd	$0x10,48(%rdi),%xmm2
	vpmuludq	%xmm9,%xmm4,%xmm1
	vpaddq	%xmm1,%xmm12,%xmm12
	vpshufd	$0x10,64(%rdi),%xmm3
	vpmuludq	%xmm8,%xmm4,%xmm0
	vpaddq	%xmm0,%xmm11,%xmm11
	vpmuludq	%xmm7,%xmm4,%xmm4
	vpaddq	%xmm4,%xmm10,%xmm10

	vpmuludq	%xmm5,%xmm2,%xmm2
	vpaddq	%xmm2,%xmm14,%xmm14
	vpmuludq	%xmm9,%xmm3,%xmm1
	vpaddq	%xmm1,%xmm13,%xmm13
	vpmuludq	%xmm8,%xmm3,%xmm0
	vpaddq	%xmm0,%xmm12,%xmm12
	vpmuludq	%xmm7,%xmm3,%xmm1
	vpaddq	%xmm1,%xmm11,%xmm11
	vpmuludq	%xmm6,%xmm3,%xmm3
	vpaddq	%xmm3,%xmm10,%xmm10

	jz	.Lshort_tail_avx

	vmovdqu	0(%rsi),%xmm0
	vmovdqu	16(%rsi),%xmm1

	vpsrldq	$6,%xmm0,%xmm2
	vpsrldq	$6,%xmm1,%xmm3
	vpunpckhqdq	%xmm1,%xmm0,%xmm4
	vpunpcklqdq	%xmm1,%xmm0,%xmm0
	vpunpcklqdq	%xmm3,%xmm2,%xmm3

	vpsrlq	$40,%xmm4,%xmm4
	vpsrlq	$26,%xmm0,%xmm1
	vpand	%xmm15,%xmm0,%xmm0
	vpsrlq	$4,%xmm3,%xmm2
	vpand	%xmm15,%xmm1,%xmm1
	vpsrlq	$30,%xmm3,%xmm3
	vpand	%xmm15,%xmm2,%xmm2
	vpand	%xmm15,%xmm3,%xmm3
	vpor	32(%rcx),%xmm4,%xmm4

	vpshufd	$0x32,-64(%rdi),%xmm9
	vpaddq	0(%r11),%xmm0,%xmm0
	vpaddq	16(%r11),%xmm1,%xmm1
	vpaddq	32(%r11),%xmm2,%xmm2
	vpaddq	48(%r11),%xmm3,%xmm3
	vpaddq	64(%r11),%xmm4,%xmm4




	vpmuludq	%xmm0,%xmm9,%xmm5
	vpaddq	%xmm5,%xmm10,%xmm10
	vpmuludq	%xmm1,%xmm9,%xmm6
	vpaddq	%xmm6,%xmm11,%xmm11
	vpmuludq	%xmm2,%xmm9,%xmm5
	vpaddq	%xmm5,%xmm12,%xmm12
	vpshufd	$0x32,-48(%rdi),%xmm7
	vpmuludq	%xmm3,%xmm9,%xmm6
	vpaddq	%xmm6,%xmm13,%xmm13
	vpmuludq	%xmm4,%xmm9,%xmm9
	vpaddq	%xmm9,%xmm14,%xmm14

	vpmuludq	%xmm3,%xmm7,%xmm5
	vpaddq	%xmm5,%xmm14,%xmm14
	vpshufd	$0x32,-32(%rdi),%xmm8
	vpmuludq	%xmm2,%xmm7,%xmm6
	vpaddq	%xmm6,%xmm13,%xmm13
	vpshufd	$0x32,-16(%rdi),%xmm9
	vpmuludq	%xmm1,%xmm7,%xmm5
	vpaddq	%xmm5,%xmm12,%xmm12
	vpmuludq	%xmm0,%xmm7,%xmm7
	vpaddq	%xmm7,%xmm11,%xmm11
	vpmuludq	%xmm4,%xmm8,%xmm8
	vpaddq	%xmm8,%xmm10,%xmm10

	vpshufd	$0x32,0(%rdi),%xmm7
	vpmuludq	%xmm2,%xmm9,%xmm6
	vpaddq	%xmm6,%xmm14,%xmm14
	vpmuludq	%xmm1,%xmm9,%xmm5
	vpaddq	%xmm5,%xmm13,%xmm13
	vpshufd	$0x32,16(%rdi),%xmm8
	vpmuludq	%xmm0,%xmm9,%xmm9
	vpaddq	%xmm9,%xmm12,%xmm12
	vpmuludq	%xmm4,%xmm7,%xmm6
	vpaddq	%xmm6,%xmm11,%xmm11
	vpshufd	$0x32,32(%rdi),%xmm9
	vpmuludq	%xmm3,%xmm7,%xmm7
	vpaddq	%xmm7,%xmm10,%xmm10

	vpmuludq	%xmm1,%xmm8,%xmm5
	vpaddq	%xmm5,%xmm14,%xmm14
	vpmuludq	%xmm0,%xmm8,%xmm8
	vpaddq	%xmm8,%xmm13,%xmm13
	vpshufd	$0x32,48(%rdi),%xmm7
	vpmuludq	%xmm4,%xmm9,%xmm6
	vpaddq	%xmm6,%xmm12,%xmm12
	vpshufd	$0x32,64(%rdi),%xmm8
	vpmuludq	%xmm3,%xmm9,%xmm5
	vpaddq	%xmm5,%xmm11,%xmm11
	vpmuludq	%xmm2,%xmm9,%xmm9
	vpaddq	%xmm9,%xmm10,%xmm10

	vpmuludq	%xmm0,%xmm7,%xmm7
	vpaddq	%xmm7,%xmm14,%xmm14
	vpmuludq	%xmm4,%xmm8,%xmm6
	vpaddq	%xmm6,%xmm13,%xmm13
	vpmuludq	%xmm3,%xmm8,%xmm5
	vpaddq	%xmm5,%xmm12,%xmm12
	vpmuludq	%xmm2,%xmm8,%xmm6
	vpaddq	%xmm6,%xmm11,%xmm11
	vpmuludq	%xmm1,%xmm8,%xmm8
	vpaddq	%xmm8,%xmm10,%xmm10

.Lshort_tail_avx:



	vpsrldq	$8,%xmm14,%xmm9
	vpsrldq	$8,%xmm13,%xmm8
	vpsrldq	$8,%xmm11,%xmm6
	vpsrldq	$8,%xmm10,%xmm5
	vpsrldq	$8,%xmm12,%xmm7
	vpaddq	%xmm8,%xmm13,%xmm13
	vpaddq	%xmm9,%xmm14,%xmm14
	vpaddq	%xmm5,%xmm10,%xmm10
	vpaddq	%xmm6,%xmm11,%xmm11
	vpaddq	%xmm7,%xmm12,%xmm12




	vpsrlq	$26,%xmm13,%xmm3
	vpand	%xmm15,%xmm13,%xmm13
	vpaddq	%xmm3,%xmm14,%xmm14

	vpsrlq	$26,%xmm10,%xmm0
	vpand	%xmm15,%xmm10,%xmm10
	vpaddq	%xmm0,%xmm11,%xmm11

	vpsrlq	$26,%xmm14,%xmm4
	vpand	%xmm15,%xmm14,%xmm14

	vpsrlq	$26,%xmm11,%xmm1
	vpand	%xmm15,%xmm11,%xmm11
	vpaddq	%xmm1,%xmm12,%xmm12

	vpaddq	%xmm4,%xmm10,%xmm10
	vpsllq	$2,%xmm4,%xmm4
	vpaddq	%xmm4,%xmm10,%xmm10

	vpsrlq	$26,%xmm12,%xmm2
	vpand	%xmm15,%xmm12,%xmm12
	vpaddq	%xmm2,%xmm13,%xmm13

	vpsrlq	$26,%xmm10,%xmm0
	vpand	%xmm15,%xmm10,%xmm10
	vpaddq	%xmm0,%xmm11,%xmm11

	vpsrlq	$26,%xmm13,%xmm3
	vpand	%xmm15,%xmm13,%xmm13
	vpaddq	%xmm3,%xmm14,%xmm14

	vmovd	%xmm10,-112(%rdi)
	vmovd	%xmm11,-108(%rdi)
	vmovd	%xmm12,-104(%rdi)
	vmovd	%xmm13,-100(%rdi)
	vmovd	%xmm14,-96(%rdi)
	leaq	88(%r11),%rsp
.cfi_def_cfa	%rsp,8
	vzeroupper
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_avx,.-poly1305_blocks_avx

.type	poly1305_emit_avx,@function
.align	32
poly1305_emit_avx:
.cfi_startproc	
	cmpl	$0,20(%rdi)
	je	.Lemit

	movl	0(%rdi),%eax
	movl	4(%rdi),%ecx
	movl	8(%rdi),%r8d
	movl	12(%rdi),%r11d
	movl	16(%rdi),%r10d

	shlq	$26,%rcx
	movq	%r8,%r9
	shlq	$52,%r8
	addq	%rcx,%rax
	shrq	$12,%r9
	addq	%rax,%r8
	adcq	$0,%r9

	shlq	$14,%r11
	movq	%r10,%rax
	shrq	$24,%r10
	addq	%r11,%r9
	shlq	$40,%rax
	addq	%rax,%r9
	adcq	$0,%r10

	movq	%r10,%rax
	movq	%r10,%rcx
	andq	$3,%r10
	shrq	$2,%rax
	andq	$-4,%rcx
	addq	%rcx,%rax
	addq	%rax,%r8
	adcq	$0,%r9
	adcq	$0,%r10

	movq	%r8,%rax
	addq	$5,%r8
	movq	%r9,%rcx
	adcq	$0,%r9
	adcq	$0,%r10
	shrq	$2,%r10
	cmovnzq	%r8,%rax
	cmovnzq	%r9,%rcx

	addq	0(%rdx),%rax
	adcq	8(%rdx),%rcx
	movq	%rax,0(%rsi)
	movq	%rcx,8(%rsi)

	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_emit_avx,.-poly1305_emit_avx
.type	poly1305_blocks_avx2,@function
.align	32
poly1305_blocks_avx2:
.cfi_startproc	
	movl	20(%rdi),%r8d
	cmpq	$128,%rdx
	jae	.Lblocks_avx2
	testl	%r8d,%r8d
	jz	.Lblocks

.Lblocks_avx2:
	andq	$-16,%rdx
	jz	.Lno_data_avx2

	vzeroupper

	testl	%r8d,%r8d
	jz	.Lbase2_64_avx2

	testq	$63,%rdx
	jz	.Leven_avx2

	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lblocks_avx2_body:

	movq	%rdx,%r15

	movq	0(%rdi),%r8
	movq	8(%rdi),%r9
	movl	16(%rdi),%ebp

	movq	24(%rdi),%r11
	movq	32(%rdi),%r13


	movl	%r8d,%r14d
	andq	$-2147483648,%r8
	movq	%r9,%r12
	movl	%r9d,%ebx
	andq	$-2147483648,%r9

	shrq	$6,%r8
	shlq	$52,%r12
	addq	%r8,%r14
	shrq	$12,%rbx
	shrq	$18,%r9
	addq	%r12,%r14
	adcq	%r9,%rbx

	movq	%rbp,%r8
	shlq	$40,%r8
	shrq	$24,%rbp
	addq	%r8,%rbx
	adcq	$0,%rbp

	movq	$-4,%r9
	movq	%rbp,%r8
	andq	%rbp,%r9
	shrq	$2,%r8
	andq	$3,%rbp
	addq	%r9,%r8
	addq	%r8,%r14
	adcq	$0,%rbx
	adcq	$0,%rbp

	movq	%r13,%r12
	movq	%r13,%rax
	shrq	$2,%r13
	addq	%r12,%r13

.Lbase2_26_pre_avx2:
	addq	0(%rsi),%r14
	adcq	8(%rsi),%rbx
	leaq	16(%rsi),%rsi
	adcq	%rcx,%rbp
	subq	$16,%r15

	call	__poly1305_block
	movq	%r12,%rax

	testq	$63,%r15
	jnz	.Lbase2_26_pre_avx2

	testq	%rcx,%rcx
	jz	.Lstore_base2_64_avx2


	movq	%r14,%rax
	movq	%r14,%rdx
	shrq	$52,%r14
	movq	%rbx,%r11
	movq	%rbx,%r12
	shrq	$26,%rdx
	andq	$0x3ffffff,%rax
	shlq	$12,%r11
	andq	$0x3ffffff,%rdx
	shrq	$14,%rbx
	orq	%r11,%r14
	shlq	$24,%rbp
	andq	$0x3ffffff,%r14
	shrq	$40,%r12
	andq	$0x3ffffff,%rbx
	orq	%r12,%rbp

	testq	%r15,%r15
	jz	.Lstore_base2_26_avx2

	vmovd	%eax,%xmm0
	vmovd	%edx,%xmm1
	vmovd	%r14d,%xmm2
	vmovd	%ebx,%xmm3
	vmovd	%ebp,%xmm4
	jmp	.Lproceed_avx2

.align	32
.Lstore_base2_64_avx2:
	movq	%r14,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rbp,16(%rdi)
	jmp	.Ldone_avx2

.align	16
.Lstore_base2_26_avx2:
	movl	%eax,0(%rdi)
	movl	%edx,4(%rdi)
	movl	%r14d,8(%rdi)
	movl	%ebx,12(%rdi)
	movl	%ebp,16(%rdi)
.align	16
.Ldone_avx2:
	movq	0(%rsp),%r15
.cfi_restore	%r15
	movq	8(%rsp),%r14
.cfi_restore	%r14
	movq	16(%rsp),%r13
.cfi_restore	%r13
	movq	24(%rsp),%r12
.cfi_restore	%r12
	movq	32(%rsp),%rbp
.cfi_restore	%rbp
	movq	40(%rsp),%rbx
.cfi_restore	%rbx
	leaq	48(%rsp),%rsp
.cfi_adjust_cfa_offset	-48
.Lno_data_avx2:
.Lblocks_avx2_epilogue:
	.byte	0xf3,0xc3
.cfi_endproc	

.align	32
.Lbase2_64_avx2:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lbase2_64_avx2_body:

	movq	%rdx,%r15

	movq	24(%rdi),%r11
	movq	32(%rdi),%r13

	movq	0(%rdi),%r14
	movq	8(%rdi),%rbx
	movl	16(%rdi),%ebp

	movq	%r13,%r12
	movq	%r13,%rax
	shrq	$2,%r13
	addq	%r12,%r13

	testq	$63,%rdx
	jz	.Linit_avx2

.Lbase2_64_pre_avx2:
	addq	0(%rsi),%r14
	adcq	8(%rsi),%rbx
	leaq	16(%rsi),%rsi
	adcq	%rcx,%rbp
	subq	$16,%r15

	call	__poly1305_block
	movq	%r12,%rax

	testq	$63,%r15
	jnz	.Lbase2_64_pre_avx2

.Linit_avx2:

	movq	%r14,%rax
	movq	%r14,%rdx
	shrq	$52,%r14
	movq	%rbx,%r8
	movq	%rbx,%r9
	shrq	$26,%rdx
	andq	$0x3ffffff,%rax
	shlq	$12,%r8
	andq	$0x3ffffff,%rdx
	shrq	$14,%rbx
	orq	%r8,%r14
	shlq	$24,%rbp
	andq	$0x3ffffff,%r14
	shrq	$40,%r9
	andq	$0x3ffffff,%rbx
	orq	%r9,%rbp

	vmovd	%eax,%xmm0
	vmovd	%edx,%xmm1
	vmovd	%r14d,%xmm2
	vmovd	%ebx,%xmm3
	vmovd	%ebp,%xmm4
	movl	$1,20(%rdi)

	call	__poly1305_init_avx

.Lproceed_avx2:
	movq	%r15,%rdx
	movl	OPENSSL_ia32cap_P+8(%rip),%r10d
	movl	$3221291008,%r11d

	movq	0(%rsp),%r15
.cfi_restore	%r15
	movq	8(%rsp),%r14
.cfi_restore	%r14
	movq	16(%rsp),%r13
.cfi_restore	%r13
	movq	24(%rsp),%r12
.cfi_restore	%r12
	movq	32(%rsp),%rbp
.cfi_restore	%rbp
	movq	40(%rsp),%rbx
.cfi_restore	%rbx
	leaq	48(%rsp),%rax
	leaq	48(%rsp),%rsp
.cfi_adjust_cfa_offset	-48
.Lbase2_64_avx2_epilogue:
	jmp	.Ldo_avx2
.cfi_endproc	

.align	32
.Leven_avx2:
.cfi_startproc	
	movl	OPENSSL_ia32cap_P+8(%rip),%r10d
	vmovd	0(%rdi),%xmm0
	vmovd	4(%rdi),%xmm1
	vmovd	8(%rdi),%xmm2
	vmovd	12(%rdi),%xmm3
	vmovd	16(%rdi),%xmm4

.Ldo_avx2:
	cmpq	$512,%rdx
	jb	.Lskip_avx512
	andl	%r11d,%r10d
	testl	$65536,%r10d
	jnz	.Lblocks_avx512
.Lskip_avx512:
	leaq	-8(%rsp),%r11
.cfi_def_cfa	%r11,16
	subq	$0x128,%rsp
	leaq	.Lconst(%rip),%rcx
	leaq	48+64(%rdi),%rdi
	vmovdqa	96(%rcx),%ymm7


	vmovdqu	-64(%rdi),%xmm9
	andq	$-512,%rsp
	vmovdqu	-48(%rdi),%xmm10
	vmovdqu	-32(%rdi),%xmm6
	vmovdqu	-16(%rdi),%xmm11
	vmovdqu	0(%rdi),%xmm12
	vmovdqu	16(%rdi),%xmm13
	leaq	144(%rsp),%rax
	vmovdqu	32(%rdi),%xmm14
	vpermd	%ymm9,%ymm7,%ymm9
	vmovdqu	48(%rdi),%xmm15
	vpermd	%ymm10,%ymm7,%ymm10
	vmovdqu	64(%rdi),%xmm5
	vpermd	%ymm6,%ymm7,%ymm6
	vmovdqa	%ymm9,0(%rsp)
	vpermd	%ymm11,%ymm7,%ymm11
	vmovdqa	%ymm10,32-144(%rax)
	vpermd	%ymm12,%ymm7,%ymm12
	vmovdqa	%ymm6,64-144(%rax)
	vpermd	%ymm13,%ymm7,%ymm13
	vmovdqa	%ymm11,96-144(%rax)
	vpermd	%ymm14,%ymm7,%ymm14
	vmovdqa	%ymm12,128-144(%rax)
	vpermd	%ymm15,%ymm7,%ymm15
	vmovdqa	%ymm13,160-144(%rax)
	vpermd	%ymm5,%ymm7,%ymm5
	vmovdqa	%ymm14,192-144(%rax)
	vmovdqa	%ymm15,224-144(%rax)
	vmovdqa	%ymm5,256-144(%rax)
	vmovdqa	64(%rcx),%ymm5



	vmovdqu	0(%rsi),%xmm7
	vmovdqu	16(%rsi),%xmm8
	vinserti128	$1,32(%rsi),%ymm7,%ymm7
	vinserti128	$1,48(%rsi),%ymm8,%ymm8
	leaq	64(%rsi),%rsi

	vpsrldq	$6,%ymm7,%ymm9
	vpsrldq	$6,%ymm8,%ymm10
	vpunpckhqdq	%ymm8,%ymm7,%ymm6
	vpunpcklqdq	%ymm10,%ymm9,%ymm9
	vpunpcklqdq	%ymm8,%ymm7,%ymm7

	vpsrlq	$30,%ymm9,%ymm10
	vpsrlq	$4,%ymm9,%ymm9
	vpsrlq	$26,%ymm7,%ymm8
	vpsrlq	$40,%ymm6,%ymm6
	vpand	%ymm5,%ymm9,%ymm9
	vpand	%ymm5,%ymm7,%ymm7
	vpand	%ymm5,%ymm8,%ymm8
	vpand	%ymm5,%ymm10,%ymm10
	vpor	32(%rcx),%ymm6,%ymm6

	vpaddq	%ymm2,%ymm9,%ymm2
	subq	$64,%rdx
	jz	.Ltail_avx2
	jmp	.Loop_avx2

.align	32
.Loop_avx2:








	vpaddq	%ymm0,%ymm7,%ymm0
	vmovdqa	0(%rsp),%ymm7
	vpaddq	%ymm1,%ymm8,%ymm1
	vmovdqa	32(%rsp),%ymm8
	vpaddq	%ymm3,%ymm10,%ymm3
	vmovdqa	96(%rsp),%ymm9
	vpaddq	%ymm4,%ymm6,%ymm4
	vmovdqa	48(%rax),%ymm10
	vmovdqa	112(%rax),%ymm5
















	vpmuludq	%ymm2,%ymm7,%ymm13
	vpmuludq	%ymm2,%ymm8,%ymm14
	vpmuludq	%ymm2,%ymm9,%ymm15
	vpmuludq	%ymm2,%ymm10,%ymm11
	vpmuludq	%ymm2,%ymm5,%ymm12

	vpmuludq	%ymm0,%ymm8,%ymm6
	vpmuludq	%ymm1,%ymm8,%ymm2
	vpaddq	%ymm6,%ymm12,%ymm12
	vpaddq	%ymm2,%ymm13,%ymm13
	vpmuludq	%ymm3,%ymm8,%ymm6
	vpmuludq	64(%rsp),%ymm4,%ymm2
	vpaddq	%ymm6,%ymm15,%ymm15
	vpaddq	%ymm2,%ymm11,%ymm11
	vmovdqa	-16(%rax),%ymm8

	vpmuludq	%ymm0,%ymm7,%ymm6
	vpmuludq	%ymm1,%ymm7,%ymm2
	vpaddq	%ymm6,%ymm11,%ymm11
	vpaddq	%ymm2,%ymm12,%ymm12
	vpmuludq	%ymm3,%ymm7,%ymm6
	vpmuludq	%ymm4,%ymm7,%ymm2
	vmovdqu	0(%rsi),%xmm7
	vpaddq	%ymm6,%ymm14,%ymm14
	vpaddq	%ymm2,%ymm15,%ymm15
	vinserti128	$1,32(%rsi),%ymm7,%ymm7

	vpmuludq	%ymm3,%ymm8,%ymm6
	vpmuludq	%ymm4,%ymm8,%ymm2
	vmovdqu	16(%rsi),%xmm8
	vpaddq	%ymm6,%ymm11,%ymm11
	vpaddq	%ymm2,%ymm12,%ymm12
	vmovdqa	16(%rax),%ymm2
	vpmuludq	%ymm1,%ymm9,%ymm6
	vpmuludq	%ymm0,%ymm9,%ymm9
	vpaddq	%ymm6,%ymm14,%ymm14
	vpaddq	%ymm9,%ymm13,%ymm13
	vinserti128	$1,48(%rsi),%ymm8,%ymm8
	leaq	64(%rsi),%rsi

	vpmuludq	%ymm1,%ymm2,%ymm6
	vpmuludq	%ymm0,%ymm2,%ymm2
	vpsrldq	$6,%ymm7,%ymm9
	vpaddq	%ymm6,%ymm15,%ymm15
	vpaddq	%ymm2,%ymm14,%ymm14
	vpmuludq	%ymm3,%ymm10,%ymm6
	vpmuludq	%ymm4,%ymm10,%ymm2
	vpsrldq	$6,%ymm8,%ymm10
	vpaddq	%ymm6,%ymm12,%ymm12
	vpaddq	%ymm2,%ymm13,%ymm13
	vpunpckhqdq	%ymm8,%ymm7,%ymm6

	vpmuludq	%ymm3,%ymm5,%ymm3
	vpmuludq	%ymm4,%ymm5,%ymm4
	vpunpcklqdq	%ymm8,%ymm7,%ymm7
	vpaddq	%ymm3,%ymm13,%ymm2
	vpaddq	%ymm4,%ymm14,%ymm3
	vpunpcklqdq	%ymm10,%ymm9,%ymm10
	vpmuludq	80(%rax),%ymm0,%ymm4
	vpmuludq	%ymm1,%ymm5,%ymm0
	vmovdqa	64(%rcx),%ymm5
	vpaddq	%ymm4,%ymm15,%ymm4
	vpaddq	%ymm0,%ymm11,%ymm0




	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpaddq	%ymm14,%ymm4,%ymm4

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpaddq	%ymm11,%ymm12,%ymm1

	vpsrlq	$26,%ymm4,%ymm15
	vpand	%ymm5,%ymm4,%ymm4

	vpsrlq	$4,%ymm10,%ymm9

	vpsrlq	$26,%ymm1,%ymm12
	vpand	%ymm5,%ymm1,%ymm1
	vpaddq	%ymm12,%ymm2,%ymm2

	vpaddq	%ymm15,%ymm0,%ymm0
	vpsllq	$2,%ymm15,%ymm15
	vpaddq	%ymm15,%ymm0,%ymm0

	vpand	%ymm5,%ymm9,%ymm9
	vpsrlq	$26,%ymm7,%ymm8

	vpsrlq	$26,%ymm2,%ymm13
	vpand	%ymm5,%ymm2,%ymm2
	vpaddq	%ymm13,%ymm3,%ymm3

	vpaddq	%ymm9,%ymm2,%ymm2
	vpsrlq	$30,%ymm10,%ymm10

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpaddq	%ymm11,%ymm1,%ymm1

	vpsrlq	$40,%ymm6,%ymm6

	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpaddq	%ymm14,%ymm4,%ymm4

	vpand	%ymm5,%ymm7,%ymm7
	vpand	%ymm5,%ymm8,%ymm8
	vpand	%ymm5,%ymm10,%ymm10
	vpor	32(%rcx),%ymm6,%ymm6

	subq	$64,%rdx
	jnz	.Loop_avx2

.byte	0x66,0x90
.Ltail_avx2:







	vpaddq	%ymm0,%ymm7,%ymm0
	vmovdqu	4(%rsp),%ymm7
	vpaddq	%ymm1,%ymm8,%ymm1
	vmovdqu	36(%rsp),%ymm8
	vpaddq	%ymm3,%ymm10,%ymm3
	vmovdqu	100(%rsp),%ymm9
	vpaddq	%ymm4,%ymm6,%ymm4
	vmovdqu	52(%rax),%ymm10
	vmovdqu	116(%rax),%ymm5

	vpmuludq	%ymm2,%ymm7,%ymm13
	vpmuludq	%ymm2,%ymm8,%ymm14
	vpmuludq	%ymm2,%ymm9,%ymm15
	vpmuludq	%ymm2,%ymm10,%ymm11
	vpmuludq	%ymm2,%ymm5,%ymm12

	vpmuludq	%ymm0,%ymm8,%ymm6
	vpmuludq	%ymm1,%ymm8,%ymm2
	vpaddq	%ymm6,%ymm12,%ymm12
	vpaddq	%ymm2,%ymm13,%ymm13
	vpmuludq	%ymm3,%ymm8,%ymm6
	vpmuludq	68(%rsp),%ymm4,%ymm2
	vpaddq	%ymm6,%ymm15,%ymm15
	vpaddq	%ymm2,%ymm11,%ymm11

	vpmuludq	%ymm0,%ymm7,%ymm6
	vpmuludq	%ymm1,%ymm7,%ymm2
	vpaddq	%ymm6,%ymm11,%ymm11
	vmovdqu	-12(%rax),%ymm8
	vpaddq	%ymm2,%ymm12,%ymm12
	vpmuludq	%ymm3,%ymm7,%ymm6
	vpmuludq	%ymm4,%ymm7,%ymm2
	vpaddq	%ymm6,%ymm14,%ymm14
	vpaddq	%ymm2,%ymm15,%ymm15

	vpmuludq	%ymm3,%ymm8,%ymm6
	vpmuludq	%ymm4,%ymm8,%ymm2
	vpaddq	%ymm6,%ymm11,%ymm11
	vpaddq	%ymm2,%ymm12,%ymm12
	vmovdqu	20(%rax),%ymm2
	vpmuludq	%ymm1,%ymm9,%ymm6
	vpmuludq	%ymm0,%ymm9,%ymm9
	vpaddq	%ymm6,%ymm14,%ymm14
	vpaddq	%ymm9,%ymm13,%ymm13

	vpmuludq	%ymm1,%ymm2,%ymm6
	vpmuludq	%ymm0,%ymm2,%ymm2
	vpaddq	%ymm6,%ymm15,%ymm15
	vpaddq	%ymm2,%ymm14,%ymm14
	vpmuludq	%ymm3,%ymm10,%ymm6
	vpmuludq	%ymm4,%ymm10,%ymm2
	vpaddq	%ymm6,%ymm12,%ymm12
	vpaddq	%ymm2,%ymm13,%ymm13

	vpmuludq	%ymm3,%ymm5,%ymm3
	vpmuludq	%ymm4,%ymm5,%ymm4
	vpaddq	%ymm3,%ymm13,%ymm2
	vpaddq	%ymm4,%ymm14,%ymm3
	vpmuludq	84(%rax),%ymm0,%ymm4
	vpmuludq	%ymm1,%ymm5,%ymm0
	vmovdqa	64(%rcx),%ymm5
	vpaddq	%ymm4,%ymm15,%ymm4
	vpaddq	%ymm0,%ymm11,%ymm0




	vpsrldq	$8,%ymm12,%ymm8
	vpsrldq	$8,%ymm2,%ymm9
	vpsrldq	$8,%ymm3,%ymm10
	vpsrldq	$8,%ymm4,%ymm6
	vpsrldq	$8,%ymm0,%ymm7
	vpaddq	%ymm8,%ymm12,%ymm12
	vpaddq	%ymm9,%ymm2,%ymm2
	vpaddq	%ymm10,%ymm3,%ymm3
	vpaddq	%ymm6,%ymm4,%ymm4
	vpaddq	%ymm7,%ymm0,%ymm0

	vpermq	$0x2,%ymm3,%ymm10
	vpermq	$0x2,%ymm4,%ymm6
	vpermq	$0x2,%ymm0,%ymm7
	vpermq	$0x2,%ymm12,%ymm8
	vpermq	$0x2,%ymm2,%ymm9
	vpaddq	%ymm10,%ymm3,%ymm3
	vpaddq	%ymm6,%ymm4,%ymm4
	vpaddq	%ymm7,%ymm0,%ymm0
	vpaddq	%ymm8,%ymm12,%ymm12
	vpaddq	%ymm9,%ymm2,%ymm2




	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpaddq	%ymm14,%ymm4,%ymm4

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpaddq	%ymm11,%ymm12,%ymm1

	vpsrlq	$26,%ymm4,%ymm15
	vpand	%ymm5,%ymm4,%ymm4

	vpsrlq	$26,%ymm1,%ymm12
	vpand	%ymm5,%ymm1,%ymm1
	vpaddq	%ymm12,%ymm2,%ymm2

	vpaddq	%ymm15,%ymm0,%ymm0
	vpsllq	$2,%ymm15,%ymm15
	vpaddq	%ymm15,%ymm0,%ymm0

	vpsrlq	$26,%ymm2,%ymm13
	vpand	%ymm5,%ymm2,%ymm2
	vpaddq	%ymm13,%ymm3,%ymm3

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpaddq	%ymm11,%ymm1,%ymm1

	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpaddq	%ymm14,%ymm4,%ymm4

	vmovd	%xmm0,-112(%rdi)
	vmovd	%xmm1,-108(%rdi)
	vmovd	%xmm2,-104(%rdi)
	vmovd	%xmm3,-100(%rdi)
	vmovd	%xmm4,-96(%rdi)
	leaq	8(%r11),%rsp
.cfi_def_cfa	%rsp,8
	vzeroupper
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_avx2,.-poly1305_blocks_avx2
.type	poly1305_blocks_avx512,@function
.align	32
poly1305_blocks_avx512:
.cfi_startproc	
.Lblocks_avx512:
	movl	$15,%eax
	kmovw	%eax,%k2
	leaq	-8(%rsp),%r11
.cfi_def_cfa	%r11,16
	subq	$0x128,%rsp
	leaq	.Lconst(%rip),%rcx
	leaq	48+64(%rdi),%rdi
	vmovdqa	96(%rcx),%ymm9


	vmovdqu	-64(%rdi),%xmm11
	andq	$-512,%rsp
	vmovdqu	-48(%rdi),%xmm12
	movq	$0x20,%rax
	vmovdqu	-32(%rdi),%xmm7
	vmovdqu	-16(%rdi),%xmm13
	vmovdqu	0(%rdi),%xmm8
	vmovdqu	16(%rdi),%xmm14
	vmovdqu	32(%rdi),%xmm10
	vmovdqu	48(%rdi),%xmm15
	vmovdqu	64(%rdi),%xmm6
	vpermd	%zmm11,%zmm9,%zmm16
	vpbroadcastq	64(%rcx),%zmm5
	vpermd	%zmm12,%zmm9,%zmm17
	vpermd	%zmm7,%zmm9,%zmm21
	vpermd	%zmm13,%zmm9,%zmm18
	vmovdqa64	%zmm16,0(%rsp){%k2}
	vpsrlq	$32,%zmm16,%zmm7
	vpermd	%zmm8,%zmm9,%zmm22
	vmovdqu64	%zmm17,0(%rsp,%rax,1){%k2}
	vpsrlq	$32,%zmm17,%zmm8
	vpermd	%zmm14,%zmm9,%zmm19
	vmovdqa64	%zmm21,64(%rsp){%k2}
	vpermd	%zmm10,%zmm9,%zmm23
	vpermd	%zmm15,%zmm9,%zmm20
	vmovdqu64	%zmm18,64(%rsp,%rax,1){%k2}
	vpermd	%zmm6,%zmm9,%zmm24
	vmovdqa64	%zmm22,128(%rsp){%k2}
	vmovdqu64	%zmm19,128(%rsp,%rax,1){%k2}
	vmovdqa64	%zmm23,192(%rsp){%k2}
	vmovdqu64	%zmm20,192(%rsp,%rax,1){%k2}
	vmovdqa64	%zmm24,256(%rsp){%k2}










	vpmuludq	%zmm7,%zmm16,%zmm11
	vpmuludq	%zmm7,%zmm17,%zmm12
	vpmuludq	%zmm7,%zmm18,%zmm13
	vpmuludq	%zmm7,%zmm19,%zmm14
	vpmuludq	%zmm7,%zmm20,%zmm15
	vpsrlq	$32,%zmm18,%zmm9

	vpmuludq	%zmm8,%zmm24,%zmm25
	vpmuludq	%zmm8,%zmm16,%zmm26
	vpmuludq	%zmm8,%zmm17,%zmm27
	vpmuludq	%zmm8,%zmm18,%zmm28
	vpmuludq	%zmm8,%zmm19,%zmm29
	vpsrlq	$32,%zmm19,%zmm10
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15

	vpmuludq	%zmm9,%zmm23,%zmm25
	vpmuludq	%zmm9,%zmm24,%zmm26
	vpmuludq	%zmm9,%zmm17,%zmm28
	vpmuludq	%zmm9,%zmm18,%zmm29
	vpmuludq	%zmm9,%zmm16,%zmm27
	vpsrlq	$32,%zmm20,%zmm6
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm27,%zmm13,%zmm13

	vpmuludq	%zmm10,%zmm22,%zmm25
	vpmuludq	%zmm10,%zmm16,%zmm28
	vpmuludq	%zmm10,%zmm17,%zmm29
	vpmuludq	%zmm10,%zmm23,%zmm26
	vpmuludq	%zmm10,%zmm24,%zmm27
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13

	vpmuludq	%zmm6,%zmm24,%zmm28
	vpmuludq	%zmm6,%zmm16,%zmm29
	vpmuludq	%zmm6,%zmm21,%zmm25
	vpmuludq	%zmm6,%zmm22,%zmm26
	vpmuludq	%zmm6,%zmm23,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13



	vmovdqu64	0(%rsi),%zmm10
	vmovdqu64	64(%rsi),%zmm6
	leaq	128(%rsi),%rsi




	vpsrlq	$26,%zmm14,%zmm28
	vpandq	%zmm5,%zmm14,%zmm14
	vpaddq	%zmm28,%zmm15,%zmm15

	vpsrlq	$26,%zmm11,%zmm25
	vpandq	%zmm5,%zmm11,%zmm11
	vpaddq	%zmm25,%zmm12,%zmm12

	vpsrlq	$26,%zmm15,%zmm29
	vpandq	%zmm5,%zmm15,%zmm15

	vpsrlq	$26,%zmm12,%zmm26
	vpandq	%zmm5,%zmm12,%zmm12
	vpaddq	%zmm26,%zmm13,%zmm13

	vpaddq	%zmm29,%zmm11,%zmm11
	vpsllq	$2,%zmm29,%zmm29
	vpaddq	%zmm29,%zmm11,%zmm11

	vpsrlq	$26,%zmm13,%zmm27
	vpandq	%zmm5,%zmm13,%zmm13
	vpaddq	%zmm27,%zmm14,%zmm14

	vpsrlq	$26,%zmm11,%zmm25
	vpandq	%zmm5,%zmm11,%zmm11
	vpaddq	%zmm25,%zmm12,%zmm12

	vpsrlq	$26,%zmm14,%zmm28
	vpandq	%zmm5,%zmm14,%zmm14
	vpaddq	%zmm28,%zmm15,%zmm15





	vpunpcklqdq	%zmm6,%zmm10,%zmm7
	vpunpckhqdq	%zmm6,%zmm10,%zmm6






	vmovdqa32	128(%rcx),%zmm25
	movl	$0x7777,%eax
	kmovw	%eax,%k1

	vpermd	%zmm16,%zmm25,%zmm16
	vpermd	%zmm17,%zmm25,%zmm17
	vpermd	%zmm18,%zmm25,%zmm18
	vpermd	%zmm19,%zmm25,%zmm19
	vpermd	%zmm20,%zmm25,%zmm20

	vpermd	%zmm11,%zmm25,%zmm16{%k1}
	vpermd	%zmm12,%zmm25,%zmm17{%k1}
	vpermd	%zmm13,%zmm25,%zmm18{%k1}
	vpermd	%zmm14,%zmm25,%zmm19{%k1}
	vpermd	%zmm15,%zmm25,%zmm20{%k1}

	vpslld	$2,%zmm17,%zmm21
	vpslld	$2,%zmm18,%zmm22
	vpslld	$2,%zmm19,%zmm23
	vpslld	$2,%zmm20,%zmm24
	vpaddd	%zmm17,%zmm21,%zmm21
	vpaddd	%zmm18,%zmm22,%zmm22
	vpaddd	%zmm19,%zmm23,%zmm23
	vpaddd	%zmm20,%zmm24,%zmm24

	vpbroadcastq	32(%rcx),%zmm30

	vpsrlq	$52,%zmm7,%zmm9
	vpsllq	$12,%zmm6,%zmm10
	vporq	%zmm10,%zmm9,%zmm9
	vpsrlq	$26,%zmm7,%zmm8
	vpsrlq	$14,%zmm6,%zmm10
	vpsrlq	$40,%zmm6,%zmm6
	vpandq	%zmm5,%zmm9,%zmm9
	vpandq	%zmm5,%zmm7,%zmm7




	vpaddq	%zmm2,%zmm9,%zmm2
	subq	$192,%rdx
	jbe	.Ltail_avx512
	jmp	.Loop_avx512

.align	32
.Loop_avx512:




























	vpmuludq	%zmm2,%zmm17,%zmm14
	vpaddq	%zmm0,%zmm7,%zmm0
	vpmuludq	%zmm2,%zmm18,%zmm15
	vpandq	%zmm5,%zmm8,%zmm8
	vpmuludq	%zmm2,%zmm23,%zmm11
	vpandq	%zmm5,%zmm10,%zmm10
	vpmuludq	%zmm2,%zmm24,%zmm12
	vporq	%zmm30,%zmm6,%zmm6
	vpmuludq	%zmm2,%zmm16,%zmm13
	vpaddq	%zmm1,%zmm8,%zmm1
	vpaddq	%zmm3,%zmm10,%zmm3
	vpaddq	%zmm4,%zmm6,%zmm4

	vmovdqu64	0(%rsi),%zmm10
	vmovdqu64	64(%rsi),%zmm6
	leaq	128(%rsi),%rsi
	vpmuludq	%zmm0,%zmm19,%zmm28
	vpmuludq	%zmm0,%zmm20,%zmm29
	vpmuludq	%zmm0,%zmm16,%zmm25
	vpmuludq	%zmm0,%zmm17,%zmm26
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12

	vpmuludq	%zmm1,%zmm18,%zmm28
	vpmuludq	%zmm1,%zmm19,%zmm29
	vpmuludq	%zmm1,%zmm24,%zmm25
	vpmuludq	%zmm0,%zmm18,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm27,%zmm13,%zmm13

	vpunpcklqdq	%zmm6,%zmm10,%zmm7
	vpunpckhqdq	%zmm6,%zmm10,%zmm6

	vpmuludq	%zmm3,%zmm16,%zmm28
	vpmuludq	%zmm3,%zmm17,%zmm29
	vpmuludq	%zmm1,%zmm16,%zmm26
	vpmuludq	%zmm1,%zmm17,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13

	vpmuludq	%zmm4,%zmm24,%zmm28
	vpmuludq	%zmm4,%zmm16,%zmm29
	vpmuludq	%zmm3,%zmm22,%zmm25
	vpmuludq	%zmm3,%zmm23,%zmm26
	vpaddq	%zmm28,%zmm14,%zmm14
	vpmuludq	%zmm3,%zmm24,%zmm27
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13

	vpmuludq	%zmm4,%zmm21,%zmm25
	vpmuludq	%zmm4,%zmm22,%zmm26
	vpmuludq	%zmm4,%zmm23,%zmm27
	vpaddq	%zmm25,%zmm11,%zmm0
	vpaddq	%zmm26,%zmm12,%zmm1
	vpaddq	%zmm27,%zmm13,%zmm2




	vpsrlq	$52,%zmm7,%zmm9
	vpsllq	$12,%zmm6,%zmm10

	vpsrlq	$26,%zmm14,%zmm3
	vpandq	%zmm5,%zmm14,%zmm14
	vpaddq	%zmm3,%zmm15,%zmm4

	vporq	%zmm10,%zmm9,%zmm9

	vpsrlq	$26,%zmm0,%zmm11
	vpandq	%zmm5,%zmm0,%zmm0
	vpaddq	%zmm11,%zmm1,%zmm1

	vpandq	%zmm5,%zmm9,%zmm9

	vpsrlq	$26,%zmm4,%zmm15
	vpandq	%zmm5,%zmm4,%zmm4

	vpsrlq	$26,%zmm1,%zmm12
	vpandq	%zmm5,%zmm1,%zmm1
	vpaddq	%zmm12,%zmm2,%zmm2

	vpaddq	%zmm15,%zmm0,%zmm0
	vpsllq	$2,%zmm15,%zmm15
	vpaddq	%zmm15,%zmm0,%zmm0

	vpaddq	%zmm9,%zmm2,%zmm2
	vpsrlq	$26,%zmm7,%zmm8

	vpsrlq	$26,%zmm2,%zmm13
	vpandq	%zmm5,%zmm2,%zmm2
	vpaddq	%zmm13,%zmm14,%zmm3

	vpsrlq	$14,%zmm6,%zmm10

	vpsrlq	$26,%zmm0,%zmm11
	vpandq	%zmm5,%zmm0,%zmm0
	vpaddq	%zmm11,%zmm1,%zmm1

	vpsrlq	$40,%zmm6,%zmm6

	vpsrlq	$26,%zmm3,%zmm14
	vpandq	%zmm5,%zmm3,%zmm3
	vpaddq	%zmm14,%zmm4,%zmm4

	vpandq	%zmm5,%zmm7,%zmm7




	subq	$128,%rdx
	ja	.Loop_avx512

.Ltail_avx512:





	vpsrlq	$32,%zmm16,%zmm16
	vpsrlq	$32,%zmm17,%zmm17
	vpsrlq	$32,%zmm18,%zmm18
	vpsrlq	$32,%zmm23,%zmm23
	vpsrlq	$32,%zmm24,%zmm24
	vpsrlq	$32,%zmm19,%zmm19
	vpsrlq	$32,%zmm20,%zmm20
	vpsrlq	$32,%zmm21,%zmm21
	vpsrlq	$32,%zmm22,%zmm22



	leaq	(%rsi,%rdx,1),%rsi


	vpaddq	%zmm0,%zmm7,%zmm0

	vpmuludq	%zmm2,%zmm17,%zmm14
	vpmuludq	%zmm2,%zmm18,%zmm15
	vpmuludq	%zmm2,%zmm23,%zmm11
	vpandq	%zmm5,%zmm8,%zmm8
	vpmuludq	%zmm2,%zmm24,%zmm12
	vpandq	%zmm5,%zmm10,%zmm10
	vpmuludq	%zmm2,%zmm16,%zmm13
	vporq	%zmm30,%zmm6,%zmm6
	vpaddq	%zmm1,%zmm8,%zmm1
	vpaddq	%zmm3,%zmm10,%zmm3
	vpaddq	%zmm4,%zmm6,%zmm4

	vmovdqu	0(%rsi),%xmm7
	vpmuludq	%zmm0,%zmm19,%zmm28
	vpmuludq	%zmm0,%zmm20,%zmm29
	vpmuludq	%zmm0,%zmm16,%zmm25
	vpmuludq	%zmm0,%zmm17,%zmm26
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12

	vmovdqu	16(%rsi),%xmm8
	vpmuludq	%zmm1,%zmm18,%zmm28
	vpmuludq	%zmm1,%zmm19,%zmm29
	vpmuludq	%zmm1,%zmm24,%zmm25
	vpmuludq	%zmm0,%zmm18,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm27,%zmm13,%zmm13

	vinserti128	$1,32(%rsi),%ymm7,%ymm7
	vpmuludq	%zmm3,%zmm16,%zmm28
	vpmuludq	%zmm3,%zmm17,%zmm29
	vpmuludq	%zmm1,%zmm16,%zmm26
	vpmuludq	%zmm1,%zmm17,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm14
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13

	vinserti128	$1,48(%rsi),%ymm8,%ymm8
	vpmuludq	%zmm4,%zmm24,%zmm28
	vpmuludq	%zmm4,%zmm16,%zmm29
	vpmuludq	%zmm3,%zmm22,%zmm25
	vpmuludq	%zmm3,%zmm23,%zmm26
	vpmuludq	%zmm3,%zmm24,%zmm27
	vpaddq	%zmm28,%zmm14,%zmm3
	vpaddq	%zmm29,%zmm15,%zmm15
	vpaddq	%zmm25,%zmm11,%zmm11
	vpaddq	%zmm26,%zmm12,%zmm12
	vpaddq	%zmm27,%zmm13,%zmm13

	vpmuludq	%zmm4,%zmm21,%zmm25
	vpmuludq	%zmm4,%zmm22,%zmm26
	vpmuludq	%zmm4,%zmm23,%zmm27
	vpaddq	%zmm25,%zmm11,%zmm0
	vpaddq	%zmm26,%zmm12,%zmm1
	vpaddq	%zmm27,%zmm13,%zmm2




	movl	$1,%eax
	vpermq	$0xb1,%zmm3,%zmm14
	vpermq	$0xb1,%zmm15,%zmm4
	vpermq	$0xb1,%zmm0,%zmm11
	vpermq	$0xb1,%zmm1,%zmm12
	vpermq	$0xb1,%zmm2,%zmm13
	vpaddq	%zmm14,%zmm3,%zmm3
	vpaddq	%zmm15,%zmm4,%zmm4
	vpaddq	%zmm11,%zmm0,%zmm0
	vpaddq	%zmm12,%zmm1,%zmm1
	vpaddq	%zmm13,%zmm2,%zmm2

	kmovw	%eax,%k3
	vpermq	$0x2,%zmm3,%zmm14
	vpermq	$0x2,%zmm4,%zmm15
	vpermq	$0x2,%zmm0,%zmm11
	vpermq	$0x2,%zmm1,%zmm12
	vpermq	$0x2,%zmm2,%zmm13
	vpaddq	%zmm14,%zmm3,%zmm3
	vpaddq	%zmm15,%zmm4,%zmm4
	vpaddq	%zmm11,%zmm0,%zmm0
	vpaddq	%zmm12,%zmm1,%zmm1
	vpaddq	%zmm13,%zmm2,%zmm2

	vextracti64x4	$0x1,%zmm3,%ymm14
	vextracti64x4	$0x1,%zmm4,%ymm15
	vextracti64x4	$0x1,%zmm0,%ymm11
	vextracti64x4	$0x1,%zmm1,%ymm12
	vextracti64x4	$0x1,%zmm2,%ymm13
	vpaddq	%zmm14,%zmm3,%zmm3{%k3}{z}
	vpaddq	%zmm15,%zmm4,%zmm4{%k3}{z}
	vpaddq	%zmm11,%zmm0,%zmm0{%k3}{z}
	vpaddq	%zmm12,%zmm1,%zmm1{%k3}{z}
	vpaddq	%zmm13,%zmm2,%zmm2{%k3}{z}



	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpsrldq	$6,%ymm7,%ymm9
	vpsrldq	$6,%ymm8,%ymm10
	vpunpckhqdq	%ymm8,%ymm7,%ymm6
	vpaddq	%ymm14,%ymm4,%ymm4

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpunpcklqdq	%ymm10,%ymm9,%ymm9
	vpunpcklqdq	%ymm8,%ymm7,%ymm7
	vpaddq	%ymm11,%ymm1,%ymm1

	vpsrlq	$26,%ymm4,%ymm15
	vpand	%ymm5,%ymm4,%ymm4

	vpsrlq	$26,%ymm1,%ymm12
	vpand	%ymm5,%ymm1,%ymm1
	vpsrlq	$30,%ymm9,%ymm10
	vpsrlq	$4,%ymm9,%ymm9
	vpaddq	%ymm12,%ymm2,%ymm2

	vpaddq	%ymm15,%ymm0,%ymm0
	vpsllq	$2,%ymm15,%ymm15
	vpsrlq	$26,%ymm7,%ymm8
	vpsrlq	$40,%ymm6,%ymm6
	vpaddq	%ymm15,%ymm0,%ymm0

	vpsrlq	$26,%ymm2,%ymm13
	vpand	%ymm5,%ymm2,%ymm2
	vpand	%ymm5,%ymm9,%ymm9
	vpand	%ymm5,%ymm7,%ymm7
	vpaddq	%ymm13,%ymm3,%ymm3

	vpsrlq	$26,%ymm0,%ymm11
	vpand	%ymm5,%ymm0,%ymm0
	vpaddq	%ymm2,%ymm9,%ymm2
	vpand	%ymm5,%ymm8,%ymm8
	vpaddq	%ymm11,%ymm1,%ymm1

	vpsrlq	$26,%ymm3,%ymm14
	vpand	%ymm5,%ymm3,%ymm3
	vpand	%ymm5,%ymm10,%ymm10
	vpor	32(%rcx),%ymm6,%ymm6
	vpaddq	%ymm14,%ymm4,%ymm4

	leaq	144(%rsp),%rax
	addq	$64,%rdx
	jnz	.Ltail_avx2

	vpsubq	%ymm9,%ymm2,%ymm2
	vmovd	%xmm0,-112(%rdi)
	vmovd	%xmm1,-108(%rdi)
	vmovd	%xmm2,-104(%rdi)
	vmovd	%xmm3,-100(%rdi)
	vmovd	%xmm4,-96(%rdi)
	vzeroall
	leaq	8(%r11),%rsp
.cfi_def_cfa	%rsp,8
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_avx512,.-poly1305_blocks_avx512
.type	poly1305_init_base2_44,@function
.align	32
poly1305_init_base2_44:
.cfi_startproc	
	xorq	%rax,%rax
	movq	%rax,0(%rdi)
	movq	%rax,8(%rdi)
	movq	%rax,16(%rdi)

.Linit_base2_44:
	leaq	poly1305_blocks_vpmadd52(%rip),%r10
	leaq	poly1305_emit_base2_44(%rip),%r11

	movq	$0x0ffffffc0fffffff,%rax
	movq	$0x0ffffffc0ffffffc,%rcx
	andq	0(%rsi),%rax
	movq	$0x00000fffffffffff,%r8
	andq	8(%rsi),%rcx
	movq	$0x00000fffffffffff,%r9
	andq	%rax,%r8
	shrdq	$44,%rcx,%rax
	movq	%r8,40(%rdi)
	andq	%r9,%rax
	shrq	$24,%rcx
	movq	%rax,48(%rdi)
	leaq	(%rax,%rax,4),%rax
	movq	%rcx,56(%rdi)
	shlq	$2,%rax
	leaq	(%rcx,%rcx,4),%rcx
	shlq	$2,%rcx
	movq	%rax,24(%rdi)
	movq	%rcx,32(%rdi)
	movq	$-1,64(%rdi)
	movq	%r10,0(%rdx)
	movq	%r11,8(%rdx)
	movl	$1,%eax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_init_base2_44,.-poly1305_init_base2_44
.type	poly1305_blocks_vpmadd52,@function
.align	32
poly1305_blocks_vpmadd52:
.cfi_startproc	
.byte	243,15,30,250
	shrq	$4,%rdx
	jz	.Lno_data_vpmadd52

	shlq	$40,%rcx
	movq	64(%rdi),%r8






	movq	$3,%rax
	movq	$1,%r10
	cmpq	$4,%rdx
	cmovaeq	%r10,%rax
	testq	%r8,%r8
	cmovnsq	%r10,%rax

	andq	%rdx,%rax
	jz	.Lblocks_vpmadd52_4x

	subq	%rax,%rdx
	movl	$7,%r10d
	movl	$1,%r11d
	kmovw	%r10d,%k7
	leaq	.L2_44_inp_permd(%rip),%r10
	kmovw	%r11d,%k1

	vmovq	%rcx,%xmm21
	vmovdqa64	0(%r10),%ymm19
	vmovdqa64	32(%r10),%ymm20
	vpermq	$0xcf,%ymm21,%ymm21
	vmovdqa64	64(%r10),%ymm22

	vmovdqu64	0(%rdi),%ymm16{%k7}{z}
	vmovdqu64	40(%rdi),%ymm3{%k7}{z}
	vmovdqu64	32(%rdi),%ymm4{%k7}{z}
	vmovdqu64	24(%rdi),%ymm5{%k7}{z}

	vmovdqa64	96(%r10),%ymm23
	vmovdqa64	128(%r10),%ymm24

	jmp	.Loop_vpmadd52

.align	32
.Loop_vpmadd52:
	vmovdqu32	0(%rsi),%xmm18
	leaq	16(%rsi),%rsi

	vpermd	%ymm18,%ymm19,%ymm18
	vpsrlvq	%ymm20,%ymm18,%ymm18
	vpandq	%ymm22,%ymm18,%ymm18
	vporq	%ymm21,%ymm18,%ymm18

	vpaddq	%ymm18,%ymm16,%ymm16

	vpermq	$0,%ymm16,%ymm0{%k7}{z}
	vpermq	$85,%ymm16,%ymm1{%k7}{z}
	vpermq	$170,%ymm16,%ymm2{%k7}{z}

	vpxord	%ymm16,%ymm16,%ymm16
	vpxord	%ymm17,%ymm17,%ymm17

	vpmadd52luq	%ymm3,%ymm0,%ymm16
	vpmadd52huq	%ymm3,%ymm0,%ymm17

	vpmadd52luq	%ymm4,%ymm1,%ymm16
	vpmadd52huq	%ymm4,%ymm1,%ymm17

	vpmadd52luq	%ymm5,%ymm2,%ymm16
	vpmadd52huq	%ymm5,%ymm2,%ymm17

	vpsrlvq	%ymm23,%ymm16,%ymm18
	vpsllvq	%ymm24,%ymm17,%ymm17
	vpandq	%ymm22,%ymm16,%ymm16

	vpaddq	%ymm18,%ymm17,%ymm17

	vpermq	$147,%ymm17,%ymm17

	vpaddq	%ymm17,%ymm16,%ymm16

	vpsrlvq	%ymm23,%ymm16,%ymm18
	vpandq	%ymm22,%ymm16,%ymm16

	vpermq	$147,%ymm18,%ymm18

	vpaddq	%ymm18,%ymm16,%ymm16

	vpermq	$147,%ymm16,%ymm18{%k1}{z}

	vpaddq	%ymm18,%ymm16,%ymm16
	vpsllq	$2,%ymm18,%ymm18

	vpaddq	%ymm18,%ymm16,%ymm16

	decq	%rax
	jnz	.Loop_vpmadd52

	vmovdqu64	%ymm16,0(%rdi){%k7}

	testq	%rdx,%rdx
	jnz	.Lblocks_vpmadd52_4x

.Lno_data_vpmadd52:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_vpmadd52,.-poly1305_blocks_vpmadd52
.type	poly1305_blocks_vpmadd52_4x,@function
.align	32
poly1305_blocks_vpmadd52_4x:
.cfi_startproc	
	shrq	$4,%rdx
	jz	.Lno_data_vpmadd52_4x

	shlq	$40,%rcx
	movq	64(%rdi),%r8

.Lblocks_vpmadd52_4x:
	vpbroadcastq	%rcx,%ymm31

	vmovdqa64	.Lx_mask44(%rip),%ymm28
	movl	$5,%eax
	vmovdqa64	.Lx_mask42(%rip),%ymm29
	kmovw	%eax,%k1

	testq	%r8,%r8
	js	.Linit_vpmadd52

	vmovq	0(%rdi),%xmm0
	vmovq	8(%rdi),%xmm1
	vmovq	16(%rdi),%xmm2

	testq	$3,%rdx
	jnz	.Lblocks_vpmadd52_2x_do

.Lblocks_vpmadd52_4x_do:
	vpbroadcastq	64(%rdi),%ymm3
	vpbroadcastq	96(%rdi),%ymm4
	vpbroadcastq	128(%rdi),%ymm5
	vpbroadcastq	160(%rdi),%ymm16

.Lblocks_vpmadd52_4x_key_loaded:
	vpsllq	$2,%ymm5,%ymm17
	vpaddq	%ymm5,%ymm17,%ymm17
	vpsllq	$2,%ymm17,%ymm17

	testq	$7,%rdx
	jz	.Lblocks_vpmadd52_8x

	vmovdqu64	0(%rsi),%ymm26
	vmovdqu64	32(%rsi),%ymm27
	leaq	64(%rsi),%rsi

	vpunpcklqdq	%ymm27,%ymm26,%ymm25
	vpunpckhqdq	%ymm27,%ymm26,%ymm27



	vpsrlq	$24,%ymm27,%ymm26
	vporq	%ymm31,%ymm26,%ymm26
	vpaddq	%ymm26,%ymm2,%ymm2
	vpandq	%ymm28,%ymm25,%ymm24
	vpsrlq	$44,%ymm25,%ymm25
	vpsllq	$20,%ymm27,%ymm27
	vporq	%ymm27,%ymm25,%ymm25
	vpandq	%ymm28,%ymm25,%ymm25

	subq	$4,%rdx
	jz	.Ltail_vpmadd52_4x
	jmp	.Loop_vpmadd52_4x
	ud2

.align	32
.Linit_vpmadd52:
	vmovq	24(%rdi),%xmm16
	vmovq	56(%rdi),%xmm2
	vmovq	32(%rdi),%xmm17
	vmovq	40(%rdi),%xmm3
	vmovq	48(%rdi),%xmm4

	vmovdqa	%ymm3,%ymm0
	vmovdqa	%ymm4,%ymm1
	vmovdqa	%ymm2,%ymm5

	movl	$2,%eax

.Lmul_init_vpmadd52:
	vpxorq	%ymm18,%ymm18,%ymm18
	vpmadd52luq	%ymm2,%ymm16,%ymm18
	vpxorq	%ymm19,%ymm19,%ymm19
	vpmadd52huq	%ymm2,%ymm16,%ymm19
	vpxorq	%ymm20,%ymm20,%ymm20
	vpmadd52luq	%ymm2,%ymm17,%ymm20
	vpxorq	%ymm21,%ymm21,%ymm21
	vpmadd52huq	%ymm2,%ymm17,%ymm21
	vpxorq	%ymm22,%ymm22,%ymm22
	vpmadd52luq	%ymm2,%ymm3,%ymm22
	vpxorq	%ymm23,%ymm23,%ymm23
	vpmadd52huq	%ymm2,%ymm3,%ymm23

	vpmadd52luq	%ymm0,%ymm3,%ymm18
	vpmadd52huq	%ymm0,%ymm3,%ymm19
	vpmadd52luq	%ymm0,%ymm4,%ymm20
	vpmadd52huq	%ymm0,%ymm4,%ymm21
	vpmadd52luq	%ymm0,%ymm5,%ymm22
	vpmadd52huq	%ymm0,%ymm5,%ymm23

	vpmadd52luq	%ymm1,%ymm17,%ymm18
	vpmadd52huq	%ymm1,%ymm17,%ymm19
	vpmadd52luq	%ymm1,%ymm3,%ymm20
	vpmadd52huq	%ymm1,%ymm3,%ymm21
	vpmadd52luq	%ymm1,%ymm4,%ymm22
	vpmadd52huq	%ymm1,%ymm4,%ymm23



	vpsrlq	$44,%ymm18,%ymm30
	vpsllq	$8,%ymm19,%ymm19
	vpandq	%ymm28,%ymm18,%ymm0
	vpaddq	%ymm30,%ymm19,%ymm19

	vpaddq	%ymm19,%ymm20,%ymm20

	vpsrlq	$44,%ymm20,%ymm30
	vpsllq	$8,%ymm21,%ymm21
	vpandq	%ymm28,%ymm20,%ymm1
	vpaddq	%ymm30,%ymm21,%ymm21

	vpaddq	%ymm21,%ymm22,%ymm22

	vpsrlq	$42,%ymm22,%ymm30
	vpsllq	$10,%ymm23,%ymm23
	vpandq	%ymm29,%ymm22,%ymm2
	vpaddq	%ymm30,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0
	vpsllq	$2,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0

	vpsrlq	$44,%ymm0,%ymm30
	vpandq	%ymm28,%ymm0,%ymm0

	vpaddq	%ymm30,%ymm1,%ymm1

	decl	%eax
	jz	.Ldone_init_vpmadd52

	vpunpcklqdq	%ymm4,%ymm1,%ymm4
	vpbroadcastq	%xmm1,%xmm1
	vpunpcklqdq	%ymm5,%ymm2,%ymm5
	vpbroadcastq	%xmm2,%xmm2
	vpunpcklqdq	%ymm3,%ymm0,%ymm3
	vpbroadcastq	%xmm0,%xmm0

	vpsllq	$2,%ymm4,%ymm16
	vpsllq	$2,%ymm5,%ymm17
	vpaddq	%ymm4,%ymm16,%ymm16
	vpaddq	%ymm5,%ymm17,%ymm17
	vpsllq	$2,%ymm16,%ymm16
	vpsllq	$2,%ymm17,%ymm17

	jmp	.Lmul_init_vpmadd52
	ud2

.align	32
.Ldone_init_vpmadd52:
	vinserti128	$1,%xmm4,%ymm1,%ymm4
	vinserti128	$1,%xmm5,%ymm2,%ymm5
	vinserti128	$1,%xmm3,%ymm0,%ymm3

	vpermq	$216,%ymm4,%ymm4
	vpermq	$216,%ymm5,%ymm5
	vpermq	$216,%ymm3,%ymm3

	vpsllq	$2,%ymm4,%ymm16
	vpaddq	%ymm4,%ymm16,%ymm16
	vpsllq	$2,%ymm16,%ymm16

	vmovq	0(%rdi),%xmm0
	vmovq	8(%rdi),%xmm1
	vmovq	16(%rdi),%xmm2

	testq	$3,%rdx
	jnz	.Ldone_init_vpmadd52_2x

	vmovdqu64	%ymm3,64(%rdi)
	vpbroadcastq	%xmm3,%ymm3
	vmovdqu64	%ymm4,96(%rdi)
	vpbroadcastq	%xmm4,%ymm4
	vmovdqu64	%ymm5,128(%rdi)
	vpbroadcastq	%xmm5,%ymm5
	vmovdqu64	%ymm16,160(%rdi)
	vpbroadcastq	%xmm16,%ymm16

	jmp	.Lblocks_vpmadd52_4x_key_loaded
	ud2

.align	32
.Ldone_init_vpmadd52_2x:
	vmovdqu64	%ymm3,64(%rdi)
	vpsrldq	$8,%ymm3,%ymm3
	vmovdqu64	%ymm4,96(%rdi)
	vpsrldq	$8,%ymm4,%ymm4
	vmovdqu64	%ymm5,128(%rdi)
	vpsrldq	$8,%ymm5,%ymm5
	vmovdqu64	%ymm16,160(%rdi)
	vpsrldq	$8,%ymm16,%ymm16
	jmp	.Lblocks_vpmadd52_2x_key_loaded
	ud2

.align	32
.Lblocks_vpmadd52_2x_do:
	vmovdqu64	128+8(%rdi),%ymm5{%k1}{z}
	vmovdqu64	160+8(%rdi),%ymm16{%k1}{z}
	vmovdqu64	64+8(%rdi),%ymm3{%k1}{z}
	vmovdqu64	96+8(%rdi),%ymm4{%k1}{z}

.Lblocks_vpmadd52_2x_key_loaded:
	vmovdqu64	0(%rsi),%ymm26
	vpxorq	%ymm27,%ymm27,%ymm27
	leaq	32(%rsi),%rsi

	vpunpcklqdq	%ymm27,%ymm26,%ymm25
	vpunpckhqdq	%ymm27,%ymm26,%ymm27



	vpsrlq	$24,%ymm27,%ymm26
	vporq	%ymm31,%ymm26,%ymm26
	vpaddq	%ymm26,%ymm2,%ymm2
	vpandq	%ymm28,%ymm25,%ymm24
	vpsrlq	$44,%ymm25,%ymm25
	vpsllq	$20,%ymm27,%ymm27
	vporq	%ymm27,%ymm25,%ymm25
	vpandq	%ymm28,%ymm25,%ymm25

	jmp	.Ltail_vpmadd52_2x
	ud2

.align	32
.Loop_vpmadd52_4x:

	vpaddq	%ymm24,%ymm0,%ymm0
	vpaddq	%ymm25,%ymm1,%ymm1

	vpxorq	%ymm18,%ymm18,%ymm18
	vpmadd52luq	%ymm2,%ymm16,%ymm18
	vpxorq	%ymm19,%ymm19,%ymm19
	vpmadd52huq	%ymm2,%ymm16,%ymm19
	vpxorq	%ymm20,%ymm20,%ymm20
	vpmadd52luq	%ymm2,%ymm17,%ymm20
	vpxorq	%ymm21,%ymm21,%ymm21
	vpmadd52huq	%ymm2,%ymm17,%ymm21
	vpxorq	%ymm22,%ymm22,%ymm22
	vpmadd52luq	%ymm2,%ymm3,%ymm22
	vpxorq	%ymm23,%ymm23,%ymm23
	vpmadd52huq	%ymm2,%ymm3,%ymm23

	vmovdqu64	0(%rsi),%ymm26
	vmovdqu64	32(%rsi),%ymm27
	leaq	64(%rsi),%rsi
	vpmadd52luq	%ymm0,%ymm3,%ymm18
	vpmadd52huq	%ymm0,%ymm3,%ymm19
	vpmadd52luq	%ymm0,%ymm4,%ymm20
	vpmadd52huq	%ymm0,%ymm4,%ymm21
	vpmadd52luq	%ymm0,%ymm5,%ymm22
	vpmadd52huq	%ymm0,%ymm5,%ymm23

	vpunpcklqdq	%ymm27,%ymm26,%ymm25
	vpunpckhqdq	%ymm27,%ymm26,%ymm27
	vpmadd52luq	%ymm1,%ymm17,%ymm18
	vpmadd52huq	%ymm1,%ymm17,%ymm19
	vpmadd52luq	%ymm1,%ymm3,%ymm20
	vpmadd52huq	%ymm1,%ymm3,%ymm21
	vpmadd52luq	%ymm1,%ymm4,%ymm22
	vpmadd52huq	%ymm1,%ymm4,%ymm23



	vpsrlq	$44,%ymm18,%ymm30
	vpsllq	$8,%ymm19,%ymm19
	vpandq	%ymm28,%ymm18,%ymm0
	vpaddq	%ymm30,%ymm19,%ymm19

	vpsrlq	$24,%ymm27,%ymm26
	vporq	%ymm31,%ymm26,%ymm26
	vpaddq	%ymm19,%ymm20,%ymm20

	vpsrlq	$44,%ymm20,%ymm30
	vpsllq	$8,%ymm21,%ymm21
	vpandq	%ymm28,%ymm20,%ymm1
	vpaddq	%ymm30,%ymm21,%ymm21

	vpandq	%ymm28,%ymm25,%ymm24
	vpsrlq	$44,%ymm25,%ymm25
	vpsllq	$20,%ymm27,%ymm27
	vpaddq	%ymm21,%ymm22,%ymm22

	vpsrlq	$42,%ymm22,%ymm30
	vpsllq	$10,%ymm23,%ymm23
	vpandq	%ymm29,%ymm22,%ymm2
	vpaddq	%ymm30,%ymm23,%ymm23

	vpaddq	%ymm26,%ymm2,%ymm2
	vpaddq	%ymm23,%ymm0,%ymm0
	vpsllq	$2,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0
	vporq	%ymm27,%ymm25,%ymm25
	vpandq	%ymm28,%ymm25,%ymm25

	vpsrlq	$44,%ymm0,%ymm30
	vpandq	%ymm28,%ymm0,%ymm0

	vpaddq	%ymm30,%ymm1,%ymm1

	subq	$4,%rdx
	jnz	.Loop_vpmadd52_4x

.Ltail_vpmadd52_4x:
	vmovdqu64	128(%rdi),%ymm5
	vmovdqu64	160(%rdi),%ymm16
	vmovdqu64	64(%rdi),%ymm3
	vmovdqu64	96(%rdi),%ymm4

.Ltail_vpmadd52_2x:
	vpsllq	$2,%ymm5,%ymm17
	vpaddq	%ymm5,%ymm17,%ymm17
	vpsllq	$2,%ymm17,%ymm17


	vpaddq	%ymm24,%ymm0,%ymm0
	vpaddq	%ymm25,%ymm1,%ymm1

	vpxorq	%ymm18,%ymm18,%ymm18
	vpmadd52luq	%ymm2,%ymm16,%ymm18
	vpxorq	%ymm19,%ymm19,%ymm19
	vpmadd52huq	%ymm2,%ymm16,%ymm19
	vpxorq	%ymm20,%ymm20,%ymm20
	vpmadd52luq	%ymm2,%ymm17,%ymm20
	vpxorq	%ymm21,%ymm21,%ymm21
	vpmadd52huq	%ymm2,%ymm17,%ymm21
	vpxorq	%ymm22,%ymm22,%ymm22
	vpmadd52luq	%ymm2,%ymm3,%ymm22
	vpxorq	%ymm23,%ymm23,%ymm23
	vpmadd52huq	%ymm2,%ymm3,%ymm23

	vpmadd52luq	%ymm0,%ymm3,%ymm18
	vpmadd52huq	%ymm0,%ymm3,%ymm19
	vpmadd52luq	%ymm0,%ymm4,%ymm20
	vpmadd52huq	%ymm0,%ymm4,%ymm21
	vpmadd52luq	%ymm0,%ymm5,%ymm22
	vpmadd52huq	%ymm0,%ymm5,%ymm23

	vpmadd52luq	%ymm1,%ymm17,%ymm18
	vpmadd52huq	%ymm1,%ymm17,%ymm19
	vpmadd52luq	%ymm1,%ymm3,%ymm20
	vpmadd52huq	%ymm1,%ymm3,%ymm21
	vpmadd52luq	%ymm1,%ymm4,%ymm22
	vpmadd52huq	%ymm1,%ymm4,%ymm23




	movl	$1,%eax
	kmovw	%eax,%k1
	vpsrldq	$8,%ymm18,%ymm24
	vpsrldq	$8,%ymm19,%ymm0
	vpsrldq	$8,%ymm20,%ymm25
	vpsrldq	$8,%ymm21,%ymm1
	vpaddq	%ymm24,%ymm18,%ymm18
	vpaddq	%ymm0,%ymm19,%ymm19
	vpsrldq	$8,%ymm22,%ymm26
	vpsrldq	$8,%ymm23,%ymm2
	vpaddq	%ymm25,%ymm20,%ymm20
	vpaddq	%ymm1,%ymm21,%ymm21
	vpermq	$0x2,%ymm18,%ymm24
	vpermq	$0x2,%ymm19,%ymm0
	vpaddq	%ymm26,%ymm22,%ymm22
	vpaddq	%ymm2,%ymm23,%ymm23

	vpermq	$0x2,%ymm20,%ymm25
	vpermq	$0x2,%ymm21,%ymm1
	vpaddq	%ymm24,%ymm18,%ymm18{%k1}{z}
	vpaddq	%ymm0,%ymm19,%ymm19{%k1}{z}
	vpermq	$0x2,%ymm22,%ymm26
	vpermq	$0x2,%ymm23,%ymm2
	vpaddq	%ymm25,%ymm20,%ymm20{%k1}{z}
	vpaddq	%ymm1,%ymm21,%ymm21{%k1}{z}
	vpaddq	%ymm26,%ymm22,%ymm22{%k1}{z}
	vpaddq	%ymm2,%ymm23,%ymm23{%k1}{z}



	vpsrlq	$44,%ymm18,%ymm30
	vpsllq	$8,%ymm19,%ymm19
	vpandq	%ymm28,%ymm18,%ymm0
	vpaddq	%ymm30,%ymm19,%ymm19

	vpaddq	%ymm19,%ymm20,%ymm20

	vpsrlq	$44,%ymm20,%ymm30
	vpsllq	$8,%ymm21,%ymm21
	vpandq	%ymm28,%ymm20,%ymm1
	vpaddq	%ymm30,%ymm21,%ymm21

	vpaddq	%ymm21,%ymm22,%ymm22

	vpsrlq	$42,%ymm22,%ymm30
	vpsllq	$10,%ymm23,%ymm23
	vpandq	%ymm29,%ymm22,%ymm2
	vpaddq	%ymm30,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0
	vpsllq	$2,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0

	vpsrlq	$44,%ymm0,%ymm30
	vpandq	%ymm28,%ymm0,%ymm0

	vpaddq	%ymm30,%ymm1,%ymm1


	subq	$2,%rdx
	ja	.Lblocks_vpmadd52_4x_do

	vmovq	%xmm0,0(%rdi)
	vmovq	%xmm1,8(%rdi)
	vmovq	%xmm2,16(%rdi)
	vzeroall

.Lno_data_vpmadd52_4x:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_vpmadd52_4x,.-poly1305_blocks_vpmadd52_4x
.type	poly1305_blocks_vpmadd52_8x,@function
.align	32
poly1305_blocks_vpmadd52_8x:
.cfi_startproc	
	shrq	$4,%rdx
	jz	.Lno_data_vpmadd52_8x

	shlq	$40,%rcx
	movq	64(%rdi),%r8

	vmovdqa64	.Lx_mask44(%rip),%ymm28
	vmovdqa64	.Lx_mask42(%rip),%ymm29

	testq	%r8,%r8
	js	.Linit_vpmadd52

	vmovq	0(%rdi),%xmm0
	vmovq	8(%rdi),%xmm1
	vmovq	16(%rdi),%xmm2

.Lblocks_vpmadd52_8x:



	vmovdqu64	128(%rdi),%ymm5
	vmovdqu64	160(%rdi),%ymm16
	vmovdqu64	64(%rdi),%ymm3
	vmovdqu64	96(%rdi),%ymm4

	vpsllq	$2,%ymm5,%ymm17
	vpaddq	%ymm5,%ymm17,%ymm17
	vpsllq	$2,%ymm17,%ymm17

	vpbroadcastq	%xmm5,%ymm8
	vpbroadcastq	%xmm3,%ymm6
	vpbroadcastq	%xmm4,%ymm7

	vpxorq	%ymm18,%ymm18,%ymm18
	vpmadd52luq	%ymm8,%ymm16,%ymm18
	vpxorq	%ymm19,%ymm19,%ymm19
	vpmadd52huq	%ymm8,%ymm16,%ymm19
	vpxorq	%ymm20,%ymm20,%ymm20
	vpmadd52luq	%ymm8,%ymm17,%ymm20
	vpxorq	%ymm21,%ymm21,%ymm21
	vpmadd52huq	%ymm8,%ymm17,%ymm21
	vpxorq	%ymm22,%ymm22,%ymm22
	vpmadd52luq	%ymm8,%ymm3,%ymm22
	vpxorq	%ymm23,%ymm23,%ymm23
	vpmadd52huq	%ymm8,%ymm3,%ymm23

	vpmadd52luq	%ymm6,%ymm3,%ymm18
	vpmadd52huq	%ymm6,%ymm3,%ymm19
	vpmadd52luq	%ymm6,%ymm4,%ymm20
	vpmadd52huq	%ymm6,%ymm4,%ymm21
	vpmadd52luq	%ymm6,%ymm5,%ymm22
	vpmadd52huq	%ymm6,%ymm5,%ymm23

	vpmadd52luq	%ymm7,%ymm17,%ymm18
	vpmadd52huq	%ymm7,%ymm17,%ymm19
	vpmadd52luq	%ymm7,%ymm3,%ymm20
	vpmadd52huq	%ymm7,%ymm3,%ymm21
	vpmadd52luq	%ymm7,%ymm4,%ymm22
	vpmadd52huq	%ymm7,%ymm4,%ymm23



	vpsrlq	$44,%ymm18,%ymm30
	vpsllq	$8,%ymm19,%ymm19
	vpandq	%ymm28,%ymm18,%ymm6
	vpaddq	%ymm30,%ymm19,%ymm19

	vpaddq	%ymm19,%ymm20,%ymm20

	vpsrlq	$44,%ymm20,%ymm30
	vpsllq	$8,%ymm21,%ymm21
	vpandq	%ymm28,%ymm20,%ymm7
	vpaddq	%ymm30,%ymm21,%ymm21

	vpaddq	%ymm21,%ymm22,%ymm22

	vpsrlq	$42,%ymm22,%ymm30
	vpsllq	$10,%ymm23,%ymm23
	vpandq	%ymm29,%ymm22,%ymm8
	vpaddq	%ymm30,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm6,%ymm6
	vpsllq	$2,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm6,%ymm6

	vpsrlq	$44,%ymm6,%ymm30
	vpandq	%ymm28,%ymm6,%ymm6

	vpaddq	%ymm30,%ymm7,%ymm7





	vpunpcklqdq	%ymm5,%ymm8,%ymm26
	vpunpckhqdq	%ymm5,%ymm8,%ymm5
	vpunpcklqdq	%ymm3,%ymm6,%ymm24
	vpunpckhqdq	%ymm3,%ymm6,%ymm3
	vpunpcklqdq	%ymm4,%ymm7,%ymm25
	vpunpckhqdq	%ymm4,%ymm7,%ymm4
	vshufi64x2	$0x44,%zmm5,%zmm26,%zmm8
	vshufi64x2	$0x44,%zmm3,%zmm24,%zmm6
	vshufi64x2	$0x44,%zmm4,%zmm25,%zmm7

	vmovdqu64	0(%rsi),%zmm26
	vmovdqu64	64(%rsi),%zmm27
	leaq	128(%rsi),%rsi

	vpsllq	$2,%zmm8,%zmm10
	vpsllq	$2,%zmm7,%zmm9
	vpaddq	%zmm8,%zmm10,%zmm10
	vpaddq	%zmm7,%zmm9,%zmm9
	vpsllq	$2,%zmm10,%zmm10
	vpsllq	$2,%zmm9,%zmm9

	vpbroadcastq	%rcx,%zmm31
	vpbroadcastq	%xmm28,%zmm28
	vpbroadcastq	%xmm29,%zmm29

	vpbroadcastq	%xmm9,%zmm16
	vpbroadcastq	%xmm10,%zmm17
	vpbroadcastq	%xmm6,%zmm3
	vpbroadcastq	%xmm7,%zmm4
	vpbroadcastq	%xmm8,%zmm5

	vpunpcklqdq	%zmm27,%zmm26,%zmm25
	vpunpckhqdq	%zmm27,%zmm26,%zmm27



	vpsrlq	$24,%zmm27,%zmm26
	vporq	%zmm31,%zmm26,%zmm26
	vpaddq	%zmm26,%zmm2,%zmm2
	vpandq	%zmm28,%zmm25,%zmm24
	vpsrlq	$44,%zmm25,%zmm25
	vpsllq	$20,%zmm27,%zmm27
	vporq	%zmm27,%zmm25,%zmm25
	vpandq	%zmm28,%zmm25,%zmm25

	subq	$8,%rdx
	jz	.Ltail_vpmadd52_8x
	jmp	.Loop_vpmadd52_8x

.align	32
.Loop_vpmadd52_8x:

	vpaddq	%zmm24,%zmm0,%zmm0
	vpaddq	%zmm25,%zmm1,%zmm1

	vpxorq	%zmm18,%zmm18,%zmm18
	vpmadd52luq	%zmm2,%zmm16,%zmm18
	vpxorq	%zmm19,%zmm19,%zmm19
	vpmadd52huq	%zmm2,%zmm16,%zmm19
	vpxorq	%zmm20,%zmm20,%zmm20
	vpmadd52luq	%zmm2,%zmm17,%zmm20
	vpxorq	%zmm21,%zmm21,%zmm21
	vpmadd52huq	%zmm2,%zmm17,%zmm21
	vpxorq	%zmm22,%zmm22,%zmm22
	vpmadd52luq	%zmm2,%zmm3,%zmm22
	vpxorq	%zmm23,%zmm23,%zmm23
	vpmadd52huq	%zmm2,%zmm3,%zmm23

	vmovdqu64	0(%rsi),%zmm26
	vmovdqu64	64(%rsi),%zmm27
	leaq	128(%rsi),%rsi
	vpmadd52luq	%zmm0,%zmm3,%zmm18
	vpmadd52huq	%zmm0,%zmm3,%zmm19
	vpmadd52luq	%zmm0,%zmm4,%zmm20
	vpmadd52huq	%zmm0,%zmm4,%zmm21
	vpmadd52luq	%zmm0,%zmm5,%zmm22
	vpmadd52huq	%zmm0,%zmm5,%zmm23

	vpunpcklqdq	%zmm27,%zmm26,%zmm25
	vpunpckhqdq	%zmm27,%zmm26,%zmm27
	vpmadd52luq	%zmm1,%zmm17,%zmm18
	vpmadd52huq	%zmm1,%zmm17,%zmm19
	vpmadd52luq	%zmm1,%zmm3,%zmm20
	vpmadd52huq	%zmm1,%zmm3,%zmm21
	vpmadd52luq	%zmm1,%zmm4,%zmm22
	vpmadd52huq	%zmm1,%zmm4,%zmm23



	vpsrlq	$44,%zmm18,%zmm30
	vpsllq	$8,%zmm19,%zmm19
	vpandq	%zmm28,%zmm18,%zmm0
	vpaddq	%zmm30,%zmm19,%zmm19

	vpsrlq	$24,%zmm27,%zmm26
	vporq	%zmm31,%zmm26,%zmm26
	vpaddq	%zmm19,%zmm20,%zmm20

	vpsrlq	$44,%zmm20,%zmm30
	vpsllq	$8,%zmm21,%zmm21
	vpandq	%zmm28,%zmm20,%zmm1
	vpaddq	%zmm30,%zmm21,%zmm21

	vpandq	%zmm28,%zmm25,%zmm24
	vpsrlq	$44,%zmm25,%zmm25
	vpsllq	$20,%zmm27,%zmm27
	vpaddq	%zmm21,%zmm22,%zmm22

	vpsrlq	$42,%zmm22,%zmm30
	vpsllq	$10,%zmm23,%zmm23
	vpandq	%zmm29,%zmm22,%zmm2
	vpaddq	%zmm30,%zmm23,%zmm23

	vpaddq	%zmm26,%zmm2,%zmm2
	vpaddq	%zmm23,%zmm0,%zmm0
	vpsllq	$2,%zmm23,%zmm23

	vpaddq	%zmm23,%zmm0,%zmm0
	vporq	%zmm27,%zmm25,%zmm25
	vpandq	%zmm28,%zmm25,%zmm25

	vpsrlq	$44,%zmm0,%zmm30
	vpandq	%zmm28,%zmm0,%zmm0

	vpaddq	%zmm30,%zmm1,%zmm1

	subq	$8,%rdx
	jnz	.Loop_vpmadd52_8x

.Ltail_vpmadd52_8x:

	vpaddq	%zmm24,%zmm0,%zmm0
	vpaddq	%zmm25,%zmm1,%zmm1

	vpxorq	%zmm18,%zmm18,%zmm18
	vpmadd52luq	%zmm2,%zmm9,%zmm18
	vpxorq	%zmm19,%zmm19,%zmm19
	vpmadd52huq	%zmm2,%zmm9,%zmm19
	vpxorq	%zmm20,%zmm20,%zmm20
	vpmadd52luq	%zmm2,%zmm10,%zmm20
	vpxorq	%zmm21,%zmm21,%zmm21
	vpmadd52huq	%zmm2,%zmm10,%zmm21
	vpxorq	%zmm22,%zmm22,%zmm22
	vpmadd52luq	%zmm2,%zmm6,%zmm22
	vpxorq	%zmm23,%zmm23,%zmm23
	vpmadd52huq	%zmm2,%zmm6,%zmm23

	vpmadd52luq	%zmm0,%zmm6,%zmm18
	vpmadd52huq	%zmm0,%zmm6,%zmm19
	vpmadd52luq	%zmm0,%zmm7,%zmm20
	vpmadd52huq	%zmm0,%zmm7,%zmm21
	vpmadd52luq	%zmm0,%zmm8,%zmm22
	vpmadd52huq	%zmm0,%zmm8,%zmm23

	vpmadd52luq	%zmm1,%zmm10,%zmm18
	vpmadd52huq	%zmm1,%zmm10,%zmm19
	vpmadd52luq	%zmm1,%zmm6,%zmm20
	vpmadd52huq	%zmm1,%zmm6,%zmm21
	vpmadd52luq	%zmm1,%zmm7,%zmm22
	vpmadd52huq	%zmm1,%zmm7,%zmm23




	movl	$1,%eax
	kmovw	%eax,%k1
	vpsrldq	$8,%zmm18,%zmm24
	vpsrldq	$8,%zmm19,%zmm0
	vpsrldq	$8,%zmm20,%zmm25
	vpsrldq	$8,%zmm21,%zmm1
	vpaddq	%zmm24,%zmm18,%zmm18
	vpaddq	%zmm0,%zmm19,%zmm19
	vpsrldq	$8,%zmm22,%zmm26
	vpsrldq	$8,%zmm23,%zmm2
	vpaddq	%zmm25,%zmm20,%zmm20
	vpaddq	%zmm1,%zmm21,%zmm21
	vpermq	$0x2,%zmm18,%zmm24
	vpermq	$0x2,%zmm19,%zmm0
	vpaddq	%zmm26,%zmm22,%zmm22
	vpaddq	%zmm2,%zmm23,%zmm23

	vpermq	$0x2,%zmm20,%zmm25
	vpermq	$0x2,%zmm21,%zmm1
	vpaddq	%zmm24,%zmm18,%zmm18
	vpaddq	%zmm0,%zmm19,%zmm19
	vpermq	$0x2,%zmm22,%zmm26
	vpermq	$0x2,%zmm23,%zmm2
	vpaddq	%zmm25,%zmm20,%zmm20
	vpaddq	%zmm1,%zmm21,%zmm21
	vextracti64x4	$1,%zmm18,%ymm24
	vextracti64x4	$1,%zmm19,%ymm0
	vpaddq	%zmm26,%zmm22,%zmm22
	vpaddq	%zmm2,%zmm23,%zmm23

	vextracti64x4	$1,%zmm20,%ymm25
	vextracti64x4	$1,%zmm21,%ymm1
	vextracti64x4	$1,%zmm22,%ymm26
	vextracti64x4	$1,%zmm23,%ymm2
	vpaddq	%ymm24,%ymm18,%ymm18{%k1}{z}
	vpaddq	%ymm0,%ymm19,%ymm19{%k1}{z}
	vpaddq	%ymm25,%ymm20,%ymm20{%k1}{z}
	vpaddq	%ymm1,%ymm21,%ymm21{%k1}{z}
	vpaddq	%ymm26,%ymm22,%ymm22{%k1}{z}
	vpaddq	%ymm2,%ymm23,%ymm23{%k1}{z}



	vpsrlq	$44,%ymm18,%ymm30
	vpsllq	$8,%ymm19,%ymm19
	vpandq	%ymm28,%ymm18,%ymm0
	vpaddq	%ymm30,%ymm19,%ymm19

	vpaddq	%ymm19,%ymm20,%ymm20

	vpsrlq	$44,%ymm20,%ymm30
	vpsllq	$8,%ymm21,%ymm21
	vpandq	%ymm28,%ymm20,%ymm1
	vpaddq	%ymm30,%ymm21,%ymm21

	vpaddq	%ymm21,%ymm22,%ymm22

	vpsrlq	$42,%ymm22,%ymm30
	vpsllq	$10,%ymm23,%ymm23
	vpandq	%ymm29,%ymm22,%ymm2
	vpaddq	%ymm30,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0
	vpsllq	$2,%ymm23,%ymm23

	vpaddq	%ymm23,%ymm0,%ymm0

	vpsrlq	$44,%ymm0,%ymm30
	vpandq	%ymm28,%ymm0,%ymm0

	vpaddq	%ymm30,%ymm1,%ymm1



	vmovq	%xmm0,0(%rdi)
	vmovq	%xmm1,8(%rdi)
	vmovq	%xmm2,16(%rdi)
	vzeroall

.Lno_data_vpmadd52_8x:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_blocks_vpmadd52_8x,.-poly1305_blocks_vpmadd52_8x
.type	poly1305_emit_base2_44,@function
.align	32
poly1305_emit_base2_44:
.cfi_startproc	
.byte	243,15,30,250
	movq	0(%rdi),%r8
	movq	8(%rdi),%r9
	movq	16(%rdi),%r10

	movq	%r9,%rax
	shrq	$20,%r9
	shlq	$44,%rax
	movq	%r10,%rcx
	shrq	$40,%r10
	shlq	$24,%rcx

	addq	%rax,%r8
	adcq	%rcx,%r9
	adcq	$0,%r10

	movq	%r8,%rax
	addq	$5,%r8
	movq	%r9,%rcx
	adcq	$0,%r9
	adcq	$0,%r10
	shrq	$2,%r10
	cmovnzq	%r8,%rax
	cmovnzq	%r9,%rcx

	addq	0(%rdx),%rax
	adcq	8(%rdx),%rcx
	movq	%rax,0(%rsi)
	movq	%rcx,8(%rsi)

	.byte	0xf3,0xc3
.cfi_endproc	
.size	poly1305_emit_base2_44,.-poly1305_emit_base2_44
.align	64
.Lconst:
.Lmask24:
.long	0x0ffffff,0,0x0ffffff,0,0x0ffffff,0,0x0ffffff,0
.L129:
.long	16777216,0,16777216,0,16777216,0,16777216,0
.Lmask26:
.long	0x3ffffff,0,0x3ffffff,0,0x3ffffff,0,0x3ffffff,0
.Lpermd_avx2:
.long	2,2,2,3,2,0,2,1
.Lpermd_avx512:
.long	0,0,0,1, 0,2,0,3, 0,4,0,5, 0,6,0,7

.L2_44_inp_permd:
.long	0,1,1,2,2,3,7,7
.L2_44_inp_shift:
.quad	0,12,24,64
.L2_44_mask:
.quad	0xfffffffffff,0xfffffffffff,0x3ffffffffff,0xffffffffffffffff
.L2_44_shift_rgt:
.quad	44,44,42,64
.L2_44_shift_lft:
.quad	8,8,10,64

.align	64
.Lx_mask44:
.quad	0xfffffffffff,0xfffffffffff,0xfffffffffff,0xfffffffffff
.quad	0xfffffffffff,0xfffffffffff,0xfffffffffff,0xfffffffffff
.Lx_mask42:
.quad	0x3ffffffffff,0x3ffffffffff,0x3ffffffffff,0x3ffffffffff
.quad	0x3ffffffffff,0x3ffffffffff,0x3ffffffffff,0x3ffffffffff
.byte	80,111,108,121,49,51,48,53,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	16
.globl	xor128_encrypt_n_pad
.type	xor128_encrypt_n_pad,@function
.align	16
xor128_encrypt_n_pad:
.cfi_startproc	
	subq	%rdx,%rsi
	subq	%rdx,%rdi
	movq	%rcx,%r10
	shrq	$4,%rcx
	jz	.Ltail_enc
	nop
.Loop_enc_xmm:
	movdqu	(%rsi,%rdx,1),%xmm0
	pxor	(%rdx),%xmm0
	movdqu	%xmm0,(%rdi,%rdx,1)
	movdqa	%xmm0,(%rdx)
	leaq	16(%rdx),%rdx
	decq	%rcx
	jnz	.Loop_enc_xmm

	andq	$15,%r10
	jz	.Ldone_enc

.Ltail_enc:
	movq	$16,%rcx
	subq	%r10,%rcx
	xorl	%eax,%eax
.Loop_enc_byte:
	movb	(%rsi,%rdx,1),%al
	xorb	(%rdx),%al
	movb	%al,(%rdi,%rdx,1)
	movb	%al,(%rdx)
	leaq	1(%rdx),%rdx
	decq	%r10
	jnz	.Loop_enc_byte

	xorl	%eax,%eax
.Loop_enc_pad:
	movb	%al,(%rdx)
	leaq	1(%rdx),%rdx
	decq	%rcx
	jnz	.Loop_enc_pad

.Ldone_enc:
	movq	%rdx,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	xor128_encrypt_n_pad,.-xor128_encrypt_n_pad

.globl	xor128_decrypt_n_pad
.type	xor128_decrypt_n_pad,@function
.align	16
xor128_decrypt_n_pad:
.cfi_startproc	
	subq	%rdx,%rsi
	subq	%rdx,%rdi
	movq	%rcx,%r10
	shrq	$4,%rcx
	jz	.Ltail_dec
	nop
.Loop_dec_xmm:
	movdqu	(%rsi,%rdx,1),%xmm0
	movdqa	(%rdx),%xmm1
	pxor	%xmm0,%xmm1
	movdqu	%xmm1,(%rdi,%rdx,1)
	movdqa	%xmm0,(%rdx)
	leaq	16(%rdx),%rdx
	decq	%rcx
	jnz	.Loop_dec_xmm

	pxor	%xmm1,%xmm1
	andq	$15,%r10
	jz	.Ldone_dec

.Ltail_dec:
	movq	$16,%rcx
	subq	%r10,%rcx
	xorl	%eax,%eax
	xorq	%r11,%r11
.Loop_dec_byte:
	movb	(%rsi,%rdx,1),%r11b
	movb	(%rdx),%al
	xorb	%r11b,%al
	movb	%al,(%rdi,%rdx,1)
	movb	%r11b,(%rdx)
	leaq	1(%rdx),%rdx
	decq	%r10
	jnz	.Loop_dec_byte

	xorl	%eax,%eax
.Loop_dec_pad:
	movb	%al,(%rdx)
	leaq	1(%rdx),%rdx
	decq	%rcx
	jnz	.Loop_dec_pad

.Ldone_dec:
	movq	%rdx,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	xor128_decrypt_n_pad,.-xor128_decrypt_n_pad
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                     node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/rc4/                          0000775 0000000 0000000 00000000000 14746647661 0025644 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/rc4/rc4-md5-x86_64.s          0000664 0000000 0000000 00000056071 14746647661 0030150 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	
.align	16

.globl	rc4_md5_enc
.type	rc4_md5_enc,@function
rc4_md5_enc:
.cfi_startproc	
	cmpq	$0,%r9
	je	.Labort
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
	subq	$40,%rsp
.cfi_adjust_cfa_offset	40
.Lbody:
	movq	%rcx,%r11
	movq	%r9,%r12
	movq	%rsi,%r13
	movq	%rdx,%r14
	movq	%r8,%r15
	xorq	%rbp,%rbp
	xorq	%rcx,%rcx

	leaq	8(%rdi),%rdi
	movb	-8(%rdi),%bpl
	movb	-4(%rdi),%cl

	incb	%bpl
	subq	%r13,%r14
	movl	(%rdi,%rbp,4),%eax
	addb	%al,%cl
	leaq	(%rdi,%rbp,4),%rsi
	shlq	$6,%r12
	addq	%r15,%r12
	movq	%r12,16(%rsp)

	movq	%r11,24(%rsp)
	movl	0(%r11),%r8d
	movl	4(%r11),%r9d
	movl	8(%r11),%r10d
	movl	12(%r11),%r11d
	jmp	.Loop

.align	16
.Loop:
	movl	%r8d,0(%rsp)
	movl	%r9d,4(%rsp)
	movl	%r10d,8(%rsp)
	movl	%r11d,%r12d
	movl	%r11d,12(%rsp)
	pxor	%xmm0,%xmm0
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	0(%r15),%r8d
	addb	%dl,%al
	movl	4(%rsi),%ebx
	addl	$3614090360,%r8d
	xorl	%r11d,%r12d
	movzbl	%al,%eax
	movl	%edx,0(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$7,%r8d
	movl	%r10d,%r12d
	movd	(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	pxor	%xmm1,%xmm1
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	4(%r15),%r11d
	addb	%dl,%bl
	movl	8(%rsi),%eax
	addl	$3905402710,%r11d
	xorl	%r10d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,4(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$12,%r11d
	movl	%r9d,%r12d
	movd	(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	8(%r15),%r10d
	addb	%dl,%al
	movl	12(%rsi),%ebx
	addl	$606105819,%r10d
	xorl	%r9d,%r12d
	movzbl	%al,%eax
	movl	%edx,8(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$17,%r10d
	movl	%r8d,%r12d
	pinsrw	$1,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	12(%r15),%r9d
	addb	%dl,%bl
	movl	16(%rsi),%eax
	addl	$3250441966,%r9d
	xorl	%r8d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,12(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$22,%r9d
	movl	%r11d,%r12d
	pinsrw	$1,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	16(%r15),%r8d
	addb	%dl,%al
	movl	20(%rsi),%ebx
	addl	$4118548399,%r8d
	xorl	%r11d,%r12d
	movzbl	%al,%eax
	movl	%edx,16(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$7,%r8d
	movl	%r10d,%r12d
	pinsrw	$2,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	20(%r15),%r11d
	addb	%dl,%bl
	movl	24(%rsi),%eax
	addl	$1200080426,%r11d
	xorl	%r10d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,20(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$12,%r11d
	movl	%r9d,%r12d
	pinsrw	$2,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	24(%r15),%r10d
	addb	%dl,%al
	movl	28(%rsi),%ebx
	addl	$2821735955,%r10d
	xorl	%r9d,%r12d
	movzbl	%al,%eax
	movl	%edx,24(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$17,%r10d
	movl	%r8d,%r12d
	pinsrw	$3,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	28(%r15),%r9d
	addb	%dl,%bl
	movl	32(%rsi),%eax
	addl	$4249261313,%r9d
	xorl	%r8d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,28(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$22,%r9d
	movl	%r11d,%r12d
	pinsrw	$3,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	32(%r15),%r8d
	addb	%dl,%al
	movl	36(%rsi),%ebx
	addl	$1770035416,%r8d
	xorl	%r11d,%r12d
	movzbl	%al,%eax
	movl	%edx,32(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$7,%r8d
	movl	%r10d,%r12d
	pinsrw	$4,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	36(%r15),%r11d
	addb	%dl,%bl
	movl	40(%rsi),%eax
	addl	$2336552879,%r11d
	xorl	%r10d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,36(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$12,%r11d
	movl	%r9d,%r12d
	pinsrw	$4,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	40(%r15),%r10d
	addb	%dl,%al
	movl	44(%rsi),%ebx
	addl	$4294925233,%r10d
	xorl	%r9d,%r12d
	movzbl	%al,%eax
	movl	%edx,40(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$17,%r10d
	movl	%r8d,%r12d
	pinsrw	$5,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	44(%r15),%r9d
	addb	%dl,%bl
	movl	48(%rsi),%eax
	addl	$2304563134,%r9d
	xorl	%r8d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,44(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$22,%r9d
	movl	%r11d,%r12d
	pinsrw	$5,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	48(%r15),%r8d
	addb	%dl,%al
	movl	52(%rsi),%ebx
	addl	$1804603682,%r8d
	xorl	%r11d,%r12d
	movzbl	%al,%eax
	movl	%edx,48(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$7,%r8d
	movl	%r10d,%r12d
	pinsrw	$6,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	52(%r15),%r11d
	addb	%dl,%bl
	movl	56(%rsi),%eax
	addl	$4254626195,%r11d
	xorl	%r10d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,52(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$12,%r11d
	movl	%r9d,%r12d
	pinsrw	$6,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	56(%r15),%r10d
	addb	%dl,%al
	movl	60(%rsi),%ebx
	addl	$2792965006,%r10d
	xorl	%r9d,%r12d
	movzbl	%al,%eax
	movl	%edx,56(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$17,%r10d
	movl	%r8d,%r12d
	pinsrw	$7,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movdqu	(%r13),%xmm2
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	60(%r15),%r9d
	addb	%dl,%bl
	movl	64(%rsi),%eax
	addl	$1236535329,%r9d
	xorl	%r8d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,60(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$22,%r9d
	movl	%r10d,%r12d
	pinsrw	$7,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	psllq	$8,%xmm1
	pxor	%xmm0,%xmm2
	pxor	%xmm1,%xmm2
	pxor	%xmm0,%xmm0
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	4(%r15),%r8d
	addb	%dl,%al
	movl	68(%rsi),%ebx
	addl	$4129170786,%r8d
	xorl	%r10d,%r12d
	movzbl	%al,%eax
	movl	%edx,64(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$5,%r8d
	movl	%r9d,%r12d
	movd	(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	pxor	%xmm1,%xmm1
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	24(%r15),%r11d
	addb	%dl,%bl
	movl	72(%rsi),%eax
	addl	$3225465664,%r11d
	xorl	%r9d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,68(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$9,%r11d
	movl	%r8d,%r12d
	movd	(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	44(%r15),%r10d
	addb	%dl,%al
	movl	76(%rsi),%ebx
	addl	$643717713,%r10d
	xorl	%r8d,%r12d
	movzbl	%al,%eax
	movl	%edx,72(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$14,%r10d
	movl	%r11d,%r12d
	pinsrw	$1,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	0(%r15),%r9d
	addb	%dl,%bl
	movl	80(%rsi),%eax
	addl	$3921069994,%r9d
	xorl	%r11d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,76(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$20,%r9d
	movl	%r10d,%r12d
	pinsrw	$1,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	20(%r15),%r8d
	addb	%dl,%al
	movl	84(%rsi),%ebx
	addl	$3593408605,%r8d
	xorl	%r10d,%r12d
	movzbl	%al,%eax
	movl	%edx,80(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$5,%r8d
	movl	%r9d,%r12d
	pinsrw	$2,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	40(%r15),%r11d
	addb	%dl,%bl
	movl	88(%rsi),%eax
	addl	$38016083,%r11d
	xorl	%r9d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,84(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$9,%r11d
	movl	%r8d,%r12d
	pinsrw	$2,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	60(%r15),%r10d
	addb	%dl,%al
	movl	92(%rsi),%ebx
	addl	$3634488961,%r10d
	xorl	%r8d,%r12d
	movzbl	%al,%eax
	movl	%edx,88(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$14,%r10d
	movl	%r11d,%r12d
	pinsrw	$3,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	16(%r15),%r9d
	addb	%dl,%bl
	movl	96(%rsi),%eax
	addl	$3889429448,%r9d
	xorl	%r11d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,92(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$20,%r9d
	movl	%r10d,%r12d
	pinsrw	$3,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	36(%r15),%r8d
	addb	%dl,%al
	movl	100(%rsi),%ebx
	addl	$568446438,%r8d
	xorl	%r10d,%r12d
	movzbl	%al,%eax
	movl	%edx,96(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$5,%r8d
	movl	%r9d,%r12d
	pinsrw	$4,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	56(%r15),%r11d
	addb	%dl,%bl
	movl	104(%rsi),%eax
	addl	$3275163606,%r11d
	xorl	%r9d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,100(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$9,%r11d
	movl	%r8d,%r12d
	pinsrw	$4,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	12(%r15),%r10d
	addb	%dl,%al
	movl	108(%rsi),%ebx
	addl	$4107603335,%r10d
	xorl	%r8d,%r12d
	movzbl	%al,%eax
	movl	%edx,104(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$14,%r10d
	movl	%r11d,%r12d
	pinsrw	$5,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	32(%r15),%r9d
	addb	%dl,%bl
	movl	112(%rsi),%eax
	addl	$1163531501,%r9d
	xorl	%r11d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,108(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$20,%r9d
	movl	%r10d,%r12d
	pinsrw	$5,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r11d,%r12d
	addl	52(%r15),%r8d
	addb	%dl,%al
	movl	116(%rsi),%ebx
	addl	$2850285829,%r8d
	xorl	%r10d,%r12d
	movzbl	%al,%eax
	movl	%edx,112(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$5,%r8d
	movl	%r9d,%r12d
	pinsrw	$6,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r10d,%r12d
	addl	8(%r15),%r11d
	addb	%dl,%bl
	movl	120(%rsi),%eax
	addl	$4243563512,%r11d
	xorl	%r9d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,116(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$9,%r11d
	movl	%r8d,%r12d
	pinsrw	$6,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	andl	%r9d,%r12d
	addl	28(%r15),%r10d
	addb	%dl,%al
	movl	124(%rsi),%ebx
	addl	$1735328473,%r10d
	xorl	%r8d,%r12d
	movzbl	%al,%eax
	movl	%edx,120(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$14,%r10d
	movl	%r11d,%r12d
	pinsrw	$7,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movdqu	16(%r13),%xmm3
	addb	$32,%bpl
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	andl	%r8d,%r12d
	addl	48(%r15),%r9d
	addb	%dl,%bl
	movl	0(%rdi,%rbp,4),%eax
	addl	$2368359562,%r9d
	xorl	%r11d,%r12d
	movzbl	%bl,%ebx
	movl	%edx,124(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$20,%r9d
	movl	%r11d,%r12d
	pinsrw	$7,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movq	%rcx,%rsi
	xorq	%rcx,%rcx
	movb	%sil,%cl
	leaq	(%rdi,%rbp,4),%rsi
	psllq	$8,%xmm1
	pxor	%xmm0,%xmm3
	pxor	%xmm1,%xmm3
	pxor	%xmm0,%xmm0
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r9d,%r12d
	addl	20(%r15),%r8d
	addb	%dl,%al
	movl	4(%rsi),%ebx
	addl	$4294588738,%r8d
	movzbl	%al,%eax
	addl	%r12d,%r8d
	movl	%edx,0(%rsi)
	addb	%bl,%cl
	roll	$4,%r8d
	movl	%r10d,%r12d
	movd	(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	pxor	%xmm1,%xmm1
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r8d,%r12d
	addl	32(%r15),%r11d
	addb	%dl,%bl
	movl	8(%rsi),%eax
	addl	$2272392833,%r11d
	movzbl	%bl,%ebx
	addl	%r12d,%r11d
	movl	%edx,4(%rsi)
	addb	%al,%cl
	roll	$11,%r11d
	movl	%r9d,%r12d
	movd	(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r11d,%r12d
	addl	44(%r15),%r10d
	addb	%dl,%al
	movl	12(%rsi),%ebx
	addl	$1839030562,%r10d
	movzbl	%al,%eax
	addl	%r12d,%r10d
	movl	%edx,8(%rsi)
	addb	%bl,%cl
	roll	$16,%r10d
	movl	%r8d,%r12d
	pinsrw	$1,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r10d,%r12d
	addl	56(%r15),%r9d
	addb	%dl,%bl
	movl	16(%rsi),%eax
	addl	$4259657740,%r9d
	movzbl	%bl,%ebx
	addl	%r12d,%r9d
	movl	%edx,12(%rsi)
	addb	%al,%cl
	roll	$23,%r9d
	movl	%r11d,%r12d
	pinsrw	$1,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r9d,%r12d
	addl	4(%r15),%r8d
	addb	%dl,%al
	movl	20(%rsi),%ebx
	addl	$2763975236,%r8d
	movzbl	%al,%eax
	addl	%r12d,%r8d
	movl	%edx,16(%rsi)
	addb	%bl,%cl
	roll	$4,%r8d
	movl	%r10d,%r12d
	pinsrw	$2,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r8d,%r12d
	addl	16(%r15),%r11d
	addb	%dl,%bl
	movl	24(%rsi),%eax
	addl	$1272893353,%r11d
	movzbl	%bl,%ebx
	addl	%r12d,%r11d
	movl	%edx,20(%rsi)
	addb	%al,%cl
	roll	$11,%r11d
	movl	%r9d,%r12d
	pinsrw	$2,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r11d,%r12d
	addl	28(%r15),%r10d
	addb	%dl,%al
	movl	28(%rsi),%ebx
	addl	$4139469664,%r10d
	movzbl	%al,%eax
	addl	%r12d,%r10d
	movl	%edx,24(%rsi)
	addb	%bl,%cl
	roll	$16,%r10d
	movl	%r8d,%r12d
	pinsrw	$3,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r10d,%r12d
	addl	40(%r15),%r9d
	addb	%dl,%bl
	movl	32(%rsi),%eax
	addl	$3200236656,%r9d
	movzbl	%bl,%ebx
	addl	%r12d,%r9d
	movl	%edx,28(%rsi)
	addb	%al,%cl
	roll	$23,%r9d
	movl	%r11d,%r12d
	pinsrw	$3,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r9d,%r12d
	addl	52(%r15),%r8d
	addb	%dl,%al
	movl	36(%rsi),%ebx
	addl	$681279174,%r8d
	movzbl	%al,%eax
	addl	%r12d,%r8d
	movl	%edx,32(%rsi)
	addb	%bl,%cl
	roll	$4,%r8d
	movl	%r10d,%r12d
	pinsrw	$4,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r8d,%r12d
	addl	0(%r15),%r11d
	addb	%dl,%bl
	movl	40(%rsi),%eax
	addl	$3936430074,%r11d
	movzbl	%bl,%ebx
	addl	%r12d,%r11d
	movl	%edx,36(%rsi)
	addb	%al,%cl
	roll	$11,%r11d
	movl	%r9d,%r12d
	pinsrw	$4,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r11d,%r12d
	addl	12(%r15),%r10d
	addb	%dl,%al
	movl	44(%rsi),%ebx
	addl	$3572445317,%r10d
	movzbl	%al,%eax
	addl	%r12d,%r10d
	movl	%edx,40(%rsi)
	addb	%bl,%cl
	roll	$16,%r10d
	movl	%r8d,%r12d
	pinsrw	$5,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r10d,%r12d
	addl	24(%r15),%r9d
	addb	%dl,%bl
	movl	48(%rsi),%eax
	addl	$76029189,%r9d
	movzbl	%bl,%ebx
	addl	%r12d,%r9d
	movl	%edx,44(%rsi)
	addb	%al,%cl
	roll	$23,%r9d
	movl	%r11d,%r12d
	pinsrw	$5,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r9d,%r12d
	addl	36(%r15),%r8d
	addb	%dl,%al
	movl	52(%rsi),%ebx
	addl	$3654602809,%r8d
	movzbl	%al,%eax
	addl	%r12d,%r8d
	movl	%edx,48(%rsi)
	addb	%bl,%cl
	roll	$4,%r8d
	movl	%r10d,%r12d
	pinsrw	$6,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r8d,%r12d
	addl	48(%r15),%r11d
	addb	%dl,%bl
	movl	56(%rsi),%eax
	addl	$3873151461,%r11d
	movzbl	%bl,%ebx
	addl	%r12d,%r11d
	movl	%edx,52(%rsi)
	addb	%al,%cl
	roll	$11,%r11d
	movl	%r9d,%r12d
	pinsrw	$6,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	xorl	%r11d,%r12d
	addl	60(%r15),%r10d
	addb	%dl,%al
	movl	60(%rsi),%ebx
	addl	$530742520,%r10d
	movzbl	%al,%eax
	addl	%r12d,%r10d
	movl	%edx,56(%rsi)
	addb	%bl,%cl
	roll	$16,%r10d
	movl	%r8d,%r12d
	pinsrw	$7,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movdqu	32(%r13),%xmm4
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	xorl	%r10d,%r12d
	addl	8(%r15),%r9d
	addb	%dl,%bl
	movl	64(%rsi),%eax
	addl	$3299628645,%r9d
	movzbl	%bl,%ebx
	addl	%r12d,%r9d
	movl	%edx,60(%rsi)
	addb	%al,%cl
	roll	$23,%r9d
	movl	$-1,%r12d
	pinsrw	$7,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	psllq	$8,%xmm1
	pxor	%xmm0,%xmm4
	pxor	%xmm1,%xmm4
	pxor	%xmm0,%xmm0
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r9d,%r12d
	addl	0(%r15),%r8d
	addb	%dl,%al
	movl	68(%rsi),%ebx
	addl	$4096336452,%r8d
	movzbl	%al,%eax
	xorl	%r10d,%r12d
	movl	%edx,64(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$6,%r8d
	movl	$-1,%r12d
	movd	(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	pxor	%xmm1,%xmm1
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r8d,%r12d
	addl	28(%r15),%r11d
	addb	%dl,%bl
	movl	72(%rsi),%eax
	addl	$1126891415,%r11d
	movzbl	%bl,%ebx
	xorl	%r9d,%r12d
	movl	%edx,68(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$10,%r11d
	movl	$-1,%r12d
	movd	(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r11d,%r12d
	addl	56(%r15),%r10d
	addb	%dl,%al
	movl	76(%rsi),%ebx
	addl	$2878612391,%r10d
	movzbl	%al,%eax
	xorl	%r8d,%r12d
	movl	%edx,72(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$15,%r10d
	movl	$-1,%r12d
	pinsrw	$1,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r10d,%r12d
	addl	20(%r15),%r9d
	addb	%dl,%bl
	movl	80(%rsi),%eax
	addl	$4237533241,%r9d
	movzbl	%bl,%ebx
	xorl	%r11d,%r12d
	movl	%edx,76(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$21,%r9d
	movl	$-1,%r12d
	pinsrw	$1,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r9d,%r12d
	addl	48(%r15),%r8d
	addb	%dl,%al
	movl	84(%rsi),%ebx
	addl	$1700485571,%r8d
	movzbl	%al,%eax
	xorl	%r10d,%r12d
	movl	%edx,80(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$6,%r8d
	movl	$-1,%r12d
	pinsrw	$2,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r8d,%r12d
	addl	12(%r15),%r11d
	addb	%dl,%bl
	movl	88(%rsi),%eax
	addl	$2399980690,%r11d
	movzbl	%bl,%ebx
	xorl	%r9d,%r12d
	movl	%edx,84(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$10,%r11d
	movl	$-1,%r12d
	pinsrw	$2,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r11d,%r12d
	addl	40(%r15),%r10d
	addb	%dl,%al
	movl	92(%rsi),%ebx
	addl	$4293915773,%r10d
	movzbl	%al,%eax
	xorl	%r8d,%r12d
	movl	%edx,88(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$15,%r10d
	movl	$-1,%r12d
	pinsrw	$3,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r10d,%r12d
	addl	4(%r15),%r9d
	addb	%dl,%bl
	movl	96(%rsi),%eax
	addl	$2240044497,%r9d
	movzbl	%bl,%ebx
	xorl	%r11d,%r12d
	movl	%edx,92(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$21,%r9d
	movl	$-1,%r12d
	pinsrw	$3,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r9d,%r12d
	addl	32(%r15),%r8d
	addb	%dl,%al
	movl	100(%rsi),%ebx
	addl	$1873313359,%r8d
	movzbl	%al,%eax
	xorl	%r10d,%r12d
	movl	%edx,96(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$6,%r8d
	movl	$-1,%r12d
	pinsrw	$4,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r8d,%r12d
	addl	60(%r15),%r11d
	addb	%dl,%bl
	movl	104(%rsi),%eax
	addl	$4264355552,%r11d
	movzbl	%bl,%ebx
	xorl	%r9d,%r12d
	movl	%edx,100(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$10,%r11d
	movl	$-1,%r12d
	pinsrw	$4,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r11d,%r12d
	addl	24(%r15),%r10d
	addb	%dl,%al
	movl	108(%rsi),%ebx
	addl	$2734768916,%r10d
	movzbl	%al,%eax
	xorl	%r8d,%r12d
	movl	%edx,104(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$15,%r10d
	movl	$-1,%r12d
	pinsrw	$5,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r10d,%r12d
	addl	52(%r15),%r9d
	addb	%dl,%bl
	movl	112(%rsi),%eax
	addl	$1309151649,%r9d
	movzbl	%bl,%ebx
	xorl	%r11d,%r12d
	movl	%edx,108(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$21,%r9d
	movl	$-1,%r12d
	pinsrw	$5,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r11d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r9d,%r12d
	addl	16(%r15),%r8d
	addb	%dl,%al
	movl	116(%rsi),%ebx
	addl	$4149444226,%r8d
	movzbl	%al,%eax
	xorl	%r10d,%r12d
	movl	%edx,112(%rsi)
	addl	%r12d,%r8d
	addb	%bl,%cl
	roll	$6,%r8d
	movl	$-1,%r12d
	pinsrw	$6,(%rdi,%rax,4),%xmm0

	addl	%r9d,%r8d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r10d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r8d,%r12d
	addl	44(%r15),%r11d
	addb	%dl,%bl
	movl	120(%rsi),%eax
	addl	$3174756917,%r11d
	movzbl	%bl,%ebx
	xorl	%r9d,%r12d
	movl	%edx,116(%rsi)
	addl	%r12d,%r11d
	addb	%al,%cl
	roll	$10,%r11d
	movl	$-1,%r12d
	pinsrw	$6,(%rdi,%rbx,4),%xmm1

	addl	%r8d,%r11d
	movl	(%rdi,%rcx,4),%edx
	xorl	%r9d,%r12d
	movl	%eax,(%rdi,%rcx,4)
	orl	%r11d,%r12d
	addl	8(%r15),%r10d
	addb	%dl,%al
	movl	124(%rsi),%ebx
	addl	$718787259,%r10d
	movzbl	%al,%eax
	xorl	%r8d,%r12d
	movl	%edx,120(%rsi)
	addl	%r12d,%r10d
	addb	%bl,%cl
	roll	$15,%r10d
	movl	$-1,%r12d
	pinsrw	$7,(%rdi,%rax,4),%xmm0

	addl	%r11d,%r10d
	movdqu	48(%r13),%xmm5
	addb	$32,%bpl
	movl	(%rdi,%rcx,4),%edx
	xorl	%r8d,%r12d
	movl	%ebx,(%rdi,%rcx,4)
	orl	%r10d,%r12d
	addl	36(%r15),%r9d
	addb	%dl,%bl
	movl	0(%rdi,%rbp,4),%eax
	addl	$3951481745,%r9d
	movzbl	%bl,%ebx
	xorl	%r11d,%r12d
	movl	%edx,124(%rsi)
	addl	%r12d,%r9d
	addb	%al,%cl
	roll	$21,%r9d
	movl	$-1,%r12d
	pinsrw	$7,(%rdi,%rbx,4),%xmm1

	addl	%r10d,%r9d
	movq	%rbp,%rsi
	xorq	%rbp,%rbp
	movb	%sil,%bpl
	movq	%rcx,%rsi
	xorq	%rcx,%rcx
	movb	%sil,%cl
	leaq	(%rdi,%rbp,4),%rsi
	psllq	$8,%xmm1
	pxor	%xmm0,%xmm5
	pxor	%xmm1,%xmm5
	addl	0(%rsp),%r8d
	addl	4(%rsp),%r9d
	addl	8(%rsp),%r10d
	addl	12(%rsp),%r11d

	movdqu	%xmm2,(%r14,%r13,1)
	movdqu	%xmm3,16(%r14,%r13,1)
	movdqu	%xmm4,32(%r14,%r13,1)
	movdqu	%xmm5,48(%r14,%r13,1)
	leaq	64(%r15),%r15
	leaq	64(%r13),%r13
	cmpq	16(%rsp),%r15
	jb	.Loop

	movq	24(%rsp),%r12
	subb	%al,%cl
	movl	%r8d,0(%r12)
	movl	%r9d,4(%r12)
	movl	%r10d,8(%r12)
	movl	%r11d,12(%r12)
	subb	$1,%bpl
	movl	%ebp,-8(%rdi)
	movl	%ecx,-4(%rdi)

	movq	40(%rsp),%r15
.cfi_restore	%r15
	movq	48(%rsp),%r14
.cfi_restore	%r14
	movq	56(%rsp),%r13
.cfi_restore	%r13
	movq	64(%rsp),%r12
.cfi_restore	%r12
	movq	72(%rsp),%rbp
.cfi_restore	%rbp
	movq	80(%rsp),%rbx
.cfi_restore	%rbx
	leaq	88(%rsp),%rsp
.cfi_adjust_cfa_offset	-88
.Lepilogue:
.Labort:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	rc4_md5_enc,.-rc4_md5_enc
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/rc4/rc4-x86_64.s              0000664 0000000 0000000 00000027171 14746647661 0027464 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	


.globl	RC4
.type	RC4,@function
.align	16
RC4:
.cfi_startproc	
.byte	243,15,30,250
	orq	%rsi,%rsi
	jne	.Lentry
	.byte	0xf3,0xc3
.Lentry:
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-24
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-32
.Lprologue:
	movq	%rsi,%r11
	movq	%rdx,%r12
	movq	%rcx,%r13
	xorq	%r10,%r10
	xorq	%rcx,%rcx

	leaq	8(%rdi),%rdi
	movb	-8(%rdi),%r10b
	movb	-4(%rdi),%cl
	cmpl	$-1,256(%rdi)
	je	.LRC4_CHAR
	movl	OPENSSL_ia32cap_P(%rip),%r8d
	xorq	%rbx,%rbx
	incb	%r10b
	subq	%r10,%rbx
	subq	%r12,%r13
	movl	(%rdi,%r10,4),%eax
	testq	$-16,%r11
	jz	.Lloop1
	btl	$30,%r8d
	jc	.Lintel
	andq	$7,%rbx
	leaq	1(%r10),%rsi
	jz	.Loop8
	subq	%rbx,%r11
.Loop8_warmup:
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	%edx,(%rdi,%r10,4)
	addb	%dl,%al
	incb	%r10b
	movl	(%rdi,%rax,4),%edx
	movl	(%rdi,%r10,4),%eax
	xorb	(%r12),%dl
	movb	%dl,(%r12,%r13,1)
	leaq	1(%r12),%r12
	decq	%rbx
	jnz	.Loop8_warmup

	leaq	1(%r10),%rsi
	jmp	.Loop8
.align	16
.Loop8:
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	0(%rdi,%rsi,4),%ebx
	rorq	$8,%r8
	movl	%edx,0(%rdi,%r10,4)
	addb	%al,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%bl,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	movl	4(%rdi,%rsi,4),%eax
	rorq	$8,%r8
	movl	%edx,4(%rdi,%r10,4)
	addb	%bl,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	8(%rdi,%rsi,4),%ebx
	rorq	$8,%r8
	movl	%edx,8(%rdi,%r10,4)
	addb	%al,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%bl,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	movl	12(%rdi,%rsi,4),%eax
	rorq	$8,%r8
	movl	%edx,12(%rdi,%r10,4)
	addb	%bl,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	16(%rdi,%rsi,4),%ebx
	rorq	$8,%r8
	movl	%edx,16(%rdi,%r10,4)
	addb	%al,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%bl,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	movl	20(%rdi,%rsi,4),%eax
	rorq	$8,%r8
	movl	%edx,20(%rdi,%r10,4)
	addb	%bl,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	24(%rdi,%rsi,4),%ebx
	rorq	$8,%r8
	movl	%edx,24(%rdi,%r10,4)
	addb	%al,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	$8,%sil
	addb	%bl,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	movl	-4(%rdi,%rsi,4),%eax
	rorq	$8,%r8
	movl	%edx,28(%rdi,%r10,4)
	addb	%bl,%dl
	movb	(%rdi,%rdx,4),%r8b
	addb	$8,%r10b
	rorq	$8,%r8
	subq	$8,%r11

	xorq	(%r12),%r8
	movq	%r8,(%r12,%r13,1)
	leaq	8(%r12),%r12

	testq	$-8,%r11
	jnz	.Loop8
	cmpq	$0,%r11
	jne	.Lloop1
	jmp	.Lexit

.align	16
.Lintel:
	testq	$-32,%r11
	jz	.Lloop1
	andq	$15,%rbx
	jz	.Loop16_is_hot
	subq	%rbx,%r11
.Loop16_warmup:
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	%edx,(%rdi,%r10,4)
	addb	%dl,%al
	incb	%r10b
	movl	(%rdi,%rax,4),%edx
	movl	(%rdi,%r10,4),%eax
	xorb	(%r12),%dl
	movb	%dl,(%r12,%r13,1)
	leaq	1(%r12),%r12
	decq	%rbx
	jnz	.Loop16_warmup

	movq	%rcx,%rbx
	xorq	%rcx,%rcx
	movb	%bl,%cl

.Loop16_is_hot:
	leaq	(%rdi,%r10,4),%rsi
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	pxor	%xmm0,%xmm0
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	4(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,0(%rsi)
	addb	%bl,%cl
	pinsrw	$0,(%rdi,%rax,4),%xmm0
	jmp	.Loop16_enter
.align	16
.Loop16:
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	pxor	%xmm0,%xmm2
	psllq	$8,%xmm1
	pxor	%xmm0,%xmm0
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	4(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,0(%rsi)
	pxor	%xmm1,%xmm2
	addb	%bl,%cl
	pinsrw	$0,(%rdi,%rax,4),%xmm0
	movdqu	%xmm2,(%r12,%r13,1)
	leaq	16(%r12),%r12
.Loop16_enter:
	movl	(%rdi,%rcx,4),%edx
	pxor	%xmm1,%xmm1
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	8(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,4(%rsi)
	addb	%al,%cl
	pinsrw	$0,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	12(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,8(%rsi)
	addb	%bl,%cl
	pinsrw	$1,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	16(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,12(%rsi)
	addb	%al,%cl
	pinsrw	$1,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	20(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,16(%rsi)
	addb	%bl,%cl
	pinsrw	$2,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	24(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,20(%rsi)
	addb	%al,%cl
	pinsrw	$2,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	28(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,24(%rsi)
	addb	%bl,%cl
	pinsrw	$3,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	32(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,28(%rsi)
	addb	%al,%cl
	pinsrw	$3,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	36(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,32(%rsi)
	addb	%bl,%cl
	pinsrw	$4,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	40(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,36(%rsi)
	addb	%al,%cl
	pinsrw	$4,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	44(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,40(%rsi)
	addb	%bl,%cl
	pinsrw	$5,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	48(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,44(%rsi)
	addb	%al,%cl
	pinsrw	$5,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	52(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,48(%rsi)
	addb	%bl,%cl
	pinsrw	$6,(%rdi,%rax,4),%xmm0
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movl	56(%rsi),%eax
	movzbl	%bl,%ebx
	movl	%edx,52(%rsi)
	addb	%al,%cl
	pinsrw	$6,(%rdi,%rbx,4),%xmm1
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	addb	%dl,%al
	movl	60(%rsi),%ebx
	movzbl	%al,%eax
	movl	%edx,56(%rsi)
	addb	%bl,%cl
	pinsrw	$7,(%rdi,%rax,4),%xmm0
	addb	$16,%r10b
	movdqu	(%r12),%xmm2
	movl	(%rdi,%rcx,4),%edx
	movl	%ebx,(%rdi,%rcx,4)
	addb	%dl,%bl
	movzbl	%bl,%ebx
	movl	%edx,60(%rsi)
	leaq	(%rdi,%r10,4),%rsi
	pinsrw	$7,(%rdi,%rbx,4),%xmm1
	movl	(%rsi),%eax
	movq	%rcx,%rbx
	xorq	%rcx,%rcx
	subq	$16,%r11
	movb	%bl,%cl
	testq	$-16,%r11
	jnz	.Loop16

	psllq	$8,%xmm1
	pxor	%xmm0,%xmm2
	pxor	%xmm1,%xmm2
	movdqu	%xmm2,(%r12,%r13,1)
	leaq	16(%r12),%r12

	cmpq	$0,%r11
	jne	.Lloop1
	jmp	.Lexit

.align	16
.Lloop1:
	addb	%al,%cl
	movl	(%rdi,%rcx,4),%edx
	movl	%eax,(%rdi,%rcx,4)
	movl	%edx,(%rdi,%r10,4)
	addb	%dl,%al
	incb	%r10b
	movl	(%rdi,%rax,4),%edx
	movl	(%rdi,%r10,4),%eax
	xorb	(%r12),%dl
	movb	%dl,(%r12,%r13,1)
	leaq	1(%r12),%r12
	decq	%r11
	jnz	.Lloop1
	jmp	.Lexit

.align	16
.LRC4_CHAR:
	addb	$1,%r10b
	movzbl	(%rdi,%r10,1),%eax
	testq	$-8,%r11
	jz	.Lcloop1
	jmp	.Lcloop8
.align	16
.Lcloop8:
	movl	(%r12),%r8d
	movl	4(%r12),%r9d
	addb	%al,%cl
	leaq	1(%r10),%rsi
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%sil,%esi
	movzbl	(%rdi,%rsi,1),%ebx
	movb	%al,(%rdi,%rcx,1)
	cmpq	%rsi,%rcx
	movb	%dl,(%rdi,%r10,1)
	jne	.Lcmov0
	movq	%rax,%rbx
.Lcmov0:
	addb	%al,%dl
	xorb	(%rdi,%rdx,1),%r8b
	rorl	$8,%r8d
	addb	%bl,%cl
	leaq	1(%rsi),%r10
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%r10b,%r10d
	movzbl	(%rdi,%r10,1),%eax
	movb	%bl,(%rdi,%rcx,1)
	cmpq	%r10,%rcx
	movb	%dl,(%rdi,%rsi,1)
	jne	.Lcmov1
	movq	%rbx,%rax
.Lcmov1:
	addb	%bl,%dl
	xorb	(%rdi,%rdx,1),%r8b
	rorl	$8,%r8d
	addb	%al,%cl
	leaq	1(%r10),%rsi
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%sil,%esi
	movzbl	(%rdi,%rsi,1),%ebx
	movb	%al,(%rdi,%rcx,1)
	cmpq	%rsi,%rcx
	movb	%dl,(%rdi,%r10,1)
	jne	.Lcmov2
	movq	%rax,%rbx
.Lcmov2:
	addb	%al,%dl
	xorb	(%rdi,%rdx,1),%r8b
	rorl	$8,%r8d
	addb	%bl,%cl
	leaq	1(%rsi),%r10
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%r10b,%r10d
	movzbl	(%rdi,%r10,1),%eax
	movb	%bl,(%rdi,%rcx,1)
	cmpq	%r10,%rcx
	movb	%dl,(%rdi,%rsi,1)
	jne	.Lcmov3
	movq	%rbx,%rax
.Lcmov3:
	addb	%bl,%dl
	xorb	(%rdi,%rdx,1),%r8b
	rorl	$8,%r8d
	addb	%al,%cl
	leaq	1(%r10),%rsi
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%sil,%esi
	movzbl	(%rdi,%rsi,1),%ebx
	movb	%al,(%rdi,%rcx,1)
	cmpq	%rsi,%rcx
	movb	%dl,(%rdi,%r10,1)
	jne	.Lcmov4
	movq	%rax,%rbx
.Lcmov4:
	addb	%al,%dl
	xorb	(%rdi,%rdx,1),%r9b
	rorl	$8,%r9d
	addb	%bl,%cl
	leaq	1(%rsi),%r10
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%r10b,%r10d
	movzbl	(%rdi,%r10,1),%eax
	movb	%bl,(%rdi,%rcx,1)
	cmpq	%r10,%rcx
	movb	%dl,(%rdi,%rsi,1)
	jne	.Lcmov5
	movq	%rbx,%rax
.Lcmov5:
	addb	%bl,%dl
	xorb	(%rdi,%rdx,1),%r9b
	rorl	$8,%r9d
	addb	%al,%cl
	leaq	1(%r10),%rsi
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%sil,%esi
	movzbl	(%rdi,%rsi,1),%ebx
	movb	%al,(%rdi,%rcx,1)
	cmpq	%rsi,%rcx
	movb	%dl,(%rdi,%r10,1)
	jne	.Lcmov6
	movq	%rax,%rbx
.Lcmov6:
	addb	%al,%dl
	xorb	(%rdi,%rdx,1),%r9b
	rorl	$8,%r9d
	addb	%bl,%cl
	leaq	1(%rsi),%r10
	movzbl	(%rdi,%rcx,1),%edx
	movzbl	%r10b,%r10d
	movzbl	(%rdi,%r10,1),%eax
	movb	%bl,(%rdi,%rcx,1)
	cmpq	%r10,%rcx
	movb	%dl,(%rdi,%rsi,1)
	jne	.Lcmov7
	movq	%rbx,%rax
.Lcmov7:
	addb	%bl,%dl
	xorb	(%rdi,%rdx,1),%r9b
	rorl	$8,%r9d
	leaq	-8(%r11),%r11
	movl	%r8d,(%r13)
	leaq	8(%r12),%r12
	movl	%r9d,4(%r13)
	leaq	8(%r13),%r13

	testq	$-8,%r11
	jnz	.Lcloop8
	cmpq	$0,%r11
	jne	.Lcloop1
	jmp	.Lexit
.align	16
.Lcloop1:
	addb	%al,%cl
	movzbl	%cl,%ecx
	movzbl	(%rdi,%rcx,1),%edx
	movb	%al,(%rdi,%rcx,1)
	movb	%dl,(%rdi,%r10,1)
	addb	%al,%dl
	addb	$1,%r10b
	movzbl	%dl,%edx
	movzbl	%r10b,%r10d
	movzbl	(%rdi,%rdx,1),%edx
	movzbl	(%rdi,%r10,1),%eax
	xorb	(%r12),%dl
	leaq	1(%r12),%r12
	movb	%dl,(%r13)
	leaq	1(%r13),%r13
	subq	$1,%r11
	jnz	.Lcloop1
	jmp	.Lexit

.align	16
.Lexit:
	subb	$1,%r10b
	movl	%r10d,-8(%rdi)
	movl	%ecx,-4(%rdi)

	movq	(%rsp),%r13
.cfi_restore	%r13
	movq	8(%rsp),%r12
.cfi_restore	%r12
	movq	16(%rsp),%rbx
.cfi_restore	%rbx
	addq	$24,%rsp
.cfi_adjust_cfa_offset	-24
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	RC4,.-RC4
.globl	RC4_set_key
.type	RC4_set_key,@function
.align	16
RC4_set_key:
.cfi_startproc	
.byte	243,15,30,250
	leaq	8(%rdi),%rdi
	leaq	(%rdx,%rsi,1),%rdx
	negq	%rsi
	movq	%rsi,%rcx
	xorl	%eax,%eax
	xorq	%r9,%r9
	xorq	%r10,%r10
	xorq	%r11,%r11

	movl	OPENSSL_ia32cap_P(%rip),%r8d
	btl	$20,%r8d
	jc	.Lc1stloop
	jmp	.Lw1stloop

.align	16
.Lw1stloop:
	movl	%eax,(%rdi,%rax,4)
	addb	$1,%al
	jnc	.Lw1stloop

	xorq	%r9,%r9
	xorq	%r8,%r8
.align	16
.Lw2ndloop:
	movl	(%rdi,%r9,4),%r10d
	addb	(%rdx,%rsi,1),%r8b
	addb	%r10b,%r8b
	addq	$1,%rsi
	movl	(%rdi,%r8,4),%r11d
	cmovzq	%rcx,%rsi
	movl	%r10d,(%rdi,%r8,4)
	movl	%r11d,(%rdi,%r9,4)
	addb	$1,%r9b
	jnc	.Lw2ndloop
	jmp	.Lexit_key

.align	16
.Lc1stloop:
	movb	%al,(%rdi,%rax,1)
	addb	$1,%al
	jnc	.Lc1stloop

	xorq	%r9,%r9
	xorq	%r8,%r8
.align	16
.Lc2ndloop:
	movb	(%rdi,%r9,1),%r10b
	addb	(%rdx,%rsi,1),%r8b
	addb	%r10b,%r8b
	addq	$1,%rsi
	movb	(%rdi,%r8,1),%r11b
	jnz	.Lcnowrap
	movq	%rcx,%rsi
.Lcnowrap:
	movb	%r10b,(%rdi,%r8,1)
	movb	%r11b,(%rdi,%r9,1)
	addb	$1,%r9b
	jnc	.Lc2ndloop
	movl	$-1,256(%rdi)

.align	16
.Lexit_key:
	xorl	%eax,%eax
	movl	%eax,-8(%rdi)
	movl	%eax,-4(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	RC4_set_key,.-RC4_set_key

.globl	RC4_options
.type	RC4_options,@function
.align	16
RC4_options:
.cfi_startproc	
.byte	243,15,30,250
	leaq	.Lopts(%rip),%rax
	movl	OPENSSL_ia32cap_P(%rip),%edx
	btl	$20,%edx
	jc	.L8xchar
	btl	$30,%edx
	jnc	.Ldone
	addq	$25,%rax
	.byte	0xf3,0xc3
.L8xchar:
	addq	$12,%rax
.Ldone:
	.byte	0xf3,0xc3
.cfi_endproc	
.align	64
.Lopts:
.byte	114,99,52,40,56,120,44,105,110,116,41,0
.byte	114,99,52,40,56,120,44,99,104,97,114,41,0
.byte	114,99,52,40,49,54,120,44,105,110,116,41,0
.byte	82,67,52,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	64
.size	RC4_options,.-RC4_options
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/                          0000775 0000000 0000000 00000000000 14746647661 0025727 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/keccak1600-x86_64.s       0000664 0000000 0000000 00000020762 14746647661 0030606 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	

.type	__KeccakF1600,@function
.align	32
__KeccakF1600:
.cfi_startproc	
	movq	60(%rdi),%rax
	movq	68(%rdi),%rbx
	movq	76(%rdi),%rcx
	movq	84(%rdi),%rdx
	movq	92(%rdi),%rbp
	jmp	.Loop

.align	32
.Loop:
	movq	-100(%rdi),%r8
	movq	-52(%rdi),%r9
	movq	-4(%rdi),%r10
	movq	44(%rdi),%r11

	xorq	-84(%rdi),%rcx
	xorq	-76(%rdi),%rdx
	xorq	%r8,%rax
	xorq	-92(%rdi),%rbx
	xorq	-44(%rdi),%rcx
	xorq	-60(%rdi),%rax
	movq	%rbp,%r12
	xorq	-68(%rdi),%rbp

	xorq	%r10,%rcx
	xorq	-20(%rdi),%rax
	xorq	-36(%rdi),%rdx
	xorq	%r9,%rbx
	xorq	-28(%rdi),%rbp

	xorq	36(%rdi),%rcx
	xorq	20(%rdi),%rax
	xorq	4(%rdi),%rdx
	xorq	-12(%rdi),%rbx
	xorq	12(%rdi),%rbp

	movq	%rcx,%r13
	rolq	$1,%rcx
	xorq	%rax,%rcx
	xorq	%r11,%rdx

	rolq	$1,%rax
	xorq	%rdx,%rax
	xorq	28(%rdi),%rbx

	rolq	$1,%rdx
	xorq	%rbx,%rdx
	xorq	52(%rdi),%rbp

	rolq	$1,%rbx
	xorq	%rbp,%rbx

	rolq	$1,%rbp
	xorq	%r13,%rbp
	xorq	%rcx,%r9
	xorq	%rdx,%r10
	rolq	$44,%r9
	xorq	%rbp,%r11
	xorq	%rax,%r12
	rolq	$43,%r10
	xorq	%rbx,%r8
	movq	%r9,%r13
	rolq	$21,%r11
	orq	%r10,%r9
	xorq	%r8,%r9
	rolq	$14,%r12

	xorq	(%r15),%r9
	leaq	8(%r15),%r15

	movq	%r12,%r14
	andq	%r11,%r12
	movq	%r9,-100(%rsi)
	xorq	%r10,%r12
	notq	%r10
	movq	%r12,-84(%rsi)

	orq	%r11,%r10
	movq	76(%rdi),%r12
	xorq	%r13,%r10
	movq	%r10,-92(%rsi)

	andq	%r8,%r13
	movq	-28(%rdi),%r9
	xorq	%r14,%r13
	movq	-20(%rdi),%r10
	movq	%r13,-68(%rsi)

	orq	%r8,%r14
	movq	-76(%rdi),%r8
	xorq	%r11,%r14
	movq	28(%rdi),%r11
	movq	%r14,-76(%rsi)


	xorq	%rbp,%r8
	xorq	%rdx,%r12
	rolq	$28,%r8
	xorq	%rcx,%r11
	xorq	%rax,%r9
	rolq	$61,%r12
	rolq	$45,%r11
	xorq	%rbx,%r10
	rolq	$20,%r9
	movq	%r8,%r13
	orq	%r12,%r8
	rolq	$3,%r10

	xorq	%r11,%r8
	movq	%r8,-36(%rsi)

	movq	%r9,%r14
	andq	%r13,%r9
	movq	-92(%rdi),%r8
	xorq	%r12,%r9
	notq	%r12
	movq	%r9,-28(%rsi)

	orq	%r11,%r12
	movq	-44(%rdi),%r9
	xorq	%r10,%r12
	movq	%r12,-44(%rsi)

	andq	%r10,%r11
	movq	60(%rdi),%r12
	xorq	%r14,%r11
	movq	%r11,-52(%rsi)

	orq	%r10,%r14
	movq	4(%rdi),%r10
	xorq	%r13,%r14
	movq	52(%rdi),%r11
	movq	%r14,-60(%rsi)


	xorq	%rbp,%r10
	xorq	%rax,%r11
	rolq	$25,%r10
	xorq	%rdx,%r9
	rolq	$8,%r11
	xorq	%rbx,%r12
	rolq	$6,%r9
	xorq	%rcx,%r8
	rolq	$18,%r12
	movq	%r10,%r13
	andq	%r11,%r10
	rolq	$1,%r8

	notq	%r11
	xorq	%r9,%r10
	movq	%r10,-12(%rsi)

	movq	%r12,%r14
	andq	%r11,%r12
	movq	-12(%rdi),%r10
	xorq	%r13,%r12
	movq	%r12,-4(%rsi)

	orq	%r9,%r13
	movq	84(%rdi),%r12
	xorq	%r8,%r13
	movq	%r13,-20(%rsi)

	andq	%r8,%r9
	xorq	%r14,%r9
	movq	%r9,12(%rsi)

	orq	%r8,%r14
	movq	-60(%rdi),%r9
	xorq	%r11,%r14
	movq	36(%rdi),%r11
	movq	%r14,4(%rsi)


	movq	-68(%rdi),%r8

	xorq	%rcx,%r10
	xorq	%rdx,%r11
	rolq	$10,%r10
	xorq	%rbx,%r9
	rolq	$15,%r11
	xorq	%rbp,%r12
	rolq	$36,%r9
	xorq	%rax,%r8
	rolq	$56,%r12
	movq	%r10,%r13
	orq	%r11,%r10
	rolq	$27,%r8

	notq	%r11
	xorq	%r9,%r10
	movq	%r10,28(%rsi)

	movq	%r12,%r14
	orq	%r11,%r12
	xorq	%r13,%r12
	movq	%r12,36(%rsi)

	andq	%r9,%r13
	xorq	%r8,%r13
	movq	%r13,20(%rsi)

	orq	%r8,%r9
	xorq	%r14,%r9
	movq	%r9,52(%rsi)

	andq	%r14,%r8
	xorq	%r11,%r8
	movq	%r8,44(%rsi)


	xorq	-84(%rdi),%rdx
	xorq	-36(%rdi),%rbp
	rolq	$62,%rdx
	xorq	68(%rdi),%rcx
	rolq	$55,%rbp
	xorq	12(%rdi),%rax
	rolq	$2,%rcx
	xorq	20(%rdi),%rbx
	xchgq	%rsi,%rdi
	rolq	$39,%rax
	rolq	$41,%rbx
	movq	%rdx,%r13
	andq	%rbp,%rdx
	notq	%rbp
	xorq	%rcx,%rdx
	movq	%rdx,92(%rdi)

	movq	%rax,%r14
	andq	%rbp,%rax
	xorq	%r13,%rax
	movq	%rax,60(%rdi)

	orq	%rcx,%r13
	xorq	%rbx,%r13
	movq	%r13,84(%rdi)

	andq	%rbx,%rcx
	xorq	%r14,%rcx
	movq	%rcx,76(%rdi)

	orq	%r14,%rbx
	xorq	%rbp,%rbx
	movq	%rbx,68(%rdi)

	movq	%rdx,%rbp
	movq	%r13,%rdx

	testq	$255,%r15
	jnz	.Loop

	leaq	-192(%r15),%r15
	.byte	0xf3,0xc3
.cfi_endproc	
.size	__KeccakF1600,.-__KeccakF1600

.type	KeccakF1600,@function
.align	32
KeccakF1600:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56

	leaq	100(%rdi),%rdi
	subq	$200,%rsp
.cfi_adjust_cfa_offset	200

	notq	-92(%rdi)
	notq	-84(%rdi)
	notq	-36(%rdi)
	notq	-4(%rdi)
	notq	36(%rdi)
	notq	60(%rdi)

	leaq	iotas(%rip),%r15
	leaq	100(%rsp),%rsi

	call	__KeccakF1600

	notq	-92(%rdi)
	notq	-84(%rdi)
	notq	-36(%rdi)
	notq	-4(%rdi)
	notq	36(%rdi)
	notq	60(%rdi)
	leaq	-100(%rdi),%rdi

	addq	$200,%rsp
.cfi_adjust_cfa_offset	-200

	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.cfi_endproc	
.size	KeccakF1600,.-KeccakF1600
.globl	SHA3_absorb
.type	SHA3_absorb,@function
.align	32
SHA3_absorb:
.cfi_startproc	
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56

	leaq	100(%rdi),%rdi
	subq	$232,%rsp
.cfi_adjust_cfa_offset	232

	movq	%rsi,%r9
	leaq	100(%rsp),%rsi

	notq	-92(%rdi)
	notq	-84(%rdi)
	notq	-36(%rdi)
	notq	-4(%rdi)
	notq	36(%rdi)
	notq	60(%rdi)
	leaq	iotas(%rip),%r15

	movq	%rcx,216-100(%rsi)

.Loop_absorb:
	cmpq	%rcx,%rdx
	jc	.Ldone_absorb

	shrq	$3,%rcx
	leaq	-100(%rdi),%r8

.Lblock_absorb:
	movq	(%r9),%rax
	leaq	8(%r9),%r9
	xorq	(%r8),%rax
	leaq	8(%r8),%r8
	subq	$8,%rdx
	movq	%rax,-8(%r8)
	subq	$1,%rcx
	jnz	.Lblock_absorb

	movq	%r9,200-100(%rsi)
	movq	%rdx,208-100(%rsi)
	call	__KeccakF1600
	movq	200-100(%rsi),%r9
	movq	208-100(%rsi),%rdx
	movq	216-100(%rsi),%rcx
	jmp	.Loop_absorb

.align	32
.Ldone_absorb:
	movq	%rdx,%rax

	notq	-92(%rdi)
	notq	-84(%rdi)
	notq	-36(%rdi)
	notq	-4(%rdi)
	notq	36(%rdi)
	notq	60(%rdi)

	addq	$232,%rsp
.cfi_adjust_cfa_offset	-232

	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.cfi_endproc	
.size	SHA3_absorb,.-SHA3_absorb
.globl	SHA3_squeeze
.type	SHA3_squeeze,@function
.align	32
SHA3_squeeze:
.cfi_startproc	
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-16
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-24
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-32

	shrq	$3,%rcx
	movq	%rdi,%r8
	movq	%rsi,%r12
	movq	%rdx,%r13
	movq	%rcx,%r14
	jmp	.Loop_squeeze

.align	32
.Loop_squeeze:
	cmpq	$8,%r13
	jb	.Ltail_squeeze

	movq	(%r8),%rax
	leaq	8(%r8),%r8
	movq	%rax,(%r12)
	leaq	8(%r12),%r12
	subq	$8,%r13
	jz	.Ldone_squeeze

	subq	$1,%rcx
	jnz	.Loop_squeeze

	call	KeccakF1600
	movq	%rdi,%r8
	movq	%r14,%rcx
	jmp	.Loop_squeeze

.Ltail_squeeze:
	movq	%r8,%rsi
	movq	%r12,%rdi
	movq	%r13,%rcx
.byte	0xf3,0xa4

.Ldone_squeeze:
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	.byte	0xf3,0xc3
.cfi_endproc	
.size	SHA3_squeeze,.-SHA3_squeeze
.align	256
.quad	0,0,0,0,0,0,0,0
.type	iotas,@object
iotas:
.quad	0x0000000000000001
.quad	0x0000000000008082
.quad	0x800000000000808a
.quad	0x8000000080008000
.quad	0x000000000000808b
.quad	0x0000000080000001
.quad	0x8000000080008081
.quad	0x8000000000008009
.quad	0x000000000000008a
.quad	0x0000000000000088
.quad	0x0000000080008009
.quad	0x000000008000000a
.quad	0x000000008000808b
.quad	0x800000000000008b
.quad	0x8000000000008089
.quad	0x8000000000008003
.quad	0x8000000000008002
.quad	0x8000000000000080
.quad	0x000000000000800a
.quad	0x800000008000000a
.quad	0x8000000080008081
.quad	0x8000000000008080
.quad	0x0000000080000001
.quad	0x8000000080008008
.size	iotas,.-iotas
.byte	75,101,99,99,97,107,45,49,54,48,48,32,97,98,115,111,114,98,32,97,110,100,32,115,113,117,101,101,122,101,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
              node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/sha1-mb-x86_64.s          0000664 0000000 0000000 00000445133 14746647661 0030311 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	



.globl	sha1_multi_block
.type	sha1_multi_block,@function
.align	32
sha1_multi_block:
.cfi_startproc	
	movq	OPENSSL_ia32cap_P+4(%rip),%rcx
	btq	$61,%rcx
	jc	_shaext_shortcut
	testl	$268435456,%ecx
	jnz	_avx_shortcut
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbx,-24
	subq	$288,%rsp
	andq	$-256,%rsp
	movq	%rax,272(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x90,0x02,0x06,0x23,0x08
.Lbody:
	leaq	K_XX_XX(%rip),%rbp
	leaq	256(%rsp),%rbx

.Loop_grande:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r9

	movq	32(%rsi),%r10

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r10

	movq	48(%rsi),%r11

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r11
	testl	%edx,%edx
	jz	.Ldone

	movdqu	0(%rdi),%xmm10
	leaq	128(%rsp),%rax
	movdqu	32(%rdi),%xmm11
	movdqu	64(%rdi),%xmm12
	movdqu	96(%rdi),%xmm13
	movdqu	128(%rdi),%xmm14
	movdqa	96(%rbp),%xmm5
	movdqa	-32(%rbp),%xmm15
	jmp	.Loop

.align	32
.Loop:
	movd	(%r8),%xmm0
	leaq	64(%r8),%r8
	movd	(%r9),%xmm2
	leaq	64(%r9),%r9
	movd	(%r10),%xmm3
	leaq	64(%r10),%r10
	movd	(%r11),%xmm4
	leaq	64(%r11),%r11
	punpckldq	%xmm3,%xmm0
	movd	-60(%r8),%xmm1
	punpckldq	%xmm4,%xmm2
	movd	-60(%r9),%xmm9
	punpckldq	%xmm2,%xmm0
	movd	-60(%r10),%xmm8
.byte	102,15,56,0,197
	movd	-60(%r11),%xmm7
	punpckldq	%xmm8,%xmm1
	movdqa	%xmm10,%xmm8
	paddd	%xmm15,%xmm14
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm11,%xmm7
	movdqa	%xmm11,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm13,%xmm7
	pand	%xmm12,%xmm6
	punpckldq	%xmm9,%xmm1
	movdqa	%xmm10,%xmm9

	movdqa	%xmm0,0-128(%rax)
	paddd	%xmm0,%xmm14
	movd	-56(%r8),%xmm2
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm11,%xmm7

	por	%xmm9,%xmm8
	movd	-56(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
.byte	102,15,56,0,205
	movd	-56(%r10),%xmm8
	por	%xmm7,%xmm11
	movd	-56(%r11),%xmm7
	punpckldq	%xmm8,%xmm2
	movdqa	%xmm14,%xmm8
	paddd	%xmm15,%xmm13
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm10,%xmm7
	movdqa	%xmm10,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm12,%xmm7
	pand	%xmm11,%xmm6
	punpckldq	%xmm9,%xmm2
	movdqa	%xmm14,%xmm9

	movdqa	%xmm1,16-128(%rax)
	paddd	%xmm1,%xmm13
	movd	-52(%r8),%xmm3
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm10,%xmm7

	por	%xmm9,%xmm8
	movd	-52(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
.byte	102,15,56,0,213
	movd	-52(%r10),%xmm8
	por	%xmm7,%xmm10
	movd	-52(%r11),%xmm7
	punpckldq	%xmm8,%xmm3
	movdqa	%xmm13,%xmm8
	paddd	%xmm15,%xmm12
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm14,%xmm7
	movdqa	%xmm14,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm11,%xmm7
	pand	%xmm10,%xmm6
	punpckldq	%xmm9,%xmm3
	movdqa	%xmm13,%xmm9

	movdqa	%xmm2,32-128(%rax)
	paddd	%xmm2,%xmm12
	movd	-48(%r8),%xmm4
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm14,%xmm7

	por	%xmm9,%xmm8
	movd	-48(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
.byte	102,15,56,0,221
	movd	-48(%r10),%xmm8
	por	%xmm7,%xmm14
	movd	-48(%r11),%xmm7
	punpckldq	%xmm8,%xmm4
	movdqa	%xmm12,%xmm8
	paddd	%xmm15,%xmm11
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm13,%xmm7
	movdqa	%xmm13,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm10,%xmm7
	pand	%xmm14,%xmm6
	punpckldq	%xmm9,%xmm4
	movdqa	%xmm12,%xmm9

	movdqa	%xmm3,48-128(%rax)
	paddd	%xmm3,%xmm11
	movd	-44(%r8),%xmm0
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm13,%xmm7

	por	%xmm9,%xmm8
	movd	-44(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
.byte	102,15,56,0,229
	movd	-44(%r10),%xmm8
	por	%xmm7,%xmm13
	movd	-44(%r11),%xmm7
	punpckldq	%xmm8,%xmm0
	movdqa	%xmm11,%xmm8
	paddd	%xmm15,%xmm10
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm12,%xmm7
	movdqa	%xmm12,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm14,%xmm7
	pand	%xmm13,%xmm6
	punpckldq	%xmm9,%xmm0
	movdqa	%xmm11,%xmm9

	movdqa	%xmm4,64-128(%rax)
	paddd	%xmm4,%xmm10
	movd	-40(%r8),%xmm1
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm12,%xmm7

	por	%xmm9,%xmm8
	movd	-40(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
.byte	102,15,56,0,197
	movd	-40(%r10),%xmm8
	por	%xmm7,%xmm12
	movd	-40(%r11),%xmm7
	punpckldq	%xmm8,%xmm1
	movdqa	%xmm10,%xmm8
	paddd	%xmm15,%xmm14
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm11,%xmm7
	movdqa	%xmm11,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm13,%xmm7
	pand	%xmm12,%xmm6
	punpckldq	%xmm9,%xmm1
	movdqa	%xmm10,%xmm9

	movdqa	%xmm0,80-128(%rax)
	paddd	%xmm0,%xmm14
	movd	-36(%r8),%xmm2
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm11,%xmm7

	por	%xmm9,%xmm8
	movd	-36(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
.byte	102,15,56,0,205
	movd	-36(%r10),%xmm8
	por	%xmm7,%xmm11
	movd	-36(%r11),%xmm7
	punpckldq	%xmm8,%xmm2
	movdqa	%xmm14,%xmm8
	paddd	%xmm15,%xmm13
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm10,%xmm7
	movdqa	%xmm10,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm12,%xmm7
	pand	%xmm11,%xmm6
	punpckldq	%xmm9,%xmm2
	movdqa	%xmm14,%xmm9

	movdqa	%xmm1,96-128(%rax)
	paddd	%xmm1,%xmm13
	movd	-32(%r8),%xmm3
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm10,%xmm7

	por	%xmm9,%xmm8
	movd	-32(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
.byte	102,15,56,0,213
	movd	-32(%r10),%xmm8
	por	%xmm7,%xmm10
	movd	-32(%r11),%xmm7
	punpckldq	%xmm8,%xmm3
	movdqa	%xmm13,%xmm8
	paddd	%xmm15,%xmm12
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm14,%xmm7
	movdqa	%xmm14,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm11,%xmm7
	pand	%xmm10,%xmm6
	punpckldq	%xmm9,%xmm3
	movdqa	%xmm13,%xmm9

	movdqa	%xmm2,112-128(%rax)
	paddd	%xmm2,%xmm12
	movd	-28(%r8),%xmm4
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm14,%xmm7

	por	%xmm9,%xmm8
	movd	-28(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
.byte	102,15,56,0,221
	movd	-28(%r10),%xmm8
	por	%xmm7,%xmm14
	movd	-28(%r11),%xmm7
	punpckldq	%xmm8,%xmm4
	movdqa	%xmm12,%xmm8
	paddd	%xmm15,%xmm11
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm13,%xmm7
	movdqa	%xmm13,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm10,%xmm7
	pand	%xmm14,%xmm6
	punpckldq	%xmm9,%xmm4
	movdqa	%xmm12,%xmm9

	movdqa	%xmm3,128-128(%rax)
	paddd	%xmm3,%xmm11
	movd	-24(%r8),%xmm0
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm13,%xmm7

	por	%xmm9,%xmm8
	movd	-24(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
.byte	102,15,56,0,229
	movd	-24(%r10),%xmm8
	por	%xmm7,%xmm13
	movd	-24(%r11),%xmm7
	punpckldq	%xmm8,%xmm0
	movdqa	%xmm11,%xmm8
	paddd	%xmm15,%xmm10
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm12,%xmm7
	movdqa	%xmm12,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm14,%xmm7
	pand	%xmm13,%xmm6
	punpckldq	%xmm9,%xmm0
	movdqa	%xmm11,%xmm9

	movdqa	%xmm4,144-128(%rax)
	paddd	%xmm4,%xmm10
	movd	-20(%r8),%xmm1
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm12,%xmm7

	por	%xmm9,%xmm8
	movd	-20(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
.byte	102,15,56,0,197
	movd	-20(%r10),%xmm8
	por	%xmm7,%xmm12
	movd	-20(%r11),%xmm7
	punpckldq	%xmm8,%xmm1
	movdqa	%xmm10,%xmm8
	paddd	%xmm15,%xmm14
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm11,%xmm7
	movdqa	%xmm11,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm13,%xmm7
	pand	%xmm12,%xmm6
	punpckldq	%xmm9,%xmm1
	movdqa	%xmm10,%xmm9

	movdqa	%xmm0,160-128(%rax)
	paddd	%xmm0,%xmm14
	movd	-16(%r8),%xmm2
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm11,%xmm7

	por	%xmm9,%xmm8
	movd	-16(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
.byte	102,15,56,0,205
	movd	-16(%r10),%xmm8
	por	%xmm7,%xmm11
	movd	-16(%r11),%xmm7
	punpckldq	%xmm8,%xmm2
	movdqa	%xmm14,%xmm8
	paddd	%xmm15,%xmm13
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm10,%xmm7
	movdqa	%xmm10,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm12,%xmm7
	pand	%xmm11,%xmm6
	punpckldq	%xmm9,%xmm2
	movdqa	%xmm14,%xmm9

	movdqa	%xmm1,176-128(%rax)
	paddd	%xmm1,%xmm13
	movd	-12(%r8),%xmm3
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm10,%xmm7

	por	%xmm9,%xmm8
	movd	-12(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
.byte	102,15,56,0,213
	movd	-12(%r10),%xmm8
	por	%xmm7,%xmm10
	movd	-12(%r11),%xmm7
	punpckldq	%xmm8,%xmm3
	movdqa	%xmm13,%xmm8
	paddd	%xmm15,%xmm12
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm14,%xmm7
	movdqa	%xmm14,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm11,%xmm7
	pand	%xmm10,%xmm6
	punpckldq	%xmm9,%xmm3
	movdqa	%xmm13,%xmm9

	movdqa	%xmm2,192-128(%rax)
	paddd	%xmm2,%xmm12
	movd	-8(%r8),%xmm4
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm14,%xmm7

	por	%xmm9,%xmm8
	movd	-8(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
.byte	102,15,56,0,221
	movd	-8(%r10),%xmm8
	por	%xmm7,%xmm14
	movd	-8(%r11),%xmm7
	punpckldq	%xmm8,%xmm4
	movdqa	%xmm12,%xmm8
	paddd	%xmm15,%xmm11
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm13,%xmm7
	movdqa	%xmm13,%xmm6
	pslld	$5,%xmm8
	pandn	%xmm10,%xmm7
	pand	%xmm14,%xmm6
	punpckldq	%xmm9,%xmm4
	movdqa	%xmm12,%xmm9

	movdqa	%xmm3,208-128(%rax)
	paddd	%xmm3,%xmm11
	movd	-4(%r8),%xmm0
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm13,%xmm7

	por	%xmm9,%xmm8
	movd	-4(%r9),%xmm9
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
.byte	102,15,56,0,229
	movd	-4(%r10),%xmm8
	por	%xmm7,%xmm13
	movdqa	0-128(%rax),%xmm1
	movd	-4(%r11),%xmm7
	punpckldq	%xmm8,%xmm0
	movdqa	%xmm11,%xmm8
	paddd	%xmm15,%xmm10
	punpckldq	%xmm7,%xmm9
	movdqa	%xmm12,%xmm7
	movdqa	%xmm12,%xmm6
	pslld	$5,%xmm8
	prefetcht0	63(%r8)
	pandn	%xmm14,%xmm7
	pand	%xmm13,%xmm6
	punpckldq	%xmm9,%xmm0
	movdqa	%xmm11,%xmm9

	movdqa	%xmm4,224-128(%rax)
	paddd	%xmm4,%xmm10
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6
	movdqa	%xmm12,%xmm7
	prefetcht0	63(%r9)

	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm10
	prefetcht0	63(%r10)

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
.byte	102,15,56,0,197
	prefetcht0	63(%r11)
	por	%xmm7,%xmm12
	movdqa	16-128(%rax),%xmm2
	pxor	%xmm3,%xmm1
	movdqa	32-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	pxor	128-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	movdqa	%xmm11,%xmm7
	pslld	$5,%xmm8
	pxor	%xmm3,%xmm1
	movdqa	%xmm11,%xmm6
	pandn	%xmm13,%xmm7
	movdqa	%xmm1,%xmm5
	pand	%xmm12,%xmm6
	movdqa	%xmm10,%xmm9
	psrld	$31,%xmm5
	paddd	%xmm1,%xmm1

	movdqa	%xmm0,240-128(%rax)
	paddd	%xmm0,%xmm14
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6

	movdqa	%xmm11,%xmm7
	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	48-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	pxor	144-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	movdqa	%xmm10,%xmm7
	pslld	$5,%xmm8
	pxor	%xmm4,%xmm2
	movdqa	%xmm10,%xmm6
	pandn	%xmm12,%xmm7
	movdqa	%xmm2,%xmm5
	pand	%xmm11,%xmm6
	movdqa	%xmm14,%xmm9
	psrld	$31,%xmm5
	paddd	%xmm2,%xmm2

	movdqa	%xmm1,0-128(%rax)
	paddd	%xmm1,%xmm13
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6

	movdqa	%xmm10,%xmm7
	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	64-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	pxor	160-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	movdqa	%xmm14,%xmm7
	pslld	$5,%xmm8
	pxor	%xmm0,%xmm3
	movdqa	%xmm14,%xmm6
	pandn	%xmm11,%xmm7
	movdqa	%xmm3,%xmm5
	pand	%xmm10,%xmm6
	movdqa	%xmm13,%xmm9
	psrld	$31,%xmm5
	paddd	%xmm3,%xmm3

	movdqa	%xmm2,16-128(%rax)
	paddd	%xmm2,%xmm12
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6

	movdqa	%xmm14,%xmm7
	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	80-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	pxor	176-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	movdqa	%xmm13,%xmm7
	pslld	$5,%xmm8
	pxor	%xmm1,%xmm4
	movdqa	%xmm13,%xmm6
	pandn	%xmm10,%xmm7
	movdqa	%xmm4,%xmm5
	pand	%xmm14,%xmm6
	movdqa	%xmm12,%xmm9
	psrld	$31,%xmm5
	paddd	%xmm4,%xmm4

	movdqa	%xmm3,32-128(%rax)
	paddd	%xmm3,%xmm11
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6

	movdqa	%xmm13,%xmm7
	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	96-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	pxor	192-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	movdqa	%xmm12,%xmm7
	pslld	$5,%xmm8
	pxor	%xmm2,%xmm0
	movdqa	%xmm12,%xmm6
	pandn	%xmm14,%xmm7
	movdqa	%xmm0,%xmm5
	pand	%xmm13,%xmm6
	movdqa	%xmm11,%xmm9
	psrld	$31,%xmm5
	paddd	%xmm0,%xmm0

	movdqa	%xmm4,48-128(%rax)
	paddd	%xmm4,%xmm10
	psrld	$27,%xmm9
	pxor	%xmm7,%xmm6

	movdqa	%xmm12,%xmm7
	por	%xmm9,%xmm8
	pslld	$30,%xmm7
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	movdqa	0(%rbp),%xmm15
	pxor	%xmm3,%xmm1
	movdqa	112-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	208-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,64-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	128-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	224-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,80-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	144-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	240-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,96-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	160-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	0-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,112-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	176-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	16-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,128-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	192-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	32-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,144-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	208-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	48-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,160-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	224-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	64-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,176-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	240-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	80-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,192-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	0-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	96-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,208-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	16-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	112-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,224-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	32-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	128-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,240-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	48-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	144-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,0-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	64-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	160-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,16-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	80-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	176-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,32-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	96-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	192-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,48-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	112-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	208-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,64-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	128-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	224-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,80-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	144-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	240-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,96-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	160-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	0-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,112-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	movdqa	32(%rbp),%xmm15
	pxor	%xmm3,%xmm1
	movdqa	176-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm7
	pxor	16-128(%rax),%xmm1
	pxor	%xmm3,%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	movdqa	%xmm10,%xmm9
	pand	%xmm12,%xmm7

	movdqa	%xmm13,%xmm6
	movdqa	%xmm1,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm14
	pxor	%xmm12,%xmm6

	movdqa	%xmm0,128-128(%rax)
	paddd	%xmm0,%xmm14
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm11,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm1,%xmm1
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	192-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm7
	pxor	32-128(%rax),%xmm2
	pxor	%xmm4,%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	movdqa	%xmm14,%xmm9
	pand	%xmm11,%xmm7

	movdqa	%xmm12,%xmm6
	movdqa	%xmm2,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm13
	pxor	%xmm11,%xmm6

	movdqa	%xmm1,144-128(%rax)
	paddd	%xmm1,%xmm13
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm10,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm2,%xmm2
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	208-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm7
	pxor	48-128(%rax),%xmm3
	pxor	%xmm0,%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	movdqa	%xmm13,%xmm9
	pand	%xmm10,%xmm7

	movdqa	%xmm11,%xmm6
	movdqa	%xmm3,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm12
	pxor	%xmm10,%xmm6

	movdqa	%xmm2,160-128(%rax)
	paddd	%xmm2,%xmm12
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm14,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm3,%xmm3
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	224-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm7
	pxor	64-128(%rax),%xmm4
	pxor	%xmm1,%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	movdqa	%xmm12,%xmm9
	pand	%xmm14,%xmm7

	movdqa	%xmm10,%xmm6
	movdqa	%xmm4,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm11
	pxor	%xmm14,%xmm6

	movdqa	%xmm3,176-128(%rax)
	paddd	%xmm3,%xmm11
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm13,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm4,%xmm4
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	240-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm7
	pxor	80-128(%rax),%xmm0
	pxor	%xmm2,%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	movdqa	%xmm11,%xmm9
	pand	%xmm13,%xmm7

	movdqa	%xmm14,%xmm6
	movdqa	%xmm0,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm10
	pxor	%xmm13,%xmm6

	movdqa	%xmm4,192-128(%rax)
	paddd	%xmm4,%xmm10
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm12,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm0,%xmm0
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	0-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm7
	pxor	96-128(%rax),%xmm1
	pxor	%xmm3,%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	movdqa	%xmm10,%xmm9
	pand	%xmm12,%xmm7

	movdqa	%xmm13,%xmm6
	movdqa	%xmm1,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm14
	pxor	%xmm12,%xmm6

	movdqa	%xmm0,208-128(%rax)
	paddd	%xmm0,%xmm14
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm11,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm1,%xmm1
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	16-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm7
	pxor	112-128(%rax),%xmm2
	pxor	%xmm4,%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	movdqa	%xmm14,%xmm9
	pand	%xmm11,%xmm7

	movdqa	%xmm12,%xmm6
	movdqa	%xmm2,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm13
	pxor	%xmm11,%xmm6

	movdqa	%xmm1,224-128(%rax)
	paddd	%xmm1,%xmm13
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm10,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm2,%xmm2
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	32-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm7
	pxor	128-128(%rax),%xmm3
	pxor	%xmm0,%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	movdqa	%xmm13,%xmm9
	pand	%xmm10,%xmm7

	movdqa	%xmm11,%xmm6
	movdqa	%xmm3,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm12
	pxor	%xmm10,%xmm6

	movdqa	%xmm2,240-128(%rax)
	paddd	%xmm2,%xmm12
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm14,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm3,%xmm3
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	48-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm7
	pxor	144-128(%rax),%xmm4
	pxor	%xmm1,%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	movdqa	%xmm12,%xmm9
	pand	%xmm14,%xmm7

	movdqa	%xmm10,%xmm6
	movdqa	%xmm4,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm11
	pxor	%xmm14,%xmm6

	movdqa	%xmm3,0-128(%rax)
	paddd	%xmm3,%xmm11
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm13,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm4,%xmm4
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	64-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm7
	pxor	160-128(%rax),%xmm0
	pxor	%xmm2,%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	movdqa	%xmm11,%xmm9
	pand	%xmm13,%xmm7

	movdqa	%xmm14,%xmm6
	movdqa	%xmm0,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm10
	pxor	%xmm13,%xmm6

	movdqa	%xmm4,16-128(%rax)
	paddd	%xmm4,%xmm10
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm12,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm0,%xmm0
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	80-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm7
	pxor	176-128(%rax),%xmm1
	pxor	%xmm3,%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	movdqa	%xmm10,%xmm9
	pand	%xmm12,%xmm7

	movdqa	%xmm13,%xmm6
	movdqa	%xmm1,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm14
	pxor	%xmm12,%xmm6

	movdqa	%xmm0,32-128(%rax)
	paddd	%xmm0,%xmm14
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm11,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm1,%xmm1
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	96-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm7
	pxor	192-128(%rax),%xmm2
	pxor	%xmm4,%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	movdqa	%xmm14,%xmm9
	pand	%xmm11,%xmm7

	movdqa	%xmm12,%xmm6
	movdqa	%xmm2,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm13
	pxor	%xmm11,%xmm6

	movdqa	%xmm1,48-128(%rax)
	paddd	%xmm1,%xmm13
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm10,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm2,%xmm2
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	112-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm7
	pxor	208-128(%rax),%xmm3
	pxor	%xmm0,%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	movdqa	%xmm13,%xmm9
	pand	%xmm10,%xmm7

	movdqa	%xmm11,%xmm6
	movdqa	%xmm3,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm12
	pxor	%xmm10,%xmm6

	movdqa	%xmm2,64-128(%rax)
	paddd	%xmm2,%xmm12
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm14,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm3,%xmm3
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	128-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm7
	pxor	224-128(%rax),%xmm4
	pxor	%xmm1,%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	movdqa	%xmm12,%xmm9
	pand	%xmm14,%xmm7

	movdqa	%xmm10,%xmm6
	movdqa	%xmm4,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm11
	pxor	%xmm14,%xmm6

	movdqa	%xmm3,80-128(%rax)
	paddd	%xmm3,%xmm11
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm13,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm4,%xmm4
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	144-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm7
	pxor	240-128(%rax),%xmm0
	pxor	%xmm2,%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	movdqa	%xmm11,%xmm9
	pand	%xmm13,%xmm7

	movdqa	%xmm14,%xmm6
	movdqa	%xmm0,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm10
	pxor	%xmm13,%xmm6

	movdqa	%xmm4,96-128(%rax)
	paddd	%xmm4,%xmm10
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm12,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm0,%xmm0
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	160-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm7
	pxor	0-128(%rax),%xmm1
	pxor	%xmm3,%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	movdqa	%xmm10,%xmm9
	pand	%xmm12,%xmm7

	movdqa	%xmm13,%xmm6
	movdqa	%xmm1,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm14
	pxor	%xmm12,%xmm6

	movdqa	%xmm0,112-128(%rax)
	paddd	%xmm0,%xmm14
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm11,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm1,%xmm1
	paddd	%xmm6,%xmm14

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	176-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm7
	pxor	16-128(%rax),%xmm2
	pxor	%xmm4,%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	movdqa	%xmm14,%xmm9
	pand	%xmm11,%xmm7

	movdqa	%xmm12,%xmm6
	movdqa	%xmm2,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm13
	pxor	%xmm11,%xmm6

	movdqa	%xmm1,128-128(%rax)
	paddd	%xmm1,%xmm13
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm10,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm2,%xmm2
	paddd	%xmm6,%xmm13

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	192-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm7
	pxor	32-128(%rax),%xmm3
	pxor	%xmm0,%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	movdqa	%xmm13,%xmm9
	pand	%xmm10,%xmm7

	movdqa	%xmm11,%xmm6
	movdqa	%xmm3,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm12
	pxor	%xmm10,%xmm6

	movdqa	%xmm2,144-128(%rax)
	paddd	%xmm2,%xmm12
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm14,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm3,%xmm3
	paddd	%xmm6,%xmm12

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	208-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm7
	pxor	48-128(%rax),%xmm4
	pxor	%xmm1,%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	movdqa	%xmm12,%xmm9
	pand	%xmm14,%xmm7

	movdqa	%xmm10,%xmm6
	movdqa	%xmm4,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm11
	pxor	%xmm14,%xmm6

	movdqa	%xmm3,160-128(%rax)
	paddd	%xmm3,%xmm11
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm13,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm4,%xmm4
	paddd	%xmm6,%xmm11

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	224-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm7
	pxor	64-128(%rax),%xmm0
	pxor	%xmm2,%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	movdqa	%xmm11,%xmm9
	pand	%xmm13,%xmm7

	movdqa	%xmm14,%xmm6
	movdqa	%xmm0,%xmm5
	psrld	$27,%xmm9
	paddd	%xmm7,%xmm10
	pxor	%xmm13,%xmm6

	movdqa	%xmm4,176-128(%rax)
	paddd	%xmm4,%xmm10
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	pand	%xmm12,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	paddd	%xmm0,%xmm0
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	movdqa	64(%rbp),%xmm15
	pxor	%xmm3,%xmm1
	movdqa	240-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	80-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,192-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	0-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	96-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,208-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	16-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	112-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,224-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	32-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	128-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,240-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	48-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	144-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,0-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	64-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	160-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,16-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	80-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	176-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,32-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	96-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	192-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	movdqa	%xmm2,48-128(%rax)
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	112-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	208-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	movdqa	%xmm3,64-128(%rax)
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	128-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	224-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	movdqa	%xmm4,80-128(%rax)
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	144-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	240-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	movdqa	%xmm0,96-128(%rax)
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	160-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	0-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	movdqa	%xmm1,112-128(%rax)
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	176-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	16-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	192-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	32-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	pxor	%xmm2,%xmm0
	movdqa	208-128(%rax),%xmm2

	movdqa	%xmm11,%xmm8
	movdqa	%xmm14,%xmm6
	pxor	48-128(%rax),%xmm0
	paddd	%xmm15,%xmm10
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	paddd	%xmm4,%xmm10
	pxor	%xmm2,%xmm0
	psrld	$27,%xmm9
	pxor	%xmm13,%xmm6
	movdqa	%xmm12,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm0,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm10
	paddd	%xmm0,%xmm0

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm5,%xmm0
	por	%xmm7,%xmm12
	pxor	%xmm3,%xmm1
	movdqa	224-128(%rax),%xmm3

	movdqa	%xmm10,%xmm8
	movdqa	%xmm13,%xmm6
	pxor	64-128(%rax),%xmm1
	paddd	%xmm15,%xmm14
	pslld	$5,%xmm8
	pxor	%xmm11,%xmm6

	movdqa	%xmm10,%xmm9
	paddd	%xmm0,%xmm14
	pxor	%xmm3,%xmm1
	psrld	$27,%xmm9
	pxor	%xmm12,%xmm6
	movdqa	%xmm11,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm1,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm14
	paddd	%xmm1,%xmm1

	psrld	$2,%xmm11
	paddd	%xmm8,%xmm14
	por	%xmm5,%xmm1
	por	%xmm7,%xmm11
	pxor	%xmm4,%xmm2
	movdqa	240-128(%rax),%xmm4

	movdqa	%xmm14,%xmm8
	movdqa	%xmm12,%xmm6
	pxor	80-128(%rax),%xmm2
	paddd	%xmm15,%xmm13
	pslld	$5,%xmm8
	pxor	%xmm10,%xmm6

	movdqa	%xmm14,%xmm9
	paddd	%xmm1,%xmm13
	pxor	%xmm4,%xmm2
	psrld	$27,%xmm9
	pxor	%xmm11,%xmm6
	movdqa	%xmm10,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm2,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm13
	paddd	%xmm2,%xmm2

	psrld	$2,%xmm10
	paddd	%xmm8,%xmm13
	por	%xmm5,%xmm2
	por	%xmm7,%xmm10
	pxor	%xmm0,%xmm3
	movdqa	0-128(%rax),%xmm0

	movdqa	%xmm13,%xmm8
	movdqa	%xmm11,%xmm6
	pxor	96-128(%rax),%xmm3
	paddd	%xmm15,%xmm12
	pslld	$5,%xmm8
	pxor	%xmm14,%xmm6

	movdqa	%xmm13,%xmm9
	paddd	%xmm2,%xmm12
	pxor	%xmm0,%xmm3
	psrld	$27,%xmm9
	pxor	%xmm10,%xmm6
	movdqa	%xmm14,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm3,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm12
	paddd	%xmm3,%xmm3

	psrld	$2,%xmm14
	paddd	%xmm8,%xmm12
	por	%xmm5,%xmm3
	por	%xmm7,%xmm14
	pxor	%xmm1,%xmm4
	movdqa	16-128(%rax),%xmm1

	movdqa	%xmm12,%xmm8
	movdqa	%xmm10,%xmm6
	pxor	112-128(%rax),%xmm4
	paddd	%xmm15,%xmm11
	pslld	$5,%xmm8
	pxor	%xmm13,%xmm6

	movdqa	%xmm12,%xmm9
	paddd	%xmm3,%xmm11
	pxor	%xmm1,%xmm4
	psrld	$27,%xmm9
	pxor	%xmm14,%xmm6
	movdqa	%xmm13,%xmm7

	pslld	$30,%xmm7
	movdqa	%xmm4,%xmm5
	por	%xmm9,%xmm8
	psrld	$31,%xmm5
	paddd	%xmm6,%xmm11
	paddd	%xmm4,%xmm4

	psrld	$2,%xmm13
	paddd	%xmm8,%xmm11
	por	%xmm5,%xmm4
	por	%xmm7,%xmm13
	movdqa	%xmm11,%xmm8
	paddd	%xmm15,%xmm10
	movdqa	%xmm14,%xmm6
	pslld	$5,%xmm8
	pxor	%xmm12,%xmm6

	movdqa	%xmm11,%xmm9
	paddd	%xmm4,%xmm10
	psrld	$27,%xmm9
	movdqa	%xmm12,%xmm7
	pxor	%xmm13,%xmm6

	pslld	$30,%xmm7
	por	%xmm9,%xmm8
	paddd	%xmm6,%xmm10

	psrld	$2,%xmm12
	paddd	%xmm8,%xmm10
	por	%xmm7,%xmm12
	movdqa	(%rbx),%xmm0
	movl	$1,%ecx
	cmpl	0(%rbx),%ecx
	pxor	%xmm8,%xmm8
	cmovgeq	%rbp,%r8
	cmpl	4(%rbx),%ecx
	movdqa	%xmm0,%xmm1
	cmovgeq	%rbp,%r9
	cmpl	8(%rbx),%ecx
	pcmpgtd	%xmm8,%xmm1
	cmovgeq	%rbp,%r10
	cmpl	12(%rbx),%ecx
	paddd	%xmm1,%xmm0
	cmovgeq	%rbp,%r11

	movdqu	0(%rdi),%xmm6
	pand	%xmm1,%xmm10
	movdqu	32(%rdi),%xmm7
	pand	%xmm1,%xmm11
	paddd	%xmm6,%xmm10
	movdqu	64(%rdi),%xmm8
	pand	%xmm1,%xmm12
	paddd	%xmm7,%xmm11
	movdqu	96(%rdi),%xmm9
	pand	%xmm1,%xmm13
	paddd	%xmm8,%xmm12
	movdqu	128(%rdi),%xmm5
	pand	%xmm1,%xmm14
	movdqu	%xmm10,0(%rdi)
	paddd	%xmm9,%xmm13
	movdqu	%xmm11,32(%rdi)
	paddd	%xmm5,%xmm14
	movdqu	%xmm12,64(%rdi)
	movdqu	%xmm13,96(%rdi)
	movdqu	%xmm14,128(%rdi)

	movdqa	%xmm0,(%rbx)
	movdqa	96(%rbp),%xmm5
	movdqa	-32(%rbp),%xmm15
	decl	%edx
	jnz	.Loop

	movl	280(%rsp),%edx
	leaq	16(%rdi),%rdi
	leaq	64(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande

.Ldone:
	movq	272(%rsp),%rax
.cfi_def_cfa	%rax,8
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_multi_block,.-sha1_multi_block
.type	sha1_multi_block_shaext,@function
.align	32
sha1_multi_block_shaext:
.cfi_startproc	
_shaext_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	subq	$288,%rsp
	shll	$1,%edx
	andq	$-256,%rsp
	leaq	64(%rdi),%rdi
	movq	%rax,272(%rsp)
.Lbody_shaext:
	leaq	256(%rsp),%rbx
	movdqa	K_XX_XX+128(%rip),%xmm3

.Loop_grande_shaext:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rsp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rsp,%r9
	testl	%edx,%edx
	jz	.Ldone_shaext

	movq	0-64(%rdi),%xmm0
	movq	32-64(%rdi),%xmm4
	movq	64-64(%rdi),%xmm5
	movq	96-64(%rdi),%xmm6
	movq	128-64(%rdi),%xmm7

	punpckldq	%xmm4,%xmm0
	punpckldq	%xmm6,%xmm5

	movdqa	%xmm0,%xmm8
	punpcklqdq	%xmm5,%xmm0
	punpckhqdq	%xmm5,%xmm8

	pshufd	$63,%xmm7,%xmm1
	pshufd	$127,%xmm7,%xmm9
	pshufd	$27,%xmm0,%xmm0
	pshufd	$27,%xmm8,%xmm8
	jmp	.Loop_shaext

.align	32
.Loop_shaext:
	movdqu	0(%r8),%xmm4
	movdqu	0(%r9),%xmm11
	movdqu	16(%r8),%xmm5
	movdqu	16(%r9),%xmm12
	movdqu	32(%r8),%xmm6
.byte	102,15,56,0,227
	movdqu	32(%r9),%xmm13
.byte	102,68,15,56,0,219
	movdqu	48(%r8),%xmm7
	leaq	64(%r8),%r8
.byte	102,15,56,0,235
	movdqu	48(%r9),%xmm14
	leaq	64(%r9),%r9
.byte	102,68,15,56,0,227

	movdqa	%xmm1,80(%rsp)
	paddd	%xmm4,%xmm1
	movdqa	%xmm9,112(%rsp)
	paddd	%xmm11,%xmm9
	movdqa	%xmm0,64(%rsp)
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,96(%rsp)
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,0
.byte	15,56,200,213
.byte	69,15,58,204,193,0
.byte	69,15,56,200,212
.byte	102,15,56,0,243
	prefetcht0	127(%r8)
.byte	15,56,201,229
.byte	102,68,15,56,0,235
	prefetcht0	127(%r9)
.byte	69,15,56,201,220

.byte	102,15,56,0,251
	movdqa	%xmm0,%xmm1
.byte	102,68,15,56,0,243
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,0
.byte	15,56,200,206
.byte	69,15,58,204,194,0
.byte	69,15,56,200,205
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
	pxor	%xmm13,%xmm11
.byte	69,15,56,201,229
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,0
.byte	15,56,200,215
.byte	69,15,58,204,193,0
.byte	69,15,56,200,214
.byte	15,56,202,231
.byte	69,15,56,202,222
	pxor	%xmm7,%xmm5
.byte	15,56,201,247
	pxor	%xmm14,%xmm12
.byte	69,15,56,201,238
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,0
.byte	15,56,200,204
.byte	69,15,58,204,194,0
.byte	69,15,56,200,203
.byte	15,56,202,236
.byte	69,15,56,202,227
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
	pxor	%xmm11,%xmm13
.byte	69,15,56,201,243
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,0
.byte	15,56,200,213
.byte	69,15,58,204,193,0
.byte	69,15,56,200,212
.byte	15,56,202,245
.byte	69,15,56,202,236
	pxor	%xmm5,%xmm7
.byte	15,56,201,229
	pxor	%xmm12,%xmm14
.byte	69,15,56,201,220
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,1
.byte	15,56,200,206
.byte	69,15,58,204,194,1
.byte	69,15,56,200,205
.byte	15,56,202,254
.byte	69,15,56,202,245
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
	pxor	%xmm13,%xmm11
.byte	69,15,56,201,229
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,1
.byte	15,56,200,215
.byte	69,15,58,204,193,1
.byte	69,15,56,200,214
.byte	15,56,202,231
.byte	69,15,56,202,222
	pxor	%xmm7,%xmm5
.byte	15,56,201,247
	pxor	%xmm14,%xmm12
.byte	69,15,56,201,238
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,1
.byte	15,56,200,204
.byte	69,15,58,204,194,1
.byte	69,15,56,200,203
.byte	15,56,202,236
.byte	69,15,56,202,227
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
	pxor	%xmm11,%xmm13
.byte	69,15,56,201,243
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,1
.byte	15,56,200,213
.byte	69,15,58,204,193,1
.byte	69,15,56,200,212
.byte	15,56,202,245
.byte	69,15,56,202,236
	pxor	%xmm5,%xmm7
.byte	15,56,201,229
	pxor	%xmm12,%xmm14
.byte	69,15,56,201,220
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,1
.byte	15,56,200,206
.byte	69,15,58,204,194,1
.byte	69,15,56,200,205
.byte	15,56,202,254
.byte	69,15,56,202,245
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
	pxor	%xmm13,%xmm11
.byte	69,15,56,201,229
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,2
.byte	15,56,200,215
.byte	69,15,58,204,193,2
.byte	69,15,56,200,214
.byte	15,56,202,231
.byte	69,15,56,202,222
	pxor	%xmm7,%xmm5
.byte	15,56,201,247
	pxor	%xmm14,%xmm12
.byte	69,15,56,201,238
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,2
.byte	15,56,200,204
.byte	69,15,58,204,194,2
.byte	69,15,56,200,203
.byte	15,56,202,236
.byte	69,15,56,202,227
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
	pxor	%xmm11,%xmm13
.byte	69,15,56,201,243
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,2
.byte	15,56,200,213
.byte	69,15,58,204,193,2
.byte	69,15,56,200,212
.byte	15,56,202,245
.byte	69,15,56,202,236
	pxor	%xmm5,%xmm7
.byte	15,56,201,229
	pxor	%xmm12,%xmm14
.byte	69,15,56,201,220
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,2
.byte	15,56,200,206
.byte	69,15,58,204,194,2
.byte	69,15,56,200,205
.byte	15,56,202,254
.byte	69,15,56,202,245
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
	pxor	%xmm13,%xmm11
.byte	69,15,56,201,229
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,2
.byte	15,56,200,215
.byte	69,15,58,204,193,2
.byte	69,15,56,200,214
.byte	15,56,202,231
.byte	69,15,56,202,222
	pxor	%xmm7,%xmm5
.byte	15,56,201,247
	pxor	%xmm14,%xmm12
.byte	69,15,56,201,238
	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,3
.byte	15,56,200,204
.byte	69,15,58,204,194,3
.byte	69,15,56,200,203
.byte	15,56,202,236
.byte	69,15,56,202,227
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
	pxor	%xmm11,%xmm13
.byte	69,15,56,201,243
	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,3
.byte	15,56,200,213
.byte	69,15,58,204,193,3
.byte	69,15,56,200,212
.byte	15,56,202,245
.byte	69,15,56,202,236
	pxor	%xmm5,%xmm7
	pxor	%xmm12,%xmm14

	movl	$1,%ecx
	pxor	%xmm4,%xmm4
	cmpl	0(%rbx),%ecx
	cmovgeq	%rsp,%r8

	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,3
.byte	15,56,200,206
.byte	69,15,58,204,194,3
.byte	69,15,56,200,205
.byte	15,56,202,254
.byte	69,15,56,202,245

	cmpl	4(%rbx),%ecx
	cmovgeq	%rsp,%r9
	movq	(%rbx),%xmm6

	movdqa	%xmm0,%xmm2
	movdqa	%xmm8,%xmm10
.byte	15,58,204,193,3
.byte	15,56,200,215
.byte	69,15,58,204,193,3
.byte	69,15,56,200,214

	pshufd	$0x00,%xmm6,%xmm11
	pshufd	$0x55,%xmm6,%xmm12
	movdqa	%xmm6,%xmm7
	pcmpgtd	%xmm4,%xmm11
	pcmpgtd	%xmm4,%xmm12

	movdqa	%xmm0,%xmm1
	movdqa	%xmm8,%xmm9
.byte	15,58,204,194,3
.byte	15,56,200,204
.byte	69,15,58,204,194,3
.byte	68,15,56,200,204

	pcmpgtd	%xmm4,%xmm7
	pand	%xmm11,%xmm0
	pand	%xmm11,%xmm1
	pand	%xmm12,%xmm8
	pand	%xmm12,%xmm9
	paddd	%xmm7,%xmm6

	paddd	64(%rsp),%xmm0
	paddd	80(%rsp),%xmm1
	paddd	96(%rsp),%xmm8
	paddd	112(%rsp),%xmm9

	movq	%xmm6,(%rbx)
	decl	%edx
	jnz	.Loop_shaext

	movl	280(%rsp),%edx

	pshufd	$27,%xmm0,%xmm0
	pshufd	$27,%xmm8,%xmm8

	movdqa	%xmm0,%xmm6
	punpckldq	%xmm8,%xmm0
	punpckhdq	%xmm8,%xmm6
	punpckhdq	%xmm9,%xmm1
	movq	%xmm0,0-64(%rdi)
	psrldq	$8,%xmm0
	movq	%xmm6,64-64(%rdi)
	psrldq	$8,%xmm6
	movq	%xmm0,32-64(%rdi)
	psrldq	$8,%xmm1
	movq	%xmm6,96-64(%rdi)
	movq	%xmm1,128-64(%rdi)

	leaq	8(%rdi),%rdi
	leaq	32(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande_shaext

.Ldone_shaext:

	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_shaext:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_multi_block_shaext,.-sha1_multi_block_shaext
.type	sha1_multi_block_avx,@function
.align	32
sha1_multi_block_avx:
.cfi_startproc	
_avx_shortcut:
	shrq	$32,%rcx
	cmpl	$2,%edx
	jb	.Lavx
	testl	$32,%ecx
	jnz	_avx2_shortcut
	jmp	.Lavx
.align	32
.Lavx:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	subq	$288,%rsp
	andq	$-256,%rsp
	movq	%rax,272(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x90,0x02,0x06,0x23,0x08
.Lbody_avx:
	leaq	K_XX_XX(%rip),%rbp
	leaq	256(%rsp),%rbx

	vzeroupper
.Loop_grande_avx:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r9

	movq	32(%rsi),%r10

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r10

	movq	48(%rsi),%r11

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r11
	testl	%edx,%edx
	jz	.Ldone_avx

	vmovdqu	0(%rdi),%xmm10
	leaq	128(%rsp),%rax
	vmovdqu	32(%rdi),%xmm11
	vmovdqu	64(%rdi),%xmm12
	vmovdqu	96(%rdi),%xmm13
	vmovdqu	128(%rdi),%xmm14
	vmovdqu	96(%rbp),%xmm5
	jmp	.Loop_avx

.align	32
.Loop_avx:
	vmovdqa	-32(%rbp),%xmm15
	vmovd	(%r8),%xmm0
	leaq	64(%r8),%r8
	vmovd	(%r9),%xmm2
	leaq	64(%r9),%r9
	vpinsrd	$1,(%r10),%xmm0,%xmm0
	leaq	64(%r10),%r10
	vpinsrd	$1,(%r11),%xmm2,%xmm2
	leaq	64(%r11),%r11
	vmovd	-60(%r8),%xmm1
	vpunpckldq	%xmm2,%xmm0,%xmm0
	vmovd	-60(%r9),%xmm9
	vpshufb	%xmm5,%xmm0,%xmm0
	vpinsrd	$1,-60(%r10),%xmm1,%xmm1
	vpinsrd	$1,-60(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpandn	%xmm13,%xmm11,%xmm7
	vpand	%xmm12,%xmm11,%xmm6

	vmovdqa	%xmm0,0-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpunpckldq	%xmm9,%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-56(%r8),%xmm2

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-56(%r9),%xmm9
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpshufb	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpinsrd	$1,-56(%r10),%xmm2,%xmm2
	vpinsrd	$1,-56(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpandn	%xmm12,%xmm10,%xmm7
	vpand	%xmm11,%xmm10,%xmm6

	vmovdqa	%xmm1,16-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpunpckldq	%xmm9,%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-52(%r8),%xmm3

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-52(%r9),%xmm9
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpshufb	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpinsrd	$1,-52(%r10),%xmm3,%xmm3
	vpinsrd	$1,-52(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpandn	%xmm11,%xmm14,%xmm7
	vpand	%xmm10,%xmm14,%xmm6

	vmovdqa	%xmm2,32-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpunpckldq	%xmm9,%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-48(%r8),%xmm4

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-48(%r9),%xmm9
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpshufb	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpinsrd	$1,-48(%r10),%xmm4,%xmm4
	vpinsrd	$1,-48(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpandn	%xmm10,%xmm13,%xmm7
	vpand	%xmm14,%xmm13,%xmm6

	vmovdqa	%xmm3,48-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpunpckldq	%xmm9,%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-44(%r8),%xmm0

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-44(%r9),%xmm9
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpshufb	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpinsrd	$1,-44(%r10),%xmm0,%xmm0
	vpinsrd	$1,-44(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpandn	%xmm14,%xmm12,%xmm7
	vpand	%xmm13,%xmm12,%xmm6

	vmovdqa	%xmm4,64-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpunpckldq	%xmm9,%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-40(%r8),%xmm1

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-40(%r9),%xmm9
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpshufb	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpinsrd	$1,-40(%r10),%xmm1,%xmm1
	vpinsrd	$1,-40(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpandn	%xmm13,%xmm11,%xmm7
	vpand	%xmm12,%xmm11,%xmm6

	vmovdqa	%xmm0,80-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpunpckldq	%xmm9,%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-36(%r8),%xmm2

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-36(%r9),%xmm9
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpshufb	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpinsrd	$1,-36(%r10),%xmm2,%xmm2
	vpinsrd	$1,-36(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpandn	%xmm12,%xmm10,%xmm7
	vpand	%xmm11,%xmm10,%xmm6

	vmovdqa	%xmm1,96-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpunpckldq	%xmm9,%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-32(%r8),%xmm3

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-32(%r9),%xmm9
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpshufb	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpinsrd	$1,-32(%r10),%xmm3,%xmm3
	vpinsrd	$1,-32(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpandn	%xmm11,%xmm14,%xmm7
	vpand	%xmm10,%xmm14,%xmm6

	vmovdqa	%xmm2,112-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpunpckldq	%xmm9,%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-28(%r8),%xmm4

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-28(%r9),%xmm9
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpshufb	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpinsrd	$1,-28(%r10),%xmm4,%xmm4
	vpinsrd	$1,-28(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpandn	%xmm10,%xmm13,%xmm7
	vpand	%xmm14,%xmm13,%xmm6

	vmovdqa	%xmm3,128-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpunpckldq	%xmm9,%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-24(%r8),%xmm0

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-24(%r9),%xmm9
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpshufb	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpinsrd	$1,-24(%r10),%xmm0,%xmm0
	vpinsrd	$1,-24(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpandn	%xmm14,%xmm12,%xmm7
	vpand	%xmm13,%xmm12,%xmm6

	vmovdqa	%xmm4,144-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpunpckldq	%xmm9,%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-20(%r8),%xmm1

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-20(%r9),%xmm9
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpshufb	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpinsrd	$1,-20(%r10),%xmm1,%xmm1
	vpinsrd	$1,-20(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpandn	%xmm13,%xmm11,%xmm7
	vpand	%xmm12,%xmm11,%xmm6

	vmovdqa	%xmm0,160-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpunpckldq	%xmm9,%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-16(%r8),%xmm2

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-16(%r9),%xmm9
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpshufb	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpinsrd	$1,-16(%r10),%xmm2,%xmm2
	vpinsrd	$1,-16(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpandn	%xmm12,%xmm10,%xmm7
	vpand	%xmm11,%xmm10,%xmm6

	vmovdqa	%xmm1,176-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpunpckldq	%xmm9,%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-12(%r8),%xmm3

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-12(%r9),%xmm9
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpshufb	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpinsrd	$1,-12(%r10),%xmm3,%xmm3
	vpinsrd	$1,-12(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpandn	%xmm11,%xmm14,%xmm7
	vpand	%xmm10,%xmm14,%xmm6

	vmovdqa	%xmm2,192-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpunpckldq	%xmm9,%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-8(%r8),%xmm4

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-8(%r9),%xmm9
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpshufb	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpinsrd	$1,-8(%r10),%xmm4,%xmm4
	vpinsrd	$1,-8(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpandn	%xmm10,%xmm13,%xmm7
	vpand	%xmm14,%xmm13,%xmm6

	vmovdqa	%xmm3,208-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpunpckldq	%xmm9,%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vmovd	-4(%r8),%xmm0

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vmovd	-4(%r9),%xmm9
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpshufb	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vmovdqa	0-128(%rax),%xmm1
	vpinsrd	$1,-4(%r10),%xmm0,%xmm0
	vpinsrd	$1,-4(%r11),%xmm9,%xmm9
	vpaddd	%xmm15,%xmm10,%xmm10
	prefetcht0	63(%r8)
	vpslld	$5,%xmm11,%xmm8
	vpandn	%xmm14,%xmm12,%xmm7
	vpand	%xmm13,%xmm12,%xmm6

	vmovdqa	%xmm4,224-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpunpckldq	%xmm9,%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	prefetcht0	63(%r9)
	vpxor	%xmm7,%xmm6,%xmm6

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	prefetcht0	63(%r10)
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	prefetcht0	63(%r11)
	vpshufb	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vmovdqa	16-128(%rax),%xmm2
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	32-128(%rax),%xmm3

	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpandn	%xmm13,%xmm11,%xmm7

	vpand	%xmm12,%xmm11,%xmm6

	vmovdqa	%xmm0,240-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	128-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1


	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11

	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	48-128(%rax),%xmm4

	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpandn	%xmm12,%xmm10,%xmm7

	vpand	%xmm11,%xmm10,%xmm6

	vmovdqa	%xmm1,0-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	144-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2


	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10

	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	64-128(%rax),%xmm0

	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpandn	%xmm11,%xmm14,%xmm7

	vpand	%xmm10,%xmm14,%xmm6

	vmovdqa	%xmm2,16-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	160-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3


	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14

	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	80-128(%rax),%xmm1

	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpandn	%xmm10,%xmm13,%xmm7

	vpand	%xmm14,%xmm13,%xmm6

	vmovdqa	%xmm3,32-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	176-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4


	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13

	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	96-128(%rax),%xmm2

	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpandn	%xmm14,%xmm12,%xmm7

	vpand	%xmm13,%xmm12,%xmm6

	vmovdqa	%xmm4,48-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	192-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm7,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0


	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12

	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vmovdqa	0(%rbp),%xmm15
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	112-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,64-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	208-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	128-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,80-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	224-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	144-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,96-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	240-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	160-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,112-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	0-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	176-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,128-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	16-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	192-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,144-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	32-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	208-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,160-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	48-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	224-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,176-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	64-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	240-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,192-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	80-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	0-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,208-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	96-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	16-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,224-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	112-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	32-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,240-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	128-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	48-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,0-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	144-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	64-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,16-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	160-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	80-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,32-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	176-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	96-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,48-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	192-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	112-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,64-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	208-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	128-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,80-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	224-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	144-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,96-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	240-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	160-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,112-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	0-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vmovdqa	32(%rbp),%xmm15
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	176-128(%rax),%xmm3

	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpand	%xmm12,%xmm13,%xmm7
	vpxor	16-128(%rax),%xmm1,%xmm1

	vpaddd	%xmm7,%xmm14,%xmm14
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm13,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vmovdqu	%xmm0,128-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm1,%xmm5
	vpand	%xmm11,%xmm6,%xmm6
	vpaddd	%xmm1,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	192-128(%rax),%xmm4

	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpand	%xmm11,%xmm12,%xmm7
	vpxor	32-128(%rax),%xmm2,%xmm2

	vpaddd	%xmm7,%xmm13,%xmm13
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm12,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vmovdqu	%xmm1,144-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm2,%xmm5
	vpand	%xmm10,%xmm6,%xmm6
	vpaddd	%xmm2,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	208-128(%rax),%xmm0

	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpand	%xmm10,%xmm11,%xmm7
	vpxor	48-128(%rax),%xmm3,%xmm3

	vpaddd	%xmm7,%xmm12,%xmm12
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm11,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vmovdqu	%xmm2,160-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm3,%xmm5
	vpand	%xmm14,%xmm6,%xmm6
	vpaddd	%xmm3,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	224-128(%rax),%xmm1

	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpand	%xmm14,%xmm10,%xmm7
	vpxor	64-128(%rax),%xmm4,%xmm4

	vpaddd	%xmm7,%xmm11,%xmm11
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm10,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vmovdqu	%xmm3,176-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm4,%xmm5
	vpand	%xmm13,%xmm6,%xmm6
	vpaddd	%xmm4,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	240-128(%rax),%xmm2

	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpand	%xmm13,%xmm14,%xmm7
	vpxor	80-128(%rax),%xmm0,%xmm0

	vpaddd	%xmm7,%xmm10,%xmm10
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm14,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vmovdqu	%xmm4,192-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm0,%xmm5
	vpand	%xmm12,%xmm6,%xmm6
	vpaddd	%xmm0,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	0-128(%rax),%xmm3

	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpand	%xmm12,%xmm13,%xmm7
	vpxor	96-128(%rax),%xmm1,%xmm1

	vpaddd	%xmm7,%xmm14,%xmm14
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm13,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vmovdqu	%xmm0,208-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm1,%xmm5
	vpand	%xmm11,%xmm6,%xmm6
	vpaddd	%xmm1,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	16-128(%rax),%xmm4

	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpand	%xmm11,%xmm12,%xmm7
	vpxor	112-128(%rax),%xmm2,%xmm2

	vpaddd	%xmm7,%xmm13,%xmm13
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm12,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vmovdqu	%xmm1,224-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm2,%xmm5
	vpand	%xmm10,%xmm6,%xmm6
	vpaddd	%xmm2,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	32-128(%rax),%xmm0

	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpand	%xmm10,%xmm11,%xmm7
	vpxor	128-128(%rax),%xmm3,%xmm3

	vpaddd	%xmm7,%xmm12,%xmm12
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm11,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vmovdqu	%xmm2,240-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm3,%xmm5
	vpand	%xmm14,%xmm6,%xmm6
	vpaddd	%xmm3,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	48-128(%rax),%xmm1

	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpand	%xmm14,%xmm10,%xmm7
	vpxor	144-128(%rax),%xmm4,%xmm4

	vpaddd	%xmm7,%xmm11,%xmm11
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm10,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vmovdqu	%xmm3,0-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm4,%xmm5
	vpand	%xmm13,%xmm6,%xmm6
	vpaddd	%xmm4,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	64-128(%rax),%xmm2

	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpand	%xmm13,%xmm14,%xmm7
	vpxor	160-128(%rax),%xmm0,%xmm0

	vpaddd	%xmm7,%xmm10,%xmm10
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm14,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vmovdqu	%xmm4,16-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm0,%xmm5
	vpand	%xmm12,%xmm6,%xmm6
	vpaddd	%xmm0,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	80-128(%rax),%xmm3

	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpand	%xmm12,%xmm13,%xmm7
	vpxor	176-128(%rax),%xmm1,%xmm1

	vpaddd	%xmm7,%xmm14,%xmm14
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm13,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vmovdqu	%xmm0,32-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm1,%xmm5
	vpand	%xmm11,%xmm6,%xmm6
	vpaddd	%xmm1,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	96-128(%rax),%xmm4

	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpand	%xmm11,%xmm12,%xmm7
	vpxor	192-128(%rax),%xmm2,%xmm2

	vpaddd	%xmm7,%xmm13,%xmm13
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm12,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vmovdqu	%xmm1,48-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm2,%xmm5
	vpand	%xmm10,%xmm6,%xmm6
	vpaddd	%xmm2,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	112-128(%rax),%xmm0

	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpand	%xmm10,%xmm11,%xmm7
	vpxor	208-128(%rax),%xmm3,%xmm3

	vpaddd	%xmm7,%xmm12,%xmm12
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm11,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vmovdqu	%xmm2,64-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm3,%xmm5
	vpand	%xmm14,%xmm6,%xmm6
	vpaddd	%xmm3,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	128-128(%rax),%xmm1

	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpand	%xmm14,%xmm10,%xmm7
	vpxor	224-128(%rax),%xmm4,%xmm4

	vpaddd	%xmm7,%xmm11,%xmm11
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm10,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vmovdqu	%xmm3,80-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm4,%xmm5
	vpand	%xmm13,%xmm6,%xmm6
	vpaddd	%xmm4,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	144-128(%rax),%xmm2

	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpand	%xmm13,%xmm14,%xmm7
	vpxor	240-128(%rax),%xmm0,%xmm0

	vpaddd	%xmm7,%xmm10,%xmm10
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm14,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vmovdqu	%xmm4,96-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm0,%xmm5
	vpand	%xmm12,%xmm6,%xmm6
	vpaddd	%xmm0,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	160-128(%rax),%xmm3

	vpaddd	%xmm15,%xmm14,%xmm14
	vpslld	$5,%xmm10,%xmm8
	vpand	%xmm12,%xmm13,%xmm7
	vpxor	0-128(%rax),%xmm1,%xmm1

	vpaddd	%xmm7,%xmm14,%xmm14
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm13,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vmovdqu	%xmm0,112-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm1,%xmm5
	vpand	%xmm11,%xmm6,%xmm6
	vpaddd	%xmm1,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpaddd	%xmm6,%xmm14,%xmm14

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	176-128(%rax),%xmm4

	vpaddd	%xmm15,%xmm13,%xmm13
	vpslld	$5,%xmm14,%xmm8
	vpand	%xmm11,%xmm12,%xmm7
	vpxor	16-128(%rax),%xmm2,%xmm2

	vpaddd	%xmm7,%xmm13,%xmm13
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm12,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vmovdqu	%xmm1,128-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm2,%xmm5
	vpand	%xmm10,%xmm6,%xmm6
	vpaddd	%xmm2,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpaddd	%xmm6,%xmm13,%xmm13

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	192-128(%rax),%xmm0

	vpaddd	%xmm15,%xmm12,%xmm12
	vpslld	$5,%xmm13,%xmm8
	vpand	%xmm10,%xmm11,%xmm7
	vpxor	32-128(%rax),%xmm3,%xmm3

	vpaddd	%xmm7,%xmm12,%xmm12
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm11,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vmovdqu	%xmm2,144-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm3,%xmm5
	vpand	%xmm14,%xmm6,%xmm6
	vpaddd	%xmm3,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpaddd	%xmm6,%xmm12,%xmm12

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	208-128(%rax),%xmm1

	vpaddd	%xmm15,%xmm11,%xmm11
	vpslld	$5,%xmm12,%xmm8
	vpand	%xmm14,%xmm10,%xmm7
	vpxor	48-128(%rax),%xmm4,%xmm4

	vpaddd	%xmm7,%xmm11,%xmm11
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm10,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vmovdqu	%xmm3,160-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm4,%xmm5
	vpand	%xmm13,%xmm6,%xmm6
	vpaddd	%xmm4,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpaddd	%xmm6,%xmm11,%xmm11

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	224-128(%rax),%xmm2

	vpaddd	%xmm15,%xmm10,%xmm10
	vpslld	$5,%xmm11,%xmm8
	vpand	%xmm13,%xmm14,%xmm7
	vpxor	64-128(%rax),%xmm0,%xmm0

	vpaddd	%xmm7,%xmm10,%xmm10
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm14,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vmovdqu	%xmm4,176-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpor	%xmm9,%xmm8,%xmm8
	vpsrld	$31,%xmm0,%xmm5
	vpand	%xmm12,%xmm6,%xmm6
	vpaddd	%xmm0,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vmovdqa	64(%rbp),%xmm15
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	240-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,192-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	80-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	0-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,208-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	96-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	16-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,224-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	112-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	32-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,240-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	128-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	48-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,0-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	144-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	64-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,16-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	160-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	80-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,32-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	176-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	96-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vmovdqa	%xmm2,48-128(%rax)
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	192-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	112-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vmovdqa	%xmm3,64-128(%rax)
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	208-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	128-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vmovdqa	%xmm4,80-128(%rax)
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	224-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	144-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vmovdqa	%xmm0,96-128(%rax)
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	240-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	160-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vmovdqa	%xmm1,112-128(%rax)
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	0-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	176-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	16-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	192-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	32-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpxor	%xmm2,%xmm0,%xmm0
	vmovdqa	208-128(%rax),%xmm2

	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	48-128(%rax),%xmm0,%xmm0
	vpsrld	$27,%xmm11,%xmm9
	vpxor	%xmm13,%xmm6,%xmm6
	vpxor	%xmm2,%xmm0,%xmm0

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10
	vpsrld	$31,%xmm0,%xmm5
	vpaddd	%xmm0,%xmm0,%xmm0

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm5,%xmm0,%xmm0
	vpor	%xmm7,%xmm12,%xmm12
	vpxor	%xmm3,%xmm1,%xmm1
	vmovdqa	224-128(%rax),%xmm3

	vpslld	$5,%xmm10,%xmm8
	vpaddd	%xmm15,%xmm14,%xmm14
	vpxor	%xmm11,%xmm13,%xmm6
	vpaddd	%xmm0,%xmm14,%xmm14
	vpxor	64-128(%rax),%xmm1,%xmm1
	vpsrld	$27,%xmm10,%xmm9
	vpxor	%xmm12,%xmm6,%xmm6
	vpxor	%xmm3,%xmm1,%xmm1

	vpslld	$30,%xmm11,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm14,%xmm14
	vpsrld	$31,%xmm1,%xmm5
	vpaddd	%xmm1,%xmm1,%xmm1

	vpsrld	$2,%xmm11,%xmm11
	vpaddd	%xmm8,%xmm14,%xmm14
	vpor	%xmm5,%xmm1,%xmm1
	vpor	%xmm7,%xmm11,%xmm11
	vpxor	%xmm4,%xmm2,%xmm2
	vmovdqa	240-128(%rax),%xmm4

	vpslld	$5,%xmm14,%xmm8
	vpaddd	%xmm15,%xmm13,%xmm13
	vpxor	%xmm10,%xmm12,%xmm6
	vpaddd	%xmm1,%xmm13,%xmm13
	vpxor	80-128(%rax),%xmm2,%xmm2
	vpsrld	$27,%xmm14,%xmm9
	vpxor	%xmm11,%xmm6,%xmm6
	vpxor	%xmm4,%xmm2,%xmm2

	vpslld	$30,%xmm10,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm13,%xmm13
	vpsrld	$31,%xmm2,%xmm5
	vpaddd	%xmm2,%xmm2,%xmm2

	vpsrld	$2,%xmm10,%xmm10
	vpaddd	%xmm8,%xmm13,%xmm13
	vpor	%xmm5,%xmm2,%xmm2
	vpor	%xmm7,%xmm10,%xmm10
	vpxor	%xmm0,%xmm3,%xmm3
	vmovdqa	0-128(%rax),%xmm0

	vpslld	$5,%xmm13,%xmm8
	vpaddd	%xmm15,%xmm12,%xmm12
	vpxor	%xmm14,%xmm11,%xmm6
	vpaddd	%xmm2,%xmm12,%xmm12
	vpxor	96-128(%rax),%xmm3,%xmm3
	vpsrld	$27,%xmm13,%xmm9
	vpxor	%xmm10,%xmm6,%xmm6
	vpxor	%xmm0,%xmm3,%xmm3

	vpslld	$30,%xmm14,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12
	vpsrld	$31,%xmm3,%xmm5
	vpaddd	%xmm3,%xmm3,%xmm3

	vpsrld	$2,%xmm14,%xmm14
	vpaddd	%xmm8,%xmm12,%xmm12
	vpor	%xmm5,%xmm3,%xmm3
	vpor	%xmm7,%xmm14,%xmm14
	vpxor	%xmm1,%xmm4,%xmm4
	vmovdqa	16-128(%rax),%xmm1

	vpslld	$5,%xmm12,%xmm8
	vpaddd	%xmm15,%xmm11,%xmm11
	vpxor	%xmm13,%xmm10,%xmm6
	vpaddd	%xmm3,%xmm11,%xmm11
	vpxor	112-128(%rax),%xmm4,%xmm4
	vpsrld	$27,%xmm12,%xmm9
	vpxor	%xmm14,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm4

	vpslld	$30,%xmm13,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm11,%xmm11
	vpsrld	$31,%xmm4,%xmm5
	vpaddd	%xmm4,%xmm4,%xmm4

	vpsrld	$2,%xmm13,%xmm13
	vpaddd	%xmm8,%xmm11,%xmm11
	vpor	%xmm5,%xmm4,%xmm4
	vpor	%xmm7,%xmm13,%xmm13
	vpslld	$5,%xmm11,%xmm8
	vpaddd	%xmm15,%xmm10,%xmm10
	vpxor	%xmm12,%xmm14,%xmm6

	vpsrld	$27,%xmm11,%xmm9
	vpaddd	%xmm4,%xmm10,%xmm10
	vpxor	%xmm13,%xmm6,%xmm6

	vpslld	$30,%xmm12,%xmm7
	vpor	%xmm9,%xmm8,%xmm8
	vpaddd	%xmm6,%xmm10,%xmm10

	vpsrld	$2,%xmm12,%xmm12
	vpaddd	%xmm8,%xmm10,%xmm10
	vpor	%xmm7,%xmm12,%xmm12
	movl	$1,%ecx
	cmpl	0(%rbx),%ecx
	cmovgeq	%rbp,%r8
	cmpl	4(%rbx),%ecx
	cmovgeq	%rbp,%r9
	cmpl	8(%rbx),%ecx
	cmovgeq	%rbp,%r10
	cmpl	12(%rbx),%ecx
	cmovgeq	%rbp,%r11
	vmovdqu	(%rbx),%xmm6
	vpxor	%xmm8,%xmm8,%xmm8
	vmovdqa	%xmm6,%xmm7
	vpcmpgtd	%xmm8,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6

	vpand	%xmm7,%xmm10,%xmm10
	vpand	%xmm7,%xmm11,%xmm11
	vpaddd	0(%rdi),%xmm10,%xmm10
	vpand	%xmm7,%xmm12,%xmm12
	vpaddd	32(%rdi),%xmm11,%xmm11
	vpand	%xmm7,%xmm13,%xmm13
	vpaddd	64(%rdi),%xmm12,%xmm12
	vpand	%xmm7,%xmm14,%xmm14
	vpaddd	96(%rdi),%xmm13,%xmm13
	vpaddd	128(%rdi),%xmm14,%xmm14
	vmovdqu	%xmm10,0(%rdi)
	vmovdqu	%xmm11,32(%rdi)
	vmovdqu	%xmm12,64(%rdi)
	vmovdqu	%xmm13,96(%rdi)
	vmovdqu	%xmm14,128(%rdi)

	vmovdqu	%xmm6,(%rbx)
	vmovdqu	96(%rbp),%xmm5
	decl	%edx
	jnz	.Loop_avx

	movl	280(%rsp),%edx
	leaq	16(%rdi),%rdi
	leaq	64(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande_avx

.Ldone_avx:
	movq	272(%rsp),%rax
.cfi_def_cfa	%rax,8
	vzeroupper
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_multi_block_avx,.-sha1_multi_block_avx
.type	sha1_multi_block_avx2,@function
.align	32
sha1_multi_block_avx2:
.cfi_startproc	
_avx2_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	subq	$576,%rsp
	andq	$-256,%rsp
	movq	%rax,544(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xa0,0x04,0x06,0x23,0x08
.Lbody_avx2:
	leaq	K_XX_XX(%rip),%rbp
	shrl	$1,%edx

	vzeroupper
.Loop_grande_avx2:
	movl	%edx,552(%rsp)
	xorl	%edx,%edx
	leaq	512(%rsp),%rbx

	movq	0(%rsi),%r12

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r12

	movq	16(%rsi),%r13

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r13

	movq	32(%rsi),%r14

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r14

	movq	48(%rsi),%r15

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r15

	movq	64(%rsi),%r8

	movl	72(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,16(%rbx)
	cmovleq	%rbp,%r8

	movq	80(%rsi),%r9

	movl	88(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,20(%rbx)
	cmovleq	%rbp,%r9

	movq	96(%rsi),%r10

	movl	104(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,24(%rbx)
	cmovleq	%rbp,%r10

	movq	112(%rsi),%r11

	movl	120(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,28(%rbx)
	cmovleq	%rbp,%r11
	vmovdqu	0(%rdi),%ymm0
	leaq	128(%rsp),%rax
	vmovdqu	32(%rdi),%ymm1
	leaq	256+128(%rsp),%rbx
	vmovdqu	64(%rdi),%ymm2
	vmovdqu	96(%rdi),%ymm3
	vmovdqu	128(%rdi),%ymm4
	vmovdqu	96(%rbp),%ymm9
	jmp	.Loop_avx2

.align	32
.Loop_avx2:
	vmovdqa	-32(%rbp),%ymm15
	vmovd	(%r12),%xmm10
	leaq	64(%r12),%r12
	vmovd	(%r8),%xmm12
	leaq	64(%r8),%r8
	vmovd	(%r13),%xmm7
	leaq	64(%r13),%r13
	vmovd	(%r9),%xmm6
	leaq	64(%r9),%r9
	vpinsrd	$1,(%r14),%xmm10,%xmm10
	leaq	64(%r14),%r14
	vpinsrd	$1,(%r10),%xmm12,%xmm12
	leaq	64(%r10),%r10
	vpinsrd	$1,(%r15),%xmm7,%xmm7
	leaq	64(%r15),%r15
	vpunpckldq	%ymm7,%ymm10,%ymm10
	vpinsrd	$1,(%r11),%xmm6,%xmm6
	leaq	64(%r11),%r11
	vpunpckldq	%ymm6,%ymm12,%ymm12
	vmovd	-60(%r12),%xmm11
	vinserti128	$1,%xmm12,%ymm10,%ymm10
	vmovd	-60(%r8),%xmm8
	vpshufb	%ymm9,%ymm10,%ymm10
	vmovd	-60(%r13),%xmm7
	vmovd	-60(%r9),%xmm6
	vpinsrd	$1,-60(%r14),%xmm11,%xmm11
	vpinsrd	$1,-60(%r10),%xmm8,%xmm8
	vpinsrd	$1,-60(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm11,%ymm11
	vpinsrd	$1,-60(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpandn	%ymm3,%ymm1,%ymm6
	vpand	%ymm2,%ymm1,%ymm5

	vmovdqa	%ymm10,0-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vinserti128	$1,%xmm8,%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-56(%r12),%xmm12

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-56(%r8),%xmm8
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpshufb	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vmovd	-56(%r13),%xmm7
	vmovd	-56(%r9),%xmm6
	vpinsrd	$1,-56(%r14),%xmm12,%xmm12
	vpinsrd	$1,-56(%r10),%xmm8,%xmm8
	vpinsrd	$1,-56(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm12,%ymm12
	vpinsrd	$1,-56(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpandn	%ymm2,%ymm0,%ymm6
	vpand	%ymm1,%ymm0,%ymm5

	vmovdqa	%ymm11,32-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vinserti128	$1,%xmm8,%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-52(%r12),%xmm13

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-52(%r8),%xmm8
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpshufb	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vmovd	-52(%r13),%xmm7
	vmovd	-52(%r9),%xmm6
	vpinsrd	$1,-52(%r14),%xmm13,%xmm13
	vpinsrd	$1,-52(%r10),%xmm8,%xmm8
	vpinsrd	$1,-52(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm13,%ymm13
	vpinsrd	$1,-52(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpandn	%ymm1,%ymm4,%ymm6
	vpand	%ymm0,%ymm4,%ymm5

	vmovdqa	%ymm12,64-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vinserti128	$1,%xmm8,%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-48(%r12),%xmm14

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-48(%r8),%xmm8
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpshufb	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vmovd	-48(%r13),%xmm7
	vmovd	-48(%r9),%xmm6
	vpinsrd	$1,-48(%r14),%xmm14,%xmm14
	vpinsrd	$1,-48(%r10),%xmm8,%xmm8
	vpinsrd	$1,-48(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm14,%ymm14
	vpinsrd	$1,-48(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpandn	%ymm0,%ymm3,%ymm6
	vpand	%ymm4,%ymm3,%ymm5

	vmovdqa	%ymm13,96-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vinserti128	$1,%xmm8,%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-44(%r12),%xmm10

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-44(%r8),%xmm8
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpshufb	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vmovd	-44(%r13),%xmm7
	vmovd	-44(%r9),%xmm6
	vpinsrd	$1,-44(%r14),%xmm10,%xmm10
	vpinsrd	$1,-44(%r10),%xmm8,%xmm8
	vpinsrd	$1,-44(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm10,%ymm10
	vpinsrd	$1,-44(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpandn	%ymm4,%ymm2,%ymm6
	vpand	%ymm3,%ymm2,%ymm5

	vmovdqa	%ymm14,128-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vinserti128	$1,%xmm8,%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-40(%r12),%xmm11

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-40(%r8),%xmm8
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpshufb	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovd	-40(%r13),%xmm7
	vmovd	-40(%r9),%xmm6
	vpinsrd	$1,-40(%r14),%xmm11,%xmm11
	vpinsrd	$1,-40(%r10),%xmm8,%xmm8
	vpinsrd	$1,-40(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm11,%ymm11
	vpinsrd	$1,-40(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpandn	%ymm3,%ymm1,%ymm6
	vpand	%ymm2,%ymm1,%ymm5

	vmovdqa	%ymm10,160-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vinserti128	$1,%xmm8,%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-36(%r12),%xmm12

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-36(%r8),%xmm8
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpshufb	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vmovd	-36(%r13),%xmm7
	vmovd	-36(%r9),%xmm6
	vpinsrd	$1,-36(%r14),%xmm12,%xmm12
	vpinsrd	$1,-36(%r10),%xmm8,%xmm8
	vpinsrd	$1,-36(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm12,%ymm12
	vpinsrd	$1,-36(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpandn	%ymm2,%ymm0,%ymm6
	vpand	%ymm1,%ymm0,%ymm5

	vmovdqa	%ymm11,192-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vinserti128	$1,%xmm8,%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-32(%r12),%xmm13

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-32(%r8),%xmm8
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpshufb	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vmovd	-32(%r13),%xmm7
	vmovd	-32(%r9),%xmm6
	vpinsrd	$1,-32(%r14),%xmm13,%xmm13
	vpinsrd	$1,-32(%r10),%xmm8,%xmm8
	vpinsrd	$1,-32(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm13,%ymm13
	vpinsrd	$1,-32(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpandn	%ymm1,%ymm4,%ymm6
	vpand	%ymm0,%ymm4,%ymm5

	vmovdqa	%ymm12,224-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vinserti128	$1,%xmm8,%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-28(%r12),%xmm14

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-28(%r8),%xmm8
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpshufb	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vmovd	-28(%r13),%xmm7
	vmovd	-28(%r9),%xmm6
	vpinsrd	$1,-28(%r14),%xmm14,%xmm14
	vpinsrd	$1,-28(%r10),%xmm8,%xmm8
	vpinsrd	$1,-28(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm14,%ymm14
	vpinsrd	$1,-28(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpandn	%ymm0,%ymm3,%ymm6
	vpand	%ymm4,%ymm3,%ymm5

	vmovdqa	%ymm13,256-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vinserti128	$1,%xmm8,%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-24(%r12),%xmm10

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-24(%r8),%xmm8
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpshufb	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vmovd	-24(%r13),%xmm7
	vmovd	-24(%r9),%xmm6
	vpinsrd	$1,-24(%r14),%xmm10,%xmm10
	vpinsrd	$1,-24(%r10),%xmm8,%xmm8
	vpinsrd	$1,-24(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm10,%ymm10
	vpinsrd	$1,-24(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpandn	%ymm4,%ymm2,%ymm6
	vpand	%ymm3,%ymm2,%ymm5

	vmovdqa	%ymm14,288-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vinserti128	$1,%xmm8,%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-20(%r12),%xmm11

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-20(%r8),%xmm8
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpshufb	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovd	-20(%r13),%xmm7
	vmovd	-20(%r9),%xmm6
	vpinsrd	$1,-20(%r14),%xmm11,%xmm11
	vpinsrd	$1,-20(%r10),%xmm8,%xmm8
	vpinsrd	$1,-20(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm11,%ymm11
	vpinsrd	$1,-20(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpandn	%ymm3,%ymm1,%ymm6
	vpand	%ymm2,%ymm1,%ymm5

	vmovdqa	%ymm10,320-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vinserti128	$1,%xmm8,%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-16(%r12),%xmm12

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-16(%r8),%xmm8
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpshufb	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vmovd	-16(%r13),%xmm7
	vmovd	-16(%r9),%xmm6
	vpinsrd	$1,-16(%r14),%xmm12,%xmm12
	vpinsrd	$1,-16(%r10),%xmm8,%xmm8
	vpinsrd	$1,-16(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm12,%ymm12
	vpinsrd	$1,-16(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpandn	%ymm2,%ymm0,%ymm6
	vpand	%ymm1,%ymm0,%ymm5

	vmovdqa	%ymm11,352-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vinserti128	$1,%xmm8,%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-12(%r12),%xmm13

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-12(%r8),%xmm8
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpshufb	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vmovd	-12(%r13),%xmm7
	vmovd	-12(%r9),%xmm6
	vpinsrd	$1,-12(%r14),%xmm13,%xmm13
	vpinsrd	$1,-12(%r10),%xmm8,%xmm8
	vpinsrd	$1,-12(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm13,%ymm13
	vpinsrd	$1,-12(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpandn	%ymm1,%ymm4,%ymm6
	vpand	%ymm0,%ymm4,%ymm5

	vmovdqa	%ymm12,384-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vinserti128	$1,%xmm8,%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-8(%r12),%xmm14

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-8(%r8),%xmm8
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpshufb	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vmovd	-8(%r13),%xmm7
	vmovd	-8(%r9),%xmm6
	vpinsrd	$1,-8(%r14),%xmm14,%xmm14
	vpinsrd	$1,-8(%r10),%xmm8,%xmm8
	vpinsrd	$1,-8(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm14,%ymm14
	vpinsrd	$1,-8(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpandn	%ymm0,%ymm3,%ymm6
	vpand	%ymm4,%ymm3,%ymm5

	vmovdqa	%ymm13,416-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vinserti128	$1,%xmm8,%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vmovd	-4(%r12),%xmm10

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vmovd	-4(%r8),%xmm8
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpshufb	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vmovdqa	0-128(%rax),%ymm11
	vmovd	-4(%r13),%xmm7
	vmovd	-4(%r9),%xmm6
	vpinsrd	$1,-4(%r14),%xmm10,%xmm10
	vpinsrd	$1,-4(%r10),%xmm8,%xmm8
	vpinsrd	$1,-4(%r15),%xmm7,%xmm7
	vpunpckldq	%ymm7,%ymm10,%ymm10
	vpinsrd	$1,-4(%r11),%xmm6,%xmm6
	vpunpckldq	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm15,%ymm0,%ymm0
	prefetcht0	63(%r12)
	vpslld	$5,%ymm1,%ymm7
	vpandn	%ymm4,%ymm2,%ymm6
	vpand	%ymm3,%ymm2,%ymm5

	vmovdqa	%ymm14,448-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vinserti128	$1,%xmm8,%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	prefetcht0	63(%r13)
	vpxor	%ymm6,%ymm5,%ymm5

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	prefetcht0	63(%r14)
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	prefetcht0	63(%r15)
	vpshufb	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovdqa	32-128(%rax),%ymm12
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	64-128(%rax),%ymm13

	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpandn	%ymm3,%ymm1,%ymm6
	prefetcht0	63(%r8)
	vpand	%ymm2,%ymm1,%ymm5

	vmovdqa	%ymm10,480-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	256-256-128(%rbx),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11
	prefetcht0	63(%r9)

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	prefetcht0	63(%r10)
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	prefetcht0	63(%r11)
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	96-128(%rax),%ymm14

	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpandn	%ymm2,%ymm0,%ymm6

	vpand	%ymm1,%ymm0,%ymm5

	vmovdqa	%ymm11,0-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	288-256-128(%rbx),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12


	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0

	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	128-128(%rax),%ymm10

	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpandn	%ymm1,%ymm4,%ymm6

	vpand	%ymm0,%ymm4,%ymm5

	vmovdqa	%ymm12,32-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	320-256-128(%rbx),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13


	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4

	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	160-128(%rax),%ymm11

	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpandn	%ymm0,%ymm3,%ymm6

	vpand	%ymm4,%ymm3,%ymm5

	vmovdqa	%ymm13,64-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	352-256-128(%rbx),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14


	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3

	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	192-128(%rax),%ymm12

	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpandn	%ymm4,%ymm2,%ymm6

	vpand	%ymm3,%ymm2,%ymm5

	vmovdqa	%ymm14,96-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	384-256-128(%rbx),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm6,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10


	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2

	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovdqa	0(%rbp),%ymm15
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	224-128(%rax),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,128-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	416-256-128(%rbx),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	256-256-128(%rbx),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,160-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	448-256-128(%rbx),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	288-256-128(%rbx),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,192-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	480-256-128(%rbx),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	320-256-128(%rbx),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,224-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	0-128(%rax),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	352-256-128(%rbx),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,256-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	32-128(%rax),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	384-256-128(%rbx),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,288-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	64-128(%rax),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	416-256-128(%rbx),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,320-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	96-128(%rax),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	448-256-128(%rbx),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,352-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	128-128(%rax),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	480-256-128(%rbx),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,384-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	160-128(%rax),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	0-128(%rax),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,416-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	192-128(%rax),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	32-128(%rax),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,448-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	224-128(%rax),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	64-128(%rax),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,480-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	256-256-128(%rbx),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	96-128(%rax),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,0-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	288-256-128(%rbx),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	128-128(%rax),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,32-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	320-256-128(%rbx),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	160-128(%rax),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,64-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	352-256-128(%rbx),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	192-128(%rax),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,96-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	384-256-128(%rbx),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	224-128(%rax),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,128-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	416-256-128(%rbx),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	256-256-128(%rbx),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,160-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	448-256-128(%rbx),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	288-256-128(%rbx),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,192-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	480-256-128(%rbx),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	320-256-128(%rbx),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,224-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	0-128(%rax),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovdqa	32(%rbp),%ymm15
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	352-256-128(%rbx),%ymm13

	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpand	%ymm2,%ymm3,%ymm6
	vpxor	32-128(%rax),%ymm11,%ymm11

	vpaddd	%ymm6,%ymm4,%ymm4
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm3,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vmovdqu	%ymm10,256-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm11,%ymm9
	vpand	%ymm1,%ymm5,%ymm5
	vpaddd	%ymm11,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	384-256-128(%rbx),%ymm14

	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpand	%ymm1,%ymm2,%ymm6
	vpxor	64-128(%rax),%ymm12,%ymm12

	vpaddd	%ymm6,%ymm3,%ymm3
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm2,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vmovdqu	%ymm11,288-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm12,%ymm9
	vpand	%ymm0,%ymm5,%ymm5
	vpaddd	%ymm12,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	416-256-128(%rbx),%ymm10

	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpand	%ymm0,%ymm1,%ymm6
	vpxor	96-128(%rax),%ymm13,%ymm13

	vpaddd	%ymm6,%ymm2,%ymm2
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm1,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vmovdqu	%ymm12,320-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm13,%ymm9
	vpand	%ymm4,%ymm5,%ymm5
	vpaddd	%ymm13,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	448-256-128(%rbx),%ymm11

	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpand	%ymm4,%ymm0,%ymm6
	vpxor	128-128(%rax),%ymm14,%ymm14

	vpaddd	%ymm6,%ymm1,%ymm1
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm0,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vmovdqu	%ymm13,352-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm14,%ymm9
	vpand	%ymm3,%ymm5,%ymm5
	vpaddd	%ymm14,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	480-256-128(%rbx),%ymm12

	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpand	%ymm3,%ymm4,%ymm6
	vpxor	160-128(%rax),%ymm10,%ymm10

	vpaddd	%ymm6,%ymm0,%ymm0
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm4,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vmovdqu	%ymm14,384-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm10,%ymm9
	vpand	%ymm2,%ymm5,%ymm5
	vpaddd	%ymm10,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	0-128(%rax),%ymm13

	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpand	%ymm2,%ymm3,%ymm6
	vpxor	192-128(%rax),%ymm11,%ymm11

	vpaddd	%ymm6,%ymm4,%ymm4
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm3,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vmovdqu	%ymm10,416-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm11,%ymm9
	vpand	%ymm1,%ymm5,%ymm5
	vpaddd	%ymm11,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	32-128(%rax),%ymm14

	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpand	%ymm1,%ymm2,%ymm6
	vpxor	224-128(%rax),%ymm12,%ymm12

	vpaddd	%ymm6,%ymm3,%ymm3
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm2,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vmovdqu	%ymm11,448-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm12,%ymm9
	vpand	%ymm0,%ymm5,%ymm5
	vpaddd	%ymm12,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	64-128(%rax),%ymm10

	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpand	%ymm0,%ymm1,%ymm6
	vpxor	256-256-128(%rbx),%ymm13,%ymm13

	vpaddd	%ymm6,%ymm2,%ymm2
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm1,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vmovdqu	%ymm12,480-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm13,%ymm9
	vpand	%ymm4,%ymm5,%ymm5
	vpaddd	%ymm13,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	96-128(%rax),%ymm11

	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpand	%ymm4,%ymm0,%ymm6
	vpxor	288-256-128(%rbx),%ymm14,%ymm14

	vpaddd	%ymm6,%ymm1,%ymm1
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm0,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vmovdqu	%ymm13,0-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm14,%ymm9
	vpand	%ymm3,%ymm5,%ymm5
	vpaddd	%ymm14,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	128-128(%rax),%ymm12

	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpand	%ymm3,%ymm4,%ymm6
	vpxor	320-256-128(%rbx),%ymm10,%ymm10

	vpaddd	%ymm6,%ymm0,%ymm0
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm4,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vmovdqu	%ymm14,32-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm10,%ymm9
	vpand	%ymm2,%ymm5,%ymm5
	vpaddd	%ymm10,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	160-128(%rax),%ymm13

	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpand	%ymm2,%ymm3,%ymm6
	vpxor	352-256-128(%rbx),%ymm11,%ymm11

	vpaddd	%ymm6,%ymm4,%ymm4
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm3,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vmovdqu	%ymm10,64-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm11,%ymm9
	vpand	%ymm1,%ymm5,%ymm5
	vpaddd	%ymm11,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	192-128(%rax),%ymm14

	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpand	%ymm1,%ymm2,%ymm6
	vpxor	384-256-128(%rbx),%ymm12,%ymm12

	vpaddd	%ymm6,%ymm3,%ymm3
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm2,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vmovdqu	%ymm11,96-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm12,%ymm9
	vpand	%ymm0,%ymm5,%ymm5
	vpaddd	%ymm12,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	224-128(%rax),%ymm10

	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpand	%ymm0,%ymm1,%ymm6
	vpxor	416-256-128(%rbx),%ymm13,%ymm13

	vpaddd	%ymm6,%ymm2,%ymm2
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm1,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vmovdqu	%ymm12,128-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm13,%ymm9
	vpand	%ymm4,%ymm5,%ymm5
	vpaddd	%ymm13,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	256-256-128(%rbx),%ymm11

	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpand	%ymm4,%ymm0,%ymm6
	vpxor	448-256-128(%rbx),%ymm14,%ymm14

	vpaddd	%ymm6,%ymm1,%ymm1
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm0,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vmovdqu	%ymm13,160-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm14,%ymm9
	vpand	%ymm3,%ymm5,%ymm5
	vpaddd	%ymm14,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	288-256-128(%rbx),%ymm12

	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpand	%ymm3,%ymm4,%ymm6
	vpxor	480-256-128(%rbx),%ymm10,%ymm10

	vpaddd	%ymm6,%ymm0,%ymm0
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm4,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vmovdqu	%ymm14,192-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm10,%ymm9
	vpand	%ymm2,%ymm5,%ymm5
	vpaddd	%ymm10,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	320-256-128(%rbx),%ymm13

	vpaddd	%ymm15,%ymm4,%ymm4
	vpslld	$5,%ymm0,%ymm7
	vpand	%ymm2,%ymm3,%ymm6
	vpxor	0-128(%rax),%ymm11,%ymm11

	vpaddd	%ymm6,%ymm4,%ymm4
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm3,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vmovdqu	%ymm10,224-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm11,%ymm9
	vpand	%ymm1,%ymm5,%ymm5
	vpaddd	%ymm11,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpaddd	%ymm5,%ymm4,%ymm4

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	352-256-128(%rbx),%ymm14

	vpaddd	%ymm15,%ymm3,%ymm3
	vpslld	$5,%ymm4,%ymm7
	vpand	%ymm1,%ymm2,%ymm6
	vpxor	32-128(%rax),%ymm12,%ymm12

	vpaddd	%ymm6,%ymm3,%ymm3
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm2,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vmovdqu	%ymm11,256-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm12,%ymm9
	vpand	%ymm0,%ymm5,%ymm5
	vpaddd	%ymm12,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpaddd	%ymm5,%ymm3,%ymm3

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	384-256-128(%rbx),%ymm10

	vpaddd	%ymm15,%ymm2,%ymm2
	vpslld	$5,%ymm3,%ymm7
	vpand	%ymm0,%ymm1,%ymm6
	vpxor	64-128(%rax),%ymm13,%ymm13

	vpaddd	%ymm6,%ymm2,%ymm2
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm1,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vmovdqu	%ymm12,288-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm13,%ymm9
	vpand	%ymm4,%ymm5,%ymm5
	vpaddd	%ymm13,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpaddd	%ymm5,%ymm2,%ymm2

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	416-256-128(%rbx),%ymm11

	vpaddd	%ymm15,%ymm1,%ymm1
	vpslld	$5,%ymm2,%ymm7
	vpand	%ymm4,%ymm0,%ymm6
	vpxor	96-128(%rax),%ymm14,%ymm14

	vpaddd	%ymm6,%ymm1,%ymm1
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm0,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vmovdqu	%ymm13,320-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm14,%ymm9
	vpand	%ymm3,%ymm5,%ymm5
	vpaddd	%ymm14,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpaddd	%ymm5,%ymm1,%ymm1

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	448-256-128(%rbx),%ymm12

	vpaddd	%ymm15,%ymm0,%ymm0
	vpslld	$5,%ymm1,%ymm7
	vpand	%ymm3,%ymm4,%ymm6
	vpxor	128-128(%rax),%ymm10,%ymm10

	vpaddd	%ymm6,%ymm0,%ymm0
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm4,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vmovdqu	%ymm14,352-256-128(%rbx)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm10,%ymm9
	vpand	%ymm2,%ymm5,%ymm5
	vpaddd	%ymm10,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vmovdqa	64(%rbp),%ymm15
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	480-256-128(%rbx),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,384-256-128(%rbx)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	160-128(%rax),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	0-128(%rax),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,416-256-128(%rbx)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	192-128(%rax),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	32-128(%rax),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,448-256-128(%rbx)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	224-128(%rax),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	64-128(%rax),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,480-256-128(%rbx)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	256-256-128(%rbx),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	96-128(%rax),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,0-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	288-256-128(%rbx),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	128-128(%rax),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,32-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	320-256-128(%rbx),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	160-128(%rax),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,64-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	352-256-128(%rbx),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	192-128(%rax),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vmovdqa	%ymm12,96-128(%rax)
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	384-256-128(%rbx),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	224-128(%rax),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vmovdqa	%ymm13,128-128(%rax)
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	416-256-128(%rbx),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	256-256-128(%rbx),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vmovdqa	%ymm14,160-128(%rax)
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	448-256-128(%rbx),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	288-256-128(%rbx),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vmovdqa	%ymm10,192-128(%rax)
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	480-256-128(%rbx),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	320-256-128(%rbx),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vmovdqa	%ymm11,224-128(%rax)
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	0-128(%rax),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	352-256-128(%rbx),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	32-128(%rax),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	384-256-128(%rbx),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	64-128(%rax),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpxor	%ymm12,%ymm10,%ymm10
	vmovdqa	416-256-128(%rbx),%ymm12

	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	96-128(%rax),%ymm10,%ymm10
	vpsrld	$27,%ymm1,%ymm8
	vpxor	%ymm3,%ymm5,%ymm5
	vpxor	%ymm12,%ymm10,%ymm10

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0
	vpsrld	$31,%ymm10,%ymm9
	vpaddd	%ymm10,%ymm10,%ymm10

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm9,%ymm10,%ymm10
	vpor	%ymm6,%ymm2,%ymm2
	vpxor	%ymm13,%ymm11,%ymm11
	vmovdqa	448-256-128(%rbx),%ymm13

	vpslld	$5,%ymm0,%ymm7
	vpaddd	%ymm15,%ymm4,%ymm4
	vpxor	%ymm1,%ymm3,%ymm5
	vpaddd	%ymm10,%ymm4,%ymm4
	vpxor	128-128(%rax),%ymm11,%ymm11
	vpsrld	$27,%ymm0,%ymm8
	vpxor	%ymm2,%ymm5,%ymm5
	vpxor	%ymm13,%ymm11,%ymm11

	vpslld	$30,%ymm1,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm4,%ymm4
	vpsrld	$31,%ymm11,%ymm9
	vpaddd	%ymm11,%ymm11,%ymm11

	vpsrld	$2,%ymm1,%ymm1
	vpaddd	%ymm7,%ymm4,%ymm4
	vpor	%ymm9,%ymm11,%ymm11
	vpor	%ymm6,%ymm1,%ymm1
	vpxor	%ymm14,%ymm12,%ymm12
	vmovdqa	480-256-128(%rbx),%ymm14

	vpslld	$5,%ymm4,%ymm7
	vpaddd	%ymm15,%ymm3,%ymm3
	vpxor	%ymm0,%ymm2,%ymm5
	vpaddd	%ymm11,%ymm3,%ymm3
	vpxor	160-128(%rax),%ymm12,%ymm12
	vpsrld	$27,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm14,%ymm12,%ymm12

	vpslld	$30,%ymm0,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm3,%ymm3
	vpsrld	$31,%ymm12,%ymm9
	vpaddd	%ymm12,%ymm12,%ymm12

	vpsrld	$2,%ymm0,%ymm0
	vpaddd	%ymm7,%ymm3,%ymm3
	vpor	%ymm9,%ymm12,%ymm12
	vpor	%ymm6,%ymm0,%ymm0
	vpxor	%ymm10,%ymm13,%ymm13
	vmovdqa	0-128(%rax),%ymm10

	vpslld	$5,%ymm3,%ymm7
	vpaddd	%ymm15,%ymm2,%ymm2
	vpxor	%ymm4,%ymm1,%ymm5
	vpaddd	%ymm12,%ymm2,%ymm2
	vpxor	192-128(%rax),%ymm13,%ymm13
	vpsrld	$27,%ymm3,%ymm8
	vpxor	%ymm0,%ymm5,%ymm5
	vpxor	%ymm10,%ymm13,%ymm13

	vpslld	$30,%ymm4,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm2,%ymm2
	vpsrld	$31,%ymm13,%ymm9
	vpaddd	%ymm13,%ymm13,%ymm13

	vpsrld	$2,%ymm4,%ymm4
	vpaddd	%ymm7,%ymm2,%ymm2
	vpor	%ymm9,%ymm13,%ymm13
	vpor	%ymm6,%ymm4,%ymm4
	vpxor	%ymm11,%ymm14,%ymm14
	vmovdqa	32-128(%rax),%ymm11

	vpslld	$5,%ymm2,%ymm7
	vpaddd	%ymm15,%ymm1,%ymm1
	vpxor	%ymm3,%ymm0,%ymm5
	vpaddd	%ymm13,%ymm1,%ymm1
	vpxor	224-128(%rax),%ymm14,%ymm14
	vpsrld	$27,%ymm2,%ymm8
	vpxor	%ymm4,%ymm5,%ymm5
	vpxor	%ymm11,%ymm14,%ymm14

	vpslld	$30,%ymm3,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm1,%ymm1
	vpsrld	$31,%ymm14,%ymm9
	vpaddd	%ymm14,%ymm14,%ymm14

	vpsrld	$2,%ymm3,%ymm3
	vpaddd	%ymm7,%ymm1,%ymm1
	vpor	%ymm9,%ymm14,%ymm14
	vpor	%ymm6,%ymm3,%ymm3
	vpslld	$5,%ymm1,%ymm7
	vpaddd	%ymm15,%ymm0,%ymm0
	vpxor	%ymm2,%ymm4,%ymm5

	vpsrld	$27,%ymm1,%ymm8
	vpaddd	%ymm14,%ymm0,%ymm0
	vpxor	%ymm3,%ymm5,%ymm5

	vpslld	$30,%ymm2,%ymm6
	vpor	%ymm8,%ymm7,%ymm7
	vpaddd	%ymm5,%ymm0,%ymm0

	vpsrld	$2,%ymm2,%ymm2
	vpaddd	%ymm7,%ymm0,%ymm0
	vpor	%ymm6,%ymm2,%ymm2
	movl	$1,%ecx
	leaq	512(%rsp),%rbx
	cmpl	0(%rbx),%ecx
	cmovgeq	%rbp,%r12
	cmpl	4(%rbx),%ecx
	cmovgeq	%rbp,%r13
	cmpl	8(%rbx),%ecx
	cmovgeq	%rbp,%r14
	cmpl	12(%rbx),%ecx
	cmovgeq	%rbp,%r15
	cmpl	16(%rbx),%ecx
	cmovgeq	%rbp,%r8
	cmpl	20(%rbx),%ecx
	cmovgeq	%rbp,%r9
	cmpl	24(%rbx),%ecx
	cmovgeq	%rbp,%r10
	cmpl	28(%rbx),%ecx
	cmovgeq	%rbp,%r11
	vmovdqu	(%rbx),%ymm5
	vpxor	%ymm7,%ymm7,%ymm7
	vmovdqa	%ymm5,%ymm6
	vpcmpgtd	%ymm7,%ymm6,%ymm6
	vpaddd	%ymm6,%ymm5,%ymm5

	vpand	%ymm6,%ymm0,%ymm0
	vpand	%ymm6,%ymm1,%ymm1
	vpaddd	0(%rdi),%ymm0,%ymm0
	vpand	%ymm6,%ymm2,%ymm2
	vpaddd	32(%rdi),%ymm1,%ymm1
	vpand	%ymm6,%ymm3,%ymm3
	vpaddd	64(%rdi),%ymm2,%ymm2
	vpand	%ymm6,%ymm4,%ymm4
	vpaddd	96(%rdi),%ymm3,%ymm3
	vpaddd	128(%rdi),%ymm4,%ymm4
	vmovdqu	%ymm0,0(%rdi)
	vmovdqu	%ymm1,32(%rdi)
	vmovdqu	%ymm2,64(%rdi)
	vmovdqu	%ymm3,96(%rdi)
	vmovdqu	%ymm4,128(%rdi)

	vmovdqu	%ymm5,(%rbx)
	leaq	256+128(%rsp),%rbx
	vmovdqu	96(%rbp),%ymm9
	decl	%edx
	jnz	.Loop_avx2







.Ldone_avx2:
	movq	544(%rsp),%rax
.cfi_def_cfa	%rax,8
	vzeroupper
	movq	-48(%rax),%r15
.cfi_restore	%r15
	movq	-40(%rax),%r14
.cfi_restore	%r14
	movq	-32(%rax),%r13
.cfi_restore	%r13
	movq	-24(%rax),%r12
.cfi_restore	%r12
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx2:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_multi_block_avx2,.-sha1_multi_block_avx2

.align	256
.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
K_XX_XX:
.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.byte	0xf,0xe,0xd,0xc,0xb,0xa,0x9,0x8,0x7,0x6,0x5,0x4,0x3,0x2,0x1,0x0
.byte	83,72,65,49,32,109,117,108,116,105,45,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                                                                                                     node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/sha1-x86_64.s             0000664 0000000 0000000 00000307360 14746647661 0027714 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	


.globl	sha1_block_data_order
.type	sha1_block_data_order,@function
.align	16
sha1_block_data_order:
.cfi_startproc	
	movl	OPENSSL_ia32cap_P+0(%rip),%r9d
	movl	OPENSSL_ia32cap_P+4(%rip),%r8d
	movl	OPENSSL_ia32cap_P+8(%rip),%r10d
	testl	$512,%r8d
	jz	.Lialu
	testl	$536870912,%r10d
	jnz	_shaext_shortcut
	andl	$296,%r10d
	cmpl	$296,%r10d
	je	_avx2_shortcut
	andl	$268435456,%r8d
	andl	$1073741824,%r9d
	orl	%r9d,%r8d
	cmpl	$1342177280,%r8d
	je	_avx_shortcut
	jmp	_ssse3_shortcut

.align	16
.Lialu:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	movq	%rdi,%r8
	subq	$72,%rsp
	movq	%rsi,%r9
	andq	$-64,%rsp
	movq	%rdx,%r10
	movq	%rax,64(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xc0,0x00,0x06,0x23,0x08
.Lprologue:

	movl	0(%r8),%esi
	movl	4(%r8),%edi
	movl	8(%r8),%r11d
	movl	12(%r8),%r12d
	movl	16(%r8),%r13d
	jmp	.Lloop

.align	16
.Lloop:
	movl	0(%r9),%edx
	bswapl	%edx
	movl	4(%r9),%ebp
	movl	%r12d,%eax
	movl	%edx,0(%rsp)
	movl	%esi,%ecx
	bswapl	%ebp
	xorl	%r11d,%eax
	roll	$5,%ecx
	andl	%edi,%eax
	leal	1518500249(%rdx,%r13,1),%r13d
	addl	%ecx,%r13d
	xorl	%r12d,%eax
	roll	$30,%edi
	addl	%eax,%r13d
	movl	8(%r9),%r14d
	movl	%r11d,%eax
	movl	%ebp,4(%rsp)
	movl	%r13d,%ecx
	bswapl	%r14d
	xorl	%edi,%eax
	roll	$5,%ecx
	andl	%esi,%eax
	leal	1518500249(%rbp,%r12,1),%r12d
	addl	%ecx,%r12d
	xorl	%r11d,%eax
	roll	$30,%esi
	addl	%eax,%r12d
	movl	12(%r9),%edx
	movl	%edi,%eax
	movl	%r14d,8(%rsp)
	movl	%r12d,%ecx
	bswapl	%edx
	xorl	%esi,%eax
	roll	$5,%ecx
	andl	%r13d,%eax
	leal	1518500249(%r14,%r11,1),%r11d
	addl	%ecx,%r11d
	xorl	%edi,%eax
	roll	$30,%r13d
	addl	%eax,%r11d
	movl	16(%r9),%ebp
	movl	%esi,%eax
	movl	%edx,12(%rsp)
	movl	%r11d,%ecx
	bswapl	%ebp
	xorl	%r13d,%eax
	roll	$5,%ecx
	andl	%r12d,%eax
	leal	1518500249(%rdx,%rdi,1),%edi
	addl	%ecx,%edi
	xorl	%esi,%eax
	roll	$30,%r12d
	addl	%eax,%edi
	movl	20(%r9),%r14d
	movl	%r13d,%eax
	movl	%ebp,16(%rsp)
	movl	%edi,%ecx
	bswapl	%r14d
	xorl	%r12d,%eax
	roll	$5,%ecx
	andl	%r11d,%eax
	leal	1518500249(%rbp,%rsi,1),%esi
	addl	%ecx,%esi
	xorl	%r13d,%eax
	roll	$30,%r11d
	addl	%eax,%esi
	movl	24(%r9),%edx
	movl	%r12d,%eax
	movl	%r14d,20(%rsp)
	movl	%esi,%ecx
	bswapl	%edx
	xorl	%r11d,%eax
	roll	$5,%ecx
	andl	%edi,%eax
	leal	1518500249(%r14,%r13,1),%r13d
	addl	%ecx,%r13d
	xorl	%r12d,%eax
	roll	$30,%edi
	addl	%eax,%r13d
	movl	28(%r9),%ebp
	movl	%r11d,%eax
	movl	%edx,24(%rsp)
	movl	%r13d,%ecx
	bswapl	%ebp
	xorl	%edi,%eax
	roll	$5,%ecx
	andl	%esi,%eax
	leal	1518500249(%rdx,%r12,1),%r12d
	addl	%ecx,%r12d
	xorl	%r11d,%eax
	roll	$30,%esi
	addl	%eax,%r12d
	movl	32(%r9),%r14d
	movl	%edi,%eax
	movl	%ebp,28(%rsp)
	movl	%r12d,%ecx
	bswapl	%r14d
	xorl	%esi,%eax
	roll	$5,%ecx
	andl	%r13d,%eax
	leal	1518500249(%rbp,%r11,1),%r11d
	addl	%ecx,%r11d
	xorl	%edi,%eax
	roll	$30,%r13d
	addl	%eax,%r11d
	movl	36(%r9),%edx
	movl	%esi,%eax
	movl	%r14d,32(%rsp)
	movl	%r11d,%ecx
	bswapl	%edx
	xorl	%r13d,%eax
	roll	$5,%ecx
	andl	%r12d,%eax
	leal	1518500249(%r14,%rdi,1),%edi
	addl	%ecx,%edi
	xorl	%esi,%eax
	roll	$30,%r12d
	addl	%eax,%edi
	movl	40(%r9),%ebp
	movl	%r13d,%eax
	movl	%edx,36(%rsp)
	movl	%edi,%ecx
	bswapl	%ebp
	xorl	%r12d,%eax
	roll	$5,%ecx
	andl	%r11d,%eax
	leal	1518500249(%rdx,%rsi,1),%esi
	addl	%ecx,%esi
	xorl	%r13d,%eax
	roll	$30,%r11d
	addl	%eax,%esi
	movl	44(%r9),%r14d
	movl	%r12d,%eax
	movl	%ebp,40(%rsp)
	movl	%esi,%ecx
	bswapl	%r14d
	xorl	%r11d,%eax
	roll	$5,%ecx
	andl	%edi,%eax
	leal	1518500249(%rbp,%r13,1),%r13d
	addl	%ecx,%r13d
	xorl	%r12d,%eax
	roll	$30,%edi
	addl	%eax,%r13d
	movl	48(%r9),%edx
	movl	%r11d,%eax
	movl	%r14d,44(%rsp)
	movl	%r13d,%ecx
	bswapl	%edx
	xorl	%edi,%eax
	roll	$5,%ecx
	andl	%esi,%eax
	leal	1518500249(%r14,%r12,1),%r12d
	addl	%ecx,%r12d
	xorl	%r11d,%eax
	roll	$30,%esi
	addl	%eax,%r12d
	movl	52(%r9),%ebp
	movl	%edi,%eax
	movl	%edx,48(%rsp)
	movl	%r12d,%ecx
	bswapl	%ebp
	xorl	%esi,%eax
	roll	$5,%ecx
	andl	%r13d,%eax
	leal	1518500249(%rdx,%r11,1),%r11d
	addl	%ecx,%r11d
	xorl	%edi,%eax
	roll	$30,%r13d
	addl	%eax,%r11d
	movl	56(%r9),%r14d
	movl	%esi,%eax
	movl	%ebp,52(%rsp)
	movl	%r11d,%ecx
	bswapl	%r14d
	xorl	%r13d,%eax
	roll	$5,%ecx
	andl	%r12d,%eax
	leal	1518500249(%rbp,%rdi,1),%edi
	addl	%ecx,%edi
	xorl	%esi,%eax
	roll	$30,%r12d
	addl	%eax,%edi
	movl	60(%r9),%edx
	movl	%r13d,%eax
	movl	%r14d,56(%rsp)
	movl	%edi,%ecx
	bswapl	%edx
	xorl	%r12d,%eax
	roll	$5,%ecx
	andl	%r11d,%eax
	leal	1518500249(%r14,%rsi,1),%esi
	addl	%ecx,%esi
	xorl	%r13d,%eax
	roll	$30,%r11d
	addl	%eax,%esi
	xorl	0(%rsp),%ebp
	movl	%r12d,%eax
	movl	%edx,60(%rsp)
	movl	%esi,%ecx
	xorl	8(%rsp),%ebp
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	32(%rsp),%ebp
	andl	%edi,%eax
	leal	1518500249(%rdx,%r13,1),%r13d
	roll	$30,%edi
	xorl	%r12d,%eax
	addl	%ecx,%r13d
	roll	$1,%ebp
	addl	%eax,%r13d
	xorl	4(%rsp),%r14d
	movl	%r11d,%eax
	movl	%ebp,0(%rsp)
	movl	%r13d,%ecx
	xorl	12(%rsp),%r14d
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	36(%rsp),%r14d
	andl	%esi,%eax
	leal	1518500249(%rbp,%r12,1),%r12d
	roll	$30,%esi
	xorl	%r11d,%eax
	addl	%ecx,%r12d
	roll	$1,%r14d
	addl	%eax,%r12d
	xorl	8(%rsp),%edx
	movl	%edi,%eax
	movl	%r14d,4(%rsp)
	movl	%r12d,%ecx
	xorl	16(%rsp),%edx
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	40(%rsp),%edx
	andl	%r13d,%eax
	leal	1518500249(%r14,%r11,1),%r11d
	roll	$30,%r13d
	xorl	%edi,%eax
	addl	%ecx,%r11d
	roll	$1,%edx
	addl	%eax,%r11d
	xorl	12(%rsp),%ebp
	movl	%esi,%eax
	movl	%edx,8(%rsp)
	movl	%r11d,%ecx
	xorl	20(%rsp),%ebp
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	44(%rsp),%ebp
	andl	%r12d,%eax
	leal	1518500249(%rdx,%rdi,1),%edi
	roll	$30,%r12d
	xorl	%esi,%eax
	addl	%ecx,%edi
	roll	$1,%ebp
	addl	%eax,%edi
	xorl	16(%rsp),%r14d
	movl	%r13d,%eax
	movl	%ebp,12(%rsp)
	movl	%edi,%ecx
	xorl	24(%rsp),%r14d
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	48(%rsp),%r14d
	andl	%r11d,%eax
	leal	1518500249(%rbp,%rsi,1),%esi
	roll	$30,%r11d
	xorl	%r13d,%eax
	addl	%ecx,%esi
	roll	$1,%r14d
	addl	%eax,%esi
	xorl	20(%rsp),%edx
	movl	%edi,%eax
	movl	%r14d,16(%rsp)
	movl	%esi,%ecx
	xorl	28(%rsp),%edx
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	52(%rsp),%edx
	leal	1859775393(%r14,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%edx
	xorl	24(%rsp),%ebp
	movl	%esi,%eax
	movl	%edx,20(%rsp)
	movl	%r13d,%ecx
	xorl	32(%rsp),%ebp
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	56(%rsp),%ebp
	leal	1859775393(%rdx,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%ebp
	xorl	28(%rsp),%r14d
	movl	%r13d,%eax
	movl	%ebp,24(%rsp)
	movl	%r12d,%ecx
	xorl	36(%rsp),%r14d
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	60(%rsp),%r14d
	leal	1859775393(%rbp,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%r14d
	xorl	32(%rsp),%edx
	movl	%r12d,%eax
	movl	%r14d,28(%rsp)
	movl	%r11d,%ecx
	xorl	40(%rsp),%edx
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	0(%rsp),%edx
	leal	1859775393(%r14,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%edx
	xorl	36(%rsp),%ebp
	movl	%r11d,%eax
	movl	%edx,32(%rsp)
	movl	%edi,%ecx
	xorl	44(%rsp),%ebp
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	4(%rsp),%ebp
	leal	1859775393(%rdx,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%ebp
	xorl	40(%rsp),%r14d
	movl	%edi,%eax
	movl	%ebp,36(%rsp)
	movl	%esi,%ecx
	xorl	48(%rsp),%r14d
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	8(%rsp),%r14d
	leal	1859775393(%rbp,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%r14d
	xorl	44(%rsp),%edx
	movl	%esi,%eax
	movl	%r14d,40(%rsp)
	movl	%r13d,%ecx
	xorl	52(%rsp),%edx
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	12(%rsp),%edx
	leal	1859775393(%r14,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%edx
	xorl	48(%rsp),%ebp
	movl	%r13d,%eax
	movl	%edx,44(%rsp)
	movl	%r12d,%ecx
	xorl	56(%rsp),%ebp
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	16(%rsp),%ebp
	leal	1859775393(%rdx,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%ebp
	xorl	52(%rsp),%r14d
	movl	%r12d,%eax
	movl	%ebp,48(%rsp)
	movl	%r11d,%ecx
	xorl	60(%rsp),%r14d
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	20(%rsp),%r14d
	leal	1859775393(%rbp,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%r14d
	xorl	56(%rsp),%edx
	movl	%r11d,%eax
	movl	%r14d,52(%rsp)
	movl	%edi,%ecx
	xorl	0(%rsp),%edx
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	24(%rsp),%edx
	leal	1859775393(%r14,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%edx
	xorl	60(%rsp),%ebp
	movl	%edi,%eax
	movl	%edx,56(%rsp)
	movl	%esi,%ecx
	xorl	4(%rsp),%ebp
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	28(%rsp),%ebp
	leal	1859775393(%rdx,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%ebp
	xorl	0(%rsp),%r14d
	movl	%esi,%eax
	movl	%ebp,60(%rsp)
	movl	%r13d,%ecx
	xorl	8(%rsp),%r14d
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	32(%rsp),%r14d
	leal	1859775393(%rbp,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%r14d
	xorl	4(%rsp),%edx
	movl	%r13d,%eax
	movl	%r14d,0(%rsp)
	movl	%r12d,%ecx
	xorl	12(%rsp),%edx
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	36(%rsp),%edx
	leal	1859775393(%r14,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%edx
	xorl	8(%rsp),%ebp
	movl	%r12d,%eax
	movl	%edx,4(%rsp)
	movl	%r11d,%ecx
	xorl	16(%rsp),%ebp
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	40(%rsp),%ebp
	leal	1859775393(%rdx,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%ebp
	xorl	12(%rsp),%r14d
	movl	%r11d,%eax
	movl	%ebp,8(%rsp)
	movl	%edi,%ecx
	xorl	20(%rsp),%r14d
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	44(%rsp),%r14d
	leal	1859775393(%rbp,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%r14d
	xorl	16(%rsp),%edx
	movl	%edi,%eax
	movl	%r14d,12(%rsp)
	movl	%esi,%ecx
	xorl	24(%rsp),%edx
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	48(%rsp),%edx
	leal	1859775393(%r14,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%edx
	xorl	20(%rsp),%ebp
	movl	%esi,%eax
	movl	%edx,16(%rsp)
	movl	%r13d,%ecx
	xorl	28(%rsp),%ebp
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	52(%rsp),%ebp
	leal	1859775393(%rdx,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%ebp
	xorl	24(%rsp),%r14d
	movl	%r13d,%eax
	movl	%ebp,20(%rsp)
	movl	%r12d,%ecx
	xorl	32(%rsp),%r14d
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	56(%rsp),%r14d
	leal	1859775393(%rbp,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%r14d
	xorl	28(%rsp),%edx
	movl	%r12d,%eax
	movl	%r14d,24(%rsp)
	movl	%r11d,%ecx
	xorl	36(%rsp),%edx
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	60(%rsp),%edx
	leal	1859775393(%r14,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%edx
	xorl	32(%rsp),%ebp
	movl	%r11d,%eax
	movl	%edx,28(%rsp)
	movl	%edi,%ecx
	xorl	40(%rsp),%ebp
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	0(%rsp),%ebp
	leal	1859775393(%rdx,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%ebp
	xorl	36(%rsp),%r14d
	movl	%r12d,%eax
	movl	%ebp,32(%rsp)
	movl	%r12d,%ebx
	xorl	44(%rsp),%r14d
	andl	%r11d,%eax
	movl	%esi,%ecx
	xorl	4(%rsp),%r14d
	leal	-1894007588(%rbp,%r13,1),%r13d
	xorl	%r11d,%ebx
	roll	$5,%ecx
	addl	%eax,%r13d
	roll	$1,%r14d
	andl	%edi,%ebx
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%ebx,%r13d
	xorl	40(%rsp),%edx
	movl	%r11d,%eax
	movl	%r14d,36(%rsp)
	movl	%r11d,%ebx
	xorl	48(%rsp),%edx
	andl	%edi,%eax
	movl	%r13d,%ecx
	xorl	8(%rsp),%edx
	leal	-1894007588(%r14,%r12,1),%r12d
	xorl	%edi,%ebx
	roll	$5,%ecx
	addl	%eax,%r12d
	roll	$1,%edx
	andl	%esi,%ebx
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%ebx,%r12d
	xorl	44(%rsp),%ebp
	movl	%edi,%eax
	movl	%edx,40(%rsp)
	movl	%edi,%ebx
	xorl	52(%rsp),%ebp
	andl	%esi,%eax
	movl	%r12d,%ecx
	xorl	12(%rsp),%ebp
	leal	-1894007588(%rdx,%r11,1),%r11d
	xorl	%esi,%ebx
	roll	$5,%ecx
	addl	%eax,%r11d
	roll	$1,%ebp
	andl	%r13d,%ebx
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%ebx,%r11d
	xorl	48(%rsp),%r14d
	movl	%esi,%eax
	movl	%ebp,44(%rsp)
	movl	%esi,%ebx
	xorl	56(%rsp),%r14d
	andl	%r13d,%eax
	movl	%r11d,%ecx
	xorl	16(%rsp),%r14d
	leal	-1894007588(%rbp,%rdi,1),%edi
	xorl	%r13d,%ebx
	roll	$5,%ecx
	addl	%eax,%edi
	roll	$1,%r14d
	andl	%r12d,%ebx
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%ebx,%edi
	xorl	52(%rsp),%edx
	movl	%r13d,%eax
	movl	%r14d,48(%rsp)
	movl	%r13d,%ebx
	xorl	60(%rsp),%edx
	andl	%r12d,%eax
	movl	%edi,%ecx
	xorl	20(%rsp),%edx
	leal	-1894007588(%r14,%rsi,1),%esi
	xorl	%r12d,%ebx
	roll	$5,%ecx
	addl	%eax,%esi
	roll	$1,%edx
	andl	%r11d,%ebx
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%ebx,%esi
	xorl	56(%rsp),%ebp
	movl	%r12d,%eax
	movl	%edx,52(%rsp)
	movl	%r12d,%ebx
	xorl	0(%rsp),%ebp
	andl	%r11d,%eax
	movl	%esi,%ecx
	xorl	24(%rsp),%ebp
	leal	-1894007588(%rdx,%r13,1),%r13d
	xorl	%r11d,%ebx
	roll	$5,%ecx
	addl	%eax,%r13d
	roll	$1,%ebp
	andl	%edi,%ebx
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%ebx,%r13d
	xorl	60(%rsp),%r14d
	movl	%r11d,%eax
	movl	%ebp,56(%rsp)
	movl	%r11d,%ebx
	xorl	4(%rsp),%r14d
	andl	%edi,%eax
	movl	%r13d,%ecx
	xorl	28(%rsp),%r14d
	leal	-1894007588(%rbp,%r12,1),%r12d
	xorl	%edi,%ebx
	roll	$5,%ecx
	addl	%eax,%r12d
	roll	$1,%r14d
	andl	%esi,%ebx
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%ebx,%r12d
	xorl	0(%rsp),%edx
	movl	%edi,%eax
	movl	%r14d,60(%rsp)
	movl	%edi,%ebx
	xorl	8(%rsp),%edx
	andl	%esi,%eax
	movl	%r12d,%ecx
	xorl	32(%rsp),%edx
	leal	-1894007588(%r14,%r11,1),%r11d
	xorl	%esi,%ebx
	roll	$5,%ecx
	addl	%eax,%r11d
	roll	$1,%edx
	andl	%r13d,%ebx
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%ebx,%r11d
	xorl	4(%rsp),%ebp
	movl	%esi,%eax
	movl	%edx,0(%rsp)
	movl	%esi,%ebx
	xorl	12(%rsp),%ebp
	andl	%r13d,%eax
	movl	%r11d,%ecx
	xorl	36(%rsp),%ebp
	leal	-1894007588(%rdx,%rdi,1),%edi
	xorl	%r13d,%ebx
	roll	$5,%ecx
	addl	%eax,%edi
	roll	$1,%ebp
	andl	%r12d,%ebx
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%ebx,%edi
	xorl	8(%rsp),%r14d
	movl	%r13d,%eax
	movl	%ebp,4(%rsp)
	movl	%r13d,%ebx
	xorl	16(%rsp),%r14d
	andl	%r12d,%eax
	movl	%edi,%ecx
	xorl	40(%rsp),%r14d
	leal	-1894007588(%rbp,%rsi,1),%esi
	xorl	%r12d,%ebx
	roll	$5,%ecx
	addl	%eax,%esi
	roll	$1,%r14d
	andl	%r11d,%ebx
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%ebx,%esi
	xorl	12(%rsp),%edx
	movl	%r12d,%eax
	movl	%r14d,8(%rsp)
	movl	%r12d,%ebx
	xorl	20(%rsp),%edx
	andl	%r11d,%eax
	movl	%esi,%ecx
	xorl	44(%rsp),%edx
	leal	-1894007588(%r14,%r13,1),%r13d
	xorl	%r11d,%ebx
	roll	$5,%ecx
	addl	%eax,%r13d
	roll	$1,%edx
	andl	%edi,%ebx
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%ebx,%r13d
	xorl	16(%rsp),%ebp
	movl	%r11d,%eax
	movl	%edx,12(%rsp)
	movl	%r11d,%ebx
	xorl	24(%rsp),%ebp
	andl	%edi,%eax
	movl	%r13d,%ecx
	xorl	48(%rsp),%ebp
	leal	-1894007588(%rdx,%r12,1),%r12d
	xorl	%edi,%ebx
	roll	$5,%ecx
	addl	%eax,%r12d
	roll	$1,%ebp
	andl	%esi,%ebx
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%ebx,%r12d
	xorl	20(%rsp),%r14d
	movl	%edi,%eax
	movl	%ebp,16(%rsp)
	movl	%edi,%ebx
	xorl	28(%rsp),%r14d
	andl	%esi,%eax
	movl	%r12d,%ecx
	xorl	52(%rsp),%r14d
	leal	-1894007588(%rbp,%r11,1),%r11d
	xorl	%esi,%ebx
	roll	$5,%ecx
	addl	%eax,%r11d
	roll	$1,%r14d
	andl	%r13d,%ebx
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%ebx,%r11d
	xorl	24(%rsp),%edx
	movl	%esi,%eax
	movl	%r14d,20(%rsp)
	movl	%esi,%ebx
	xorl	32(%rsp),%edx
	andl	%r13d,%eax
	movl	%r11d,%ecx
	xorl	56(%rsp),%edx
	leal	-1894007588(%r14,%rdi,1),%edi
	xorl	%r13d,%ebx
	roll	$5,%ecx
	addl	%eax,%edi
	roll	$1,%edx
	andl	%r12d,%ebx
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%ebx,%edi
	xorl	28(%rsp),%ebp
	movl	%r13d,%eax
	movl	%edx,24(%rsp)
	movl	%r13d,%ebx
	xorl	36(%rsp),%ebp
	andl	%r12d,%eax
	movl	%edi,%ecx
	xorl	60(%rsp),%ebp
	leal	-1894007588(%rdx,%rsi,1),%esi
	xorl	%r12d,%ebx
	roll	$5,%ecx
	addl	%eax,%esi
	roll	$1,%ebp
	andl	%r11d,%ebx
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%ebx,%esi
	xorl	32(%rsp),%r14d
	movl	%r12d,%eax
	movl	%ebp,28(%rsp)
	movl	%r12d,%ebx
	xorl	40(%rsp),%r14d
	andl	%r11d,%eax
	movl	%esi,%ecx
	xorl	0(%rsp),%r14d
	leal	-1894007588(%rbp,%r13,1),%r13d
	xorl	%r11d,%ebx
	roll	$5,%ecx
	addl	%eax,%r13d
	roll	$1,%r14d
	andl	%edi,%ebx
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%ebx,%r13d
	xorl	36(%rsp),%edx
	movl	%r11d,%eax
	movl	%r14d,32(%rsp)
	movl	%r11d,%ebx
	xorl	44(%rsp),%edx
	andl	%edi,%eax
	movl	%r13d,%ecx
	xorl	4(%rsp),%edx
	leal	-1894007588(%r14,%r12,1),%r12d
	xorl	%edi,%ebx
	roll	$5,%ecx
	addl	%eax,%r12d
	roll	$1,%edx
	andl	%esi,%ebx
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%ebx,%r12d
	xorl	40(%rsp),%ebp
	movl	%edi,%eax
	movl	%edx,36(%rsp)
	movl	%edi,%ebx
	xorl	48(%rsp),%ebp
	andl	%esi,%eax
	movl	%r12d,%ecx
	xorl	8(%rsp),%ebp
	leal	-1894007588(%rdx,%r11,1),%r11d
	xorl	%esi,%ebx
	roll	$5,%ecx
	addl	%eax,%r11d
	roll	$1,%ebp
	andl	%r13d,%ebx
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%ebx,%r11d
	xorl	44(%rsp),%r14d
	movl	%esi,%eax
	movl	%ebp,40(%rsp)
	movl	%esi,%ebx
	xorl	52(%rsp),%r14d
	andl	%r13d,%eax
	movl	%r11d,%ecx
	xorl	12(%rsp),%r14d
	leal	-1894007588(%rbp,%rdi,1),%edi
	xorl	%r13d,%ebx
	roll	$5,%ecx
	addl	%eax,%edi
	roll	$1,%r14d
	andl	%r12d,%ebx
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%ebx,%edi
	xorl	48(%rsp),%edx
	movl	%r13d,%eax
	movl	%r14d,44(%rsp)
	movl	%r13d,%ebx
	xorl	56(%rsp),%edx
	andl	%r12d,%eax
	movl	%edi,%ecx
	xorl	16(%rsp),%edx
	leal	-1894007588(%r14,%rsi,1),%esi
	xorl	%r12d,%ebx
	roll	$5,%ecx
	addl	%eax,%esi
	roll	$1,%edx
	andl	%r11d,%ebx
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%ebx,%esi
	xorl	52(%rsp),%ebp
	movl	%edi,%eax
	movl	%edx,48(%rsp)
	movl	%esi,%ecx
	xorl	60(%rsp),%ebp
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	20(%rsp),%ebp
	leal	-899497514(%rdx,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%ebp
	xorl	56(%rsp),%r14d
	movl	%esi,%eax
	movl	%ebp,52(%rsp)
	movl	%r13d,%ecx
	xorl	0(%rsp),%r14d
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	24(%rsp),%r14d
	leal	-899497514(%rbp,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%r14d
	xorl	60(%rsp),%edx
	movl	%r13d,%eax
	movl	%r14d,56(%rsp)
	movl	%r12d,%ecx
	xorl	4(%rsp),%edx
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	28(%rsp),%edx
	leal	-899497514(%r14,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%edx
	xorl	0(%rsp),%ebp
	movl	%r12d,%eax
	movl	%edx,60(%rsp)
	movl	%r11d,%ecx
	xorl	8(%rsp),%ebp
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	32(%rsp),%ebp
	leal	-899497514(%rdx,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%ebp
	xorl	4(%rsp),%r14d
	movl	%r11d,%eax
	movl	%ebp,0(%rsp)
	movl	%edi,%ecx
	xorl	12(%rsp),%r14d
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	36(%rsp),%r14d
	leal	-899497514(%rbp,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%r14d
	xorl	8(%rsp),%edx
	movl	%edi,%eax
	movl	%r14d,4(%rsp)
	movl	%esi,%ecx
	xorl	16(%rsp),%edx
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	40(%rsp),%edx
	leal	-899497514(%r14,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%edx
	xorl	12(%rsp),%ebp
	movl	%esi,%eax
	movl	%edx,8(%rsp)
	movl	%r13d,%ecx
	xorl	20(%rsp),%ebp
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	44(%rsp),%ebp
	leal	-899497514(%rdx,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%ebp
	xorl	16(%rsp),%r14d
	movl	%r13d,%eax
	movl	%ebp,12(%rsp)
	movl	%r12d,%ecx
	xorl	24(%rsp),%r14d
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	48(%rsp),%r14d
	leal	-899497514(%rbp,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%r14d
	xorl	20(%rsp),%edx
	movl	%r12d,%eax
	movl	%r14d,16(%rsp)
	movl	%r11d,%ecx
	xorl	28(%rsp),%edx
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	52(%rsp),%edx
	leal	-899497514(%r14,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%edx
	xorl	24(%rsp),%ebp
	movl	%r11d,%eax
	movl	%edx,20(%rsp)
	movl	%edi,%ecx
	xorl	32(%rsp),%ebp
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	56(%rsp),%ebp
	leal	-899497514(%rdx,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%ebp
	xorl	28(%rsp),%r14d
	movl	%edi,%eax
	movl	%ebp,24(%rsp)
	movl	%esi,%ecx
	xorl	36(%rsp),%r14d
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	60(%rsp),%r14d
	leal	-899497514(%rbp,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%r14d
	xorl	32(%rsp),%edx
	movl	%esi,%eax
	movl	%r14d,28(%rsp)
	movl	%r13d,%ecx
	xorl	40(%rsp),%edx
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	0(%rsp),%edx
	leal	-899497514(%r14,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%edx
	xorl	36(%rsp),%ebp
	movl	%r13d,%eax

	movl	%r12d,%ecx
	xorl	44(%rsp),%ebp
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	4(%rsp),%ebp
	leal	-899497514(%rdx,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%ebp
	xorl	40(%rsp),%r14d
	movl	%r12d,%eax

	movl	%r11d,%ecx
	xorl	48(%rsp),%r14d
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	8(%rsp),%r14d
	leal	-899497514(%rbp,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%r14d
	xorl	44(%rsp),%edx
	movl	%r11d,%eax

	movl	%edi,%ecx
	xorl	52(%rsp),%edx
	xorl	%r13d,%eax
	roll	$5,%ecx
	xorl	12(%rsp),%edx
	leal	-899497514(%r14,%rsi,1),%esi
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	roll	$1,%edx
	xorl	48(%rsp),%ebp
	movl	%edi,%eax

	movl	%esi,%ecx
	xorl	56(%rsp),%ebp
	xorl	%r12d,%eax
	roll	$5,%ecx
	xorl	16(%rsp),%ebp
	leal	-899497514(%rdx,%r13,1),%r13d
	xorl	%r11d,%eax
	addl	%ecx,%r13d
	roll	$30,%edi
	addl	%eax,%r13d
	roll	$1,%ebp
	xorl	52(%rsp),%r14d
	movl	%esi,%eax

	movl	%r13d,%ecx
	xorl	60(%rsp),%r14d
	xorl	%r11d,%eax
	roll	$5,%ecx
	xorl	20(%rsp),%r14d
	leal	-899497514(%rbp,%r12,1),%r12d
	xorl	%edi,%eax
	addl	%ecx,%r12d
	roll	$30,%esi
	addl	%eax,%r12d
	roll	$1,%r14d
	xorl	56(%rsp),%edx
	movl	%r13d,%eax

	movl	%r12d,%ecx
	xorl	0(%rsp),%edx
	xorl	%edi,%eax
	roll	$5,%ecx
	xorl	24(%rsp),%edx
	leal	-899497514(%r14,%r11,1),%r11d
	xorl	%esi,%eax
	addl	%ecx,%r11d
	roll	$30,%r13d
	addl	%eax,%r11d
	roll	$1,%edx
	xorl	60(%rsp),%ebp
	movl	%r12d,%eax

	movl	%r11d,%ecx
	xorl	4(%rsp),%ebp
	xorl	%esi,%eax
	roll	$5,%ecx
	xorl	28(%rsp),%ebp
	leal	-899497514(%rdx,%rdi,1),%edi
	xorl	%r13d,%eax
	addl	%ecx,%edi
	roll	$30,%r12d
	addl	%eax,%edi
	roll	$1,%ebp
	movl	%r11d,%eax
	movl	%edi,%ecx
	xorl	%r13d,%eax
	leal	-899497514(%rbp,%rsi,1),%esi
	roll	$5,%ecx
	xorl	%r12d,%eax
	addl	%ecx,%esi
	roll	$30,%r11d
	addl	%eax,%esi
	addl	0(%r8),%esi
	addl	4(%r8),%edi
	addl	8(%r8),%r11d
	addl	12(%r8),%r12d
	addl	16(%r8),%r13d
	movl	%esi,0(%r8)
	movl	%edi,4(%r8)
	movl	%r11d,8(%r8)
	movl	%r12d,12(%r8)
	movl	%r13d,16(%r8)

	subq	$1,%r10
	leaq	64(%r9),%r9
	jnz	.Lloop

	movq	64(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_block_data_order,.-sha1_block_data_order
.type	sha1_block_data_order_shaext,@function
.align	32
sha1_block_data_order_shaext:
_shaext_shortcut:
.cfi_startproc	
	movdqu	(%rdi),%xmm0
	movd	16(%rdi),%xmm1
	movdqa	K_XX_XX+160(%rip),%xmm3

	movdqu	(%rsi),%xmm4
	pshufd	$27,%xmm0,%xmm0
	movdqu	16(%rsi),%xmm5
	pshufd	$27,%xmm1,%xmm1
	movdqu	32(%rsi),%xmm6
.byte	102,15,56,0,227
	movdqu	48(%rsi),%xmm7
.byte	102,15,56,0,235
.byte	102,15,56,0,243
	movdqa	%xmm1,%xmm9
.byte	102,15,56,0,251
	jmp	.Loop_shaext

.align	16
.Loop_shaext:
	decq	%rdx
	leaq	64(%rsi),%r8
	paddd	%xmm4,%xmm1
	cmovneq	%r8,%rsi
	movdqa	%xmm0,%xmm8
.byte	15,56,201,229
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,0
.byte	15,56,200,213
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
.byte	15,56,202,231

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,0
.byte	15,56,200,206
	pxor	%xmm7,%xmm5
.byte	15,56,202,236
.byte	15,56,201,247
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,0
.byte	15,56,200,215
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
.byte	15,56,202,245

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,0
.byte	15,56,200,204
	pxor	%xmm5,%xmm7
.byte	15,56,202,254
.byte	15,56,201,229
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,0
.byte	15,56,200,213
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
.byte	15,56,202,231

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,1
.byte	15,56,200,206
	pxor	%xmm7,%xmm5
.byte	15,56,202,236
.byte	15,56,201,247
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,1
.byte	15,56,200,215
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
.byte	15,56,202,245

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,1
.byte	15,56,200,204
	pxor	%xmm5,%xmm7
.byte	15,56,202,254
.byte	15,56,201,229
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,1
.byte	15,56,200,213
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
.byte	15,56,202,231

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,1
.byte	15,56,200,206
	pxor	%xmm7,%xmm5
.byte	15,56,202,236
.byte	15,56,201,247
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,2
.byte	15,56,200,215
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
.byte	15,56,202,245

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,2
.byte	15,56,200,204
	pxor	%xmm5,%xmm7
.byte	15,56,202,254
.byte	15,56,201,229
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,2
.byte	15,56,200,213
	pxor	%xmm6,%xmm4
.byte	15,56,201,238
.byte	15,56,202,231

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,2
.byte	15,56,200,206
	pxor	%xmm7,%xmm5
.byte	15,56,202,236
.byte	15,56,201,247
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,2
.byte	15,56,200,215
	pxor	%xmm4,%xmm6
.byte	15,56,201,252
.byte	15,56,202,245

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,3
.byte	15,56,200,204
	pxor	%xmm5,%xmm7
.byte	15,56,202,254
	movdqu	(%rsi),%xmm4
	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,3
.byte	15,56,200,213
	movdqu	16(%rsi),%xmm5
.byte	102,15,56,0,227

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,3
.byte	15,56,200,206
	movdqu	32(%rsi),%xmm6
.byte	102,15,56,0,235

	movdqa	%xmm0,%xmm2
.byte	15,58,204,193,3
.byte	15,56,200,215
	movdqu	48(%rsi),%xmm7
.byte	102,15,56,0,243

	movdqa	%xmm0,%xmm1
.byte	15,58,204,194,3
.byte	65,15,56,200,201
.byte	102,15,56,0,251

	paddd	%xmm8,%xmm0
	movdqa	%xmm1,%xmm9

	jnz	.Loop_shaext

	pshufd	$27,%xmm0,%xmm0
	pshufd	$27,%xmm1,%xmm1
	movdqu	%xmm0,(%rdi)
	movd	%xmm1,16(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_block_data_order_shaext,.-sha1_block_data_order_shaext
.type	sha1_block_data_order_ssse3,@function
.align	16
sha1_block_data_order_ssse3:
_ssse3_shortcut:
.cfi_startproc	
	movq	%rsp,%r11
.cfi_def_cfa_register	%r11
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	leaq	-64(%rsp),%rsp
	andq	$-64,%rsp
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rdx,%r10

	shlq	$6,%r10
	addq	%r9,%r10
	leaq	K_XX_XX+64(%rip),%r14

	movl	0(%r8),%eax
	movl	4(%r8),%ebx
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movl	%ebx,%esi
	movl	16(%r8),%ebp
	movl	%ecx,%edi
	xorl	%edx,%edi
	andl	%edi,%esi

	movdqa	64(%r14),%xmm6
	movdqa	-64(%r14),%xmm9
	movdqu	0(%r9),%xmm0
	movdqu	16(%r9),%xmm1
	movdqu	32(%r9),%xmm2
	movdqu	48(%r9),%xmm3
.byte	102,15,56,0,198
.byte	102,15,56,0,206
.byte	102,15,56,0,214
	addq	$64,%r9
	paddd	%xmm9,%xmm0
.byte	102,15,56,0,222
	paddd	%xmm9,%xmm1
	paddd	%xmm9,%xmm2
	movdqa	%xmm0,0(%rsp)
	psubd	%xmm9,%xmm0
	movdqa	%xmm1,16(%rsp)
	psubd	%xmm9,%xmm1
	movdqa	%xmm2,32(%rsp)
	psubd	%xmm9,%xmm2
	jmp	.Loop_ssse3
.align	16
.Loop_ssse3:
	rorl	$2,%ebx
	pshufd	$238,%xmm0,%xmm4
	xorl	%edx,%esi
	movdqa	%xmm3,%xmm8
	paddd	%xmm3,%xmm9
	movl	%eax,%edi
	addl	0(%rsp),%ebp
	punpcklqdq	%xmm1,%xmm4
	xorl	%ecx,%ebx
	roll	$5,%eax
	addl	%esi,%ebp
	psrldq	$4,%xmm8
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	pxor	%xmm0,%xmm4
	addl	%eax,%ebp
	rorl	$7,%eax
	pxor	%xmm2,%xmm8
	xorl	%ecx,%edi
	movl	%ebp,%esi
	addl	4(%rsp),%edx
	pxor	%xmm8,%xmm4
	xorl	%ebx,%eax
	roll	$5,%ebp
	movdqa	%xmm9,48(%rsp)
	addl	%edi,%edx
	andl	%eax,%esi
	movdqa	%xmm4,%xmm10
	xorl	%ebx,%eax
	addl	%ebp,%edx
	rorl	$7,%ebp
	movdqa	%xmm4,%xmm8
	xorl	%ebx,%esi
	pslldq	$12,%xmm10
	paddd	%xmm4,%xmm4
	movl	%edx,%edi
	addl	8(%rsp),%ecx
	psrld	$31,%xmm8
	xorl	%eax,%ebp
	roll	$5,%edx
	addl	%esi,%ecx
	movdqa	%xmm10,%xmm9
	andl	%ebp,%edi
	xorl	%eax,%ebp
	psrld	$30,%xmm10
	addl	%edx,%ecx
	rorl	$7,%edx
	por	%xmm8,%xmm4
	xorl	%eax,%edi
	movl	%ecx,%esi
	addl	12(%rsp),%ebx
	pslld	$2,%xmm9
	pxor	%xmm10,%xmm4
	xorl	%ebp,%edx
	movdqa	-64(%r14),%xmm10
	roll	$5,%ecx
	addl	%edi,%ebx
	andl	%edx,%esi
	pxor	%xmm9,%xmm4
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	rorl	$7,%ecx
	pshufd	$238,%xmm1,%xmm5
	xorl	%ebp,%esi
	movdqa	%xmm4,%xmm9
	paddd	%xmm4,%xmm10
	movl	%ebx,%edi
	addl	16(%rsp),%eax
	punpcklqdq	%xmm2,%xmm5
	xorl	%edx,%ecx
	roll	$5,%ebx
	addl	%esi,%eax
	psrldq	$4,%xmm9
	andl	%ecx,%edi
	xorl	%edx,%ecx
	pxor	%xmm1,%xmm5
	addl	%ebx,%eax
	rorl	$7,%ebx
	pxor	%xmm3,%xmm9
	xorl	%edx,%edi
	movl	%eax,%esi
	addl	20(%rsp),%ebp
	pxor	%xmm9,%xmm5
	xorl	%ecx,%ebx
	roll	$5,%eax
	movdqa	%xmm10,0(%rsp)
	addl	%edi,%ebp
	andl	%ebx,%esi
	movdqa	%xmm5,%xmm8
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	rorl	$7,%eax
	movdqa	%xmm5,%xmm9
	xorl	%ecx,%esi
	pslldq	$12,%xmm8
	paddd	%xmm5,%xmm5
	movl	%ebp,%edi
	addl	24(%rsp),%edx
	psrld	$31,%xmm9
	xorl	%ebx,%eax
	roll	$5,%ebp
	addl	%esi,%edx
	movdqa	%xmm8,%xmm10
	andl	%eax,%edi
	xorl	%ebx,%eax
	psrld	$30,%xmm8
	addl	%ebp,%edx
	rorl	$7,%ebp
	por	%xmm9,%xmm5
	xorl	%ebx,%edi
	movl	%edx,%esi
	addl	28(%rsp),%ecx
	pslld	$2,%xmm10
	pxor	%xmm8,%xmm5
	xorl	%eax,%ebp
	movdqa	-32(%r14),%xmm8
	roll	$5,%edx
	addl	%edi,%ecx
	andl	%ebp,%esi
	pxor	%xmm10,%xmm5
	xorl	%eax,%ebp
	addl	%edx,%ecx
	rorl	$7,%edx
	pshufd	$238,%xmm2,%xmm6
	xorl	%eax,%esi
	movdqa	%xmm5,%xmm10
	paddd	%xmm5,%xmm8
	movl	%ecx,%edi
	addl	32(%rsp),%ebx
	punpcklqdq	%xmm3,%xmm6
	xorl	%ebp,%edx
	roll	$5,%ecx
	addl	%esi,%ebx
	psrldq	$4,%xmm10
	andl	%edx,%edi
	xorl	%ebp,%edx
	pxor	%xmm2,%xmm6
	addl	%ecx,%ebx
	rorl	$7,%ecx
	pxor	%xmm4,%xmm10
	xorl	%ebp,%edi
	movl	%ebx,%esi
	addl	36(%rsp),%eax
	pxor	%xmm10,%xmm6
	xorl	%edx,%ecx
	roll	$5,%ebx
	movdqa	%xmm8,16(%rsp)
	addl	%edi,%eax
	andl	%ecx,%esi
	movdqa	%xmm6,%xmm9
	xorl	%edx,%ecx
	addl	%ebx,%eax
	rorl	$7,%ebx
	movdqa	%xmm6,%xmm10
	xorl	%edx,%esi
	pslldq	$12,%xmm9
	paddd	%xmm6,%xmm6
	movl	%eax,%edi
	addl	40(%rsp),%ebp
	psrld	$31,%xmm10
	xorl	%ecx,%ebx
	roll	$5,%eax
	addl	%esi,%ebp
	movdqa	%xmm9,%xmm8
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	psrld	$30,%xmm9
	addl	%eax,%ebp
	rorl	$7,%eax
	por	%xmm10,%xmm6
	xorl	%ecx,%edi
	movl	%ebp,%esi
	addl	44(%rsp),%edx
	pslld	$2,%xmm8
	pxor	%xmm9,%xmm6
	xorl	%ebx,%eax
	movdqa	-32(%r14),%xmm9
	roll	$5,%ebp
	addl	%edi,%edx
	andl	%eax,%esi
	pxor	%xmm8,%xmm6
	xorl	%ebx,%eax
	addl	%ebp,%edx
	rorl	$7,%ebp
	pshufd	$238,%xmm3,%xmm7
	xorl	%ebx,%esi
	movdqa	%xmm6,%xmm8
	paddd	%xmm6,%xmm9
	movl	%edx,%edi
	addl	48(%rsp),%ecx
	punpcklqdq	%xmm4,%xmm7
	xorl	%eax,%ebp
	roll	$5,%edx
	addl	%esi,%ecx
	psrldq	$4,%xmm8
	andl	%ebp,%edi
	xorl	%eax,%ebp
	pxor	%xmm3,%xmm7
	addl	%edx,%ecx
	rorl	$7,%edx
	pxor	%xmm5,%xmm8
	xorl	%eax,%edi
	movl	%ecx,%esi
	addl	52(%rsp),%ebx
	pxor	%xmm8,%xmm7
	xorl	%ebp,%edx
	roll	$5,%ecx
	movdqa	%xmm9,32(%rsp)
	addl	%edi,%ebx
	andl	%edx,%esi
	movdqa	%xmm7,%xmm10
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	rorl	$7,%ecx
	movdqa	%xmm7,%xmm8
	xorl	%ebp,%esi
	pslldq	$12,%xmm10
	paddd	%xmm7,%xmm7
	movl	%ebx,%edi
	addl	56(%rsp),%eax
	psrld	$31,%xmm8
	xorl	%edx,%ecx
	roll	$5,%ebx
	addl	%esi,%eax
	movdqa	%xmm10,%xmm9
	andl	%ecx,%edi
	xorl	%edx,%ecx
	psrld	$30,%xmm10
	addl	%ebx,%eax
	rorl	$7,%ebx
	por	%xmm8,%xmm7
	xorl	%edx,%edi
	movl	%eax,%esi
	addl	60(%rsp),%ebp
	pslld	$2,%xmm9
	pxor	%xmm10,%xmm7
	xorl	%ecx,%ebx
	movdqa	-32(%r14),%xmm10
	roll	$5,%eax
	addl	%edi,%ebp
	andl	%ebx,%esi
	pxor	%xmm9,%xmm7
	pshufd	$238,%xmm6,%xmm9
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	rorl	$7,%eax
	pxor	%xmm4,%xmm0
	xorl	%ecx,%esi
	movl	%ebp,%edi
	addl	0(%rsp),%edx
	punpcklqdq	%xmm7,%xmm9
	xorl	%ebx,%eax
	roll	$5,%ebp
	pxor	%xmm1,%xmm0
	addl	%esi,%edx
	andl	%eax,%edi
	movdqa	%xmm10,%xmm8
	xorl	%ebx,%eax
	paddd	%xmm7,%xmm10
	addl	%ebp,%edx
	pxor	%xmm9,%xmm0
	rorl	$7,%ebp
	xorl	%ebx,%edi
	movl	%edx,%esi
	addl	4(%rsp),%ecx
	movdqa	%xmm0,%xmm9
	xorl	%eax,%ebp
	roll	$5,%edx
	movdqa	%xmm10,48(%rsp)
	addl	%edi,%ecx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	pslld	$2,%xmm0
	addl	%edx,%ecx
	rorl	$7,%edx
	psrld	$30,%xmm9
	xorl	%eax,%esi
	movl	%ecx,%edi
	addl	8(%rsp),%ebx
	por	%xmm9,%xmm0
	xorl	%ebp,%edx
	roll	$5,%ecx
	pshufd	$238,%xmm7,%xmm10
	addl	%esi,%ebx
	andl	%edx,%edi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	12(%rsp),%eax
	xorl	%ebp,%edi
	movl	%ebx,%esi
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	rorl	$7,%ecx
	addl	%ebx,%eax
	pxor	%xmm5,%xmm1
	addl	16(%rsp),%ebp
	xorl	%ecx,%esi
	punpcklqdq	%xmm0,%xmm10
	movl	%eax,%edi
	roll	$5,%eax
	pxor	%xmm2,%xmm1
	addl	%esi,%ebp
	xorl	%ecx,%edi
	movdqa	%xmm8,%xmm9
	rorl	$7,%ebx
	paddd	%xmm0,%xmm8
	addl	%eax,%ebp
	pxor	%xmm10,%xmm1
	addl	20(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	roll	$5,%ebp
	movdqa	%xmm1,%xmm10
	addl	%edi,%edx
	xorl	%ebx,%esi
	movdqa	%xmm8,0(%rsp)
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	24(%rsp),%ecx
	pslld	$2,%xmm1
	xorl	%eax,%esi
	movl	%edx,%edi
	psrld	$30,%xmm10
	roll	$5,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	rorl	$7,%ebp
	por	%xmm10,%xmm1
	addl	%edx,%ecx
	addl	28(%rsp),%ebx
	pshufd	$238,%xmm0,%xmm8
	xorl	%ebp,%edi
	movl	%ecx,%esi
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	rorl	$7,%edx
	addl	%ecx,%ebx
	pxor	%xmm6,%xmm2
	addl	32(%rsp),%eax
	xorl	%edx,%esi
	punpcklqdq	%xmm1,%xmm8
	movl	%ebx,%edi
	roll	$5,%ebx
	pxor	%xmm3,%xmm2
	addl	%esi,%eax
	xorl	%edx,%edi
	movdqa	0(%r14),%xmm10
	rorl	$7,%ecx
	paddd	%xmm1,%xmm9
	addl	%ebx,%eax
	pxor	%xmm8,%xmm2
	addl	36(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	roll	$5,%eax
	movdqa	%xmm2,%xmm8
	addl	%edi,%ebp
	xorl	%ecx,%esi
	movdqa	%xmm9,16(%rsp)
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	40(%rsp),%edx
	pslld	$2,%xmm2
	xorl	%ebx,%esi
	movl	%ebp,%edi
	psrld	$30,%xmm8
	roll	$5,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	rorl	$7,%eax
	por	%xmm8,%xmm2
	addl	%ebp,%edx
	addl	44(%rsp),%ecx
	pshufd	$238,%xmm1,%xmm9
	xorl	%eax,%edi
	movl	%edx,%esi
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	rorl	$7,%ebp
	addl	%edx,%ecx
	pxor	%xmm7,%xmm3
	addl	48(%rsp),%ebx
	xorl	%ebp,%esi
	punpcklqdq	%xmm2,%xmm9
	movl	%ecx,%edi
	roll	$5,%ecx
	pxor	%xmm4,%xmm3
	addl	%esi,%ebx
	xorl	%ebp,%edi
	movdqa	%xmm10,%xmm8
	rorl	$7,%edx
	paddd	%xmm2,%xmm10
	addl	%ecx,%ebx
	pxor	%xmm9,%xmm3
	addl	52(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	roll	$5,%ebx
	movdqa	%xmm3,%xmm9
	addl	%edi,%eax
	xorl	%edx,%esi
	movdqa	%xmm10,32(%rsp)
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	56(%rsp),%ebp
	pslld	$2,%xmm3
	xorl	%ecx,%esi
	movl	%eax,%edi
	psrld	$30,%xmm9
	roll	$5,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	rorl	$7,%ebx
	por	%xmm9,%xmm3
	addl	%eax,%ebp
	addl	60(%rsp),%edx
	pshufd	$238,%xmm2,%xmm10
	xorl	%ebx,%edi
	movl	%ebp,%esi
	roll	$5,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	rorl	$7,%eax
	addl	%ebp,%edx
	pxor	%xmm0,%xmm4
	addl	0(%rsp),%ecx
	xorl	%eax,%esi
	punpcklqdq	%xmm3,%xmm10
	movl	%edx,%edi
	roll	$5,%edx
	pxor	%xmm5,%xmm4
	addl	%esi,%ecx
	xorl	%eax,%edi
	movdqa	%xmm8,%xmm9
	rorl	$7,%ebp
	paddd	%xmm3,%xmm8
	addl	%edx,%ecx
	pxor	%xmm10,%xmm4
	addl	4(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	roll	$5,%ecx
	movdqa	%xmm4,%xmm10
	addl	%edi,%ebx
	xorl	%ebp,%esi
	movdqa	%xmm8,48(%rsp)
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	8(%rsp),%eax
	pslld	$2,%xmm4
	xorl	%edx,%esi
	movl	%ebx,%edi
	psrld	$30,%xmm10
	roll	$5,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	rorl	$7,%ecx
	por	%xmm10,%xmm4
	addl	%ebx,%eax
	addl	12(%rsp),%ebp
	pshufd	$238,%xmm3,%xmm8
	xorl	%ecx,%edi
	movl	%eax,%esi
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	rorl	$7,%ebx
	addl	%eax,%ebp
	pxor	%xmm1,%xmm5
	addl	16(%rsp),%edx
	xorl	%ebx,%esi
	punpcklqdq	%xmm4,%xmm8
	movl	%ebp,%edi
	roll	$5,%ebp
	pxor	%xmm6,%xmm5
	addl	%esi,%edx
	xorl	%ebx,%edi
	movdqa	%xmm9,%xmm10
	rorl	$7,%eax
	paddd	%xmm4,%xmm9
	addl	%ebp,%edx
	pxor	%xmm8,%xmm5
	addl	20(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	roll	$5,%edx
	movdqa	%xmm5,%xmm8
	addl	%edi,%ecx
	xorl	%eax,%esi
	movdqa	%xmm9,0(%rsp)
	rorl	$7,%ebp
	addl	%edx,%ecx
	addl	24(%rsp),%ebx
	pslld	$2,%xmm5
	xorl	%ebp,%esi
	movl	%ecx,%edi
	psrld	$30,%xmm8
	roll	$5,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	rorl	$7,%edx
	por	%xmm8,%xmm5
	addl	%ecx,%ebx
	addl	28(%rsp),%eax
	pshufd	$238,%xmm4,%xmm9
	rorl	$7,%ecx
	movl	%ebx,%esi
	xorl	%edx,%edi
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%ecx,%esi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	pxor	%xmm2,%xmm6
	addl	32(%rsp),%ebp
	andl	%ecx,%esi
	xorl	%edx,%ecx
	rorl	$7,%ebx
	punpcklqdq	%xmm5,%xmm9
	movl	%eax,%edi
	xorl	%ecx,%esi
	pxor	%xmm7,%xmm6
	roll	$5,%eax
	addl	%esi,%ebp
	movdqa	%xmm10,%xmm8
	xorl	%ebx,%edi
	paddd	%xmm5,%xmm10
	xorl	%ecx,%ebx
	pxor	%xmm9,%xmm6
	addl	%eax,%ebp
	addl	36(%rsp),%edx
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	rorl	$7,%eax
	movdqa	%xmm6,%xmm9
	movl	%ebp,%esi
	xorl	%ebx,%edi
	movdqa	%xmm10,16(%rsp)
	roll	$5,%ebp
	addl	%edi,%edx
	xorl	%eax,%esi
	pslld	$2,%xmm6
	xorl	%ebx,%eax
	addl	%ebp,%edx
	psrld	$30,%xmm9
	addl	40(%rsp),%ecx
	andl	%eax,%esi
	xorl	%ebx,%eax
	por	%xmm9,%xmm6
	rorl	$7,%ebp
	movl	%edx,%edi
	xorl	%eax,%esi
	roll	$5,%edx
	pshufd	$238,%xmm5,%xmm10
	addl	%esi,%ecx
	xorl	%ebp,%edi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	addl	44(%rsp),%ebx
	andl	%ebp,%edi
	xorl	%eax,%ebp
	rorl	$7,%edx
	movl	%ecx,%esi
	xorl	%ebp,%edi
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%edx,%esi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	pxor	%xmm3,%xmm7
	addl	48(%rsp),%eax
	andl	%edx,%esi
	xorl	%ebp,%edx
	rorl	$7,%ecx
	punpcklqdq	%xmm6,%xmm10
	movl	%ebx,%edi
	xorl	%edx,%esi
	pxor	%xmm0,%xmm7
	roll	$5,%ebx
	addl	%esi,%eax
	movdqa	32(%r14),%xmm9
	xorl	%ecx,%edi
	paddd	%xmm6,%xmm8
	xorl	%edx,%ecx
	pxor	%xmm10,%xmm7
	addl	%ebx,%eax
	addl	52(%rsp),%ebp
	andl	%ecx,%edi
	xorl	%edx,%ecx
	rorl	$7,%ebx
	movdqa	%xmm7,%xmm10
	movl	%eax,%esi
	xorl	%ecx,%edi
	movdqa	%xmm8,32(%rsp)
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ebx,%esi
	pslld	$2,%xmm7
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	psrld	$30,%xmm10
	addl	56(%rsp),%edx
	andl	%ebx,%esi
	xorl	%ecx,%ebx
	por	%xmm10,%xmm7
	rorl	$7,%eax
	movl	%ebp,%edi
	xorl	%ebx,%esi
	roll	$5,%ebp
	pshufd	$238,%xmm6,%xmm8
	addl	%esi,%edx
	xorl	%eax,%edi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	addl	60(%rsp),%ecx
	andl	%eax,%edi
	xorl	%ebx,%eax
	rorl	$7,%ebp
	movl	%edx,%esi
	xorl	%eax,%edi
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%ebp,%esi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	pxor	%xmm4,%xmm0
	addl	0(%rsp),%ebx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	rorl	$7,%edx
	punpcklqdq	%xmm7,%xmm8
	movl	%ecx,%edi
	xorl	%ebp,%esi
	pxor	%xmm1,%xmm0
	roll	$5,%ecx
	addl	%esi,%ebx
	movdqa	%xmm9,%xmm10
	xorl	%edx,%edi
	paddd	%xmm7,%xmm9
	xorl	%ebp,%edx
	pxor	%xmm8,%xmm0
	addl	%ecx,%ebx
	addl	4(%rsp),%eax
	andl	%edx,%edi
	xorl	%ebp,%edx
	rorl	$7,%ecx
	movdqa	%xmm0,%xmm8
	movl	%ebx,%esi
	xorl	%edx,%edi
	movdqa	%xmm9,48(%rsp)
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%ecx,%esi
	pslld	$2,%xmm0
	xorl	%edx,%ecx
	addl	%ebx,%eax
	psrld	$30,%xmm8
	addl	8(%rsp),%ebp
	andl	%ecx,%esi
	xorl	%edx,%ecx
	por	%xmm8,%xmm0
	rorl	$7,%ebx
	movl	%eax,%edi
	xorl	%ecx,%esi
	roll	$5,%eax
	pshufd	$238,%xmm7,%xmm9
	addl	%esi,%ebp
	xorl	%ebx,%edi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	addl	12(%rsp),%edx
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	rorl	$7,%eax
	movl	%ebp,%esi
	xorl	%ebx,%edi
	roll	$5,%ebp
	addl	%edi,%edx
	xorl	%eax,%esi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	pxor	%xmm5,%xmm1
	addl	16(%rsp),%ecx
	andl	%eax,%esi
	xorl	%ebx,%eax
	rorl	$7,%ebp
	punpcklqdq	%xmm0,%xmm9
	movl	%edx,%edi
	xorl	%eax,%esi
	pxor	%xmm2,%xmm1
	roll	$5,%edx
	addl	%esi,%ecx
	movdqa	%xmm10,%xmm8
	xorl	%ebp,%edi
	paddd	%xmm0,%xmm10
	xorl	%eax,%ebp
	pxor	%xmm9,%xmm1
	addl	%edx,%ecx
	addl	20(%rsp),%ebx
	andl	%ebp,%edi
	xorl	%eax,%ebp
	rorl	$7,%edx
	movdqa	%xmm1,%xmm9
	movl	%ecx,%esi
	xorl	%ebp,%edi
	movdqa	%xmm10,0(%rsp)
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%edx,%esi
	pslld	$2,%xmm1
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	psrld	$30,%xmm9
	addl	24(%rsp),%eax
	andl	%edx,%esi
	xorl	%ebp,%edx
	por	%xmm9,%xmm1
	rorl	$7,%ecx
	movl	%ebx,%edi
	xorl	%edx,%esi
	roll	$5,%ebx
	pshufd	$238,%xmm0,%xmm10
	addl	%esi,%eax
	xorl	%ecx,%edi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	addl	28(%rsp),%ebp
	andl	%ecx,%edi
	xorl	%edx,%ecx
	rorl	$7,%ebx
	movl	%eax,%esi
	xorl	%ecx,%edi
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ebx,%esi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	pxor	%xmm6,%xmm2
	addl	32(%rsp),%edx
	andl	%ebx,%esi
	xorl	%ecx,%ebx
	rorl	$7,%eax
	punpcklqdq	%xmm1,%xmm10
	movl	%ebp,%edi
	xorl	%ebx,%esi
	pxor	%xmm3,%xmm2
	roll	$5,%ebp
	addl	%esi,%edx
	movdqa	%xmm8,%xmm9
	xorl	%eax,%edi
	paddd	%xmm1,%xmm8
	xorl	%ebx,%eax
	pxor	%xmm10,%xmm2
	addl	%ebp,%edx
	addl	36(%rsp),%ecx
	andl	%eax,%edi
	xorl	%ebx,%eax
	rorl	$7,%ebp
	movdqa	%xmm2,%xmm10
	movl	%edx,%esi
	xorl	%eax,%edi
	movdqa	%xmm8,16(%rsp)
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%ebp,%esi
	pslld	$2,%xmm2
	xorl	%eax,%ebp
	addl	%edx,%ecx
	psrld	$30,%xmm10
	addl	40(%rsp),%ebx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	por	%xmm10,%xmm2
	rorl	$7,%edx
	movl	%ecx,%edi
	xorl	%ebp,%esi
	roll	$5,%ecx
	pshufd	$238,%xmm1,%xmm8
	addl	%esi,%ebx
	xorl	%edx,%edi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	44(%rsp),%eax
	andl	%edx,%edi
	xorl	%ebp,%edx
	rorl	$7,%ecx
	movl	%ebx,%esi
	xorl	%edx,%edi
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	addl	%ebx,%eax
	pxor	%xmm7,%xmm3
	addl	48(%rsp),%ebp
	xorl	%ecx,%esi
	punpcklqdq	%xmm2,%xmm8
	movl	%eax,%edi
	roll	$5,%eax
	pxor	%xmm4,%xmm3
	addl	%esi,%ebp
	xorl	%ecx,%edi
	movdqa	%xmm9,%xmm10
	rorl	$7,%ebx
	paddd	%xmm2,%xmm9
	addl	%eax,%ebp
	pxor	%xmm8,%xmm3
	addl	52(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	roll	$5,%ebp
	movdqa	%xmm3,%xmm8
	addl	%edi,%edx
	xorl	%ebx,%esi
	movdqa	%xmm9,32(%rsp)
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	56(%rsp),%ecx
	pslld	$2,%xmm3
	xorl	%eax,%esi
	movl	%edx,%edi
	psrld	$30,%xmm8
	roll	$5,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	rorl	$7,%ebp
	por	%xmm8,%xmm3
	addl	%edx,%ecx
	addl	60(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	0(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	roll	$5,%ebx
	paddd	%xmm3,%xmm10
	addl	%esi,%eax
	xorl	%edx,%edi
	movdqa	%xmm10,48(%rsp)
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	4(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	8(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	roll	$5,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	12(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	rorl	$7,%ebp
	addl	%edx,%ecx
	cmpq	%r10,%r9
	je	.Ldone_ssse3
	movdqa	64(%r14),%xmm6
	movdqa	-64(%r14),%xmm9
	movdqu	0(%r9),%xmm0
	movdqu	16(%r9),%xmm1
	movdqu	32(%r9),%xmm2
	movdqu	48(%r9),%xmm3
.byte	102,15,56,0,198
	addq	$64,%r9
	addl	16(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
.byte	102,15,56,0,206
	roll	$5,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	rorl	$7,%edx
	paddd	%xmm9,%xmm0
	addl	%ecx,%ebx
	addl	20(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	movdqa	%xmm0,0(%rsp)
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	rorl	$7,%ecx
	psubd	%xmm9,%xmm0
	addl	%ebx,%eax
	addl	24(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	roll	$5,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	28(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	roll	$5,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	32(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
.byte	102,15,56,0,214
	roll	$5,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	rorl	$7,%ebp
	paddd	%xmm9,%xmm1
	addl	%edx,%ecx
	addl	36(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	movdqa	%xmm1,16(%rsp)
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	rorl	$7,%edx
	psubd	%xmm9,%xmm1
	addl	%ecx,%ebx
	addl	40(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	roll	$5,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	44(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	48(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
.byte	102,15,56,0,222
	roll	$5,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	rorl	$7,%eax
	paddd	%xmm9,%xmm2
	addl	%ebp,%edx
	addl	52(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	movdqa	%xmm2,32(%rsp)
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	rorl	$7,%ebp
	psubd	%xmm9,%xmm2
	addl	%edx,%ecx
	addl	56(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	roll	$5,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	60(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	roll	$5,%ebx
	addl	%edi,%eax
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	0(%r8),%eax
	addl	4(%r8),%esi
	addl	8(%r8),%ecx
	addl	12(%r8),%edx
	movl	%eax,0(%r8)
	addl	16(%r8),%ebp
	movl	%esi,4(%r8)
	movl	%esi,%ebx
	movl	%ecx,8(%r8)
	movl	%ecx,%edi
	movl	%edx,12(%r8)
	xorl	%edx,%edi
	movl	%ebp,16(%r8)
	andl	%edi,%esi
	jmp	.Loop_ssse3

.align	16
.Ldone_ssse3:
	addl	16(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	roll	$5,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	20(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	roll	$5,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	24(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	roll	$5,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	28(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	roll	$5,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	32(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
	roll	$5,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	rorl	$7,%ebp
	addl	%edx,%ecx
	addl	36(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	roll	$5,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	40(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	roll	$5,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	44(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	roll	$5,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	rorl	$7,%ebx
	addl	%eax,%ebp
	addl	48(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	roll	$5,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	rorl	$7,%eax
	addl	%ebp,%edx
	addl	52(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	roll	$5,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	rorl	$7,%ebp
	addl	%edx,%ecx
	addl	56(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	roll	$5,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	rorl	$7,%edx
	addl	%ecx,%ebx
	addl	60(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	roll	$5,%ebx
	addl	%edi,%eax
	rorl	$7,%ecx
	addl	%ebx,%eax
	addl	0(%r8),%eax
	addl	4(%r8),%esi
	addl	8(%r8),%ecx
	movl	%eax,0(%r8)
	addl	12(%r8),%edx
	movl	%esi,4(%r8)
	addl	16(%r8),%ebp
	movl	%ecx,8(%r8)
	movl	%edx,12(%r8)
	movl	%ebp,16(%r8)
	movq	-40(%r11),%r14
.cfi_restore	%r14
	movq	-32(%r11),%r13
.cfi_restore	%r13
	movq	-24(%r11),%r12
.cfi_restore	%r12
	movq	-16(%r11),%rbp
.cfi_restore	%rbp
	movq	-8(%r11),%rbx
.cfi_restore	%rbx
	leaq	(%r11),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_ssse3:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_block_data_order_ssse3,.-sha1_block_data_order_ssse3
.type	sha1_block_data_order_avx,@function
.align	16
sha1_block_data_order_avx:
_avx_shortcut:
.cfi_startproc	
	movq	%rsp,%r11
.cfi_def_cfa_register	%r11
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	leaq	-64(%rsp),%rsp
	vzeroupper
	andq	$-64,%rsp
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rdx,%r10

	shlq	$6,%r10
	addq	%r9,%r10
	leaq	K_XX_XX+64(%rip),%r14

	movl	0(%r8),%eax
	movl	4(%r8),%ebx
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movl	%ebx,%esi
	movl	16(%r8),%ebp
	movl	%ecx,%edi
	xorl	%edx,%edi
	andl	%edi,%esi

	vmovdqa	64(%r14),%xmm6
	vmovdqa	-64(%r14),%xmm11
	vmovdqu	0(%r9),%xmm0
	vmovdqu	16(%r9),%xmm1
	vmovdqu	32(%r9),%xmm2
	vmovdqu	48(%r9),%xmm3
	vpshufb	%xmm6,%xmm0,%xmm0
	addq	$64,%r9
	vpshufb	%xmm6,%xmm1,%xmm1
	vpshufb	%xmm6,%xmm2,%xmm2
	vpshufb	%xmm6,%xmm3,%xmm3
	vpaddd	%xmm11,%xmm0,%xmm4
	vpaddd	%xmm11,%xmm1,%xmm5
	vpaddd	%xmm11,%xmm2,%xmm6
	vmovdqa	%xmm4,0(%rsp)
	vmovdqa	%xmm5,16(%rsp)
	vmovdqa	%xmm6,32(%rsp)
	jmp	.Loop_avx
.align	16
.Loop_avx:
	shrdl	$2,%ebx,%ebx
	xorl	%edx,%esi
	vpalignr	$8,%xmm0,%xmm1,%xmm4
	movl	%eax,%edi
	addl	0(%rsp),%ebp
	vpaddd	%xmm3,%xmm11,%xmm9
	xorl	%ecx,%ebx
	shldl	$5,%eax,%eax
	vpsrldq	$4,%xmm3,%xmm8
	addl	%esi,%ebp
	andl	%ebx,%edi
	vpxor	%xmm0,%xmm4,%xmm4
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	vpxor	%xmm2,%xmm8,%xmm8
	shrdl	$7,%eax,%eax
	xorl	%ecx,%edi
	movl	%ebp,%esi
	addl	4(%rsp),%edx
	vpxor	%xmm8,%xmm4,%xmm4
	xorl	%ebx,%eax
	shldl	$5,%ebp,%ebp
	vmovdqa	%xmm9,48(%rsp)
	addl	%edi,%edx
	andl	%eax,%esi
	vpsrld	$31,%xmm4,%xmm8
	xorl	%ebx,%eax
	addl	%ebp,%edx
	shrdl	$7,%ebp,%ebp
	xorl	%ebx,%esi
	vpslldq	$12,%xmm4,%xmm10
	vpaddd	%xmm4,%xmm4,%xmm4
	movl	%edx,%edi
	addl	8(%rsp),%ecx
	xorl	%eax,%ebp
	shldl	$5,%edx,%edx
	vpsrld	$30,%xmm10,%xmm9
	vpor	%xmm8,%xmm4,%xmm4
	addl	%esi,%ecx
	andl	%ebp,%edi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	vpslld	$2,%xmm10,%xmm10
	vpxor	%xmm9,%xmm4,%xmm4
	shrdl	$7,%edx,%edx
	xorl	%eax,%edi
	movl	%ecx,%esi
	addl	12(%rsp),%ebx
	vpxor	%xmm10,%xmm4,%xmm4
	xorl	%ebp,%edx
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	andl	%edx,%esi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	shrdl	$7,%ecx,%ecx
	xorl	%ebp,%esi
	vpalignr	$8,%xmm1,%xmm2,%xmm5
	movl	%ebx,%edi
	addl	16(%rsp),%eax
	vpaddd	%xmm4,%xmm11,%xmm9
	xorl	%edx,%ecx
	shldl	$5,%ebx,%ebx
	vpsrldq	$4,%xmm4,%xmm8
	addl	%esi,%eax
	andl	%ecx,%edi
	vpxor	%xmm1,%xmm5,%xmm5
	xorl	%edx,%ecx
	addl	%ebx,%eax
	vpxor	%xmm3,%xmm8,%xmm8
	shrdl	$7,%ebx,%ebx
	xorl	%edx,%edi
	movl	%eax,%esi
	addl	20(%rsp),%ebp
	vpxor	%xmm8,%xmm5,%xmm5
	xorl	%ecx,%ebx
	shldl	$5,%eax,%eax
	vmovdqa	%xmm9,0(%rsp)
	addl	%edi,%ebp
	andl	%ebx,%esi
	vpsrld	$31,%xmm5,%xmm8
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	shrdl	$7,%eax,%eax
	xorl	%ecx,%esi
	vpslldq	$12,%xmm5,%xmm10
	vpaddd	%xmm5,%xmm5,%xmm5
	movl	%ebp,%edi
	addl	24(%rsp),%edx
	xorl	%ebx,%eax
	shldl	$5,%ebp,%ebp
	vpsrld	$30,%xmm10,%xmm9
	vpor	%xmm8,%xmm5,%xmm5
	addl	%esi,%edx
	andl	%eax,%edi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	vpslld	$2,%xmm10,%xmm10
	vpxor	%xmm9,%xmm5,%xmm5
	shrdl	$7,%ebp,%ebp
	xorl	%ebx,%edi
	movl	%edx,%esi
	addl	28(%rsp),%ecx
	vpxor	%xmm10,%xmm5,%xmm5
	xorl	%eax,%ebp
	shldl	$5,%edx,%edx
	vmovdqa	-32(%r14),%xmm11
	addl	%edi,%ecx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	shrdl	$7,%edx,%edx
	xorl	%eax,%esi
	vpalignr	$8,%xmm2,%xmm3,%xmm6
	movl	%ecx,%edi
	addl	32(%rsp),%ebx
	vpaddd	%xmm5,%xmm11,%xmm9
	xorl	%ebp,%edx
	shldl	$5,%ecx,%ecx
	vpsrldq	$4,%xmm5,%xmm8
	addl	%esi,%ebx
	andl	%edx,%edi
	vpxor	%xmm2,%xmm6,%xmm6
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	vpxor	%xmm4,%xmm8,%xmm8
	shrdl	$7,%ecx,%ecx
	xorl	%ebp,%edi
	movl	%ebx,%esi
	addl	36(%rsp),%eax
	vpxor	%xmm8,%xmm6,%xmm6
	xorl	%edx,%ecx
	shldl	$5,%ebx,%ebx
	vmovdqa	%xmm9,16(%rsp)
	addl	%edi,%eax
	andl	%ecx,%esi
	vpsrld	$31,%xmm6,%xmm8
	xorl	%edx,%ecx
	addl	%ebx,%eax
	shrdl	$7,%ebx,%ebx
	xorl	%edx,%esi
	vpslldq	$12,%xmm6,%xmm10
	vpaddd	%xmm6,%xmm6,%xmm6
	movl	%eax,%edi
	addl	40(%rsp),%ebp
	xorl	%ecx,%ebx
	shldl	$5,%eax,%eax
	vpsrld	$30,%xmm10,%xmm9
	vpor	%xmm8,%xmm6,%xmm6
	addl	%esi,%ebp
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	vpslld	$2,%xmm10,%xmm10
	vpxor	%xmm9,%xmm6,%xmm6
	shrdl	$7,%eax,%eax
	xorl	%ecx,%edi
	movl	%ebp,%esi
	addl	44(%rsp),%edx
	vpxor	%xmm10,%xmm6,%xmm6
	xorl	%ebx,%eax
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	andl	%eax,%esi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	shrdl	$7,%ebp,%ebp
	xorl	%ebx,%esi
	vpalignr	$8,%xmm3,%xmm4,%xmm7
	movl	%edx,%edi
	addl	48(%rsp),%ecx
	vpaddd	%xmm6,%xmm11,%xmm9
	xorl	%eax,%ebp
	shldl	$5,%edx,%edx
	vpsrldq	$4,%xmm6,%xmm8
	addl	%esi,%ecx
	andl	%ebp,%edi
	vpxor	%xmm3,%xmm7,%xmm7
	xorl	%eax,%ebp
	addl	%edx,%ecx
	vpxor	%xmm5,%xmm8,%xmm8
	shrdl	$7,%edx,%edx
	xorl	%eax,%edi
	movl	%ecx,%esi
	addl	52(%rsp),%ebx
	vpxor	%xmm8,%xmm7,%xmm7
	xorl	%ebp,%edx
	shldl	$5,%ecx,%ecx
	vmovdqa	%xmm9,32(%rsp)
	addl	%edi,%ebx
	andl	%edx,%esi
	vpsrld	$31,%xmm7,%xmm8
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	shrdl	$7,%ecx,%ecx
	xorl	%ebp,%esi
	vpslldq	$12,%xmm7,%xmm10
	vpaddd	%xmm7,%xmm7,%xmm7
	movl	%ebx,%edi
	addl	56(%rsp),%eax
	xorl	%edx,%ecx
	shldl	$5,%ebx,%ebx
	vpsrld	$30,%xmm10,%xmm9
	vpor	%xmm8,%xmm7,%xmm7
	addl	%esi,%eax
	andl	%ecx,%edi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	vpslld	$2,%xmm10,%xmm10
	vpxor	%xmm9,%xmm7,%xmm7
	shrdl	$7,%ebx,%ebx
	xorl	%edx,%edi
	movl	%eax,%esi
	addl	60(%rsp),%ebp
	vpxor	%xmm10,%xmm7,%xmm7
	xorl	%ecx,%ebx
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	andl	%ebx,%esi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	vpalignr	$8,%xmm6,%xmm7,%xmm8
	vpxor	%xmm4,%xmm0,%xmm0
	shrdl	$7,%eax,%eax
	xorl	%ecx,%esi
	movl	%ebp,%edi
	addl	0(%rsp),%edx
	vpxor	%xmm1,%xmm0,%xmm0
	xorl	%ebx,%eax
	shldl	$5,%ebp,%ebp
	vpaddd	%xmm7,%xmm11,%xmm9
	addl	%esi,%edx
	andl	%eax,%edi
	vpxor	%xmm8,%xmm0,%xmm0
	xorl	%ebx,%eax
	addl	%ebp,%edx
	shrdl	$7,%ebp,%ebp
	xorl	%ebx,%edi
	vpsrld	$30,%xmm0,%xmm8
	vmovdqa	%xmm9,48(%rsp)
	movl	%edx,%esi
	addl	4(%rsp),%ecx
	xorl	%eax,%ebp
	shldl	$5,%edx,%edx
	vpslld	$2,%xmm0,%xmm0
	addl	%edi,%ecx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	shrdl	$7,%edx,%edx
	xorl	%eax,%esi
	movl	%ecx,%edi
	addl	8(%rsp),%ebx
	vpor	%xmm8,%xmm0,%xmm0
	xorl	%ebp,%edx
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	andl	%edx,%edi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	12(%rsp),%eax
	xorl	%ebp,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	vpalignr	$8,%xmm7,%xmm0,%xmm8
	vpxor	%xmm5,%xmm1,%xmm1
	addl	16(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	shldl	$5,%eax,%eax
	vpxor	%xmm2,%xmm1,%xmm1
	addl	%esi,%ebp
	xorl	%ecx,%edi
	vpaddd	%xmm0,%xmm11,%xmm9
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	vpxor	%xmm8,%xmm1,%xmm1
	addl	20(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	shldl	$5,%ebp,%ebp
	vpsrld	$30,%xmm1,%xmm8
	vmovdqa	%xmm9,0(%rsp)
	addl	%edi,%edx
	xorl	%ebx,%esi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vpslld	$2,%xmm1,%xmm1
	addl	24(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
	shldl	$5,%edx,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vpor	%xmm8,%xmm1,%xmm1
	addl	28(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	vpalignr	$8,%xmm0,%xmm1,%xmm8
	vpxor	%xmm6,%xmm2,%xmm2
	addl	32(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	shldl	$5,%ebx,%ebx
	vpxor	%xmm3,%xmm2,%xmm2
	addl	%esi,%eax
	xorl	%edx,%edi
	vpaddd	%xmm1,%xmm11,%xmm9
	vmovdqa	0(%r14),%xmm11
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	vpxor	%xmm8,%xmm2,%xmm2
	addl	36(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	shldl	$5,%eax,%eax
	vpsrld	$30,%xmm2,%xmm8
	vmovdqa	%xmm9,16(%rsp)
	addl	%edi,%ebp
	xorl	%ecx,%esi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	vpslld	$2,%xmm2,%xmm2
	addl	40(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	shldl	$5,%ebp,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vpor	%xmm8,%xmm2,%xmm2
	addl	44(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vpalignr	$8,%xmm1,%xmm2,%xmm8
	vpxor	%xmm7,%xmm3,%xmm3
	addl	48(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	vpxor	%xmm4,%xmm3,%xmm3
	addl	%esi,%ebx
	xorl	%ebp,%edi
	vpaddd	%xmm2,%xmm11,%xmm9
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	vpxor	%xmm8,%xmm3,%xmm3
	addl	52(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	vpsrld	$30,%xmm3,%xmm8
	vmovdqa	%xmm9,32(%rsp)
	addl	%edi,%eax
	xorl	%edx,%esi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	vpslld	$2,%xmm3,%xmm3
	addl	56(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	shldl	$5,%eax,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	vpor	%xmm8,%xmm3,%xmm3
	addl	60(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vpalignr	$8,%xmm2,%xmm3,%xmm8
	vpxor	%xmm0,%xmm4,%xmm4
	addl	0(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
	shldl	$5,%edx,%edx
	vpxor	%xmm5,%xmm4,%xmm4
	addl	%esi,%ecx
	xorl	%eax,%edi
	vpaddd	%xmm3,%xmm11,%xmm9
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vpxor	%xmm8,%xmm4,%xmm4
	addl	4(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	shldl	$5,%ecx,%ecx
	vpsrld	$30,%xmm4,%xmm8
	vmovdqa	%xmm9,48(%rsp)
	addl	%edi,%ebx
	xorl	%ebp,%esi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	vpslld	$2,%xmm4,%xmm4
	addl	8(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	vpor	%xmm8,%xmm4,%xmm4
	addl	12(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	vpalignr	$8,%xmm3,%xmm4,%xmm8
	vpxor	%xmm1,%xmm5,%xmm5
	addl	16(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	shldl	$5,%ebp,%ebp
	vpxor	%xmm6,%xmm5,%xmm5
	addl	%esi,%edx
	xorl	%ebx,%edi
	vpaddd	%xmm4,%xmm11,%xmm9
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vpxor	%xmm8,%xmm5,%xmm5
	addl	20(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	shldl	$5,%edx,%edx
	vpsrld	$30,%xmm5,%xmm8
	vmovdqa	%xmm9,0(%rsp)
	addl	%edi,%ecx
	xorl	%eax,%esi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vpslld	$2,%xmm5,%xmm5
	addl	24(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	vpor	%xmm8,%xmm5,%xmm5
	addl	28(%rsp),%eax
	shrdl	$7,%ecx,%ecx
	movl	%ebx,%esi
	xorl	%edx,%edi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%ecx,%esi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	vpalignr	$8,%xmm4,%xmm5,%xmm8
	vpxor	%xmm2,%xmm6,%xmm6
	addl	32(%rsp),%ebp
	andl	%ecx,%esi
	xorl	%edx,%ecx
	shrdl	$7,%ebx,%ebx
	vpxor	%xmm7,%xmm6,%xmm6
	movl	%eax,%edi
	xorl	%ecx,%esi
	vpaddd	%xmm5,%xmm11,%xmm9
	shldl	$5,%eax,%eax
	addl	%esi,%ebp
	vpxor	%xmm8,%xmm6,%xmm6
	xorl	%ebx,%edi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	addl	36(%rsp),%edx
	vpsrld	$30,%xmm6,%xmm8
	vmovdqa	%xmm9,16(%rsp)
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	shrdl	$7,%eax,%eax
	movl	%ebp,%esi
	vpslld	$2,%xmm6,%xmm6
	xorl	%ebx,%edi
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	xorl	%eax,%esi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	addl	40(%rsp),%ecx
	andl	%eax,%esi
	vpor	%xmm8,%xmm6,%xmm6
	xorl	%ebx,%eax
	shrdl	$7,%ebp,%ebp
	movl	%edx,%edi
	xorl	%eax,%esi
	shldl	$5,%edx,%edx
	addl	%esi,%ecx
	xorl	%ebp,%edi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	addl	44(%rsp),%ebx
	andl	%ebp,%edi
	xorl	%eax,%ebp
	shrdl	$7,%edx,%edx
	movl	%ecx,%esi
	xorl	%ebp,%edi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%edx,%esi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	vpalignr	$8,%xmm5,%xmm6,%xmm8
	vpxor	%xmm3,%xmm7,%xmm7
	addl	48(%rsp),%eax
	andl	%edx,%esi
	xorl	%ebp,%edx
	shrdl	$7,%ecx,%ecx
	vpxor	%xmm0,%xmm7,%xmm7
	movl	%ebx,%edi
	xorl	%edx,%esi
	vpaddd	%xmm6,%xmm11,%xmm9
	vmovdqa	32(%r14),%xmm11
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	vpxor	%xmm8,%xmm7,%xmm7
	xorl	%ecx,%edi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	addl	52(%rsp),%ebp
	vpsrld	$30,%xmm7,%xmm8
	vmovdqa	%xmm9,32(%rsp)
	andl	%ecx,%edi
	xorl	%edx,%ecx
	shrdl	$7,%ebx,%ebx
	movl	%eax,%esi
	vpslld	$2,%xmm7,%xmm7
	xorl	%ecx,%edi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ebx,%esi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	addl	56(%rsp),%edx
	andl	%ebx,%esi
	vpor	%xmm8,%xmm7,%xmm7
	xorl	%ecx,%ebx
	shrdl	$7,%eax,%eax
	movl	%ebp,%edi
	xorl	%ebx,%esi
	shldl	$5,%ebp,%ebp
	addl	%esi,%edx
	xorl	%eax,%edi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	addl	60(%rsp),%ecx
	andl	%eax,%edi
	xorl	%ebx,%eax
	shrdl	$7,%ebp,%ebp
	movl	%edx,%esi
	xorl	%eax,%edi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%ebp,%esi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	vpalignr	$8,%xmm6,%xmm7,%xmm8
	vpxor	%xmm4,%xmm0,%xmm0
	addl	0(%rsp),%ebx
	andl	%ebp,%esi
	xorl	%eax,%ebp
	shrdl	$7,%edx,%edx
	vpxor	%xmm1,%xmm0,%xmm0
	movl	%ecx,%edi
	xorl	%ebp,%esi
	vpaddd	%xmm7,%xmm11,%xmm9
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	vpxor	%xmm8,%xmm0,%xmm0
	xorl	%edx,%edi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	4(%rsp),%eax
	vpsrld	$30,%xmm0,%xmm8
	vmovdqa	%xmm9,48(%rsp)
	andl	%edx,%edi
	xorl	%ebp,%edx
	shrdl	$7,%ecx,%ecx
	movl	%ebx,%esi
	vpslld	$2,%xmm0,%xmm0
	xorl	%edx,%edi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%ecx,%esi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	addl	8(%rsp),%ebp
	andl	%ecx,%esi
	vpor	%xmm8,%xmm0,%xmm0
	xorl	%edx,%ecx
	shrdl	$7,%ebx,%ebx
	movl	%eax,%edi
	xorl	%ecx,%esi
	shldl	$5,%eax,%eax
	addl	%esi,%ebp
	xorl	%ebx,%edi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	addl	12(%rsp),%edx
	andl	%ebx,%edi
	xorl	%ecx,%ebx
	shrdl	$7,%eax,%eax
	movl	%ebp,%esi
	xorl	%ebx,%edi
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	xorl	%eax,%esi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	vpalignr	$8,%xmm7,%xmm0,%xmm8
	vpxor	%xmm5,%xmm1,%xmm1
	addl	16(%rsp),%ecx
	andl	%eax,%esi
	xorl	%ebx,%eax
	shrdl	$7,%ebp,%ebp
	vpxor	%xmm2,%xmm1,%xmm1
	movl	%edx,%edi
	xorl	%eax,%esi
	vpaddd	%xmm0,%xmm11,%xmm9
	shldl	$5,%edx,%edx
	addl	%esi,%ecx
	vpxor	%xmm8,%xmm1,%xmm1
	xorl	%ebp,%edi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	addl	20(%rsp),%ebx
	vpsrld	$30,%xmm1,%xmm8
	vmovdqa	%xmm9,0(%rsp)
	andl	%ebp,%edi
	xorl	%eax,%ebp
	shrdl	$7,%edx,%edx
	movl	%ecx,%esi
	vpslld	$2,%xmm1,%xmm1
	xorl	%ebp,%edi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%edx,%esi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	24(%rsp),%eax
	andl	%edx,%esi
	vpor	%xmm8,%xmm1,%xmm1
	xorl	%ebp,%edx
	shrdl	$7,%ecx,%ecx
	movl	%ebx,%edi
	xorl	%edx,%esi
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	xorl	%ecx,%edi
	xorl	%edx,%ecx
	addl	%ebx,%eax
	addl	28(%rsp),%ebp
	andl	%ecx,%edi
	xorl	%edx,%ecx
	shrdl	$7,%ebx,%ebx
	movl	%eax,%esi
	xorl	%ecx,%edi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ebx,%esi
	xorl	%ecx,%ebx
	addl	%eax,%ebp
	vpalignr	$8,%xmm0,%xmm1,%xmm8
	vpxor	%xmm6,%xmm2,%xmm2
	addl	32(%rsp),%edx
	andl	%ebx,%esi
	xorl	%ecx,%ebx
	shrdl	$7,%eax,%eax
	vpxor	%xmm3,%xmm2,%xmm2
	movl	%ebp,%edi
	xorl	%ebx,%esi
	vpaddd	%xmm1,%xmm11,%xmm9
	shldl	$5,%ebp,%ebp
	addl	%esi,%edx
	vpxor	%xmm8,%xmm2,%xmm2
	xorl	%eax,%edi
	xorl	%ebx,%eax
	addl	%ebp,%edx
	addl	36(%rsp),%ecx
	vpsrld	$30,%xmm2,%xmm8
	vmovdqa	%xmm9,16(%rsp)
	andl	%eax,%edi
	xorl	%ebx,%eax
	shrdl	$7,%ebp,%ebp
	movl	%edx,%esi
	vpslld	$2,%xmm2,%xmm2
	xorl	%eax,%edi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%ebp,%esi
	xorl	%eax,%ebp
	addl	%edx,%ecx
	addl	40(%rsp),%ebx
	andl	%ebp,%esi
	vpor	%xmm8,%xmm2,%xmm2
	xorl	%eax,%ebp
	shrdl	$7,%edx,%edx
	movl	%ecx,%edi
	xorl	%ebp,%esi
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	xorl	%edx,%edi
	xorl	%ebp,%edx
	addl	%ecx,%ebx
	addl	44(%rsp),%eax
	andl	%edx,%edi
	xorl	%ebp,%edx
	shrdl	$7,%ecx,%ecx
	movl	%ebx,%esi
	xorl	%edx,%edi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	addl	%ebx,%eax
	vpalignr	$8,%xmm1,%xmm2,%xmm8
	vpxor	%xmm7,%xmm3,%xmm3
	addl	48(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	shldl	$5,%eax,%eax
	vpxor	%xmm4,%xmm3,%xmm3
	addl	%esi,%ebp
	xorl	%ecx,%edi
	vpaddd	%xmm2,%xmm11,%xmm9
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	vpxor	%xmm8,%xmm3,%xmm3
	addl	52(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	shldl	$5,%ebp,%ebp
	vpsrld	$30,%xmm3,%xmm8
	vmovdqa	%xmm9,32(%rsp)
	addl	%edi,%edx
	xorl	%ebx,%esi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vpslld	$2,%xmm3,%xmm3
	addl	56(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
	shldl	$5,%edx,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vpor	%xmm8,%xmm3,%xmm3
	addl	60(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	0(%rsp),%eax
	vpaddd	%xmm3,%xmm11,%xmm9
	xorl	%edx,%esi
	movl	%ebx,%edi
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	vmovdqa	%xmm9,48(%rsp)
	xorl	%edx,%edi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	4(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	addl	8(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	shldl	$5,%ebp,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	addl	12(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	cmpq	%r10,%r9
	je	.Ldone_avx
	vmovdqa	64(%r14),%xmm6
	vmovdqa	-64(%r14),%xmm11
	vmovdqu	0(%r9),%xmm0
	vmovdqu	16(%r9),%xmm1
	vmovdqu	32(%r9),%xmm2
	vmovdqu	48(%r9),%xmm3
	vpshufb	%xmm6,%xmm0,%xmm0
	addq	$64,%r9
	addl	16(%rsp),%ebx
	xorl	%ebp,%esi
	vpshufb	%xmm6,%xmm1,%xmm1
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	vpaddd	%xmm11,%xmm0,%xmm4
	addl	%esi,%ebx
	xorl	%ebp,%edi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	vmovdqa	%xmm4,0(%rsp)
	addl	20(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	24(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	shldl	$5,%eax,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	addl	28(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	addl	32(%rsp),%ecx
	xorl	%eax,%esi
	vpshufb	%xmm6,%xmm2,%xmm2
	movl	%edx,%edi
	shldl	$5,%edx,%edx
	vpaddd	%xmm11,%xmm1,%xmm5
	addl	%esi,%ecx
	xorl	%eax,%edi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	vmovdqa	%xmm5,16(%rsp)
	addl	36(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	40(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	44(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	addl	48(%rsp),%edx
	xorl	%ebx,%esi
	vpshufb	%xmm6,%xmm3,%xmm3
	movl	%ebp,%edi
	shldl	$5,%ebp,%ebp
	vpaddd	%xmm11,%xmm2,%xmm6
	addl	%esi,%edx
	xorl	%ebx,%edi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	vmovdqa	%xmm6,32(%rsp)
	addl	52(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	addl	56(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	60(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	0(%r8),%eax
	addl	4(%r8),%esi
	addl	8(%r8),%ecx
	addl	12(%r8),%edx
	movl	%eax,0(%r8)
	addl	16(%r8),%ebp
	movl	%esi,4(%r8)
	movl	%esi,%ebx
	movl	%ecx,8(%r8)
	movl	%ecx,%edi
	movl	%edx,12(%r8)
	xorl	%edx,%edi
	movl	%ebp,16(%r8)
	andl	%edi,%esi
	jmp	.Loop_avx

.align	16
.Ldone_avx:
	addl	16(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	20(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	xorl	%edx,%esi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	24(%rsp),%ebp
	xorl	%ecx,%esi
	movl	%eax,%edi
	shldl	$5,%eax,%eax
	addl	%esi,%ebp
	xorl	%ecx,%edi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	addl	28(%rsp),%edx
	xorl	%ebx,%edi
	movl	%ebp,%esi
	shldl	$5,%ebp,%ebp
	addl	%edi,%edx
	xorl	%ebx,%esi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	addl	32(%rsp),%ecx
	xorl	%eax,%esi
	movl	%edx,%edi
	shldl	$5,%edx,%edx
	addl	%esi,%ecx
	xorl	%eax,%edi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	addl	36(%rsp),%ebx
	xorl	%ebp,%edi
	movl	%ecx,%esi
	shldl	$5,%ecx,%ecx
	addl	%edi,%ebx
	xorl	%ebp,%esi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	40(%rsp),%eax
	xorl	%edx,%esi
	movl	%ebx,%edi
	shldl	$5,%ebx,%ebx
	addl	%esi,%eax
	xorl	%edx,%edi
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	addl	44(%rsp),%ebp
	xorl	%ecx,%edi
	movl	%eax,%esi
	shldl	$5,%eax,%eax
	addl	%edi,%ebp
	xorl	%ecx,%esi
	shrdl	$7,%ebx,%ebx
	addl	%eax,%ebp
	addl	48(%rsp),%edx
	xorl	%ebx,%esi
	movl	%ebp,%edi
	shldl	$5,%ebp,%ebp
	addl	%esi,%edx
	xorl	%ebx,%edi
	shrdl	$7,%eax,%eax
	addl	%ebp,%edx
	addl	52(%rsp),%ecx
	xorl	%eax,%edi
	movl	%edx,%esi
	shldl	$5,%edx,%edx
	addl	%edi,%ecx
	xorl	%eax,%esi
	shrdl	$7,%ebp,%ebp
	addl	%edx,%ecx
	addl	56(%rsp),%ebx
	xorl	%ebp,%esi
	movl	%ecx,%edi
	shldl	$5,%ecx,%ecx
	addl	%esi,%ebx
	xorl	%ebp,%edi
	shrdl	$7,%edx,%edx
	addl	%ecx,%ebx
	addl	60(%rsp),%eax
	xorl	%edx,%edi
	movl	%ebx,%esi
	shldl	$5,%ebx,%ebx
	addl	%edi,%eax
	shrdl	$7,%ecx,%ecx
	addl	%ebx,%eax
	vzeroupper

	addl	0(%r8),%eax
	addl	4(%r8),%esi
	addl	8(%r8),%ecx
	movl	%eax,0(%r8)
	addl	12(%r8),%edx
	movl	%esi,4(%r8)
	addl	16(%r8),%ebp
	movl	%ecx,8(%r8)
	movl	%edx,12(%r8)
	movl	%ebp,16(%r8)
	movq	-40(%r11),%r14
.cfi_restore	%r14
	movq	-32(%r11),%r13
.cfi_restore	%r13
	movq	-24(%r11),%r12
.cfi_restore	%r12
	movq	-16(%r11),%rbp
.cfi_restore	%rbp
	movq	-8(%r11),%rbx
.cfi_restore	%rbx
	leaq	(%r11),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_block_data_order_avx,.-sha1_block_data_order_avx
.type	sha1_block_data_order_avx2,@function
.align	16
sha1_block_data_order_avx2:
_avx2_shortcut:
.cfi_startproc	
	movq	%rsp,%r11
.cfi_def_cfa_register	%r11
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	vzeroupper
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rdx,%r10

	leaq	-640(%rsp),%rsp
	shlq	$6,%r10
	leaq	64(%r9),%r13
	andq	$-128,%rsp
	addq	%r9,%r10
	leaq	K_XX_XX+64(%rip),%r14

	movl	0(%r8),%eax
	cmpq	%r10,%r13
	cmovaeq	%r9,%r13
	movl	4(%r8),%ebp
	movl	8(%r8),%ecx
	movl	12(%r8),%edx
	movl	16(%r8),%esi
	vmovdqu	64(%r14),%ymm6

	vmovdqu	(%r9),%xmm0
	vmovdqu	16(%r9),%xmm1
	vmovdqu	32(%r9),%xmm2
	vmovdqu	48(%r9),%xmm3
	leaq	64(%r9),%r9
	vinserti128	$1,(%r13),%ymm0,%ymm0
	vinserti128	$1,16(%r13),%ymm1,%ymm1
	vpshufb	%ymm6,%ymm0,%ymm0
	vinserti128	$1,32(%r13),%ymm2,%ymm2
	vpshufb	%ymm6,%ymm1,%ymm1
	vinserti128	$1,48(%r13),%ymm3,%ymm3
	vpshufb	%ymm6,%ymm2,%ymm2
	vmovdqu	-64(%r14),%ymm11
	vpshufb	%ymm6,%ymm3,%ymm3

	vpaddd	%ymm11,%ymm0,%ymm4
	vpaddd	%ymm11,%ymm1,%ymm5
	vmovdqu	%ymm4,0(%rsp)
	vpaddd	%ymm11,%ymm2,%ymm6
	vmovdqu	%ymm5,32(%rsp)
	vpaddd	%ymm11,%ymm3,%ymm7
	vmovdqu	%ymm6,64(%rsp)
	vmovdqu	%ymm7,96(%rsp)
	vpalignr	$8,%ymm0,%ymm1,%ymm4
	vpsrldq	$4,%ymm3,%ymm8
	vpxor	%ymm0,%ymm4,%ymm4
	vpxor	%ymm2,%ymm8,%ymm8
	vpxor	%ymm8,%ymm4,%ymm4
	vpsrld	$31,%ymm4,%ymm8
	vpslldq	$12,%ymm4,%ymm10
	vpaddd	%ymm4,%ymm4,%ymm4
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm4,%ymm4
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm4,%ymm4
	vpxor	%ymm10,%ymm4,%ymm4
	vpaddd	%ymm11,%ymm4,%ymm9
	vmovdqu	%ymm9,128(%rsp)
	vpalignr	$8,%ymm1,%ymm2,%ymm5
	vpsrldq	$4,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm3,%ymm8,%ymm8
	vpxor	%ymm8,%ymm5,%ymm5
	vpsrld	$31,%ymm5,%ymm8
	vmovdqu	-32(%r14),%ymm11
	vpslldq	$12,%ymm5,%ymm10
	vpaddd	%ymm5,%ymm5,%ymm5
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm5,%ymm5
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm5,%ymm5
	vpxor	%ymm10,%ymm5,%ymm5
	vpaddd	%ymm11,%ymm5,%ymm9
	vmovdqu	%ymm9,160(%rsp)
	vpalignr	$8,%ymm2,%ymm3,%ymm6
	vpsrldq	$4,%ymm5,%ymm8
	vpxor	%ymm2,%ymm6,%ymm6
	vpxor	%ymm4,%ymm8,%ymm8
	vpxor	%ymm8,%ymm6,%ymm6
	vpsrld	$31,%ymm6,%ymm8
	vpslldq	$12,%ymm6,%ymm10
	vpaddd	%ymm6,%ymm6,%ymm6
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm6,%ymm6
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm6,%ymm6
	vpxor	%ymm10,%ymm6,%ymm6
	vpaddd	%ymm11,%ymm6,%ymm9
	vmovdqu	%ymm9,192(%rsp)
	vpalignr	$8,%ymm3,%ymm4,%ymm7
	vpsrldq	$4,%ymm6,%ymm8
	vpxor	%ymm3,%ymm7,%ymm7
	vpxor	%ymm5,%ymm8,%ymm8
	vpxor	%ymm8,%ymm7,%ymm7
	vpsrld	$31,%ymm7,%ymm8
	vpslldq	$12,%ymm7,%ymm10
	vpaddd	%ymm7,%ymm7,%ymm7
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm7,%ymm7
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm7,%ymm7
	vpxor	%ymm10,%ymm7,%ymm7
	vpaddd	%ymm11,%ymm7,%ymm9
	vmovdqu	%ymm9,224(%rsp)
	leaq	128(%rsp),%r13
	jmp	.Loop_avx2
.align	32
.Loop_avx2:
	rorxl	$2,%ebp,%ebx
	andnl	%edx,%ebp,%edi
	andl	%ecx,%ebp
	xorl	%edi,%ebp
	jmp	.Lalign32_1
.align	32
.Lalign32_1:
	vpalignr	$8,%ymm6,%ymm7,%ymm8
	vpxor	%ymm4,%ymm0,%ymm0
	addl	-128(%r13),%esi
	andnl	%ecx,%eax,%edi
	vpxor	%ymm1,%ymm0,%ymm0
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	vpxor	%ymm8,%ymm0,%ymm0
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	vpsrld	$30,%ymm0,%ymm8
	vpslld	$2,%ymm0,%ymm0
	addl	-124(%r13),%edx
	andnl	%ebx,%esi,%edi
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	vpor	%ymm8,%ymm0,%ymm0
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-120(%r13),%ecx
	andnl	%ebp,%edx,%edi
	vpaddd	%ymm11,%ymm0,%ymm9
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	vmovdqu	%ymm9,256(%rsp)
	addl	%r12d,%ecx
	xorl	%edi,%edx
	addl	-116(%r13),%ebx
	andnl	%eax,%ecx,%edi
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	addl	-96(%r13),%ebp
	andnl	%esi,%ebx,%edi
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	andl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	vpalignr	$8,%ymm7,%ymm0,%ymm8
	vpxor	%ymm5,%ymm1,%ymm1
	addl	-92(%r13),%eax
	andnl	%edx,%ebp,%edi
	vpxor	%ymm2,%ymm1,%ymm1
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	vpxor	%ymm8,%ymm1,%ymm1
	andl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edi,%ebp
	vpsrld	$30,%ymm1,%ymm8
	vpslld	$2,%ymm1,%ymm1
	addl	-88(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	vpor	%ymm8,%ymm1,%ymm1
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	-84(%r13),%edx
	andnl	%ebx,%esi,%edi
	vpaddd	%ymm11,%ymm1,%ymm9
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	vmovdqu	%ymm9,288(%rsp)
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-64(%r13),%ecx
	andnl	%ebp,%edx,%edi
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%edi,%edx
	addl	-60(%r13),%ebx
	andnl	%eax,%ecx,%edi
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	vpalignr	$8,%ymm0,%ymm1,%ymm8
	vpxor	%ymm6,%ymm2,%ymm2
	addl	-56(%r13),%ebp
	andnl	%esi,%ebx,%edi
	vpxor	%ymm3,%ymm2,%ymm2
	vmovdqu	0(%r14),%ymm11
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	vpxor	%ymm8,%ymm2,%ymm2
	andl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	vpsrld	$30,%ymm2,%ymm8
	vpslld	$2,%ymm2,%ymm2
	addl	-52(%r13),%eax
	andnl	%edx,%ebp,%edi
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	andl	%ecx,%ebp
	vpor	%ymm8,%ymm2,%ymm2
	addl	%r12d,%eax
	xorl	%edi,%ebp
	addl	-32(%r13),%esi
	andnl	%ecx,%eax,%edi
	vpaddd	%ymm11,%ymm2,%ymm9
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	vmovdqu	%ymm9,320(%rsp)
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	-28(%r13),%edx
	andnl	%ebx,%esi,%edi
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-24(%r13),%ecx
	andnl	%ebp,%edx,%edi
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%edi,%edx
	vpalignr	$8,%ymm1,%ymm2,%ymm8
	vpxor	%ymm7,%ymm3,%ymm3
	addl	-20(%r13),%ebx
	andnl	%eax,%ecx,%edi
	vpxor	%ymm4,%ymm3,%ymm3
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	vpxor	%ymm8,%ymm3,%ymm3
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	vpsrld	$30,%ymm3,%ymm8
	vpslld	$2,%ymm3,%ymm3
	addl	0(%r13),%ebp
	andnl	%esi,%ebx,%edi
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	andl	%edx,%ebx
	vpor	%ymm8,%ymm3,%ymm3
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	addl	4(%r13),%eax
	andnl	%edx,%ebp,%edi
	vpaddd	%ymm11,%ymm3,%ymm9
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	andl	%ecx,%ebp
	vmovdqu	%ymm9,352(%rsp)
	addl	%r12d,%eax
	xorl	%edi,%ebp
	addl	8(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	12(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	vpalignr	$8,%ymm2,%ymm3,%ymm8
	vpxor	%ymm0,%ymm4,%ymm4
	addl	32(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	vpxor	%ymm8,%ymm4,%ymm4
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	36(%r13),%ebx
	vpsrld	$30,%ymm4,%ymm8
	vpslld	$2,%ymm4,%ymm4
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	vpor	%ymm8,%ymm4,%ymm4
	addl	40(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	vpaddd	%ymm11,%ymm4,%ymm9
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	44(%r13),%eax
	vmovdqu	%ymm9,384(%rsp)
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	64(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	vpalignr	$8,%ymm3,%ymm4,%ymm8
	vpxor	%ymm1,%ymm5,%ymm5
	addl	68(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	vpxor	%ymm6,%ymm5,%ymm5
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	vpxor	%ymm8,%ymm5,%ymm5
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	72(%r13),%ecx
	vpsrld	$30,%ymm5,%ymm8
	vpslld	$2,%ymm5,%ymm5
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	vpor	%ymm8,%ymm5,%ymm5
	addl	76(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	vpaddd	%ymm11,%ymm5,%ymm9
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	96(%r13),%ebp
	vmovdqu	%ymm9,416(%rsp)
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	100(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	vpalignr	$8,%ymm4,%ymm5,%ymm8
	vpxor	%ymm2,%ymm6,%ymm6
	addl	104(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	vpxor	%ymm7,%ymm6,%ymm6
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	vpxor	%ymm8,%ymm6,%ymm6
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	108(%r13),%edx
	leaq	256(%r13),%r13
	vpsrld	$30,%ymm6,%ymm8
	vpslld	$2,%ymm6,%ymm6
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	vpor	%ymm8,%ymm6,%ymm6
	addl	-128(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	vpaddd	%ymm11,%ymm6,%ymm9
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-124(%r13),%ebx
	vmovdqu	%ymm9,448(%rsp)
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-120(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	vpalignr	$8,%ymm5,%ymm6,%ymm8
	vpxor	%ymm3,%ymm7,%ymm7
	addl	-116(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	vpxor	%ymm0,%ymm7,%ymm7
	vmovdqu	32(%r14),%ymm11
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	vpxor	%ymm8,%ymm7,%ymm7
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-96(%r13),%esi
	vpsrld	$30,%ymm7,%ymm8
	vpslld	$2,%ymm7,%ymm7
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	vpor	%ymm8,%ymm7,%ymm7
	addl	-92(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	vpaddd	%ymm11,%ymm7,%ymm9
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	-88(%r13),%ecx
	vmovdqu	%ymm9,480(%rsp)
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-84(%r13),%ebx
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	andl	%edi,%ecx
	jmp	.Lalign32_2
.align	32
.Lalign32_2:
	vpalignr	$8,%ymm6,%ymm7,%ymm8
	vpxor	%ymm4,%ymm0,%ymm0
	addl	-64(%r13),%ebp
	xorl	%esi,%ecx
	vpxor	%ymm1,%ymm0,%ymm0
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	vpxor	%ymm8,%ymm0,%ymm0
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	vpsrld	$30,%ymm0,%ymm8
	vpslld	$2,%ymm0,%ymm0
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	-60(%r13),%eax
	xorl	%edx,%ebx
	movl	%ecx,%edi
	xorl	%edx,%edi
	vpor	%ymm8,%ymm0,%ymm0
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	vpaddd	%ymm11,%ymm0,%ymm9
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	-56(%r13),%esi
	xorl	%ecx,%ebp
	vmovdqu	%ymm9,512(%rsp)
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	addl	-52(%r13),%edx
	xorl	%ebx,%eax
	movl	%ebp,%edi
	xorl	%ebx,%edi
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	andl	%edi,%esi
	addl	-32(%r13),%ecx
	xorl	%ebp,%esi
	movl	%eax,%edi
	xorl	%ebp,%edi
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	andl	%edi,%edx
	vpalignr	$8,%ymm7,%ymm0,%ymm8
	vpxor	%ymm5,%ymm1,%ymm1
	addl	-28(%r13),%ebx
	xorl	%eax,%edx
	vpxor	%ymm2,%ymm1,%ymm1
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	vpxor	%ymm8,%ymm1,%ymm1
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	vpsrld	$30,%ymm1,%ymm8
	vpslld	$2,%ymm1,%ymm1
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	-24(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	vpor	%ymm8,%ymm1,%ymm1
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	vpaddd	%ymm11,%ymm1,%ymm9
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	-20(%r13),%eax
	xorl	%edx,%ebx
	vmovdqu	%ymm9,544(%rsp)
	movl	%ecx,%edi
	xorl	%edx,%edi
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	0(%r13),%esi
	xorl	%ecx,%ebp
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	addl	4(%r13),%edx
	xorl	%ebx,%eax
	movl	%ebp,%edi
	xorl	%ebx,%edi
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	andl	%edi,%esi
	vpalignr	$8,%ymm0,%ymm1,%ymm8
	vpxor	%ymm6,%ymm2,%ymm2
	addl	8(%r13),%ecx
	xorl	%ebp,%esi
	vpxor	%ymm3,%ymm2,%ymm2
	movl	%eax,%edi
	xorl	%ebp,%edi
	leal	(%rcx,%rsi,1),%ecx
	vpxor	%ymm8,%ymm2,%ymm2
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	vpsrld	$30,%ymm2,%ymm8
	vpslld	$2,%ymm2,%ymm2
	addl	%r12d,%ecx
	andl	%edi,%edx
	addl	12(%r13),%ebx
	xorl	%eax,%edx
	movl	%esi,%edi
	xorl	%eax,%edi
	vpor	%ymm8,%ymm2,%ymm2
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	vpaddd	%ymm11,%ymm2,%ymm9
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	32(%r13),%ebp
	xorl	%esi,%ecx
	vmovdqu	%ymm9,576(%rsp)
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	36(%r13),%eax
	xorl	%edx,%ebx
	movl	%ecx,%edi
	xorl	%edx,%edi
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	40(%r13),%esi
	xorl	%ecx,%ebp
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	vpalignr	$8,%ymm1,%ymm2,%ymm8
	vpxor	%ymm7,%ymm3,%ymm3
	addl	44(%r13),%edx
	xorl	%ebx,%eax
	vpxor	%ymm4,%ymm3,%ymm3
	movl	%ebp,%edi
	xorl	%ebx,%edi
	leal	(%rdx,%rax,1),%edx
	vpxor	%ymm8,%ymm3,%ymm3
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	vpsrld	$30,%ymm3,%ymm8
	vpslld	$2,%ymm3,%ymm3
	addl	%r12d,%edx
	andl	%edi,%esi
	addl	64(%r13),%ecx
	xorl	%ebp,%esi
	movl	%eax,%edi
	xorl	%ebp,%edi
	vpor	%ymm8,%ymm3,%ymm3
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	vpaddd	%ymm11,%ymm3,%ymm9
	addl	%r12d,%ecx
	andl	%edi,%edx
	addl	68(%r13),%ebx
	xorl	%eax,%edx
	vmovdqu	%ymm9,608(%rsp)
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	72(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	76(%r13),%eax
	xorl	%edx,%ebx
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	96(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	100(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	104(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	108(%r13),%ebx
	leaq	256(%r13),%r13
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-128(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	-124(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-120(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	-116(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	-96(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-92(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-88(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	-84(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-64(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	-60(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	-56(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-52(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-32(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	-28(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-24(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	-20(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	addl	%r12d,%edx
	leaq	128(%r9),%r13
	leaq	128(%r9),%rdi
	cmpq	%r10,%r13
	cmovaeq	%r9,%r13


	addl	0(%r8),%edx
	addl	4(%r8),%esi
	addl	8(%r8),%ebp
	movl	%edx,0(%r8)
	addl	12(%r8),%ebx
	movl	%esi,4(%r8)
	movl	%edx,%eax
	addl	16(%r8),%ecx
	movl	%ebp,%r12d
	movl	%ebp,8(%r8)
	movl	%ebx,%edx

	movl	%ebx,12(%r8)
	movl	%esi,%ebp
	movl	%ecx,16(%r8)

	movl	%ecx,%esi
	movl	%r12d,%ecx


	cmpq	%r10,%r9
	je	.Ldone_avx2
	vmovdqu	64(%r14),%ymm6
	cmpq	%r10,%rdi
	ja	.Last_avx2

	vmovdqu	-64(%rdi),%xmm0
	vmovdqu	-48(%rdi),%xmm1
	vmovdqu	-32(%rdi),%xmm2
	vmovdqu	-16(%rdi),%xmm3
	vinserti128	$1,0(%r13),%ymm0,%ymm0
	vinserti128	$1,16(%r13),%ymm1,%ymm1
	vinserti128	$1,32(%r13),%ymm2,%ymm2
	vinserti128	$1,48(%r13),%ymm3,%ymm3
	jmp	.Last_avx2

.align	32
.Last_avx2:
	leaq	128+16(%rsp),%r13
	rorxl	$2,%ebp,%ebx
	andnl	%edx,%ebp,%edi
	andl	%ecx,%ebp
	xorl	%edi,%ebp
	subq	$-128,%r9
	addl	-128(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	-124(%r13),%edx
	andnl	%ebx,%esi,%edi
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-120(%r13),%ecx
	andnl	%ebp,%edx,%edi
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%edi,%edx
	addl	-116(%r13),%ebx
	andnl	%eax,%ecx,%edi
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	addl	-96(%r13),%ebp
	andnl	%esi,%ebx,%edi
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	andl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	addl	-92(%r13),%eax
	andnl	%edx,%ebp,%edi
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	andl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edi,%ebp
	addl	-88(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	-84(%r13),%edx
	andnl	%ebx,%esi,%edi
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-64(%r13),%ecx
	andnl	%ebp,%edx,%edi
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%edi,%edx
	addl	-60(%r13),%ebx
	andnl	%eax,%ecx,%edi
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	addl	-56(%r13),%ebp
	andnl	%esi,%ebx,%edi
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	andl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	addl	-52(%r13),%eax
	andnl	%edx,%ebp,%edi
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	andl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edi,%ebp
	addl	-32(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	-28(%r13),%edx
	andnl	%ebx,%esi,%edi
	addl	%eax,%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	andl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%edi,%esi
	addl	-24(%r13),%ecx
	andnl	%ebp,%edx,%edi
	addl	%esi,%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	andl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%edi,%edx
	addl	-20(%r13),%ebx
	andnl	%eax,%ecx,%edi
	addl	%edx,%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	andl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%edi,%ecx
	addl	0(%r13),%ebp
	andnl	%esi,%ebx,%edi
	addl	%ecx,%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	andl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%edi,%ebx
	addl	4(%r13),%eax
	andnl	%edx,%ebp,%edi
	addl	%ebx,%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	andl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edi,%ebp
	addl	8(%r13),%esi
	andnl	%ecx,%eax,%edi
	addl	%ebp,%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	andl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%edi,%eax
	addl	12(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	32(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	36(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	40(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	44(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	64(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	vmovdqu	-64(%r14),%ymm11
	vpshufb	%ymm6,%ymm0,%ymm0
	addl	68(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	72(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	76(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	96(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	100(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	vpshufb	%ymm6,%ymm1,%ymm1
	vpaddd	%ymm11,%ymm0,%ymm8
	addl	104(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	108(%r13),%edx
	leaq	256(%r13),%r13
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	-128(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-124(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-120(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	vmovdqu	%ymm8,0(%rsp)
	vpshufb	%ymm6,%ymm2,%ymm2
	vpaddd	%ymm11,%ymm1,%ymm9
	addl	-116(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-96(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	-92(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	addl	-88(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-84(%r13),%ebx
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	andl	%edi,%ecx
	vmovdqu	%ymm9,32(%rsp)
	vpshufb	%ymm6,%ymm3,%ymm3
	vpaddd	%ymm11,%ymm2,%ymm6
	addl	-64(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	-60(%r13),%eax
	xorl	%edx,%ebx
	movl	%ecx,%edi
	xorl	%edx,%edi
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	-56(%r13),%esi
	xorl	%ecx,%ebp
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	addl	-52(%r13),%edx
	xorl	%ebx,%eax
	movl	%ebp,%edi
	xorl	%ebx,%edi
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	andl	%edi,%esi
	addl	-32(%r13),%ecx
	xorl	%ebp,%esi
	movl	%eax,%edi
	xorl	%ebp,%edi
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	andl	%edi,%edx
	jmp	.Lalign32_3
.align	32
.Lalign32_3:
	vmovdqu	%ymm6,64(%rsp)
	vpaddd	%ymm11,%ymm3,%ymm7
	addl	-28(%r13),%ebx
	xorl	%eax,%edx
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	-24(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	-20(%r13),%eax
	xorl	%edx,%ebx
	movl	%ecx,%edi
	xorl	%edx,%edi
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	0(%r13),%esi
	xorl	%ecx,%ebp
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	addl	4(%r13),%edx
	xorl	%ebx,%eax
	movl	%ebp,%edi
	xorl	%ebx,%edi
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	andl	%edi,%esi
	vmovdqu	%ymm7,96(%rsp)
	addl	8(%r13),%ecx
	xorl	%ebp,%esi
	movl	%eax,%edi
	xorl	%ebp,%edi
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	andl	%edi,%edx
	addl	12(%r13),%ebx
	xorl	%eax,%edx
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	32(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	36(%r13),%eax
	xorl	%edx,%ebx
	movl	%ecx,%edi
	xorl	%edx,%edi
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	andl	%edi,%ebp
	addl	40(%r13),%esi
	xorl	%ecx,%ebp
	movl	%ebx,%edi
	xorl	%ecx,%edi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	andl	%edi,%eax
	vpalignr	$8,%ymm0,%ymm1,%ymm4
	addl	44(%r13),%edx
	xorl	%ebx,%eax
	movl	%ebp,%edi
	xorl	%ebx,%edi
	vpsrldq	$4,%ymm3,%ymm8
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	vpxor	%ymm0,%ymm4,%ymm4
	vpxor	%ymm2,%ymm8,%ymm8
	xorl	%ebp,%esi
	addl	%r12d,%edx
	vpxor	%ymm8,%ymm4,%ymm4
	andl	%edi,%esi
	addl	64(%r13),%ecx
	xorl	%ebp,%esi
	movl	%eax,%edi
	vpsrld	$31,%ymm4,%ymm8
	xorl	%ebp,%edi
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	vpslldq	$12,%ymm4,%ymm10
	vpaddd	%ymm4,%ymm4,%ymm4
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm4,%ymm4
	addl	%r12d,%ecx
	andl	%edi,%edx
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm4,%ymm4
	addl	68(%r13),%ebx
	xorl	%eax,%edx
	vpxor	%ymm10,%ymm4,%ymm4
	movl	%esi,%edi
	xorl	%eax,%edi
	leal	(%rbx,%rdx,1),%ebx
	vpaddd	%ymm11,%ymm4,%ymm9
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	vmovdqu	%ymm9,128(%rsp)
	addl	%r12d,%ebx
	andl	%edi,%ecx
	addl	72(%r13),%ebp
	xorl	%esi,%ecx
	movl	%edx,%edi
	xorl	%esi,%edi
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	andl	%edi,%ebx
	addl	76(%r13),%eax
	xorl	%edx,%ebx
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	vpalignr	$8,%ymm1,%ymm2,%ymm5
	addl	96(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	vpsrldq	$4,%ymm4,%ymm8
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	vpxor	%ymm1,%ymm5,%ymm5
	vpxor	%ymm3,%ymm8,%ymm8
	addl	100(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	vpxor	%ymm8,%ymm5,%ymm5
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	xorl	%ebp,%esi
	addl	%r12d,%edx
	vpsrld	$31,%ymm5,%ymm8
	vmovdqu	-32(%r14),%ymm11
	xorl	%ebx,%esi
	addl	104(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	vpslldq	$12,%ymm5,%ymm10
	vpaddd	%ymm5,%ymm5,%ymm5
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm5,%ymm5
	xorl	%eax,%edx
	addl	%r12d,%ecx
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm5,%ymm5
	xorl	%ebp,%edx
	addl	108(%r13),%ebx
	leaq	256(%r13),%r13
	vpxor	%ymm10,%ymm5,%ymm5
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	vpaddd	%ymm11,%ymm5,%ymm9
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	vmovdqu	%ymm9,160(%rsp)
	addl	-128(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	vpalignr	$8,%ymm2,%ymm3,%ymm6
	addl	-124(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	vpsrldq	$4,%ymm5,%ymm8
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	vpxor	%ymm2,%ymm6,%ymm6
	vpxor	%ymm4,%ymm8,%ymm8
	addl	-120(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	vpxor	%ymm8,%ymm6,%ymm6
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	vpsrld	$31,%ymm6,%ymm8
	xorl	%ecx,%eax
	addl	-116(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	vpslldq	$12,%ymm6,%ymm10
	vpaddd	%ymm6,%ymm6,%ymm6
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm6,%ymm6
	xorl	%ebp,%esi
	addl	%r12d,%edx
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm6,%ymm6
	xorl	%ebx,%esi
	addl	-96(%r13),%ecx
	vpxor	%ymm10,%ymm6,%ymm6
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	vpaddd	%ymm11,%ymm6,%ymm9
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	vmovdqu	%ymm9,192(%rsp)
	addl	-92(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	vpalignr	$8,%ymm3,%ymm4,%ymm7
	addl	-88(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	vpsrldq	$4,%ymm6,%ymm8
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	vpxor	%ymm3,%ymm7,%ymm7
	vpxor	%ymm5,%ymm8,%ymm8
	addl	-84(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	vpxor	%ymm8,%ymm7,%ymm7
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	vpsrld	$31,%ymm7,%ymm8
	xorl	%edx,%ebp
	addl	-64(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	vpslldq	$12,%ymm7,%ymm10
	vpaddd	%ymm7,%ymm7,%ymm7
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	vpsrld	$30,%ymm10,%ymm9
	vpor	%ymm8,%ymm7,%ymm7
	xorl	%ebx,%eax
	addl	%r12d,%esi
	vpslld	$2,%ymm10,%ymm10
	vpxor	%ymm9,%ymm7,%ymm7
	xorl	%ecx,%eax
	addl	-60(%r13),%edx
	vpxor	%ymm10,%ymm7,%ymm7
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	rorxl	$2,%esi,%eax
	vpaddd	%ymm11,%ymm7,%ymm9
	xorl	%ebp,%esi
	addl	%r12d,%edx
	xorl	%ebx,%esi
	vmovdqu	%ymm9,224(%rsp)
	addl	-56(%r13),%ecx
	leal	(%rcx,%rsi,1),%ecx
	rorxl	$27,%edx,%r12d
	rorxl	$2,%edx,%esi
	xorl	%eax,%edx
	addl	%r12d,%ecx
	xorl	%ebp,%edx
	addl	-52(%r13),%ebx
	leal	(%rbx,%rdx,1),%ebx
	rorxl	$27,%ecx,%r12d
	rorxl	$2,%ecx,%edx
	xorl	%esi,%ecx
	addl	%r12d,%ebx
	xorl	%eax,%ecx
	addl	-32(%r13),%ebp
	leal	(%rcx,%rbp,1),%ebp
	rorxl	$27,%ebx,%r12d
	rorxl	$2,%ebx,%ecx
	xorl	%edx,%ebx
	addl	%r12d,%ebp
	xorl	%esi,%ebx
	addl	-28(%r13),%eax
	leal	(%rax,%rbx,1),%eax
	rorxl	$27,%ebp,%r12d
	rorxl	$2,%ebp,%ebx
	xorl	%ecx,%ebp
	addl	%r12d,%eax
	xorl	%edx,%ebp
	addl	-24(%r13),%esi
	leal	(%rsi,%rbp,1),%esi
	rorxl	$27,%eax,%r12d
	rorxl	$2,%eax,%ebp
	xorl	%ebx,%eax
	addl	%r12d,%esi
	xorl	%ecx,%eax
	addl	-20(%r13),%edx
	leal	(%rdx,%rax,1),%edx
	rorxl	$27,%esi,%r12d
	addl	%r12d,%edx
	leaq	128(%rsp),%r13


	addl	0(%r8),%edx
	addl	4(%r8),%esi
	addl	8(%r8),%ebp
	movl	%edx,0(%r8)
	addl	12(%r8),%ebx
	movl	%esi,4(%r8)
	movl	%edx,%eax
	addl	16(%r8),%ecx
	movl	%ebp,%r12d
	movl	%ebp,8(%r8)
	movl	%ebx,%edx

	movl	%ebx,12(%r8)
	movl	%esi,%ebp
	movl	%ecx,16(%r8)

	movl	%ecx,%esi
	movl	%r12d,%ecx


	cmpq	%r10,%r9
	jbe	.Loop_avx2

.Ldone_avx2:
	vzeroupper
	movq	-40(%r11),%r14
.cfi_restore	%r14
	movq	-32(%r11),%r13
.cfi_restore	%r13
	movq	-24(%r11),%r12
.cfi_restore	%r12
	movq	-16(%r11),%rbp
.cfi_restore	%rbp
	movq	-8(%r11),%rbx
.cfi_restore	%rbx
	leaq	(%r11),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx2:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha1_block_data_order_avx2,.-sha1_block_data_order_avx2
.align	64
K_XX_XX:
.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
.long	0x5a827999,0x5a827999,0x5a827999,0x5a827999
.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
.long	0x6ed9eba1,0x6ed9eba1,0x6ed9eba1,0x6ed9eba1
.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
.long	0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc,0x8f1bbcdc
.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
.long	0xca62c1d6,0xca62c1d6,0xca62c1d6,0xca62c1d6
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.byte	0xf,0xe,0xd,0xc,0xb,0xa,0x9,0x8,0x7,0x6,0x5,0x4,0x3,0x2,0x1,0x0
.byte	83,72,65,49,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	64
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/sha256-mb-x86_64.s        0000664 0000000 0000000 00000461442 14746647661 0030466 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	



.globl	sha256_multi_block
.type	sha256_multi_block,@function
.align	32
sha256_multi_block:
.cfi_startproc	
	movq	OPENSSL_ia32cap_P+4(%rip),%rcx
	btq	$61,%rcx
	jc	_shaext_shortcut
	testl	$268435456,%ecx
	jnz	_avx_shortcut
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	subq	$288,%rsp
	andq	$-256,%rsp
	movq	%rax,272(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x90,0x02,0x06,0x23,0x08
.Lbody:
	leaq	K256+128(%rip),%rbp
	leaq	256(%rsp),%rbx
	leaq	128(%rdi),%rdi

.Loop_grande:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r9

	movq	32(%rsi),%r10

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r10

	movq	48(%rsi),%r11

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r11
	testl	%edx,%edx
	jz	.Ldone

	movdqu	0-128(%rdi),%xmm8
	leaq	128(%rsp),%rax
	movdqu	32-128(%rdi),%xmm9
	movdqu	64-128(%rdi),%xmm10
	movdqu	96-128(%rdi),%xmm11
	movdqu	128-128(%rdi),%xmm12
	movdqu	160-128(%rdi),%xmm13
	movdqu	192-128(%rdi),%xmm14
	movdqu	224-128(%rdi),%xmm15
	movdqu	.Lpbswap(%rip),%xmm6
	jmp	.Loop

.align	32
.Loop:
	movdqa	%xmm10,%xmm4
	pxor	%xmm9,%xmm4
	movd	0(%r8),%xmm5
	movd	0(%r9),%xmm0
	movd	0(%r10),%xmm1
	movd	0(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm12,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm12,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm12,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,0-128(%rax)
	paddd	%xmm15,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-128(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm12,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm14,%xmm0
	pand	%xmm13,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm8,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm9,%xmm3
	movdqa	%xmm8,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm8,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm9,%xmm15
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm15
	paddd	%xmm5,%xmm11
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm15
	paddd	%xmm7,%xmm15
	movd	4(%r8),%xmm5
	movd	4(%r9),%xmm0
	movd	4(%r10),%xmm1
	movd	4(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm11,%xmm7

	movdqa	%xmm11,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm11,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,16-128(%rax)
	paddd	%xmm14,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-96(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm11,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm13,%xmm0
	pand	%xmm12,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm15,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm8,%xmm4
	movdqa	%xmm15,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm15,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm8,%xmm14
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm14
	paddd	%xmm5,%xmm10
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm14
	paddd	%xmm7,%xmm14
	movd	8(%r8),%xmm5
	movd	8(%r9),%xmm0
	movd	8(%r10),%xmm1
	movd	8(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm10,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm10,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm10,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,32-128(%rax)
	paddd	%xmm13,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm10,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm12,%xmm0
	pand	%xmm11,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm14,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm15,%xmm3
	movdqa	%xmm14,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm14,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm15,%xmm13
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm13
	paddd	%xmm5,%xmm9
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm13
	paddd	%xmm7,%xmm13
	movd	12(%r8),%xmm5
	movd	12(%r9),%xmm0
	movd	12(%r10),%xmm1
	movd	12(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm9,%xmm7

	movdqa	%xmm9,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm9,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,48-128(%rax)
	paddd	%xmm12,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-32(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm9,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm11,%xmm0
	pand	%xmm10,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm13,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm14,%xmm4
	movdqa	%xmm13,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm13,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm14,%xmm12
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm12
	paddd	%xmm5,%xmm8
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm12
	paddd	%xmm7,%xmm12
	movd	16(%r8),%xmm5
	movd	16(%r9),%xmm0
	movd	16(%r10),%xmm1
	movd	16(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm8,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm8,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm8,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,64-128(%rax)
	paddd	%xmm11,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	0(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm8,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm10,%xmm0
	pand	%xmm9,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm12,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm13,%xmm3
	movdqa	%xmm12,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm12,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm13,%xmm11
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm11
	paddd	%xmm5,%xmm15
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm11
	paddd	%xmm7,%xmm11
	movd	20(%r8),%xmm5
	movd	20(%r9),%xmm0
	movd	20(%r10),%xmm1
	movd	20(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm15,%xmm7

	movdqa	%xmm15,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm15,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,80-128(%rax)
	paddd	%xmm10,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	32(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm15,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm9,%xmm0
	pand	%xmm8,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm11,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm12,%xmm4
	movdqa	%xmm11,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm11,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm12,%xmm10
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm10
	paddd	%xmm5,%xmm14
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm10
	paddd	%xmm7,%xmm10
	movd	24(%r8),%xmm5
	movd	24(%r9),%xmm0
	movd	24(%r10),%xmm1
	movd	24(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm14,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm14,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm14,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,96-128(%rax)
	paddd	%xmm9,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm14,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm8,%xmm0
	pand	%xmm15,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm10,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm11,%xmm3
	movdqa	%xmm10,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm10,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm11,%xmm9
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm9
	paddd	%xmm5,%xmm13
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm9
	paddd	%xmm7,%xmm9
	movd	28(%r8),%xmm5
	movd	28(%r9),%xmm0
	movd	28(%r10),%xmm1
	movd	28(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm13,%xmm7

	movdqa	%xmm13,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm13,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,112-128(%rax)
	paddd	%xmm8,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	96(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm13,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm15,%xmm0
	pand	%xmm14,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm9,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm10,%xmm4
	movdqa	%xmm9,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm9,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm10,%xmm8
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm8
	paddd	%xmm5,%xmm12
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm8
	paddd	%xmm7,%xmm8
	leaq	256(%rbp),%rbp
	movd	32(%r8),%xmm5
	movd	32(%r9),%xmm0
	movd	32(%r10),%xmm1
	movd	32(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm12,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm12,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm12,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,128-128(%rax)
	paddd	%xmm15,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-128(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm12,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm14,%xmm0
	pand	%xmm13,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm8,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm9,%xmm3
	movdqa	%xmm8,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm8,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm9,%xmm15
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm15
	paddd	%xmm5,%xmm11
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm15
	paddd	%xmm7,%xmm15
	movd	36(%r8),%xmm5
	movd	36(%r9),%xmm0
	movd	36(%r10),%xmm1
	movd	36(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm11,%xmm7

	movdqa	%xmm11,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm11,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,144-128(%rax)
	paddd	%xmm14,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-96(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm11,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm13,%xmm0
	pand	%xmm12,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm15,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm8,%xmm4
	movdqa	%xmm15,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm15,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm8,%xmm14
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm14
	paddd	%xmm5,%xmm10
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm14
	paddd	%xmm7,%xmm14
	movd	40(%r8),%xmm5
	movd	40(%r9),%xmm0
	movd	40(%r10),%xmm1
	movd	40(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm10,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm10,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm10,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,160-128(%rax)
	paddd	%xmm13,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm10,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm12,%xmm0
	pand	%xmm11,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm14,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm15,%xmm3
	movdqa	%xmm14,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm14,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm15,%xmm13
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm13
	paddd	%xmm5,%xmm9
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm13
	paddd	%xmm7,%xmm13
	movd	44(%r8),%xmm5
	movd	44(%r9),%xmm0
	movd	44(%r10),%xmm1
	movd	44(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm9,%xmm7

	movdqa	%xmm9,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm9,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,176-128(%rax)
	paddd	%xmm12,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-32(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm9,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm11,%xmm0
	pand	%xmm10,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm13,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm14,%xmm4
	movdqa	%xmm13,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm13,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm14,%xmm12
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm12
	paddd	%xmm5,%xmm8
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm12
	paddd	%xmm7,%xmm12
	movd	48(%r8),%xmm5
	movd	48(%r9),%xmm0
	movd	48(%r10),%xmm1
	movd	48(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm8,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm8,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm8,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,192-128(%rax)
	paddd	%xmm11,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	0(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm8,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm10,%xmm0
	pand	%xmm9,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm12,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm13,%xmm3
	movdqa	%xmm12,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm12,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm13,%xmm11
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm11
	paddd	%xmm5,%xmm15
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm11
	paddd	%xmm7,%xmm11
	movd	52(%r8),%xmm5
	movd	52(%r9),%xmm0
	movd	52(%r10),%xmm1
	movd	52(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm15,%xmm7

	movdqa	%xmm15,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm15,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,208-128(%rax)
	paddd	%xmm10,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	32(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm15,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm9,%xmm0
	pand	%xmm8,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm11,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm12,%xmm4
	movdqa	%xmm11,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm11,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm12,%xmm10
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm10
	paddd	%xmm5,%xmm14
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm10
	paddd	%xmm7,%xmm10
	movd	56(%r8),%xmm5
	movd	56(%r9),%xmm0
	movd	56(%r10),%xmm1
	movd	56(%r11),%xmm2
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm14,%xmm7
.byte	102,15,56,0,238
	movdqa	%xmm14,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm14,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,224-128(%rax)
	paddd	%xmm9,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm14,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm8,%xmm0
	pand	%xmm15,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm10,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm11,%xmm3
	movdqa	%xmm10,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm10,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm11,%xmm9
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm9
	paddd	%xmm5,%xmm13
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm9
	paddd	%xmm7,%xmm9
	movd	60(%r8),%xmm5
	leaq	64(%r8),%r8
	movd	60(%r9),%xmm0
	leaq	64(%r9),%r9
	movd	60(%r10),%xmm1
	leaq	64(%r10),%r10
	movd	60(%r11),%xmm2
	leaq	64(%r11),%r11
	punpckldq	%xmm1,%xmm5
	punpckldq	%xmm2,%xmm0
	punpckldq	%xmm0,%xmm5
	movdqa	%xmm13,%xmm7

	movdqa	%xmm13,%xmm2
.byte	102,15,56,0,238
	psrld	$6,%xmm7
	movdqa	%xmm13,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,240-128(%rax)
	paddd	%xmm8,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	96(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm13,%xmm0
	prefetcht0	63(%r8)
	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm15,%xmm0
	pand	%xmm14,%xmm4
	pxor	%xmm1,%xmm7

	prefetcht0	63(%r9)
	movdqa	%xmm9,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm4,%xmm0
	movdqa	%xmm10,%xmm4
	movdqa	%xmm9,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm9,%xmm4

	prefetcht0	63(%r10)
	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1

	prefetcht0	63(%r11)
	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm10,%xmm8
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm8
	paddd	%xmm5,%xmm12
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm8
	paddd	%xmm7,%xmm8
	leaq	256(%rbp),%rbp
	movdqu	0-128(%rax),%xmm5
	movl	$3,%ecx
	jmp	.Loop_16_xx
.align	32
.Loop_16_xx:
	movdqa	16-128(%rax),%xmm6
	paddd	144-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	224-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm12,%xmm7

	movdqa	%xmm12,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm12,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,0-128(%rax)
	paddd	%xmm15,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-128(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm12,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm14,%xmm0
	pand	%xmm13,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm8,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm9,%xmm3
	movdqa	%xmm8,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm8,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm9,%xmm15
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm15
	paddd	%xmm5,%xmm11
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm15
	paddd	%xmm7,%xmm15
	movdqa	32-128(%rax),%xmm5
	paddd	160-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	240-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm11,%xmm7

	movdqa	%xmm11,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm11,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,16-128(%rax)
	paddd	%xmm14,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-96(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm11,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm13,%xmm0
	pand	%xmm12,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm15,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm8,%xmm4
	movdqa	%xmm15,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm15,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm8,%xmm14
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm14
	paddd	%xmm6,%xmm10
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm14
	paddd	%xmm7,%xmm14
	movdqa	48-128(%rax),%xmm6
	paddd	176-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	0-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm10,%xmm7

	movdqa	%xmm10,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm10,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,32-128(%rax)
	paddd	%xmm13,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm10,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm12,%xmm0
	pand	%xmm11,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm14,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm15,%xmm3
	movdqa	%xmm14,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm14,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm15,%xmm13
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm13
	paddd	%xmm5,%xmm9
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm13
	paddd	%xmm7,%xmm13
	movdqa	64-128(%rax),%xmm5
	paddd	192-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	16-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm9,%xmm7

	movdqa	%xmm9,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm9,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,48-128(%rax)
	paddd	%xmm12,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-32(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm9,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm11,%xmm0
	pand	%xmm10,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm13,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm14,%xmm4
	movdqa	%xmm13,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm13,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm14,%xmm12
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm12
	paddd	%xmm6,%xmm8
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm12
	paddd	%xmm7,%xmm12
	movdqa	80-128(%rax),%xmm6
	paddd	208-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	32-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm8,%xmm7

	movdqa	%xmm8,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm8,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,64-128(%rax)
	paddd	%xmm11,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	0(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm8,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm10,%xmm0
	pand	%xmm9,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm12,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm13,%xmm3
	movdqa	%xmm12,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm12,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm13,%xmm11
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm11
	paddd	%xmm5,%xmm15
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm11
	paddd	%xmm7,%xmm11
	movdqa	96-128(%rax),%xmm5
	paddd	224-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	48-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm15,%xmm7

	movdqa	%xmm15,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm15,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,80-128(%rax)
	paddd	%xmm10,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	32(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm15,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm9,%xmm0
	pand	%xmm8,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm11,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm12,%xmm4
	movdqa	%xmm11,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm11,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm12,%xmm10
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm10
	paddd	%xmm6,%xmm14
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm10
	paddd	%xmm7,%xmm10
	movdqa	112-128(%rax),%xmm6
	paddd	240-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	64-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm14,%xmm7

	movdqa	%xmm14,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm14,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,96-128(%rax)
	paddd	%xmm9,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm14,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm8,%xmm0
	pand	%xmm15,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm10,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm11,%xmm3
	movdqa	%xmm10,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm10,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm11,%xmm9
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm9
	paddd	%xmm5,%xmm13
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm9
	paddd	%xmm7,%xmm9
	movdqa	128-128(%rax),%xmm5
	paddd	0-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	80-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm13,%xmm7

	movdqa	%xmm13,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm13,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,112-128(%rax)
	paddd	%xmm8,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	96(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm13,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm15,%xmm0
	pand	%xmm14,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm9,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm10,%xmm4
	movdqa	%xmm9,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm9,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm10,%xmm8
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm8
	paddd	%xmm6,%xmm12
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm8
	paddd	%xmm7,%xmm8
	leaq	256(%rbp),%rbp
	movdqa	144-128(%rax),%xmm6
	paddd	16-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	96-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm12,%xmm7

	movdqa	%xmm12,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm12,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,128-128(%rax)
	paddd	%xmm15,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-128(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm12,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm14,%xmm0
	pand	%xmm13,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm8,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm9,%xmm3
	movdqa	%xmm8,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm8,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm9,%xmm15
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm15
	paddd	%xmm5,%xmm11
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm15
	paddd	%xmm7,%xmm15
	movdqa	160-128(%rax),%xmm5
	paddd	32-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	112-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm11,%xmm7

	movdqa	%xmm11,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm11,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,144-128(%rax)
	paddd	%xmm14,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-96(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm11,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm13,%xmm0
	pand	%xmm12,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm15,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm8,%xmm4
	movdqa	%xmm15,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm15,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm8,%xmm14
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm14
	paddd	%xmm6,%xmm10
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm14
	paddd	%xmm7,%xmm14
	movdqa	176-128(%rax),%xmm6
	paddd	48-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	128-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm10,%xmm7

	movdqa	%xmm10,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm10,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,160-128(%rax)
	paddd	%xmm13,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm10,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm12,%xmm0
	pand	%xmm11,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm14,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm15,%xmm3
	movdqa	%xmm14,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm14,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm15,%xmm13
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm13
	paddd	%xmm5,%xmm9
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm13
	paddd	%xmm7,%xmm13
	movdqa	192-128(%rax),%xmm5
	paddd	64-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	144-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm9,%xmm7

	movdqa	%xmm9,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm9,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,176-128(%rax)
	paddd	%xmm12,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	-32(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm9,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm11,%xmm0
	pand	%xmm10,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm13,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm14,%xmm4
	movdqa	%xmm13,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm13,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm14,%xmm12
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm12
	paddd	%xmm6,%xmm8
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm12
	paddd	%xmm7,%xmm12
	movdqa	208-128(%rax),%xmm6
	paddd	80-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	160-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm8,%xmm7

	movdqa	%xmm8,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm8,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,192-128(%rax)
	paddd	%xmm11,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	0(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm8,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm8,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm10,%xmm0
	pand	%xmm9,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm12,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm12,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm13,%xmm3
	movdqa	%xmm12,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm12,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm13,%xmm11
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm11
	paddd	%xmm5,%xmm15
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm11
	paddd	%xmm7,%xmm11
	movdqa	224-128(%rax),%xmm5
	paddd	96-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	176-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm15,%xmm7

	movdqa	%xmm15,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm15,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,208-128(%rax)
	paddd	%xmm10,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	32(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm15,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm15,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm9,%xmm0
	pand	%xmm8,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm11,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm11,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm12,%xmm4
	movdqa	%xmm11,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm11,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm12,%xmm10
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm10
	paddd	%xmm6,%xmm14
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm10
	paddd	%xmm7,%xmm10
	movdqa	240-128(%rax),%xmm6
	paddd	112-128(%rax),%xmm5

	movdqa	%xmm6,%xmm7
	movdqa	%xmm6,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm6,%xmm2

	psrld	$7,%xmm1
	movdqa	192-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm3,%xmm1

	psrld	$17,%xmm3
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	psrld	$19-17,%xmm3
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm3,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm5
	movdqa	%xmm14,%xmm7

	movdqa	%xmm14,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm14,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm5,224-128(%rax)
	paddd	%xmm9,%xmm5

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	64(%rbp),%xmm5
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm14,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm14,%xmm3
	pslld	$26-21,%xmm2
	pandn	%xmm8,%xmm0
	pand	%xmm15,%xmm3
	pxor	%xmm1,%xmm7


	movdqa	%xmm10,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm10,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm5
	pxor	%xmm3,%xmm0
	movdqa	%xmm11,%xmm3
	movdqa	%xmm10,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm10,%xmm3


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm5
	pslld	$19-10,%xmm2
	pand	%xmm3,%xmm4
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm11,%xmm9
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm4,%xmm9
	paddd	%xmm5,%xmm13
	pxor	%xmm2,%xmm7

	paddd	%xmm5,%xmm9
	paddd	%xmm7,%xmm9
	movdqa	0-128(%rax),%xmm5
	paddd	128-128(%rax),%xmm6

	movdqa	%xmm5,%xmm7
	movdqa	%xmm5,%xmm1
	psrld	$3,%xmm7
	movdqa	%xmm5,%xmm2

	psrld	$7,%xmm1
	movdqa	208-128(%rax),%xmm0
	pslld	$14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$18-7,%xmm1
	movdqa	%xmm0,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$25-14,%xmm2
	pxor	%xmm1,%xmm7
	psrld	$10,%xmm0
	movdqa	%xmm4,%xmm1

	psrld	$17,%xmm4
	pxor	%xmm2,%xmm7
	pslld	$13,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	psrld	$19-17,%xmm4
	pxor	%xmm1,%xmm0
	pslld	$15-13,%xmm1
	pxor	%xmm4,%xmm0
	pxor	%xmm1,%xmm0
	paddd	%xmm0,%xmm6
	movdqa	%xmm13,%xmm7

	movdqa	%xmm13,%xmm2

	psrld	$6,%xmm7
	movdqa	%xmm13,%xmm1
	pslld	$7,%xmm2
	movdqa	%xmm6,240-128(%rax)
	paddd	%xmm8,%xmm6

	psrld	$11,%xmm1
	pxor	%xmm2,%xmm7
	pslld	$21-7,%xmm2
	paddd	96(%rbp),%xmm6
	pxor	%xmm1,%xmm7

	psrld	$25-11,%xmm1
	movdqa	%xmm13,%xmm0

	pxor	%xmm2,%xmm7
	movdqa	%xmm13,%xmm4
	pslld	$26-21,%xmm2
	pandn	%xmm15,%xmm0
	pand	%xmm14,%xmm4
	pxor	%xmm1,%xmm7


	movdqa	%xmm9,%xmm1
	pxor	%xmm2,%xmm7
	movdqa	%xmm9,%xmm2
	psrld	$2,%xmm1
	paddd	%xmm7,%xmm6
	pxor	%xmm4,%xmm0
	movdqa	%xmm10,%xmm4
	movdqa	%xmm9,%xmm7
	pslld	$10,%xmm2
	pxor	%xmm9,%xmm4


	psrld	$13,%xmm7
	pxor	%xmm2,%xmm1
	paddd	%xmm0,%xmm6
	pslld	$19-10,%xmm2
	pand	%xmm4,%xmm3
	pxor	%xmm7,%xmm1


	psrld	$22-13,%xmm7
	pxor	%xmm2,%xmm1
	movdqa	%xmm10,%xmm8
	pslld	$30-19,%xmm2
	pxor	%xmm1,%xmm7
	pxor	%xmm3,%xmm8
	paddd	%xmm6,%xmm12
	pxor	%xmm2,%xmm7

	paddd	%xmm6,%xmm8
	paddd	%xmm7,%xmm8
	leaq	256(%rbp),%rbp
	decl	%ecx
	jnz	.Loop_16_xx

	movl	$1,%ecx
	leaq	K256+128(%rip),%rbp

	movdqa	(%rbx),%xmm7
	cmpl	0(%rbx),%ecx
	pxor	%xmm0,%xmm0
	cmovgeq	%rbp,%r8
	cmpl	4(%rbx),%ecx
	movdqa	%xmm7,%xmm6
	cmovgeq	%rbp,%r9
	cmpl	8(%rbx),%ecx
	pcmpgtd	%xmm0,%xmm6
	cmovgeq	%rbp,%r10
	cmpl	12(%rbx),%ecx
	paddd	%xmm6,%xmm7
	cmovgeq	%rbp,%r11

	movdqu	0-128(%rdi),%xmm0
	pand	%xmm6,%xmm8
	movdqu	32-128(%rdi),%xmm1
	pand	%xmm6,%xmm9
	movdqu	64-128(%rdi),%xmm2
	pand	%xmm6,%xmm10
	movdqu	96-128(%rdi),%xmm5
	pand	%xmm6,%xmm11
	paddd	%xmm0,%xmm8
	movdqu	128-128(%rdi),%xmm0
	pand	%xmm6,%xmm12
	paddd	%xmm1,%xmm9
	movdqu	160-128(%rdi),%xmm1
	pand	%xmm6,%xmm13
	paddd	%xmm2,%xmm10
	movdqu	192-128(%rdi),%xmm2
	pand	%xmm6,%xmm14
	paddd	%xmm5,%xmm11
	movdqu	224-128(%rdi),%xmm5
	pand	%xmm6,%xmm15
	paddd	%xmm0,%xmm12
	paddd	%xmm1,%xmm13
	movdqu	%xmm8,0-128(%rdi)
	paddd	%xmm2,%xmm14
	movdqu	%xmm9,32-128(%rdi)
	paddd	%xmm5,%xmm15
	movdqu	%xmm10,64-128(%rdi)
	movdqu	%xmm11,96-128(%rdi)
	movdqu	%xmm12,128-128(%rdi)
	movdqu	%xmm13,160-128(%rdi)
	movdqu	%xmm14,192-128(%rdi)
	movdqu	%xmm15,224-128(%rdi)

	movdqa	%xmm7,(%rbx)
	movdqa	.Lpbswap(%rip),%xmm6
	decl	%edx
	jnz	.Loop

	movl	280(%rsp),%edx
	leaq	16(%rdi),%rdi
	leaq	64(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande

.Ldone:
	movq	272(%rsp),%rax
.cfi_def_cfa	%rax,8
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_multi_block,.-sha256_multi_block
.type	sha256_multi_block_shaext,@function
.align	32
sha256_multi_block_shaext:
.cfi_startproc	
_shaext_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	subq	$288,%rsp
	shll	$1,%edx
	andq	$-256,%rsp
	leaq	128(%rdi),%rdi
	movq	%rax,272(%rsp)
.Lbody_shaext:
	leaq	256(%rsp),%rbx
	leaq	K256_shaext+128(%rip),%rbp

.Loop_grande_shaext:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rsp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rsp,%r9
	testl	%edx,%edx
	jz	.Ldone_shaext

	movq	0-128(%rdi),%xmm12
	movq	32-128(%rdi),%xmm4
	movq	64-128(%rdi),%xmm13
	movq	96-128(%rdi),%xmm5
	movq	128-128(%rdi),%xmm8
	movq	160-128(%rdi),%xmm9
	movq	192-128(%rdi),%xmm10
	movq	224-128(%rdi),%xmm11

	punpckldq	%xmm4,%xmm12
	punpckldq	%xmm5,%xmm13
	punpckldq	%xmm9,%xmm8
	punpckldq	%xmm11,%xmm10
	movdqa	K256_shaext-16(%rip),%xmm3

	movdqa	%xmm12,%xmm14
	movdqa	%xmm13,%xmm15
	punpcklqdq	%xmm8,%xmm12
	punpcklqdq	%xmm10,%xmm13
	punpckhqdq	%xmm8,%xmm14
	punpckhqdq	%xmm10,%xmm15

	pshufd	$27,%xmm12,%xmm12
	pshufd	$27,%xmm13,%xmm13
	pshufd	$27,%xmm14,%xmm14
	pshufd	$27,%xmm15,%xmm15
	jmp	.Loop_shaext

.align	32
.Loop_shaext:
	movdqu	0(%r8),%xmm4
	movdqu	0(%r9),%xmm8
	movdqu	16(%r8),%xmm5
	movdqu	16(%r9),%xmm9
	movdqu	32(%r8),%xmm6
.byte	102,15,56,0,227
	movdqu	32(%r9),%xmm10
.byte	102,68,15,56,0,195
	movdqu	48(%r8),%xmm7
	leaq	64(%r8),%r8
	movdqu	48(%r9),%xmm11
	leaq	64(%r9),%r9

	movdqa	0-128(%rbp),%xmm0
.byte	102,15,56,0,235
	paddd	%xmm4,%xmm0
	pxor	%xmm12,%xmm4
	movdqa	%xmm0,%xmm1
	movdqa	0-128(%rbp),%xmm2
.byte	102,68,15,56,0,203
	paddd	%xmm8,%xmm2
	movdqa	%xmm13,80(%rsp)
.byte	69,15,56,203,236
	pxor	%xmm14,%xmm8
	movdqa	%xmm2,%xmm0
	movdqa	%xmm15,112(%rsp)
.byte	69,15,56,203,254
	pshufd	$0x0e,%xmm1,%xmm0
	pxor	%xmm12,%xmm4
	movdqa	%xmm12,64(%rsp)
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	pxor	%xmm14,%xmm8
	movdqa	%xmm14,96(%rsp)
	movdqa	16-128(%rbp),%xmm1
	paddd	%xmm5,%xmm1
.byte	102,15,56,0,243
.byte	69,15,56,203,247

	movdqa	%xmm1,%xmm0
	movdqa	16-128(%rbp),%xmm2
	paddd	%xmm9,%xmm2
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	prefetcht0	127(%r8)
.byte	102,15,56,0,251
.byte	102,68,15,56,0,211
	prefetcht0	127(%r9)
.byte	69,15,56,203,254
	pshufd	$0x0e,%xmm1,%xmm0
.byte	102,68,15,56,0,219
.byte	15,56,204,229
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	32-128(%rbp),%xmm1
	paddd	%xmm6,%xmm1
.byte	69,15,56,203,247

	movdqa	%xmm1,%xmm0
	movdqa	32-128(%rbp),%xmm2
	paddd	%xmm10,%xmm2
.byte	69,15,56,203,236
.byte	69,15,56,204,193
	movdqa	%xmm2,%xmm0
	movdqa	%xmm7,%xmm3
.byte	69,15,56,203,254
	pshufd	$0x0e,%xmm1,%xmm0
.byte	102,15,58,15,222,4
	paddd	%xmm3,%xmm4
	movdqa	%xmm11,%xmm3
.byte	102,65,15,58,15,218,4
.byte	15,56,204,238
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	48-128(%rbp),%xmm1
	paddd	%xmm7,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,202

	movdqa	%xmm1,%xmm0
	movdqa	48-128(%rbp),%xmm2
	paddd	%xmm3,%xmm8
	paddd	%xmm11,%xmm2
.byte	15,56,205,231
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm4,%xmm3
.byte	102,15,58,15,223,4
.byte	69,15,56,203,254
.byte	69,15,56,205,195
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm5
	movdqa	%xmm8,%xmm3
.byte	102,65,15,58,15,219,4
.byte	15,56,204,247
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	64-128(%rbp),%xmm1
	paddd	%xmm4,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,211
	movdqa	%xmm1,%xmm0
	movdqa	64-128(%rbp),%xmm2
	paddd	%xmm3,%xmm9
	paddd	%xmm8,%xmm2
.byte	15,56,205,236
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm5,%xmm3
.byte	102,15,58,15,220,4
.byte	69,15,56,203,254
.byte	69,15,56,205,200
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm6
	movdqa	%xmm9,%xmm3
.byte	102,65,15,58,15,216,4
.byte	15,56,204,252
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	80-128(%rbp),%xmm1
	paddd	%xmm5,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,216
	movdqa	%xmm1,%xmm0
	movdqa	80-128(%rbp),%xmm2
	paddd	%xmm3,%xmm10
	paddd	%xmm9,%xmm2
.byte	15,56,205,245
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm6,%xmm3
.byte	102,15,58,15,221,4
.byte	69,15,56,203,254
.byte	69,15,56,205,209
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm7
	movdqa	%xmm10,%xmm3
.byte	102,65,15,58,15,217,4
.byte	15,56,204,229
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	96-128(%rbp),%xmm1
	paddd	%xmm6,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,193
	movdqa	%xmm1,%xmm0
	movdqa	96-128(%rbp),%xmm2
	paddd	%xmm3,%xmm11
	paddd	%xmm10,%xmm2
.byte	15,56,205,254
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm7,%xmm3
.byte	102,15,58,15,222,4
.byte	69,15,56,203,254
.byte	69,15,56,205,218
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm4
	movdqa	%xmm11,%xmm3
.byte	102,65,15,58,15,218,4
.byte	15,56,204,238
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	112-128(%rbp),%xmm1
	paddd	%xmm7,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,202
	movdqa	%xmm1,%xmm0
	movdqa	112-128(%rbp),%xmm2
	paddd	%xmm3,%xmm8
	paddd	%xmm11,%xmm2
.byte	15,56,205,231
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm4,%xmm3
.byte	102,15,58,15,223,4
.byte	69,15,56,203,254
.byte	69,15,56,205,195
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm5
	movdqa	%xmm8,%xmm3
.byte	102,65,15,58,15,219,4
.byte	15,56,204,247
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	128-128(%rbp),%xmm1
	paddd	%xmm4,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,211
	movdqa	%xmm1,%xmm0
	movdqa	128-128(%rbp),%xmm2
	paddd	%xmm3,%xmm9
	paddd	%xmm8,%xmm2
.byte	15,56,205,236
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm5,%xmm3
.byte	102,15,58,15,220,4
.byte	69,15,56,203,254
.byte	69,15,56,205,200
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm6
	movdqa	%xmm9,%xmm3
.byte	102,65,15,58,15,216,4
.byte	15,56,204,252
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	144-128(%rbp),%xmm1
	paddd	%xmm5,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,216
	movdqa	%xmm1,%xmm0
	movdqa	144-128(%rbp),%xmm2
	paddd	%xmm3,%xmm10
	paddd	%xmm9,%xmm2
.byte	15,56,205,245
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm6,%xmm3
.byte	102,15,58,15,221,4
.byte	69,15,56,203,254
.byte	69,15,56,205,209
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm7
	movdqa	%xmm10,%xmm3
.byte	102,65,15,58,15,217,4
.byte	15,56,204,229
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	160-128(%rbp),%xmm1
	paddd	%xmm6,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,193
	movdqa	%xmm1,%xmm0
	movdqa	160-128(%rbp),%xmm2
	paddd	%xmm3,%xmm11
	paddd	%xmm10,%xmm2
.byte	15,56,205,254
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm7,%xmm3
.byte	102,15,58,15,222,4
.byte	69,15,56,203,254
.byte	69,15,56,205,218
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm4
	movdqa	%xmm11,%xmm3
.byte	102,65,15,58,15,218,4
.byte	15,56,204,238
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	176-128(%rbp),%xmm1
	paddd	%xmm7,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,202
	movdqa	%xmm1,%xmm0
	movdqa	176-128(%rbp),%xmm2
	paddd	%xmm3,%xmm8
	paddd	%xmm11,%xmm2
.byte	15,56,205,231
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm4,%xmm3
.byte	102,15,58,15,223,4
.byte	69,15,56,203,254
.byte	69,15,56,205,195
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm5
	movdqa	%xmm8,%xmm3
.byte	102,65,15,58,15,219,4
.byte	15,56,204,247
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	192-128(%rbp),%xmm1
	paddd	%xmm4,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,211
	movdqa	%xmm1,%xmm0
	movdqa	192-128(%rbp),%xmm2
	paddd	%xmm3,%xmm9
	paddd	%xmm8,%xmm2
.byte	15,56,205,236
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm5,%xmm3
.byte	102,15,58,15,220,4
.byte	69,15,56,203,254
.byte	69,15,56,205,200
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm6
	movdqa	%xmm9,%xmm3
.byte	102,65,15,58,15,216,4
.byte	15,56,204,252
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	208-128(%rbp),%xmm1
	paddd	%xmm5,%xmm1
.byte	69,15,56,203,247
.byte	69,15,56,204,216
	movdqa	%xmm1,%xmm0
	movdqa	208-128(%rbp),%xmm2
	paddd	%xmm3,%xmm10
	paddd	%xmm9,%xmm2
.byte	15,56,205,245
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movdqa	%xmm6,%xmm3
.byte	102,15,58,15,221,4
.byte	69,15,56,203,254
.byte	69,15,56,205,209
	pshufd	$0x0e,%xmm1,%xmm0
	paddd	%xmm3,%xmm7
	movdqa	%xmm10,%xmm3
.byte	102,65,15,58,15,217,4
	nop
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	224-128(%rbp),%xmm1
	paddd	%xmm6,%xmm1
.byte	69,15,56,203,247

	movdqa	%xmm1,%xmm0
	movdqa	224-128(%rbp),%xmm2
	paddd	%xmm3,%xmm11
	paddd	%xmm10,%xmm2
.byte	15,56,205,254
	nop
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	movl	$1,%ecx
	pxor	%xmm6,%xmm6
.byte	69,15,56,203,254
.byte	69,15,56,205,218
	pshufd	$0x0e,%xmm1,%xmm0
	movdqa	240-128(%rbp),%xmm1
	paddd	%xmm7,%xmm1
	movq	(%rbx),%xmm7
	nop
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	movdqa	240-128(%rbp),%xmm2
	paddd	%xmm11,%xmm2
.byte	69,15,56,203,247

	movdqa	%xmm1,%xmm0
	cmpl	0(%rbx),%ecx
	cmovgeq	%rsp,%r8
	cmpl	4(%rbx),%ecx
	cmovgeq	%rsp,%r9
	pshufd	$0x00,%xmm7,%xmm9
.byte	69,15,56,203,236
	movdqa	%xmm2,%xmm0
	pshufd	$0x55,%xmm7,%xmm10
	movdqa	%xmm7,%xmm11
.byte	69,15,56,203,254
	pshufd	$0x0e,%xmm1,%xmm0
	pcmpgtd	%xmm6,%xmm9
	pcmpgtd	%xmm6,%xmm10
.byte	69,15,56,203,229
	pshufd	$0x0e,%xmm2,%xmm0
	pcmpgtd	%xmm6,%xmm11
	movdqa	K256_shaext-16(%rip),%xmm3
.byte	69,15,56,203,247

	pand	%xmm9,%xmm13
	pand	%xmm10,%xmm15
	pand	%xmm9,%xmm12
	pand	%xmm10,%xmm14
	paddd	%xmm7,%xmm11

	paddd	80(%rsp),%xmm13
	paddd	112(%rsp),%xmm15
	paddd	64(%rsp),%xmm12
	paddd	96(%rsp),%xmm14

	movq	%xmm11,(%rbx)
	decl	%edx
	jnz	.Loop_shaext

	movl	280(%rsp),%edx

	pshufd	$27,%xmm12,%xmm12
	pshufd	$27,%xmm13,%xmm13
	pshufd	$27,%xmm14,%xmm14
	pshufd	$27,%xmm15,%xmm15

	movdqa	%xmm12,%xmm5
	movdqa	%xmm13,%xmm6
	punpckldq	%xmm14,%xmm12
	punpckhdq	%xmm14,%xmm5
	punpckldq	%xmm15,%xmm13
	punpckhdq	%xmm15,%xmm6

	movq	%xmm12,0-128(%rdi)
	psrldq	$8,%xmm12
	movq	%xmm5,128-128(%rdi)
	psrldq	$8,%xmm5
	movq	%xmm12,32-128(%rdi)
	movq	%xmm5,160-128(%rdi)

	movq	%xmm13,64-128(%rdi)
	psrldq	$8,%xmm13
	movq	%xmm6,192-128(%rdi)
	psrldq	$8,%xmm6
	movq	%xmm13,96-128(%rdi)
	movq	%xmm6,224-128(%rdi)

	leaq	8(%rdi),%rdi
	leaq	32(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande_shaext

.Ldone_shaext:

	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_shaext:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_multi_block_shaext,.-sha256_multi_block_shaext
.type	sha256_multi_block_avx,@function
.align	32
sha256_multi_block_avx:
.cfi_startproc	
_avx_shortcut:
	shrq	$32,%rcx
	cmpl	$2,%edx
	jb	.Lavx
	testl	$32,%ecx
	jnz	_avx2_shortcut
	jmp	.Lavx
.align	32
.Lavx:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	subq	$288,%rsp
	andq	$-256,%rsp
	movq	%rax,272(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x90,0x02,0x06,0x23,0x08
.Lbody_avx:
	leaq	K256+128(%rip),%rbp
	leaq	256(%rsp),%rbx
	leaq	128(%rdi),%rdi

.Loop_grande_avx:
	movl	%edx,280(%rsp)
	xorl	%edx,%edx

	movq	0(%rsi),%r8

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r8

	movq	16(%rsi),%r9

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r9

	movq	32(%rsi),%r10

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r10

	movq	48(%rsi),%r11

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r11
	testl	%edx,%edx
	jz	.Ldone_avx

	vmovdqu	0-128(%rdi),%xmm8
	leaq	128(%rsp),%rax
	vmovdqu	32-128(%rdi),%xmm9
	vmovdqu	64-128(%rdi),%xmm10
	vmovdqu	96-128(%rdi),%xmm11
	vmovdqu	128-128(%rdi),%xmm12
	vmovdqu	160-128(%rdi),%xmm13
	vmovdqu	192-128(%rdi),%xmm14
	vmovdqu	224-128(%rdi),%xmm15
	vmovdqu	.Lpbswap(%rip),%xmm6
	jmp	.Loop_avx

.align	32
.Loop_avx:
	vpxor	%xmm9,%xmm10,%xmm4
	vmovd	0(%r8),%xmm5
	vmovd	0(%r9),%xmm0
	vpinsrd	$1,0(%r10),%xmm5,%xmm5
	vpinsrd	$1,0(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm12,%xmm7
	vpslld	$26,%xmm12,%xmm2
	vmovdqu	%xmm5,0-128(%rax)
	vpaddd	%xmm15,%xmm5,%xmm5

	vpsrld	$11,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm12,%xmm2
	vpaddd	-128(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm12,%xmm2
	vpandn	%xmm14,%xmm12,%xmm0
	vpand	%xmm13,%xmm12,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm8,%xmm15
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm8,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm8,%xmm9,%xmm3

	vpxor	%xmm1,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm8,%xmm1

	vpslld	$19,%xmm8,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm15,%xmm7

	vpsrld	$22,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm8,%xmm2
	vpxor	%xmm4,%xmm9,%xmm15
	vpaddd	%xmm5,%xmm11,%xmm11

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm15,%xmm15
	vmovd	4(%r8),%xmm5
	vmovd	4(%r9),%xmm0
	vpinsrd	$1,4(%r10),%xmm5,%xmm5
	vpinsrd	$1,4(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm11,%xmm7
	vpslld	$26,%xmm11,%xmm2
	vmovdqu	%xmm5,16-128(%rax)
	vpaddd	%xmm14,%xmm5,%xmm5

	vpsrld	$11,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm11,%xmm2
	vpaddd	-96(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm11,%xmm2
	vpandn	%xmm13,%xmm11,%xmm0
	vpand	%xmm12,%xmm11,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm15,%xmm14
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm15,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm15,%xmm8,%xmm4

	vpxor	%xmm1,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm15,%xmm1

	vpslld	$19,%xmm15,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm14,%xmm7

	vpsrld	$22,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm15,%xmm2
	vpxor	%xmm3,%xmm8,%xmm14
	vpaddd	%xmm5,%xmm10,%xmm10

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm14,%xmm14
	vmovd	8(%r8),%xmm5
	vmovd	8(%r9),%xmm0
	vpinsrd	$1,8(%r10),%xmm5,%xmm5
	vpinsrd	$1,8(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm10,%xmm7
	vpslld	$26,%xmm10,%xmm2
	vmovdqu	%xmm5,32-128(%rax)
	vpaddd	%xmm13,%xmm5,%xmm5

	vpsrld	$11,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm10,%xmm2
	vpaddd	-64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm10,%xmm2
	vpandn	%xmm12,%xmm10,%xmm0
	vpand	%xmm11,%xmm10,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm14,%xmm13
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm14,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm14,%xmm15,%xmm3

	vpxor	%xmm1,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm14,%xmm1

	vpslld	$19,%xmm14,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm13,%xmm7

	vpsrld	$22,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm14,%xmm2
	vpxor	%xmm4,%xmm15,%xmm13
	vpaddd	%xmm5,%xmm9,%xmm9

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm13,%xmm13
	vmovd	12(%r8),%xmm5
	vmovd	12(%r9),%xmm0
	vpinsrd	$1,12(%r10),%xmm5,%xmm5
	vpinsrd	$1,12(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm9,%xmm7
	vpslld	$26,%xmm9,%xmm2
	vmovdqu	%xmm5,48-128(%rax)
	vpaddd	%xmm12,%xmm5,%xmm5

	vpsrld	$11,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm9,%xmm2
	vpaddd	-32(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm9,%xmm2
	vpandn	%xmm11,%xmm9,%xmm0
	vpand	%xmm10,%xmm9,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm13,%xmm12
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm13,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm13,%xmm14,%xmm4

	vpxor	%xmm1,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm13,%xmm1

	vpslld	$19,%xmm13,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm12,%xmm7

	vpsrld	$22,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm13,%xmm2
	vpxor	%xmm3,%xmm14,%xmm12
	vpaddd	%xmm5,%xmm8,%xmm8

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm12,%xmm12
	vmovd	16(%r8),%xmm5
	vmovd	16(%r9),%xmm0
	vpinsrd	$1,16(%r10),%xmm5,%xmm5
	vpinsrd	$1,16(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm8,%xmm7
	vpslld	$26,%xmm8,%xmm2
	vmovdqu	%xmm5,64-128(%rax)
	vpaddd	%xmm11,%xmm5,%xmm5

	vpsrld	$11,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm8,%xmm2
	vpaddd	0(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm8,%xmm2
	vpandn	%xmm10,%xmm8,%xmm0
	vpand	%xmm9,%xmm8,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm12,%xmm11
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm12,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm12,%xmm13,%xmm3

	vpxor	%xmm1,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm12,%xmm1

	vpslld	$19,%xmm12,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm11,%xmm7

	vpsrld	$22,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm12,%xmm2
	vpxor	%xmm4,%xmm13,%xmm11
	vpaddd	%xmm5,%xmm15,%xmm15

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm11,%xmm11
	vmovd	20(%r8),%xmm5
	vmovd	20(%r9),%xmm0
	vpinsrd	$1,20(%r10),%xmm5,%xmm5
	vpinsrd	$1,20(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm15,%xmm7
	vpslld	$26,%xmm15,%xmm2
	vmovdqu	%xmm5,80-128(%rax)
	vpaddd	%xmm10,%xmm5,%xmm5

	vpsrld	$11,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm15,%xmm2
	vpaddd	32(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm15,%xmm2
	vpandn	%xmm9,%xmm15,%xmm0
	vpand	%xmm8,%xmm15,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm11,%xmm10
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm11,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm11,%xmm12,%xmm4

	vpxor	%xmm1,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm11,%xmm1

	vpslld	$19,%xmm11,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm10,%xmm7

	vpsrld	$22,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm11,%xmm2
	vpxor	%xmm3,%xmm12,%xmm10
	vpaddd	%xmm5,%xmm14,%xmm14

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm10,%xmm10
	vmovd	24(%r8),%xmm5
	vmovd	24(%r9),%xmm0
	vpinsrd	$1,24(%r10),%xmm5,%xmm5
	vpinsrd	$1,24(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm14,%xmm7
	vpslld	$26,%xmm14,%xmm2
	vmovdqu	%xmm5,96-128(%rax)
	vpaddd	%xmm9,%xmm5,%xmm5

	vpsrld	$11,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm14,%xmm2
	vpaddd	64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm14,%xmm2
	vpandn	%xmm8,%xmm14,%xmm0
	vpand	%xmm15,%xmm14,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm10,%xmm9
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm10,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm10,%xmm11,%xmm3

	vpxor	%xmm1,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm10,%xmm1

	vpslld	$19,%xmm10,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm9,%xmm7

	vpsrld	$22,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm10,%xmm2
	vpxor	%xmm4,%xmm11,%xmm9
	vpaddd	%xmm5,%xmm13,%xmm13

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm9,%xmm9
	vmovd	28(%r8),%xmm5
	vmovd	28(%r9),%xmm0
	vpinsrd	$1,28(%r10),%xmm5,%xmm5
	vpinsrd	$1,28(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm13,%xmm7
	vpslld	$26,%xmm13,%xmm2
	vmovdqu	%xmm5,112-128(%rax)
	vpaddd	%xmm8,%xmm5,%xmm5

	vpsrld	$11,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm13,%xmm2
	vpaddd	96(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm13,%xmm2
	vpandn	%xmm15,%xmm13,%xmm0
	vpand	%xmm14,%xmm13,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm9,%xmm8
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm9,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm9,%xmm10,%xmm4

	vpxor	%xmm1,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm9,%xmm1

	vpslld	$19,%xmm9,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm8,%xmm7

	vpsrld	$22,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm9,%xmm2
	vpxor	%xmm3,%xmm10,%xmm8
	vpaddd	%xmm5,%xmm12,%xmm12

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm8,%xmm8
	addq	$256,%rbp
	vmovd	32(%r8),%xmm5
	vmovd	32(%r9),%xmm0
	vpinsrd	$1,32(%r10),%xmm5,%xmm5
	vpinsrd	$1,32(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm12,%xmm7
	vpslld	$26,%xmm12,%xmm2
	vmovdqu	%xmm5,128-128(%rax)
	vpaddd	%xmm15,%xmm5,%xmm5

	vpsrld	$11,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm12,%xmm2
	vpaddd	-128(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm12,%xmm2
	vpandn	%xmm14,%xmm12,%xmm0
	vpand	%xmm13,%xmm12,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm8,%xmm15
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm8,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm8,%xmm9,%xmm3

	vpxor	%xmm1,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm8,%xmm1

	vpslld	$19,%xmm8,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm15,%xmm7

	vpsrld	$22,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm8,%xmm2
	vpxor	%xmm4,%xmm9,%xmm15
	vpaddd	%xmm5,%xmm11,%xmm11

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm15,%xmm15
	vmovd	36(%r8),%xmm5
	vmovd	36(%r9),%xmm0
	vpinsrd	$1,36(%r10),%xmm5,%xmm5
	vpinsrd	$1,36(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm11,%xmm7
	vpslld	$26,%xmm11,%xmm2
	vmovdqu	%xmm5,144-128(%rax)
	vpaddd	%xmm14,%xmm5,%xmm5

	vpsrld	$11,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm11,%xmm2
	vpaddd	-96(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm11,%xmm2
	vpandn	%xmm13,%xmm11,%xmm0
	vpand	%xmm12,%xmm11,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm15,%xmm14
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm15,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm15,%xmm8,%xmm4

	vpxor	%xmm1,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm15,%xmm1

	vpslld	$19,%xmm15,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm14,%xmm7

	vpsrld	$22,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm15,%xmm2
	vpxor	%xmm3,%xmm8,%xmm14
	vpaddd	%xmm5,%xmm10,%xmm10

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm14,%xmm14
	vmovd	40(%r8),%xmm5
	vmovd	40(%r9),%xmm0
	vpinsrd	$1,40(%r10),%xmm5,%xmm5
	vpinsrd	$1,40(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm10,%xmm7
	vpslld	$26,%xmm10,%xmm2
	vmovdqu	%xmm5,160-128(%rax)
	vpaddd	%xmm13,%xmm5,%xmm5

	vpsrld	$11,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm10,%xmm2
	vpaddd	-64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm10,%xmm2
	vpandn	%xmm12,%xmm10,%xmm0
	vpand	%xmm11,%xmm10,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm14,%xmm13
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm14,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm14,%xmm15,%xmm3

	vpxor	%xmm1,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm14,%xmm1

	vpslld	$19,%xmm14,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm13,%xmm7

	vpsrld	$22,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm14,%xmm2
	vpxor	%xmm4,%xmm15,%xmm13
	vpaddd	%xmm5,%xmm9,%xmm9

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm13,%xmm13
	vmovd	44(%r8),%xmm5
	vmovd	44(%r9),%xmm0
	vpinsrd	$1,44(%r10),%xmm5,%xmm5
	vpinsrd	$1,44(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm9,%xmm7
	vpslld	$26,%xmm9,%xmm2
	vmovdqu	%xmm5,176-128(%rax)
	vpaddd	%xmm12,%xmm5,%xmm5

	vpsrld	$11,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm9,%xmm2
	vpaddd	-32(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm9,%xmm2
	vpandn	%xmm11,%xmm9,%xmm0
	vpand	%xmm10,%xmm9,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm13,%xmm12
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm13,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm13,%xmm14,%xmm4

	vpxor	%xmm1,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm13,%xmm1

	vpslld	$19,%xmm13,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm12,%xmm7

	vpsrld	$22,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm13,%xmm2
	vpxor	%xmm3,%xmm14,%xmm12
	vpaddd	%xmm5,%xmm8,%xmm8

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm12,%xmm12
	vmovd	48(%r8),%xmm5
	vmovd	48(%r9),%xmm0
	vpinsrd	$1,48(%r10),%xmm5,%xmm5
	vpinsrd	$1,48(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm8,%xmm7
	vpslld	$26,%xmm8,%xmm2
	vmovdqu	%xmm5,192-128(%rax)
	vpaddd	%xmm11,%xmm5,%xmm5

	vpsrld	$11,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm8,%xmm2
	vpaddd	0(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm8,%xmm2
	vpandn	%xmm10,%xmm8,%xmm0
	vpand	%xmm9,%xmm8,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm12,%xmm11
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm12,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm12,%xmm13,%xmm3

	vpxor	%xmm1,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm12,%xmm1

	vpslld	$19,%xmm12,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm11,%xmm7

	vpsrld	$22,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm12,%xmm2
	vpxor	%xmm4,%xmm13,%xmm11
	vpaddd	%xmm5,%xmm15,%xmm15

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm11,%xmm11
	vmovd	52(%r8),%xmm5
	vmovd	52(%r9),%xmm0
	vpinsrd	$1,52(%r10),%xmm5,%xmm5
	vpinsrd	$1,52(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm15,%xmm7
	vpslld	$26,%xmm15,%xmm2
	vmovdqu	%xmm5,208-128(%rax)
	vpaddd	%xmm10,%xmm5,%xmm5

	vpsrld	$11,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm15,%xmm2
	vpaddd	32(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm15,%xmm2
	vpandn	%xmm9,%xmm15,%xmm0
	vpand	%xmm8,%xmm15,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm11,%xmm10
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm11,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm11,%xmm12,%xmm4

	vpxor	%xmm1,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm11,%xmm1

	vpslld	$19,%xmm11,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm10,%xmm7

	vpsrld	$22,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm11,%xmm2
	vpxor	%xmm3,%xmm12,%xmm10
	vpaddd	%xmm5,%xmm14,%xmm14

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm10,%xmm10
	vmovd	56(%r8),%xmm5
	vmovd	56(%r9),%xmm0
	vpinsrd	$1,56(%r10),%xmm5,%xmm5
	vpinsrd	$1,56(%r11),%xmm0,%xmm0
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm14,%xmm7
	vpslld	$26,%xmm14,%xmm2
	vmovdqu	%xmm5,224-128(%rax)
	vpaddd	%xmm9,%xmm5,%xmm5

	vpsrld	$11,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm14,%xmm2
	vpaddd	64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm14,%xmm2
	vpandn	%xmm8,%xmm14,%xmm0
	vpand	%xmm15,%xmm14,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm10,%xmm9
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm10,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm10,%xmm11,%xmm3

	vpxor	%xmm1,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm10,%xmm1

	vpslld	$19,%xmm10,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm9,%xmm7

	vpsrld	$22,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm10,%xmm2
	vpxor	%xmm4,%xmm11,%xmm9
	vpaddd	%xmm5,%xmm13,%xmm13

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm9,%xmm9
	vmovd	60(%r8),%xmm5
	leaq	64(%r8),%r8
	vmovd	60(%r9),%xmm0
	leaq	64(%r9),%r9
	vpinsrd	$1,60(%r10),%xmm5,%xmm5
	leaq	64(%r10),%r10
	vpinsrd	$1,60(%r11),%xmm0,%xmm0
	leaq	64(%r11),%r11
	vpunpckldq	%xmm0,%xmm5,%xmm5
	vpshufb	%xmm6,%xmm5,%xmm5
	vpsrld	$6,%xmm13,%xmm7
	vpslld	$26,%xmm13,%xmm2
	vmovdqu	%xmm5,240-128(%rax)
	vpaddd	%xmm8,%xmm5,%xmm5

	vpsrld	$11,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm13,%xmm2
	vpaddd	96(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	prefetcht0	63(%r8)
	vpslld	$7,%xmm13,%xmm2
	vpandn	%xmm15,%xmm13,%xmm0
	vpand	%xmm14,%xmm13,%xmm4
	prefetcht0	63(%r9)
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm9,%xmm8
	vpxor	%xmm2,%xmm7,%xmm7
	prefetcht0	63(%r10)
	vpslld	$30,%xmm9,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm9,%xmm10,%xmm4
	prefetcht0	63(%r11)
	vpxor	%xmm1,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm9,%xmm1

	vpslld	$19,%xmm9,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm8,%xmm7

	vpsrld	$22,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm9,%xmm2
	vpxor	%xmm3,%xmm10,%xmm8
	vpaddd	%xmm5,%xmm12,%xmm12

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm8,%xmm8
	addq	$256,%rbp
	vmovdqu	0-128(%rax),%xmm5
	movl	$3,%ecx
	jmp	.Loop_16_xx_avx
.align	32
.Loop_16_xx_avx:
	vmovdqu	16-128(%rax),%xmm6
	vpaddd	144-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	224-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm12,%xmm7
	vpslld	$26,%xmm12,%xmm2
	vmovdqu	%xmm5,0-128(%rax)
	vpaddd	%xmm15,%xmm5,%xmm5

	vpsrld	$11,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm12,%xmm2
	vpaddd	-128(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm12,%xmm2
	vpandn	%xmm14,%xmm12,%xmm0
	vpand	%xmm13,%xmm12,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm8,%xmm15
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm8,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm8,%xmm9,%xmm3

	vpxor	%xmm1,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm8,%xmm1

	vpslld	$19,%xmm8,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm15,%xmm7

	vpsrld	$22,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm8,%xmm2
	vpxor	%xmm4,%xmm9,%xmm15
	vpaddd	%xmm5,%xmm11,%xmm11

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm15,%xmm15
	vmovdqu	32-128(%rax),%xmm5
	vpaddd	160-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	240-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm11,%xmm7
	vpslld	$26,%xmm11,%xmm2
	vmovdqu	%xmm6,16-128(%rax)
	vpaddd	%xmm14,%xmm6,%xmm6

	vpsrld	$11,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm11,%xmm2
	vpaddd	-96(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm11,%xmm2
	vpandn	%xmm13,%xmm11,%xmm0
	vpand	%xmm12,%xmm11,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm15,%xmm14
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm15,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm15,%xmm8,%xmm4

	vpxor	%xmm1,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm15,%xmm1

	vpslld	$19,%xmm15,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm14,%xmm7

	vpsrld	$22,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm15,%xmm2
	vpxor	%xmm3,%xmm8,%xmm14
	vpaddd	%xmm6,%xmm10,%xmm10

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm14,%xmm14
	vmovdqu	48-128(%rax),%xmm6
	vpaddd	176-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	0-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm10,%xmm7
	vpslld	$26,%xmm10,%xmm2
	vmovdqu	%xmm5,32-128(%rax)
	vpaddd	%xmm13,%xmm5,%xmm5

	vpsrld	$11,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm10,%xmm2
	vpaddd	-64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm10,%xmm2
	vpandn	%xmm12,%xmm10,%xmm0
	vpand	%xmm11,%xmm10,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm14,%xmm13
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm14,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm14,%xmm15,%xmm3

	vpxor	%xmm1,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm14,%xmm1

	vpslld	$19,%xmm14,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm13,%xmm7

	vpsrld	$22,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm14,%xmm2
	vpxor	%xmm4,%xmm15,%xmm13
	vpaddd	%xmm5,%xmm9,%xmm9

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm13,%xmm13
	vmovdqu	64-128(%rax),%xmm5
	vpaddd	192-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	16-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm9,%xmm7
	vpslld	$26,%xmm9,%xmm2
	vmovdqu	%xmm6,48-128(%rax)
	vpaddd	%xmm12,%xmm6,%xmm6

	vpsrld	$11,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm9,%xmm2
	vpaddd	-32(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm9,%xmm2
	vpandn	%xmm11,%xmm9,%xmm0
	vpand	%xmm10,%xmm9,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm13,%xmm12
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm13,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm13,%xmm14,%xmm4

	vpxor	%xmm1,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm13,%xmm1

	vpslld	$19,%xmm13,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm12,%xmm7

	vpsrld	$22,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm13,%xmm2
	vpxor	%xmm3,%xmm14,%xmm12
	vpaddd	%xmm6,%xmm8,%xmm8

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm12,%xmm12
	vmovdqu	80-128(%rax),%xmm6
	vpaddd	208-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	32-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm8,%xmm7
	vpslld	$26,%xmm8,%xmm2
	vmovdqu	%xmm5,64-128(%rax)
	vpaddd	%xmm11,%xmm5,%xmm5

	vpsrld	$11,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm8,%xmm2
	vpaddd	0(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm8,%xmm2
	vpandn	%xmm10,%xmm8,%xmm0
	vpand	%xmm9,%xmm8,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm12,%xmm11
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm12,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm12,%xmm13,%xmm3

	vpxor	%xmm1,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm12,%xmm1

	vpslld	$19,%xmm12,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm11,%xmm7

	vpsrld	$22,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm12,%xmm2
	vpxor	%xmm4,%xmm13,%xmm11
	vpaddd	%xmm5,%xmm15,%xmm15

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm11,%xmm11
	vmovdqu	96-128(%rax),%xmm5
	vpaddd	224-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	48-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm15,%xmm7
	vpslld	$26,%xmm15,%xmm2
	vmovdqu	%xmm6,80-128(%rax)
	vpaddd	%xmm10,%xmm6,%xmm6

	vpsrld	$11,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm15,%xmm2
	vpaddd	32(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm15,%xmm2
	vpandn	%xmm9,%xmm15,%xmm0
	vpand	%xmm8,%xmm15,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm11,%xmm10
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm11,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm11,%xmm12,%xmm4

	vpxor	%xmm1,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm11,%xmm1

	vpslld	$19,%xmm11,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm10,%xmm7

	vpsrld	$22,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm11,%xmm2
	vpxor	%xmm3,%xmm12,%xmm10
	vpaddd	%xmm6,%xmm14,%xmm14

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm10,%xmm10
	vmovdqu	112-128(%rax),%xmm6
	vpaddd	240-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	64-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm14,%xmm7
	vpslld	$26,%xmm14,%xmm2
	vmovdqu	%xmm5,96-128(%rax)
	vpaddd	%xmm9,%xmm5,%xmm5

	vpsrld	$11,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm14,%xmm2
	vpaddd	64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm14,%xmm2
	vpandn	%xmm8,%xmm14,%xmm0
	vpand	%xmm15,%xmm14,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm10,%xmm9
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm10,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm10,%xmm11,%xmm3

	vpxor	%xmm1,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm10,%xmm1

	vpslld	$19,%xmm10,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm9,%xmm7

	vpsrld	$22,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm10,%xmm2
	vpxor	%xmm4,%xmm11,%xmm9
	vpaddd	%xmm5,%xmm13,%xmm13

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm9,%xmm9
	vmovdqu	128-128(%rax),%xmm5
	vpaddd	0-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	80-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm13,%xmm7
	vpslld	$26,%xmm13,%xmm2
	vmovdqu	%xmm6,112-128(%rax)
	vpaddd	%xmm8,%xmm6,%xmm6

	vpsrld	$11,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm13,%xmm2
	vpaddd	96(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm13,%xmm2
	vpandn	%xmm15,%xmm13,%xmm0
	vpand	%xmm14,%xmm13,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm9,%xmm8
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm9,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm9,%xmm10,%xmm4

	vpxor	%xmm1,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm9,%xmm1

	vpslld	$19,%xmm9,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm8,%xmm7

	vpsrld	$22,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm9,%xmm2
	vpxor	%xmm3,%xmm10,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm8,%xmm8
	addq	$256,%rbp
	vmovdqu	144-128(%rax),%xmm6
	vpaddd	16-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	96-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm12,%xmm7
	vpslld	$26,%xmm12,%xmm2
	vmovdqu	%xmm5,128-128(%rax)
	vpaddd	%xmm15,%xmm5,%xmm5

	vpsrld	$11,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm12,%xmm2
	vpaddd	-128(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm12,%xmm2
	vpandn	%xmm14,%xmm12,%xmm0
	vpand	%xmm13,%xmm12,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm8,%xmm15
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm8,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm8,%xmm9,%xmm3

	vpxor	%xmm1,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm8,%xmm1

	vpslld	$19,%xmm8,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm15,%xmm7

	vpsrld	$22,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm8,%xmm2
	vpxor	%xmm4,%xmm9,%xmm15
	vpaddd	%xmm5,%xmm11,%xmm11

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm15,%xmm15
	vpaddd	%xmm7,%xmm15,%xmm15
	vmovdqu	160-128(%rax),%xmm5
	vpaddd	32-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	112-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm11,%xmm7
	vpslld	$26,%xmm11,%xmm2
	vmovdqu	%xmm6,144-128(%rax)
	vpaddd	%xmm14,%xmm6,%xmm6

	vpsrld	$11,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm11,%xmm2
	vpaddd	-96(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm11,%xmm2
	vpandn	%xmm13,%xmm11,%xmm0
	vpand	%xmm12,%xmm11,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm15,%xmm14
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm15,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm15,%xmm8,%xmm4

	vpxor	%xmm1,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm15,%xmm1

	vpslld	$19,%xmm15,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm14,%xmm7

	vpsrld	$22,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm15,%xmm2
	vpxor	%xmm3,%xmm8,%xmm14
	vpaddd	%xmm6,%xmm10,%xmm10

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm14,%xmm14
	vpaddd	%xmm7,%xmm14,%xmm14
	vmovdqu	176-128(%rax),%xmm6
	vpaddd	48-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	128-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm10,%xmm7
	vpslld	$26,%xmm10,%xmm2
	vmovdqu	%xmm5,160-128(%rax)
	vpaddd	%xmm13,%xmm5,%xmm5

	vpsrld	$11,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm10,%xmm2
	vpaddd	-64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm10,%xmm2
	vpandn	%xmm12,%xmm10,%xmm0
	vpand	%xmm11,%xmm10,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm14,%xmm13
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm14,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm14,%xmm15,%xmm3

	vpxor	%xmm1,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm14,%xmm1

	vpslld	$19,%xmm14,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm13,%xmm7

	vpsrld	$22,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm14,%xmm2
	vpxor	%xmm4,%xmm15,%xmm13
	vpaddd	%xmm5,%xmm9,%xmm9

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm13,%xmm13
	vpaddd	%xmm7,%xmm13,%xmm13
	vmovdqu	192-128(%rax),%xmm5
	vpaddd	64-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	144-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm9,%xmm7
	vpslld	$26,%xmm9,%xmm2
	vmovdqu	%xmm6,176-128(%rax)
	vpaddd	%xmm12,%xmm6,%xmm6

	vpsrld	$11,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm9,%xmm2
	vpaddd	-32(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm9,%xmm2
	vpandn	%xmm11,%xmm9,%xmm0
	vpand	%xmm10,%xmm9,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm13,%xmm12
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm13,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm13,%xmm14,%xmm4

	vpxor	%xmm1,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm13,%xmm1

	vpslld	$19,%xmm13,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm12,%xmm7

	vpsrld	$22,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm13,%xmm2
	vpxor	%xmm3,%xmm14,%xmm12
	vpaddd	%xmm6,%xmm8,%xmm8

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm12,%xmm12
	vpaddd	%xmm7,%xmm12,%xmm12
	vmovdqu	208-128(%rax),%xmm6
	vpaddd	80-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	160-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm8,%xmm7
	vpslld	$26,%xmm8,%xmm2
	vmovdqu	%xmm5,192-128(%rax)
	vpaddd	%xmm11,%xmm5,%xmm5

	vpsrld	$11,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm8,%xmm2
	vpaddd	0(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm8,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm8,%xmm2
	vpandn	%xmm10,%xmm8,%xmm0
	vpand	%xmm9,%xmm8,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm12,%xmm11
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm12,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm12,%xmm13,%xmm3

	vpxor	%xmm1,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm12,%xmm1

	vpslld	$19,%xmm12,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm11,%xmm7

	vpsrld	$22,%xmm12,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm12,%xmm2
	vpxor	%xmm4,%xmm13,%xmm11
	vpaddd	%xmm5,%xmm15,%xmm15

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm11,%xmm11
	vpaddd	%xmm7,%xmm11,%xmm11
	vmovdqu	224-128(%rax),%xmm5
	vpaddd	96-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	176-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm15,%xmm7
	vpslld	$26,%xmm15,%xmm2
	vmovdqu	%xmm6,208-128(%rax)
	vpaddd	%xmm10,%xmm6,%xmm6

	vpsrld	$11,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm15,%xmm2
	vpaddd	32(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm15,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm15,%xmm2
	vpandn	%xmm9,%xmm15,%xmm0
	vpand	%xmm8,%xmm15,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm11,%xmm10
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm11,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm11,%xmm12,%xmm4

	vpxor	%xmm1,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm11,%xmm1

	vpslld	$19,%xmm11,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm10,%xmm7

	vpsrld	$22,%xmm11,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm11,%xmm2
	vpxor	%xmm3,%xmm12,%xmm10
	vpaddd	%xmm6,%xmm14,%xmm14

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm10,%xmm10
	vpaddd	%xmm7,%xmm10,%xmm10
	vmovdqu	240-128(%rax),%xmm6
	vpaddd	112-128(%rax),%xmm5,%xmm5

	vpsrld	$3,%xmm6,%xmm7
	vpsrld	$7,%xmm6,%xmm1
	vpslld	$25,%xmm6,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm6,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm6,%xmm2
	vmovdqu	192-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm5,%xmm5
	vpxor	%xmm1,%xmm3,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm5,%xmm5
	vpsrld	$6,%xmm14,%xmm7
	vpslld	$26,%xmm14,%xmm2
	vmovdqu	%xmm5,224-128(%rax)
	vpaddd	%xmm9,%xmm5,%xmm5

	vpsrld	$11,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm14,%xmm2
	vpaddd	64(%rbp),%xmm5,%xmm5
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm14,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm14,%xmm2
	vpandn	%xmm8,%xmm14,%xmm0
	vpand	%xmm15,%xmm14,%xmm3

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm10,%xmm9
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm10,%xmm1
	vpxor	%xmm3,%xmm0,%xmm0
	vpxor	%xmm10,%xmm11,%xmm3

	vpxor	%xmm1,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm5,%xmm5

	vpsrld	$13,%xmm10,%xmm1

	vpslld	$19,%xmm10,%xmm2
	vpaddd	%xmm0,%xmm5,%xmm5
	vpand	%xmm3,%xmm4,%xmm4

	vpxor	%xmm1,%xmm9,%xmm7

	vpsrld	$22,%xmm10,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm10,%xmm2
	vpxor	%xmm4,%xmm11,%xmm9
	vpaddd	%xmm5,%xmm13,%xmm13

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm5,%xmm9,%xmm9
	vpaddd	%xmm7,%xmm9,%xmm9
	vmovdqu	0-128(%rax),%xmm5
	vpaddd	128-128(%rax),%xmm6,%xmm6

	vpsrld	$3,%xmm5,%xmm7
	vpsrld	$7,%xmm5,%xmm1
	vpslld	$25,%xmm5,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$18,%xmm5,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$14,%xmm5,%xmm2
	vmovdqu	208-128(%rax),%xmm0
	vpsrld	$10,%xmm0,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7
	vpsrld	$17,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$15,%xmm0,%xmm2
	vpaddd	%xmm7,%xmm6,%xmm6
	vpxor	%xmm1,%xmm4,%xmm7
	vpsrld	$19,%xmm0,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$13,%xmm0,%xmm2
	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7
	vpaddd	%xmm7,%xmm6,%xmm6
	vpsrld	$6,%xmm13,%xmm7
	vpslld	$26,%xmm13,%xmm2
	vmovdqu	%xmm6,240-128(%rax)
	vpaddd	%xmm8,%xmm6,%xmm6

	vpsrld	$11,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7
	vpslld	$21,%xmm13,%xmm2
	vpaddd	96(%rbp),%xmm6,%xmm6
	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$25,%xmm13,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$7,%xmm13,%xmm2
	vpandn	%xmm15,%xmm13,%xmm0
	vpand	%xmm14,%xmm13,%xmm4

	vpxor	%xmm1,%xmm7,%xmm7

	vpsrld	$2,%xmm9,%xmm8
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$30,%xmm9,%xmm1
	vpxor	%xmm4,%xmm0,%xmm0
	vpxor	%xmm9,%xmm10,%xmm4

	vpxor	%xmm1,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm6,%xmm6

	vpsrld	$13,%xmm9,%xmm1

	vpslld	$19,%xmm9,%xmm2
	vpaddd	%xmm0,%xmm6,%xmm6
	vpand	%xmm4,%xmm3,%xmm3

	vpxor	%xmm1,%xmm8,%xmm7

	vpsrld	$22,%xmm9,%xmm1
	vpxor	%xmm2,%xmm7,%xmm7

	vpslld	$10,%xmm9,%xmm2
	vpxor	%xmm3,%xmm10,%xmm8
	vpaddd	%xmm6,%xmm12,%xmm12

	vpxor	%xmm1,%xmm7,%xmm7
	vpxor	%xmm2,%xmm7,%xmm7

	vpaddd	%xmm6,%xmm8,%xmm8
	vpaddd	%xmm7,%xmm8,%xmm8
	addq	$256,%rbp
	decl	%ecx
	jnz	.Loop_16_xx_avx

	movl	$1,%ecx
	leaq	K256+128(%rip),%rbp
	cmpl	0(%rbx),%ecx
	cmovgeq	%rbp,%r8
	cmpl	4(%rbx),%ecx
	cmovgeq	%rbp,%r9
	cmpl	8(%rbx),%ecx
	cmovgeq	%rbp,%r10
	cmpl	12(%rbx),%ecx
	cmovgeq	%rbp,%r11
	vmovdqa	(%rbx),%xmm7
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa	%xmm7,%xmm6
	vpcmpgtd	%xmm0,%xmm6,%xmm6
	vpaddd	%xmm6,%xmm7,%xmm7

	vmovdqu	0-128(%rdi),%xmm0
	vpand	%xmm6,%xmm8,%xmm8
	vmovdqu	32-128(%rdi),%xmm1
	vpand	%xmm6,%xmm9,%xmm9
	vmovdqu	64-128(%rdi),%xmm2
	vpand	%xmm6,%xmm10,%xmm10
	vmovdqu	96-128(%rdi),%xmm5
	vpand	%xmm6,%xmm11,%xmm11
	vpaddd	%xmm0,%xmm8,%xmm8
	vmovdqu	128-128(%rdi),%xmm0
	vpand	%xmm6,%xmm12,%xmm12
	vpaddd	%xmm1,%xmm9,%xmm9
	vmovdqu	160-128(%rdi),%xmm1
	vpand	%xmm6,%xmm13,%xmm13
	vpaddd	%xmm2,%xmm10,%xmm10
	vmovdqu	192-128(%rdi),%xmm2
	vpand	%xmm6,%xmm14,%xmm14
	vpaddd	%xmm5,%xmm11,%xmm11
	vmovdqu	224-128(%rdi),%xmm5
	vpand	%xmm6,%xmm15,%xmm15
	vpaddd	%xmm0,%xmm12,%xmm12
	vpaddd	%xmm1,%xmm13,%xmm13
	vmovdqu	%xmm8,0-128(%rdi)
	vpaddd	%xmm2,%xmm14,%xmm14
	vmovdqu	%xmm9,32-128(%rdi)
	vpaddd	%xmm5,%xmm15,%xmm15
	vmovdqu	%xmm10,64-128(%rdi)
	vmovdqu	%xmm11,96-128(%rdi)
	vmovdqu	%xmm12,128-128(%rdi)
	vmovdqu	%xmm13,160-128(%rdi)
	vmovdqu	%xmm14,192-128(%rdi)
	vmovdqu	%xmm15,224-128(%rdi)

	vmovdqu	%xmm7,(%rbx)
	vmovdqu	.Lpbswap(%rip),%xmm6
	decl	%edx
	jnz	.Loop_avx

	movl	280(%rsp),%edx
	leaq	16(%rdi),%rdi
	leaq	64(%rsi),%rsi
	decl	%edx
	jnz	.Loop_grande_avx

.Ldone_avx:
	movq	272(%rsp),%rax
.cfi_def_cfa	%rax,8
	vzeroupper
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_multi_block_avx,.-sha256_multi_block_avx
.type	sha256_multi_block_avx2,@function
.align	32
sha256_multi_block_avx2:
.cfi_startproc	
_avx2_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	subq	$576,%rsp
	andq	$-256,%rsp
	movq	%rax,544(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xa0,0x04,0x06,0x23,0x08
.Lbody_avx2:
	leaq	K256+128(%rip),%rbp
	leaq	128(%rdi),%rdi

.Loop_grande_avx2:
	movl	%edx,552(%rsp)
	xorl	%edx,%edx
	leaq	512(%rsp),%rbx

	movq	0(%rsi),%r12

	movl	8(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,0(%rbx)
	cmovleq	%rbp,%r12

	movq	16(%rsi),%r13

	movl	24(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,4(%rbx)
	cmovleq	%rbp,%r13

	movq	32(%rsi),%r14

	movl	40(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,8(%rbx)
	cmovleq	%rbp,%r14

	movq	48(%rsi),%r15

	movl	56(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,12(%rbx)
	cmovleq	%rbp,%r15

	movq	64(%rsi),%r8

	movl	72(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,16(%rbx)
	cmovleq	%rbp,%r8

	movq	80(%rsi),%r9

	movl	88(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,20(%rbx)
	cmovleq	%rbp,%r9

	movq	96(%rsi),%r10

	movl	104(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,24(%rbx)
	cmovleq	%rbp,%r10

	movq	112(%rsi),%r11

	movl	120(%rsi),%ecx
	cmpl	%edx,%ecx
	cmovgl	%ecx,%edx
	testl	%ecx,%ecx
	movl	%ecx,28(%rbx)
	cmovleq	%rbp,%r11
	vmovdqu	0-128(%rdi),%ymm8
	leaq	128(%rsp),%rax
	vmovdqu	32-128(%rdi),%ymm9
	leaq	256+128(%rsp),%rbx
	vmovdqu	64-128(%rdi),%ymm10
	vmovdqu	96-128(%rdi),%ymm11
	vmovdqu	128-128(%rdi),%ymm12
	vmovdqu	160-128(%rdi),%ymm13
	vmovdqu	192-128(%rdi),%ymm14
	vmovdqu	224-128(%rdi),%ymm15
	vmovdqu	.Lpbswap(%rip),%ymm6
	jmp	.Loop_avx2

.align	32
.Loop_avx2:
	vpxor	%ymm9,%ymm10,%ymm4
	vmovd	0(%r12),%xmm5
	vmovd	0(%r8),%xmm0
	vmovd	0(%r13),%xmm1
	vmovd	0(%r9),%xmm2
	vpinsrd	$1,0(%r14),%xmm5,%xmm5
	vpinsrd	$1,0(%r10),%xmm0,%xmm0
	vpinsrd	$1,0(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,0(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm12,%ymm7
	vpslld	$26,%ymm12,%ymm2
	vmovdqu	%ymm5,0-128(%rax)
	vpaddd	%ymm15,%ymm5,%ymm5

	vpsrld	$11,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm12,%ymm2
	vpaddd	-128(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm12,%ymm2
	vpandn	%ymm14,%ymm12,%ymm0
	vpand	%ymm13,%ymm12,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm8,%ymm15
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm8,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm8,%ymm9,%ymm3

	vpxor	%ymm1,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm8,%ymm1

	vpslld	$19,%ymm8,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm15,%ymm7

	vpsrld	$22,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm8,%ymm2
	vpxor	%ymm4,%ymm9,%ymm15
	vpaddd	%ymm5,%ymm11,%ymm11

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm15,%ymm15
	vmovd	4(%r12),%xmm5
	vmovd	4(%r8),%xmm0
	vmovd	4(%r13),%xmm1
	vmovd	4(%r9),%xmm2
	vpinsrd	$1,4(%r14),%xmm5,%xmm5
	vpinsrd	$1,4(%r10),%xmm0,%xmm0
	vpinsrd	$1,4(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,4(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm11,%ymm7
	vpslld	$26,%ymm11,%ymm2
	vmovdqu	%ymm5,32-128(%rax)
	vpaddd	%ymm14,%ymm5,%ymm5

	vpsrld	$11,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm11,%ymm2
	vpaddd	-96(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm11,%ymm2
	vpandn	%ymm13,%ymm11,%ymm0
	vpand	%ymm12,%ymm11,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm15,%ymm14
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm15,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm15,%ymm8,%ymm4

	vpxor	%ymm1,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm15,%ymm1

	vpslld	$19,%ymm15,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm14,%ymm7

	vpsrld	$22,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm15,%ymm2
	vpxor	%ymm3,%ymm8,%ymm14
	vpaddd	%ymm5,%ymm10,%ymm10

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm14,%ymm14
	vmovd	8(%r12),%xmm5
	vmovd	8(%r8),%xmm0
	vmovd	8(%r13),%xmm1
	vmovd	8(%r9),%xmm2
	vpinsrd	$1,8(%r14),%xmm5,%xmm5
	vpinsrd	$1,8(%r10),%xmm0,%xmm0
	vpinsrd	$1,8(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,8(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm10,%ymm7
	vpslld	$26,%ymm10,%ymm2
	vmovdqu	%ymm5,64-128(%rax)
	vpaddd	%ymm13,%ymm5,%ymm5

	vpsrld	$11,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm10,%ymm2
	vpaddd	-64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm10,%ymm2
	vpandn	%ymm12,%ymm10,%ymm0
	vpand	%ymm11,%ymm10,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm14,%ymm13
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm14,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm14,%ymm15,%ymm3

	vpxor	%ymm1,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm14,%ymm1

	vpslld	$19,%ymm14,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm13,%ymm7

	vpsrld	$22,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm14,%ymm2
	vpxor	%ymm4,%ymm15,%ymm13
	vpaddd	%ymm5,%ymm9,%ymm9

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm13,%ymm13
	vmovd	12(%r12),%xmm5
	vmovd	12(%r8),%xmm0
	vmovd	12(%r13),%xmm1
	vmovd	12(%r9),%xmm2
	vpinsrd	$1,12(%r14),%xmm5,%xmm5
	vpinsrd	$1,12(%r10),%xmm0,%xmm0
	vpinsrd	$1,12(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,12(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm9,%ymm7
	vpslld	$26,%ymm9,%ymm2
	vmovdqu	%ymm5,96-128(%rax)
	vpaddd	%ymm12,%ymm5,%ymm5

	vpsrld	$11,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm9,%ymm2
	vpaddd	-32(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm9,%ymm2
	vpandn	%ymm11,%ymm9,%ymm0
	vpand	%ymm10,%ymm9,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm13,%ymm12
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm13,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm13,%ymm14,%ymm4

	vpxor	%ymm1,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm13,%ymm1

	vpslld	$19,%ymm13,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm12,%ymm7

	vpsrld	$22,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm13,%ymm2
	vpxor	%ymm3,%ymm14,%ymm12
	vpaddd	%ymm5,%ymm8,%ymm8

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm12,%ymm12
	vmovd	16(%r12),%xmm5
	vmovd	16(%r8),%xmm0
	vmovd	16(%r13),%xmm1
	vmovd	16(%r9),%xmm2
	vpinsrd	$1,16(%r14),%xmm5,%xmm5
	vpinsrd	$1,16(%r10),%xmm0,%xmm0
	vpinsrd	$1,16(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,16(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm8,%ymm7
	vpslld	$26,%ymm8,%ymm2
	vmovdqu	%ymm5,128-128(%rax)
	vpaddd	%ymm11,%ymm5,%ymm5

	vpsrld	$11,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm8,%ymm2
	vpaddd	0(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm8,%ymm2
	vpandn	%ymm10,%ymm8,%ymm0
	vpand	%ymm9,%ymm8,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm12,%ymm11
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm12,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm12,%ymm13,%ymm3

	vpxor	%ymm1,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm12,%ymm1

	vpslld	$19,%ymm12,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm11,%ymm7

	vpsrld	$22,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm12,%ymm2
	vpxor	%ymm4,%ymm13,%ymm11
	vpaddd	%ymm5,%ymm15,%ymm15

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm11,%ymm11
	vmovd	20(%r12),%xmm5
	vmovd	20(%r8),%xmm0
	vmovd	20(%r13),%xmm1
	vmovd	20(%r9),%xmm2
	vpinsrd	$1,20(%r14),%xmm5,%xmm5
	vpinsrd	$1,20(%r10),%xmm0,%xmm0
	vpinsrd	$1,20(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,20(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm15,%ymm7
	vpslld	$26,%ymm15,%ymm2
	vmovdqu	%ymm5,160-128(%rax)
	vpaddd	%ymm10,%ymm5,%ymm5

	vpsrld	$11,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm15,%ymm2
	vpaddd	32(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm15,%ymm2
	vpandn	%ymm9,%ymm15,%ymm0
	vpand	%ymm8,%ymm15,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm11,%ymm10
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm11,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm11,%ymm12,%ymm4

	vpxor	%ymm1,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm11,%ymm1

	vpslld	$19,%ymm11,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm10,%ymm7

	vpsrld	$22,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm11,%ymm2
	vpxor	%ymm3,%ymm12,%ymm10
	vpaddd	%ymm5,%ymm14,%ymm14

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm10,%ymm10
	vmovd	24(%r12),%xmm5
	vmovd	24(%r8),%xmm0
	vmovd	24(%r13),%xmm1
	vmovd	24(%r9),%xmm2
	vpinsrd	$1,24(%r14),%xmm5,%xmm5
	vpinsrd	$1,24(%r10),%xmm0,%xmm0
	vpinsrd	$1,24(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,24(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm14,%ymm7
	vpslld	$26,%ymm14,%ymm2
	vmovdqu	%ymm5,192-128(%rax)
	vpaddd	%ymm9,%ymm5,%ymm5

	vpsrld	$11,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm14,%ymm2
	vpaddd	64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm14,%ymm2
	vpandn	%ymm8,%ymm14,%ymm0
	vpand	%ymm15,%ymm14,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm10,%ymm9
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm10,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm10,%ymm11,%ymm3

	vpxor	%ymm1,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm10,%ymm1

	vpslld	$19,%ymm10,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm9,%ymm7

	vpsrld	$22,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm10,%ymm2
	vpxor	%ymm4,%ymm11,%ymm9
	vpaddd	%ymm5,%ymm13,%ymm13

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm9,%ymm9
	vmovd	28(%r12),%xmm5
	vmovd	28(%r8),%xmm0
	vmovd	28(%r13),%xmm1
	vmovd	28(%r9),%xmm2
	vpinsrd	$1,28(%r14),%xmm5,%xmm5
	vpinsrd	$1,28(%r10),%xmm0,%xmm0
	vpinsrd	$1,28(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,28(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm13,%ymm7
	vpslld	$26,%ymm13,%ymm2
	vmovdqu	%ymm5,224-128(%rax)
	vpaddd	%ymm8,%ymm5,%ymm5

	vpsrld	$11,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm13,%ymm2
	vpaddd	96(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm13,%ymm2
	vpandn	%ymm15,%ymm13,%ymm0
	vpand	%ymm14,%ymm13,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm9,%ymm8
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm9,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm9,%ymm10,%ymm4

	vpxor	%ymm1,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm9,%ymm1

	vpslld	$19,%ymm9,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm8,%ymm7

	vpsrld	$22,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm9,%ymm2
	vpxor	%ymm3,%ymm10,%ymm8
	vpaddd	%ymm5,%ymm12,%ymm12

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm8,%ymm8
	addq	$256,%rbp
	vmovd	32(%r12),%xmm5
	vmovd	32(%r8),%xmm0
	vmovd	32(%r13),%xmm1
	vmovd	32(%r9),%xmm2
	vpinsrd	$1,32(%r14),%xmm5,%xmm5
	vpinsrd	$1,32(%r10),%xmm0,%xmm0
	vpinsrd	$1,32(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,32(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm12,%ymm7
	vpslld	$26,%ymm12,%ymm2
	vmovdqu	%ymm5,256-256-128(%rbx)
	vpaddd	%ymm15,%ymm5,%ymm5

	vpsrld	$11,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm12,%ymm2
	vpaddd	-128(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm12,%ymm2
	vpandn	%ymm14,%ymm12,%ymm0
	vpand	%ymm13,%ymm12,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm8,%ymm15
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm8,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm8,%ymm9,%ymm3

	vpxor	%ymm1,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm8,%ymm1

	vpslld	$19,%ymm8,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm15,%ymm7

	vpsrld	$22,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm8,%ymm2
	vpxor	%ymm4,%ymm9,%ymm15
	vpaddd	%ymm5,%ymm11,%ymm11

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm15,%ymm15
	vmovd	36(%r12),%xmm5
	vmovd	36(%r8),%xmm0
	vmovd	36(%r13),%xmm1
	vmovd	36(%r9),%xmm2
	vpinsrd	$1,36(%r14),%xmm5,%xmm5
	vpinsrd	$1,36(%r10),%xmm0,%xmm0
	vpinsrd	$1,36(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,36(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm11,%ymm7
	vpslld	$26,%ymm11,%ymm2
	vmovdqu	%ymm5,288-256-128(%rbx)
	vpaddd	%ymm14,%ymm5,%ymm5

	vpsrld	$11,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm11,%ymm2
	vpaddd	-96(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm11,%ymm2
	vpandn	%ymm13,%ymm11,%ymm0
	vpand	%ymm12,%ymm11,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm15,%ymm14
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm15,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm15,%ymm8,%ymm4

	vpxor	%ymm1,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm15,%ymm1

	vpslld	$19,%ymm15,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm14,%ymm7

	vpsrld	$22,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm15,%ymm2
	vpxor	%ymm3,%ymm8,%ymm14
	vpaddd	%ymm5,%ymm10,%ymm10

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm14,%ymm14
	vmovd	40(%r12),%xmm5
	vmovd	40(%r8),%xmm0
	vmovd	40(%r13),%xmm1
	vmovd	40(%r9),%xmm2
	vpinsrd	$1,40(%r14),%xmm5,%xmm5
	vpinsrd	$1,40(%r10),%xmm0,%xmm0
	vpinsrd	$1,40(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,40(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm10,%ymm7
	vpslld	$26,%ymm10,%ymm2
	vmovdqu	%ymm5,320-256-128(%rbx)
	vpaddd	%ymm13,%ymm5,%ymm5

	vpsrld	$11,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm10,%ymm2
	vpaddd	-64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm10,%ymm2
	vpandn	%ymm12,%ymm10,%ymm0
	vpand	%ymm11,%ymm10,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm14,%ymm13
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm14,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm14,%ymm15,%ymm3

	vpxor	%ymm1,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm14,%ymm1

	vpslld	$19,%ymm14,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm13,%ymm7

	vpsrld	$22,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm14,%ymm2
	vpxor	%ymm4,%ymm15,%ymm13
	vpaddd	%ymm5,%ymm9,%ymm9

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm13,%ymm13
	vmovd	44(%r12),%xmm5
	vmovd	44(%r8),%xmm0
	vmovd	44(%r13),%xmm1
	vmovd	44(%r9),%xmm2
	vpinsrd	$1,44(%r14),%xmm5,%xmm5
	vpinsrd	$1,44(%r10),%xmm0,%xmm0
	vpinsrd	$1,44(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,44(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm9,%ymm7
	vpslld	$26,%ymm9,%ymm2
	vmovdqu	%ymm5,352-256-128(%rbx)
	vpaddd	%ymm12,%ymm5,%ymm5

	vpsrld	$11,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm9,%ymm2
	vpaddd	-32(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm9,%ymm2
	vpandn	%ymm11,%ymm9,%ymm0
	vpand	%ymm10,%ymm9,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm13,%ymm12
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm13,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm13,%ymm14,%ymm4

	vpxor	%ymm1,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm13,%ymm1

	vpslld	$19,%ymm13,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm12,%ymm7

	vpsrld	$22,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm13,%ymm2
	vpxor	%ymm3,%ymm14,%ymm12
	vpaddd	%ymm5,%ymm8,%ymm8

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm12,%ymm12
	vmovd	48(%r12),%xmm5
	vmovd	48(%r8),%xmm0
	vmovd	48(%r13),%xmm1
	vmovd	48(%r9),%xmm2
	vpinsrd	$1,48(%r14),%xmm5,%xmm5
	vpinsrd	$1,48(%r10),%xmm0,%xmm0
	vpinsrd	$1,48(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,48(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm8,%ymm7
	vpslld	$26,%ymm8,%ymm2
	vmovdqu	%ymm5,384-256-128(%rbx)
	vpaddd	%ymm11,%ymm5,%ymm5

	vpsrld	$11,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm8,%ymm2
	vpaddd	0(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm8,%ymm2
	vpandn	%ymm10,%ymm8,%ymm0
	vpand	%ymm9,%ymm8,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm12,%ymm11
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm12,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm12,%ymm13,%ymm3

	vpxor	%ymm1,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm12,%ymm1

	vpslld	$19,%ymm12,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm11,%ymm7

	vpsrld	$22,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm12,%ymm2
	vpxor	%ymm4,%ymm13,%ymm11
	vpaddd	%ymm5,%ymm15,%ymm15

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm11,%ymm11
	vmovd	52(%r12),%xmm5
	vmovd	52(%r8),%xmm0
	vmovd	52(%r13),%xmm1
	vmovd	52(%r9),%xmm2
	vpinsrd	$1,52(%r14),%xmm5,%xmm5
	vpinsrd	$1,52(%r10),%xmm0,%xmm0
	vpinsrd	$1,52(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,52(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm15,%ymm7
	vpslld	$26,%ymm15,%ymm2
	vmovdqu	%ymm5,416-256-128(%rbx)
	vpaddd	%ymm10,%ymm5,%ymm5

	vpsrld	$11,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm15,%ymm2
	vpaddd	32(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm15,%ymm2
	vpandn	%ymm9,%ymm15,%ymm0
	vpand	%ymm8,%ymm15,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm11,%ymm10
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm11,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm11,%ymm12,%ymm4

	vpxor	%ymm1,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm11,%ymm1

	vpslld	$19,%ymm11,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm10,%ymm7

	vpsrld	$22,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm11,%ymm2
	vpxor	%ymm3,%ymm12,%ymm10
	vpaddd	%ymm5,%ymm14,%ymm14

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm10,%ymm10
	vmovd	56(%r12),%xmm5
	vmovd	56(%r8),%xmm0
	vmovd	56(%r13),%xmm1
	vmovd	56(%r9),%xmm2
	vpinsrd	$1,56(%r14),%xmm5,%xmm5
	vpinsrd	$1,56(%r10),%xmm0,%xmm0
	vpinsrd	$1,56(%r15),%xmm1,%xmm1
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,56(%r11),%xmm2,%xmm2
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm14,%ymm7
	vpslld	$26,%ymm14,%ymm2
	vmovdqu	%ymm5,448-256-128(%rbx)
	vpaddd	%ymm9,%ymm5,%ymm5

	vpsrld	$11,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm14,%ymm2
	vpaddd	64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm14,%ymm2
	vpandn	%ymm8,%ymm14,%ymm0
	vpand	%ymm15,%ymm14,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm10,%ymm9
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm10,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm10,%ymm11,%ymm3

	vpxor	%ymm1,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm10,%ymm1

	vpslld	$19,%ymm10,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm9,%ymm7

	vpsrld	$22,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm10,%ymm2
	vpxor	%ymm4,%ymm11,%ymm9
	vpaddd	%ymm5,%ymm13,%ymm13

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm9,%ymm9
	vmovd	60(%r12),%xmm5
	leaq	64(%r12),%r12
	vmovd	60(%r8),%xmm0
	leaq	64(%r8),%r8
	vmovd	60(%r13),%xmm1
	leaq	64(%r13),%r13
	vmovd	60(%r9),%xmm2
	leaq	64(%r9),%r9
	vpinsrd	$1,60(%r14),%xmm5,%xmm5
	leaq	64(%r14),%r14
	vpinsrd	$1,60(%r10),%xmm0,%xmm0
	leaq	64(%r10),%r10
	vpinsrd	$1,60(%r15),%xmm1,%xmm1
	leaq	64(%r15),%r15
	vpunpckldq	%ymm1,%ymm5,%ymm5
	vpinsrd	$1,60(%r11),%xmm2,%xmm2
	leaq	64(%r11),%r11
	vpunpckldq	%ymm2,%ymm0,%ymm0
	vinserti128	$1,%xmm0,%ymm5,%ymm5
	vpshufb	%ymm6,%ymm5,%ymm5
	vpsrld	$6,%ymm13,%ymm7
	vpslld	$26,%ymm13,%ymm2
	vmovdqu	%ymm5,480-256-128(%rbx)
	vpaddd	%ymm8,%ymm5,%ymm5

	vpsrld	$11,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm13,%ymm2
	vpaddd	96(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	prefetcht0	63(%r12)
	vpslld	$7,%ymm13,%ymm2
	vpandn	%ymm15,%ymm13,%ymm0
	vpand	%ymm14,%ymm13,%ymm4
	prefetcht0	63(%r13)
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm9,%ymm8
	vpxor	%ymm2,%ymm7,%ymm7
	prefetcht0	63(%r14)
	vpslld	$30,%ymm9,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm9,%ymm10,%ymm4
	prefetcht0	63(%r15)
	vpxor	%ymm1,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm9,%ymm1
	prefetcht0	63(%r8)
	vpslld	$19,%ymm9,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm4,%ymm3,%ymm3
	prefetcht0	63(%r9)
	vpxor	%ymm1,%ymm8,%ymm7

	vpsrld	$22,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	prefetcht0	63(%r10)
	vpslld	$10,%ymm9,%ymm2
	vpxor	%ymm3,%ymm10,%ymm8
	vpaddd	%ymm5,%ymm12,%ymm12
	prefetcht0	63(%r11)
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm8,%ymm8
	addq	$256,%rbp
	vmovdqu	0-128(%rax),%ymm5
	movl	$3,%ecx
	jmp	.Loop_16_xx_avx2
.align	32
.Loop_16_xx_avx2:
	vmovdqu	32-128(%rax),%ymm6
	vpaddd	288-256-128(%rbx),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	448-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm12,%ymm7
	vpslld	$26,%ymm12,%ymm2
	vmovdqu	%ymm5,0-128(%rax)
	vpaddd	%ymm15,%ymm5,%ymm5

	vpsrld	$11,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm12,%ymm2
	vpaddd	-128(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm12,%ymm2
	vpandn	%ymm14,%ymm12,%ymm0
	vpand	%ymm13,%ymm12,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm8,%ymm15
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm8,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm8,%ymm9,%ymm3

	vpxor	%ymm1,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm8,%ymm1

	vpslld	$19,%ymm8,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm15,%ymm7

	vpsrld	$22,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm8,%ymm2
	vpxor	%ymm4,%ymm9,%ymm15
	vpaddd	%ymm5,%ymm11,%ymm11

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm15,%ymm15
	vmovdqu	64-128(%rax),%ymm5
	vpaddd	320-256-128(%rbx),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	480-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm11,%ymm7
	vpslld	$26,%ymm11,%ymm2
	vmovdqu	%ymm6,32-128(%rax)
	vpaddd	%ymm14,%ymm6,%ymm6

	vpsrld	$11,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm11,%ymm2
	vpaddd	-96(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm11,%ymm2
	vpandn	%ymm13,%ymm11,%ymm0
	vpand	%ymm12,%ymm11,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm15,%ymm14
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm15,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm15,%ymm8,%ymm4

	vpxor	%ymm1,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm15,%ymm1

	vpslld	$19,%ymm15,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm14,%ymm7

	vpsrld	$22,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm15,%ymm2
	vpxor	%ymm3,%ymm8,%ymm14
	vpaddd	%ymm6,%ymm10,%ymm10

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm14,%ymm14
	vmovdqu	96-128(%rax),%ymm6
	vpaddd	352-256-128(%rbx),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	0-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm10,%ymm7
	vpslld	$26,%ymm10,%ymm2
	vmovdqu	%ymm5,64-128(%rax)
	vpaddd	%ymm13,%ymm5,%ymm5

	vpsrld	$11,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm10,%ymm2
	vpaddd	-64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm10,%ymm2
	vpandn	%ymm12,%ymm10,%ymm0
	vpand	%ymm11,%ymm10,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm14,%ymm13
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm14,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm14,%ymm15,%ymm3

	vpxor	%ymm1,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm14,%ymm1

	vpslld	$19,%ymm14,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm13,%ymm7

	vpsrld	$22,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm14,%ymm2
	vpxor	%ymm4,%ymm15,%ymm13
	vpaddd	%ymm5,%ymm9,%ymm9

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm13,%ymm13
	vmovdqu	128-128(%rax),%ymm5
	vpaddd	384-256-128(%rbx),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	32-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm9,%ymm7
	vpslld	$26,%ymm9,%ymm2
	vmovdqu	%ymm6,96-128(%rax)
	vpaddd	%ymm12,%ymm6,%ymm6

	vpsrld	$11,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm9,%ymm2
	vpaddd	-32(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm9,%ymm2
	vpandn	%ymm11,%ymm9,%ymm0
	vpand	%ymm10,%ymm9,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm13,%ymm12
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm13,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm13,%ymm14,%ymm4

	vpxor	%ymm1,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm13,%ymm1

	vpslld	$19,%ymm13,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm12,%ymm7

	vpsrld	$22,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm13,%ymm2
	vpxor	%ymm3,%ymm14,%ymm12
	vpaddd	%ymm6,%ymm8,%ymm8

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm12,%ymm12
	vmovdqu	160-128(%rax),%ymm6
	vpaddd	416-256-128(%rbx),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	64-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm8,%ymm7
	vpslld	$26,%ymm8,%ymm2
	vmovdqu	%ymm5,128-128(%rax)
	vpaddd	%ymm11,%ymm5,%ymm5

	vpsrld	$11,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm8,%ymm2
	vpaddd	0(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm8,%ymm2
	vpandn	%ymm10,%ymm8,%ymm0
	vpand	%ymm9,%ymm8,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm12,%ymm11
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm12,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm12,%ymm13,%ymm3

	vpxor	%ymm1,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm12,%ymm1

	vpslld	$19,%ymm12,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm11,%ymm7

	vpsrld	$22,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm12,%ymm2
	vpxor	%ymm4,%ymm13,%ymm11
	vpaddd	%ymm5,%ymm15,%ymm15

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm11,%ymm11
	vmovdqu	192-128(%rax),%ymm5
	vpaddd	448-256-128(%rbx),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	96-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm15,%ymm7
	vpslld	$26,%ymm15,%ymm2
	vmovdqu	%ymm6,160-128(%rax)
	vpaddd	%ymm10,%ymm6,%ymm6

	vpsrld	$11,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm15,%ymm2
	vpaddd	32(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm15,%ymm2
	vpandn	%ymm9,%ymm15,%ymm0
	vpand	%ymm8,%ymm15,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm11,%ymm10
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm11,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm11,%ymm12,%ymm4

	vpxor	%ymm1,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm11,%ymm1

	vpslld	$19,%ymm11,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm10,%ymm7

	vpsrld	$22,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm11,%ymm2
	vpxor	%ymm3,%ymm12,%ymm10
	vpaddd	%ymm6,%ymm14,%ymm14

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm10,%ymm10
	vmovdqu	224-128(%rax),%ymm6
	vpaddd	480-256-128(%rbx),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	128-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm14,%ymm7
	vpslld	$26,%ymm14,%ymm2
	vmovdqu	%ymm5,192-128(%rax)
	vpaddd	%ymm9,%ymm5,%ymm5

	vpsrld	$11,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm14,%ymm2
	vpaddd	64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm14,%ymm2
	vpandn	%ymm8,%ymm14,%ymm0
	vpand	%ymm15,%ymm14,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm10,%ymm9
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm10,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm10,%ymm11,%ymm3

	vpxor	%ymm1,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm10,%ymm1

	vpslld	$19,%ymm10,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm9,%ymm7

	vpsrld	$22,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm10,%ymm2
	vpxor	%ymm4,%ymm11,%ymm9
	vpaddd	%ymm5,%ymm13,%ymm13

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm9,%ymm9
	vmovdqu	256-256-128(%rbx),%ymm5
	vpaddd	0-128(%rax),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	160-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm13,%ymm7
	vpslld	$26,%ymm13,%ymm2
	vmovdqu	%ymm6,224-128(%rax)
	vpaddd	%ymm8,%ymm6,%ymm6

	vpsrld	$11,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm13,%ymm2
	vpaddd	96(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm13,%ymm2
	vpandn	%ymm15,%ymm13,%ymm0
	vpand	%ymm14,%ymm13,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm9,%ymm8
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm9,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm9,%ymm10,%ymm4

	vpxor	%ymm1,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm9,%ymm1

	vpslld	$19,%ymm9,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm8,%ymm7

	vpsrld	$22,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm9,%ymm2
	vpxor	%ymm3,%ymm10,%ymm8
	vpaddd	%ymm6,%ymm12,%ymm12

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm8,%ymm8
	addq	$256,%rbp
	vmovdqu	288-256-128(%rbx),%ymm6
	vpaddd	32-128(%rax),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	192-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm12,%ymm7
	vpslld	$26,%ymm12,%ymm2
	vmovdqu	%ymm5,256-256-128(%rbx)
	vpaddd	%ymm15,%ymm5,%ymm5

	vpsrld	$11,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm12,%ymm2
	vpaddd	-128(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm12,%ymm2
	vpandn	%ymm14,%ymm12,%ymm0
	vpand	%ymm13,%ymm12,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm8,%ymm15
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm8,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm8,%ymm9,%ymm3

	vpxor	%ymm1,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm8,%ymm1

	vpslld	$19,%ymm8,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm15,%ymm7

	vpsrld	$22,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm8,%ymm2
	vpxor	%ymm4,%ymm9,%ymm15
	vpaddd	%ymm5,%ymm11,%ymm11

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm15,%ymm15
	vpaddd	%ymm7,%ymm15,%ymm15
	vmovdqu	320-256-128(%rbx),%ymm5
	vpaddd	64-128(%rax),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	224-128(%rax),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm11,%ymm7
	vpslld	$26,%ymm11,%ymm2
	vmovdqu	%ymm6,288-256-128(%rbx)
	vpaddd	%ymm14,%ymm6,%ymm6

	vpsrld	$11,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm11,%ymm2
	vpaddd	-96(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm11,%ymm2
	vpandn	%ymm13,%ymm11,%ymm0
	vpand	%ymm12,%ymm11,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm15,%ymm14
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm15,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm15,%ymm8,%ymm4

	vpxor	%ymm1,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm15,%ymm1

	vpslld	$19,%ymm15,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm14,%ymm7

	vpsrld	$22,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm15,%ymm2
	vpxor	%ymm3,%ymm8,%ymm14
	vpaddd	%ymm6,%ymm10,%ymm10

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm14,%ymm14
	vpaddd	%ymm7,%ymm14,%ymm14
	vmovdqu	352-256-128(%rbx),%ymm6
	vpaddd	96-128(%rax),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	256-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm10,%ymm7
	vpslld	$26,%ymm10,%ymm2
	vmovdqu	%ymm5,320-256-128(%rbx)
	vpaddd	%ymm13,%ymm5,%ymm5

	vpsrld	$11,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm10,%ymm2
	vpaddd	-64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm10,%ymm2
	vpandn	%ymm12,%ymm10,%ymm0
	vpand	%ymm11,%ymm10,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm14,%ymm13
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm14,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm14,%ymm15,%ymm3

	vpxor	%ymm1,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm14,%ymm1

	vpslld	$19,%ymm14,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm13,%ymm7

	vpsrld	$22,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm14,%ymm2
	vpxor	%ymm4,%ymm15,%ymm13
	vpaddd	%ymm5,%ymm9,%ymm9

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm13,%ymm13
	vpaddd	%ymm7,%ymm13,%ymm13
	vmovdqu	384-256-128(%rbx),%ymm5
	vpaddd	128-128(%rax),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	288-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm9,%ymm7
	vpslld	$26,%ymm9,%ymm2
	vmovdqu	%ymm6,352-256-128(%rbx)
	vpaddd	%ymm12,%ymm6,%ymm6

	vpsrld	$11,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm9,%ymm2
	vpaddd	-32(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm9,%ymm2
	vpandn	%ymm11,%ymm9,%ymm0
	vpand	%ymm10,%ymm9,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm13,%ymm12
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm13,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm13,%ymm14,%ymm4

	vpxor	%ymm1,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm13,%ymm1

	vpslld	$19,%ymm13,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm12,%ymm7

	vpsrld	$22,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm13,%ymm2
	vpxor	%ymm3,%ymm14,%ymm12
	vpaddd	%ymm6,%ymm8,%ymm8

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm12,%ymm12
	vpaddd	%ymm7,%ymm12,%ymm12
	vmovdqu	416-256-128(%rbx),%ymm6
	vpaddd	160-128(%rax),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	320-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm8,%ymm7
	vpslld	$26,%ymm8,%ymm2
	vmovdqu	%ymm5,384-256-128(%rbx)
	vpaddd	%ymm11,%ymm5,%ymm5

	vpsrld	$11,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm8,%ymm2
	vpaddd	0(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm8,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm8,%ymm2
	vpandn	%ymm10,%ymm8,%ymm0
	vpand	%ymm9,%ymm8,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm12,%ymm11
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm12,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm12,%ymm13,%ymm3

	vpxor	%ymm1,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm12,%ymm1

	vpslld	$19,%ymm12,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm11,%ymm7

	vpsrld	$22,%ymm12,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm12,%ymm2
	vpxor	%ymm4,%ymm13,%ymm11
	vpaddd	%ymm5,%ymm15,%ymm15

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm11,%ymm11
	vpaddd	%ymm7,%ymm11,%ymm11
	vmovdqu	448-256-128(%rbx),%ymm5
	vpaddd	192-128(%rax),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	352-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm15,%ymm7
	vpslld	$26,%ymm15,%ymm2
	vmovdqu	%ymm6,416-256-128(%rbx)
	vpaddd	%ymm10,%ymm6,%ymm6

	vpsrld	$11,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm15,%ymm2
	vpaddd	32(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm15,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm15,%ymm2
	vpandn	%ymm9,%ymm15,%ymm0
	vpand	%ymm8,%ymm15,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm11,%ymm10
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm11,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm11,%ymm12,%ymm4

	vpxor	%ymm1,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm11,%ymm1

	vpslld	$19,%ymm11,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm10,%ymm7

	vpsrld	$22,%ymm11,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm11,%ymm2
	vpxor	%ymm3,%ymm12,%ymm10
	vpaddd	%ymm6,%ymm14,%ymm14

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm10,%ymm10
	vpaddd	%ymm7,%ymm10,%ymm10
	vmovdqu	480-256-128(%rbx),%ymm6
	vpaddd	224-128(%rax),%ymm5,%ymm5

	vpsrld	$3,%ymm6,%ymm7
	vpsrld	$7,%ymm6,%ymm1
	vpslld	$25,%ymm6,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm6,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm6,%ymm2
	vmovdqu	384-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm5,%ymm5
	vpxor	%ymm1,%ymm3,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm5,%ymm5
	vpsrld	$6,%ymm14,%ymm7
	vpslld	$26,%ymm14,%ymm2
	vmovdqu	%ymm5,448-256-128(%rbx)
	vpaddd	%ymm9,%ymm5,%ymm5

	vpsrld	$11,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm14,%ymm2
	vpaddd	64(%rbp),%ymm5,%ymm5
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm14,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm14,%ymm2
	vpandn	%ymm8,%ymm14,%ymm0
	vpand	%ymm15,%ymm14,%ymm3

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm10,%ymm9
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm10,%ymm1
	vpxor	%ymm3,%ymm0,%ymm0
	vpxor	%ymm10,%ymm11,%ymm3

	vpxor	%ymm1,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm5,%ymm5

	vpsrld	$13,%ymm10,%ymm1

	vpslld	$19,%ymm10,%ymm2
	vpaddd	%ymm0,%ymm5,%ymm5
	vpand	%ymm3,%ymm4,%ymm4

	vpxor	%ymm1,%ymm9,%ymm7

	vpsrld	$22,%ymm10,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm10,%ymm2
	vpxor	%ymm4,%ymm11,%ymm9
	vpaddd	%ymm5,%ymm13,%ymm13

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm5,%ymm9,%ymm9
	vpaddd	%ymm7,%ymm9,%ymm9
	vmovdqu	0-128(%rax),%ymm5
	vpaddd	256-256-128(%rbx),%ymm6,%ymm6

	vpsrld	$3,%ymm5,%ymm7
	vpsrld	$7,%ymm5,%ymm1
	vpslld	$25,%ymm5,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$18,%ymm5,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$14,%ymm5,%ymm2
	vmovdqu	416-256-128(%rbx),%ymm0
	vpsrld	$10,%ymm0,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7
	vpsrld	$17,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$15,%ymm0,%ymm2
	vpaddd	%ymm7,%ymm6,%ymm6
	vpxor	%ymm1,%ymm4,%ymm7
	vpsrld	$19,%ymm0,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$13,%ymm0,%ymm2
	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7
	vpaddd	%ymm7,%ymm6,%ymm6
	vpsrld	$6,%ymm13,%ymm7
	vpslld	$26,%ymm13,%ymm2
	vmovdqu	%ymm6,480-256-128(%rbx)
	vpaddd	%ymm8,%ymm6,%ymm6

	vpsrld	$11,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7
	vpslld	$21,%ymm13,%ymm2
	vpaddd	96(%rbp),%ymm6,%ymm6
	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$25,%ymm13,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$7,%ymm13,%ymm2
	vpandn	%ymm15,%ymm13,%ymm0
	vpand	%ymm14,%ymm13,%ymm4

	vpxor	%ymm1,%ymm7,%ymm7

	vpsrld	$2,%ymm9,%ymm8
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$30,%ymm9,%ymm1
	vpxor	%ymm4,%ymm0,%ymm0
	vpxor	%ymm9,%ymm10,%ymm4

	vpxor	%ymm1,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm6,%ymm6

	vpsrld	$13,%ymm9,%ymm1

	vpslld	$19,%ymm9,%ymm2
	vpaddd	%ymm0,%ymm6,%ymm6
	vpand	%ymm4,%ymm3,%ymm3

	vpxor	%ymm1,%ymm8,%ymm7

	vpsrld	$22,%ymm9,%ymm1
	vpxor	%ymm2,%ymm7,%ymm7

	vpslld	$10,%ymm9,%ymm2
	vpxor	%ymm3,%ymm10,%ymm8
	vpaddd	%ymm6,%ymm12,%ymm12

	vpxor	%ymm1,%ymm7,%ymm7
	vpxor	%ymm2,%ymm7,%ymm7

	vpaddd	%ymm6,%ymm8,%ymm8
	vpaddd	%ymm7,%ymm8,%ymm8
	addq	$256,%rbp
	decl	%ecx
	jnz	.Loop_16_xx_avx2

	movl	$1,%ecx
	leaq	512(%rsp),%rbx
	leaq	K256+128(%rip),%rbp
	cmpl	0(%rbx),%ecx
	cmovgeq	%rbp,%r12
	cmpl	4(%rbx),%ecx
	cmovgeq	%rbp,%r13
	cmpl	8(%rbx),%ecx
	cmovgeq	%rbp,%r14
	cmpl	12(%rbx),%ecx
	cmovgeq	%rbp,%r15
	cmpl	16(%rbx),%ecx
	cmovgeq	%rbp,%r8
	cmpl	20(%rbx),%ecx
	cmovgeq	%rbp,%r9
	cmpl	24(%rbx),%ecx
	cmovgeq	%rbp,%r10
	cmpl	28(%rbx),%ecx
	cmovgeq	%rbp,%r11
	vmovdqa	(%rbx),%ymm7
	vpxor	%ymm0,%ymm0,%ymm0
	vmovdqa	%ymm7,%ymm6
	vpcmpgtd	%ymm0,%ymm6,%ymm6
	vpaddd	%ymm6,%ymm7,%ymm7

	vmovdqu	0-128(%rdi),%ymm0
	vpand	%ymm6,%ymm8,%ymm8
	vmovdqu	32-128(%rdi),%ymm1
	vpand	%ymm6,%ymm9,%ymm9
	vmovdqu	64-128(%rdi),%ymm2
	vpand	%ymm6,%ymm10,%ymm10
	vmovdqu	96-128(%rdi),%ymm5
	vpand	%ymm6,%ymm11,%ymm11
	vpaddd	%ymm0,%ymm8,%ymm8
	vmovdqu	128-128(%rdi),%ymm0
	vpand	%ymm6,%ymm12,%ymm12
	vpaddd	%ymm1,%ymm9,%ymm9
	vmovdqu	160-128(%rdi),%ymm1
	vpand	%ymm6,%ymm13,%ymm13
	vpaddd	%ymm2,%ymm10,%ymm10
	vmovdqu	192-128(%rdi),%ymm2
	vpand	%ymm6,%ymm14,%ymm14
	vpaddd	%ymm5,%ymm11,%ymm11
	vmovdqu	224-128(%rdi),%ymm5
	vpand	%ymm6,%ymm15,%ymm15
	vpaddd	%ymm0,%ymm12,%ymm12
	vpaddd	%ymm1,%ymm13,%ymm13
	vmovdqu	%ymm8,0-128(%rdi)
	vpaddd	%ymm2,%ymm14,%ymm14
	vmovdqu	%ymm9,32-128(%rdi)
	vpaddd	%ymm5,%ymm15,%ymm15
	vmovdqu	%ymm10,64-128(%rdi)
	vmovdqu	%ymm11,96-128(%rdi)
	vmovdqu	%ymm12,128-128(%rdi)
	vmovdqu	%ymm13,160-128(%rdi)
	vmovdqu	%ymm14,192-128(%rdi)
	vmovdqu	%ymm15,224-128(%rdi)

	vmovdqu	%ymm7,(%rbx)
	leaq	256+128(%rsp),%rbx
	vmovdqu	.Lpbswap(%rip),%ymm6
	decl	%edx
	jnz	.Loop_avx2







.Ldone_avx2:
	movq	544(%rsp),%rax
.cfi_def_cfa	%rax,8
	vzeroupper
	movq	-48(%rax),%r15
.cfi_restore	%r15
	movq	-40(%rax),%r14
.cfi_restore	%r14
	movq	-32(%rax),%r13
.cfi_restore	%r13
	movq	-24(%rax),%r12
.cfi_restore	%r12
	movq	-16(%rax),%rbp
.cfi_restore	%rbp
	movq	-8(%rax),%rbx
.cfi_restore	%rbx
	leaq	(%rax),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx2:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_multi_block_avx2,.-sha256_multi_block_avx2
.align	256
K256:
.long	1116352408,1116352408,1116352408,1116352408
.long	1116352408,1116352408,1116352408,1116352408
.long	1899447441,1899447441,1899447441,1899447441
.long	1899447441,1899447441,1899447441,1899447441
.long	3049323471,3049323471,3049323471,3049323471
.long	3049323471,3049323471,3049323471,3049323471
.long	3921009573,3921009573,3921009573,3921009573
.long	3921009573,3921009573,3921009573,3921009573
.long	961987163,961987163,961987163,961987163
.long	961987163,961987163,961987163,961987163
.long	1508970993,1508970993,1508970993,1508970993
.long	1508970993,1508970993,1508970993,1508970993
.long	2453635748,2453635748,2453635748,2453635748
.long	2453635748,2453635748,2453635748,2453635748
.long	2870763221,2870763221,2870763221,2870763221
.long	2870763221,2870763221,2870763221,2870763221
.long	3624381080,3624381080,3624381080,3624381080
.long	3624381080,3624381080,3624381080,3624381080
.long	310598401,310598401,310598401,310598401
.long	310598401,310598401,310598401,310598401
.long	607225278,607225278,607225278,607225278
.long	607225278,607225278,607225278,607225278
.long	1426881987,1426881987,1426881987,1426881987
.long	1426881987,1426881987,1426881987,1426881987
.long	1925078388,1925078388,1925078388,1925078388
.long	1925078388,1925078388,1925078388,1925078388
.long	2162078206,2162078206,2162078206,2162078206
.long	2162078206,2162078206,2162078206,2162078206
.long	2614888103,2614888103,2614888103,2614888103
.long	2614888103,2614888103,2614888103,2614888103
.long	3248222580,3248222580,3248222580,3248222580
.long	3248222580,3248222580,3248222580,3248222580
.long	3835390401,3835390401,3835390401,3835390401
.long	3835390401,3835390401,3835390401,3835390401
.long	4022224774,4022224774,4022224774,4022224774
.long	4022224774,4022224774,4022224774,4022224774
.long	264347078,264347078,264347078,264347078
.long	264347078,264347078,264347078,264347078
.long	604807628,604807628,604807628,604807628
.long	604807628,604807628,604807628,604807628
.long	770255983,770255983,770255983,770255983
.long	770255983,770255983,770255983,770255983
.long	1249150122,1249150122,1249150122,1249150122
.long	1249150122,1249150122,1249150122,1249150122
.long	1555081692,1555081692,1555081692,1555081692
.long	1555081692,1555081692,1555081692,1555081692
.long	1996064986,1996064986,1996064986,1996064986
.long	1996064986,1996064986,1996064986,1996064986
.long	2554220882,2554220882,2554220882,2554220882
.long	2554220882,2554220882,2554220882,2554220882
.long	2821834349,2821834349,2821834349,2821834349
.long	2821834349,2821834349,2821834349,2821834349
.long	2952996808,2952996808,2952996808,2952996808
.long	2952996808,2952996808,2952996808,2952996808
.long	3210313671,3210313671,3210313671,3210313671
.long	3210313671,3210313671,3210313671,3210313671
.long	3336571891,3336571891,3336571891,3336571891
.long	3336571891,3336571891,3336571891,3336571891
.long	3584528711,3584528711,3584528711,3584528711
.long	3584528711,3584528711,3584528711,3584528711
.long	113926993,113926993,113926993,113926993
.long	113926993,113926993,113926993,113926993
.long	338241895,338241895,338241895,338241895
.long	338241895,338241895,338241895,338241895
.long	666307205,666307205,666307205,666307205
.long	666307205,666307205,666307205,666307205
.long	773529912,773529912,773529912,773529912
.long	773529912,773529912,773529912,773529912
.long	1294757372,1294757372,1294757372,1294757372
.long	1294757372,1294757372,1294757372,1294757372
.long	1396182291,1396182291,1396182291,1396182291
.long	1396182291,1396182291,1396182291,1396182291
.long	1695183700,1695183700,1695183700,1695183700
.long	1695183700,1695183700,1695183700,1695183700
.long	1986661051,1986661051,1986661051,1986661051
.long	1986661051,1986661051,1986661051,1986661051
.long	2177026350,2177026350,2177026350,2177026350
.long	2177026350,2177026350,2177026350,2177026350
.long	2456956037,2456956037,2456956037,2456956037
.long	2456956037,2456956037,2456956037,2456956037
.long	2730485921,2730485921,2730485921,2730485921
.long	2730485921,2730485921,2730485921,2730485921
.long	2820302411,2820302411,2820302411,2820302411
.long	2820302411,2820302411,2820302411,2820302411
.long	3259730800,3259730800,3259730800,3259730800
.long	3259730800,3259730800,3259730800,3259730800
.long	3345764771,3345764771,3345764771,3345764771
.long	3345764771,3345764771,3345764771,3345764771
.long	3516065817,3516065817,3516065817,3516065817
.long	3516065817,3516065817,3516065817,3516065817
.long	3600352804,3600352804,3600352804,3600352804
.long	3600352804,3600352804,3600352804,3600352804
.long	4094571909,4094571909,4094571909,4094571909
.long	4094571909,4094571909,4094571909,4094571909
.long	275423344,275423344,275423344,275423344
.long	275423344,275423344,275423344,275423344
.long	430227734,430227734,430227734,430227734
.long	430227734,430227734,430227734,430227734
.long	506948616,506948616,506948616,506948616
.long	506948616,506948616,506948616,506948616
.long	659060556,659060556,659060556,659060556
.long	659060556,659060556,659060556,659060556
.long	883997877,883997877,883997877,883997877
.long	883997877,883997877,883997877,883997877
.long	958139571,958139571,958139571,958139571
.long	958139571,958139571,958139571,958139571
.long	1322822218,1322822218,1322822218,1322822218
.long	1322822218,1322822218,1322822218,1322822218
.long	1537002063,1537002063,1537002063,1537002063
.long	1537002063,1537002063,1537002063,1537002063
.long	1747873779,1747873779,1747873779,1747873779
.long	1747873779,1747873779,1747873779,1747873779
.long	1955562222,1955562222,1955562222,1955562222
.long	1955562222,1955562222,1955562222,1955562222
.long	2024104815,2024104815,2024104815,2024104815
.long	2024104815,2024104815,2024104815,2024104815
.long	2227730452,2227730452,2227730452,2227730452
.long	2227730452,2227730452,2227730452,2227730452
.long	2361852424,2361852424,2361852424,2361852424
.long	2361852424,2361852424,2361852424,2361852424
.long	2428436474,2428436474,2428436474,2428436474
.long	2428436474,2428436474,2428436474,2428436474
.long	2756734187,2756734187,2756734187,2756734187
.long	2756734187,2756734187,2756734187,2756734187
.long	3204031479,3204031479,3204031479,3204031479
.long	3204031479,3204031479,3204031479,3204031479
.long	3329325298,3329325298,3329325298,3329325298
.long	3329325298,3329325298,3329325298,3329325298
.Lpbswap:
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
K256_shaext:
.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
.long	0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5
.long	0xd807aa98,0x12835b01,0x243185be,0x550c7dc3
.long	0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174
.long	0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc
.long	0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da
.long	0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7
.long	0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967
.long	0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13
.long	0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85
.long	0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3
.long	0xd192e819,0xd6990624,0xf40e3585,0x106aa070
.long	0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5
.long	0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3
.long	0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208
.long	0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2
.byte	83,72,65,50,53,54,32,109,117,108,116,105,45,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                              node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/sha256-x86_64.s           0000664 0000000 0000000 00000277164 14746647661 0030100 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	


.globl	sha256_block_data_order
.type	sha256_block_data_order,@function
.align	16
sha256_block_data_order:
.cfi_startproc	
	leaq	OPENSSL_ia32cap_P(%rip),%r11
	movl	0(%r11),%r9d
	movl	4(%r11),%r10d
	movl	8(%r11),%r11d
	testl	$536870912,%r11d
	jnz	_shaext_shortcut
	andl	$296,%r11d
	cmpl	$296,%r11d
	je	.Lavx2_shortcut
	andl	$1073741824,%r9d
	andl	$268435968,%r10d
	orl	%r9d,%r10d
	cmpl	$1342177792,%r10d
	je	.Lavx_shortcut
	testl	$512,%r10d
	jnz	.Lssse3_shortcut
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$64+32,%rsp
	leaq	(%rsi,%rdx,4),%rdx
	andq	$-64,%rsp
	movq	%rdi,64+0(%rsp)
	movq	%rsi,64+8(%rsp)
	movq	%rdx,64+16(%rsp)
	movq	%rax,88(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
.Lprologue:

	movl	0(%rdi),%eax
	movl	4(%rdi),%ebx
	movl	8(%rdi),%ecx
	movl	12(%rdi),%edx
	movl	16(%rdi),%r8d
	movl	20(%rdi),%r9d
	movl	24(%rdi),%r10d
	movl	28(%rdi),%r11d
	jmp	.Lloop

.align	16
.Lloop:
	movl	%ebx,%edi
	leaq	K256(%rip),%rbp
	xorl	%ecx,%edi
	movl	0(%rsi),%r12d
	movl	%r8d,%r13d
	movl	%eax,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r9d,%r15d

	xorl	%r8d,%r13d
	rorl	$9,%r14d
	xorl	%r10d,%r15d

	movl	%r12d,0(%rsp)
	xorl	%eax,%r14d
	andl	%r8d,%r15d

	rorl	$5,%r13d
	addl	%r11d,%r12d
	xorl	%r10d,%r15d

	rorl	$11,%r14d
	xorl	%r8d,%r13d
	addl	%r15d,%r12d

	movl	%eax,%r15d
	addl	(%rbp),%r12d
	xorl	%eax,%r14d

	xorl	%ebx,%r15d
	rorl	$6,%r13d
	movl	%ebx,%r11d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r11d
	addl	%r12d,%edx
	addl	%r12d,%r11d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r11d
	movl	4(%rsi),%r12d
	movl	%edx,%r13d
	movl	%r11d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r8d,%edi

	xorl	%edx,%r13d
	rorl	$9,%r14d
	xorl	%r9d,%edi

	movl	%r12d,4(%rsp)
	xorl	%r11d,%r14d
	andl	%edx,%edi

	rorl	$5,%r13d
	addl	%r10d,%r12d
	xorl	%r9d,%edi

	rorl	$11,%r14d
	xorl	%edx,%r13d
	addl	%edi,%r12d

	movl	%r11d,%edi
	addl	(%rbp),%r12d
	xorl	%r11d,%r14d

	xorl	%eax,%edi
	rorl	$6,%r13d
	movl	%eax,%r10d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r10d
	addl	%r12d,%ecx
	addl	%r12d,%r10d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r10d
	movl	8(%rsi),%r12d
	movl	%ecx,%r13d
	movl	%r10d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%edx,%r15d

	xorl	%ecx,%r13d
	rorl	$9,%r14d
	xorl	%r8d,%r15d

	movl	%r12d,8(%rsp)
	xorl	%r10d,%r14d
	andl	%ecx,%r15d

	rorl	$5,%r13d
	addl	%r9d,%r12d
	xorl	%r8d,%r15d

	rorl	$11,%r14d
	xorl	%ecx,%r13d
	addl	%r15d,%r12d

	movl	%r10d,%r15d
	addl	(%rbp),%r12d
	xorl	%r10d,%r14d

	xorl	%r11d,%r15d
	rorl	$6,%r13d
	movl	%r11d,%r9d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r9d
	addl	%r12d,%ebx
	addl	%r12d,%r9d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r9d
	movl	12(%rsi),%r12d
	movl	%ebx,%r13d
	movl	%r9d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%ecx,%edi

	xorl	%ebx,%r13d
	rorl	$9,%r14d
	xorl	%edx,%edi

	movl	%r12d,12(%rsp)
	xorl	%r9d,%r14d
	andl	%ebx,%edi

	rorl	$5,%r13d
	addl	%r8d,%r12d
	xorl	%edx,%edi

	rorl	$11,%r14d
	xorl	%ebx,%r13d
	addl	%edi,%r12d

	movl	%r9d,%edi
	addl	(%rbp),%r12d
	xorl	%r9d,%r14d

	xorl	%r10d,%edi
	rorl	$6,%r13d
	movl	%r10d,%r8d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r8d
	addl	%r12d,%eax
	addl	%r12d,%r8d

	leaq	20(%rbp),%rbp
	addl	%r14d,%r8d
	movl	16(%rsi),%r12d
	movl	%eax,%r13d
	movl	%r8d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%ebx,%r15d

	xorl	%eax,%r13d
	rorl	$9,%r14d
	xorl	%ecx,%r15d

	movl	%r12d,16(%rsp)
	xorl	%r8d,%r14d
	andl	%eax,%r15d

	rorl	$5,%r13d
	addl	%edx,%r12d
	xorl	%ecx,%r15d

	rorl	$11,%r14d
	xorl	%eax,%r13d
	addl	%r15d,%r12d

	movl	%r8d,%r15d
	addl	(%rbp),%r12d
	xorl	%r8d,%r14d

	xorl	%r9d,%r15d
	rorl	$6,%r13d
	movl	%r9d,%edx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%edx
	addl	%r12d,%r11d
	addl	%r12d,%edx

	leaq	4(%rbp),%rbp
	addl	%r14d,%edx
	movl	20(%rsi),%r12d
	movl	%r11d,%r13d
	movl	%edx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%eax,%edi

	xorl	%r11d,%r13d
	rorl	$9,%r14d
	xorl	%ebx,%edi

	movl	%r12d,20(%rsp)
	xorl	%edx,%r14d
	andl	%r11d,%edi

	rorl	$5,%r13d
	addl	%ecx,%r12d
	xorl	%ebx,%edi

	rorl	$11,%r14d
	xorl	%r11d,%r13d
	addl	%edi,%r12d

	movl	%edx,%edi
	addl	(%rbp),%r12d
	xorl	%edx,%r14d

	xorl	%r8d,%edi
	rorl	$6,%r13d
	movl	%r8d,%ecx

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%ecx
	addl	%r12d,%r10d
	addl	%r12d,%ecx

	leaq	4(%rbp),%rbp
	addl	%r14d,%ecx
	movl	24(%rsi),%r12d
	movl	%r10d,%r13d
	movl	%ecx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r11d,%r15d

	xorl	%r10d,%r13d
	rorl	$9,%r14d
	xorl	%eax,%r15d

	movl	%r12d,24(%rsp)
	xorl	%ecx,%r14d
	andl	%r10d,%r15d

	rorl	$5,%r13d
	addl	%ebx,%r12d
	xorl	%eax,%r15d

	rorl	$11,%r14d
	xorl	%r10d,%r13d
	addl	%r15d,%r12d

	movl	%ecx,%r15d
	addl	(%rbp),%r12d
	xorl	%ecx,%r14d

	xorl	%edx,%r15d
	rorl	$6,%r13d
	movl	%edx,%ebx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%ebx
	addl	%r12d,%r9d
	addl	%r12d,%ebx

	leaq	4(%rbp),%rbp
	addl	%r14d,%ebx
	movl	28(%rsi),%r12d
	movl	%r9d,%r13d
	movl	%ebx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r10d,%edi

	xorl	%r9d,%r13d
	rorl	$9,%r14d
	xorl	%r11d,%edi

	movl	%r12d,28(%rsp)
	xorl	%ebx,%r14d
	andl	%r9d,%edi

	rorl	$5,%r13d
	addl	%eax,%r12d
	xorl	%r11d,%edi

	rorl	$11,%r14d
	xorl	%r9d,%r13d
	addl	%edi,%r12d

	movl	%ebx,%edi
	addl	(%rbp),%r12d
	xorl	%ebx,%r14d

	xorl	%ecx,%edi
	rorl	$6,%r13d
	movl	%ecx,%eax

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%eax
	addl	%r12d,%r8d
	addl	%r12d,%eax

	leaq	20(%rbp),%rbp
	addl	%r14d,%eax
	movl	32(%rsi),%r12d
	movl	%r8d,%r13d
	movl	%eax,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r9d,%r15d

	xorl	%r8d,%r13d
	rorl	$9,%r14d
	xorl	%r10d,%r15d

	movl	%r12d,32(%rsp)
	xorl	%eax,%r14d
	andl	%r8d,%r15d

	rorl	$5,%r13d
	addl	%r11d,%r12d
	xorl	%r10d,%r15d

	rorl	$11,%r14d
	xorl	%r8d,%r13d
	addl	%r15d,%r12d

	movl	%eax,%r15d
	addl	(%rbp),%r12d
	xorl	%eax,%r14d

	xorl	%ebx,%r15d
	rorl	$6,%r13d
	movl	%ebx,%r11d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r11d
	addl	%r12d,%edx
	addl	%r12d,%r11d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r11d
	movl	36(%rsi),%r12d
	movl	%edx,%r13d
	movl	%r11d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r8d,%edi

	xorl	%edx,%r13d
	rorl	$9,%r14d
	xorl	%r9d,%edi

	movl	%r12d,36(%rsp)
	xorl	%r11d,%r14d
	andl	%edx,%edi

	rorl	$5,%r13d
	addl	%r10d,%r12d
	xorl	%r9d,%edi

	rorl	$11,%r14d
	xorl	%edx,%r13d
	addl	%edi,%r12d

	movl	%r11d,%edi
	addl	(%rbp),%r12d
	xorl	%r11d,%r14d

	xorl	%eax,%edi
	rorl	$6,%r13d
	movl	%eax,%r10d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r10d
	addl	%r12d,%ecx
	addl	%r12d,%r10d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r10d
	movl	40(%rsi),%r12d
	movl	%ecx,%r13d
	movl	%r10d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%edx,%r15d

	xorl	%ecx,%r13d
	rorl	$9,%r14d
	xorl	%r8d,%r15d

	movl	%r12d,40(%rsp)
	xorl	%r10d,%r14d
	andl	%ecx,%r15d

	rorl	$5,%r13d
	addl	%r9d,%r12d
	xorl	%r8d,%r15d

	rorl	$11,%r14d
	xorl	%ecx,%r13d
	addl	%r15d,%r12d

	movl	%r10d,%r15d
	addl	(%rbp),%r12d
	xorl	%r10d,%r14d

	xorl	%r11d,%r15d
	rorl	$6,%r13d
	movl	%r11d,%r9d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r9d
	addl	%r12d,%ebx
	addl	%r12d,%r9d

	leaq	4(%rbp),%rbp
	addl	%r14d,%r9d
	movl	44(%rsi),%r12d
	movl	%ebx,%r13d
	movl	%r9d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%ecx,%edi

	xorl	%ebx,%r13d
	rorl	$9,%r14d
	xorl	%edx,%edi

	movl	%r12d,44(%rsp)
	xorl	%r9d,%r14d
	andl	%ebx,%edi

	rorl	$5,%r13d
	addl	%r8d,%r12d
	xorl	%edx,%edi

	rorl	$11,%r14d
	xorl	%ebx,%r13d
	addl	%edi,%r12d

	movl	%r9d,%edi
	addl	(%rbp),%r12d
	xorl	%r9d,%r14d

	xorl	%r10d,%edi
	rorl	$6,%r13d
	movl	%r10d,%r8d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r8d
	addl	%r12d,%eax
	addl	%r12d,%r8d

	leaq	20(%rbp),%rbp
	addl	%r14d,%r8d
	movl	48(%rsi),%r12d
	movl	%eax,%r13d
	movl	%r8d,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%ebx,%r15d

	xorl	%eax,%r13d
	rorl	$9,%r14d
	xorl	%ecx,%r15d

	movl	%r12d,48(%rsp)
	xorl	%r8d,%r14d
	andl	%eax,%r15d

	rorl	$5,%r13d
	addl	%edx,%r12d
	xorl	%ecx,%r15d

	rorl	$11,%r14d
	xorl	%eax,%r13d
	addl	%r15d,%r12d

	movl	%r8d,%r15d
	addl	(%rbp),%r12d
	xorl	%r8d,%r14d

	xorl	%r9d,%r15d
	rorl	$6,%r13d
	movl	%r9d,%edx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%edx
	addl	%r12d,%r11d
	addl	%r12d,%edx

	leaq	4(%rbp),%rbp
	addl	%r14d,%edx
	movl	52(%rsi),%r12d
	movl	%r11d,%r13d
	movl	%edx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%eax,%edi

	xorl	%r11d,%r13d
	rorl	$9,%r14d
	xorl	%ebx,%edi

	movl	%r12d,52(%rsp)
	xorl	%edx,%r14d
	andl	%r11d,%edi

	rorl	$5,%r13d
	addl	%ecx,%r12d
	xorl	%ebx,%edi

	rorl	$11,%r14d
	xorl	%r11d,%r13d
	addl	%edi,%r12d

	movl	%edx,%edi
	addl	(%rbp),%r12d
	xorl	%edx,%r14d

	xorl	%r8d,%edi
	rorl	$6,%r13d
	movl	%r8d,%ecx

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%ecx
	addl	%r12d,%r10d
	addl	%r12d,%ecx

	leaq	4(%rbp),%rbp
	addl	%r14d,%ecx
	movl	56(%rsi),%r12d
	movl	%r10d,%r13d
	movl	%ecx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r11d,%r15d

	xorl	%r10d,%r13d
	rorl	$9,%r14d
	xorl	%eax,%r15d

	movl	%r12d,56(%rsp)
	xorl	%ecx,%r14d
	andl	%r10d,%r15d

	rorl	$5,%r13d
	addl	%ebx,%r12d
	xorl	%eax,%r15d

	rorl	$11,%r14d
	xorl	%r10d,%r13d
	addl	%r15d,%r12d

	movl	%ecx,%r15d
	addl	(%rbp),%r12d
	xorl	%ecx,%r14d

	xorl	%edx,%r15d
	rorl	$6,%r13d
	movl	%edx,%ebx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%ebx
	addl	%r12d,%r9d
	addl	%r12d,%ebx

	leaq	4(%rbp),%rbp
	addl	%r14d,%ebx
	movl	60(%rsi),%r12d
	movl	%r9d,%r13d
	movl	%ebx,%r14d
	bswapl	%r12d
	rorl	$14,%r13d
	movl	%r10d,%edi

	xorl	%r9d,%r13d
	rorl	$9,%r14d
	xorl	%r11d,%edi

	movl	%r12d,60(%rsp)
	xorl	%ebx,%r14d
	andl	%r9d,%edi

	rorl	$5,%r13d
	addl	%eax,%r12d
	xorl	%r11d,%edi

	rorl	$11,%r14d
	xorl	%r9d,%r13d
	addl	%edi,%r12d

	movl	%ebx,%edi
	addl	(%rbp),%r12d
	xorl	%ebx,%r14d

	xorl	%ecx,%edi
	rorl	$6,%r13d
	movl	%ecx,%eax

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%eax
	addl	%r12d,%r8d
	addl	%r12d,%eax

	leaq	20(%rbp),%rbp
	jmp	.Lrounds_16_xx
.align	16
.Lrounds_16_xx:
	movl	4(%rsp),%r13d
	movl	56(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%eax
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	36(%rsp),%r12d

	addl	0(%rsp),%r12d
	movl	%r8d,%r13d
	addl	%r15d,%r12d
	movl	%eax,%r14d
	rorl	$14,%r13d
	movl	%r9d,%r15d

	xorl	%r8d,%r13d
	rorl	$9,%r14d
	xorl	%r10d,%r15d

	movl	%r12d,0(%rsp)
	xorl	%eax,%r14d
	andl	%r8d,%r15d

	rorl	$5,%r13d
	addl	%r11d,%r12d
	xorl	%r10d,%r15d

	rorl	$11,%r14d
	xorl	%r8d,%r13d
	addl	%r15d,%r12d

	movl	%eax,%r15d
	addl	(%rbp),%r12d
	xorl	%eax,%r14d

	xorl	%ebx,%r15d
	rorl	$6,%r13d
	movl	%ebx,%r11d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r11d
	addl	%r12d,%edx
	addl	%r12d,%r11d

	leaq	4(%rbp),%rbp
	movl	8(%rsp),%r13d
	movl	60(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r11d
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	40(%rsp),%r12d

	addl	4(%rsp),%r12d
	movl	%edx,%r13d
	addl	%edi,%r12d
	movl	%r11d,%r14d
	rorl	$14,%r13d
	movl	%r8d,%edi

	xorl	%edx,%r13d
	rorl	$9,%r14d
	xorl	%r9d,%edi

	movl	%r12d,4(%rsp)
	xorl	%r11d,%r14d
	andl	%edx,%edi

	rorl	$5,%r13d
	addl	%r10d,%r12d
	xorl	%r9d,%edi

	rorl	$11,%r14d
	xorl	%edx,%r13d
	addl	%edi,%r12d

	movl	%r11d,%edi
	addl	(%rbp),%r12d
	xorl	%r11d,%r14d

	xorl	%eax,%edi
	rorl	$6,%r13d
	movl	%eax,%r10d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r10d
	addl	%r12d,%ecx
	addl	%r12d,%r10d

	leaq	4(%rbp),%rbp
	movl	12(%rsp),%r13d
	movl	0(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r10d
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	44(%rsp),%r12d

	addl	8(%rsp),%r12d
	movl	%ecx,%r13d
	addl	%r15d,%r12d
	movl	%r10d,%r14d
	rorl	$14,%r13d
	movl	%edx,%r15d

	xorl	%ecx,%r13d
	rorl	$9,%r14d
	xorl	%r8d,%r15d

	movl	%r12d,8(%rsp)
	xorl	%r10d,%r14d
	andl	%ecx,%r15d

	rorl	$5,%r13d
	addl	%r9d,%r12d
	xorl	%r8d,%r15d

	rorl	$11,%r14d
	xorl	%ecx,%r13d
	addl	%r15d,%r12d

	movl	%r10d,%r15d
	addl	(%rbp),%r12d
	xorl	%r10d,%r14d

	xorl	%r11d,%r15d
	rorl	$6,%r13d
	movl	%r11d,%r9d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r9d
	addl	%r12d,%ebx
	addl	%r12d,%r9d

	leaq	4(%rbp),%rbp
	movl	16(%rsp),%r13d
	movl	4(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r9d
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	48(%rsp),%r12d

	addl	12(%rsp),%r12d
	movl	%ebx,%r13d
	addl	%edi,%r12d
	movl	%r9d,%r14d
	rorl	$14,%r13d
	movl	%ecx,%edi

	xorl	%ebx,%r13d
	rorl	$9,%r14d
	xorl	%edx,%edi

	movl	%r12d,12(%rsp)
	xorl	%r9d,%r14d
	andl	%ebx,%edi

	rorl	$5,%r13d
	addl	%r8d,%r12d
	xorl	%edx,%edi

	rorl	$11,%r14d
	xorl	%ebx,%r13d
	addl	%edi,%r12d

	movl	%r9d,%edi
	addl	(%rbp),%r12d
	xorl	%r9d,%r14d

	xorl	%r10d,%edi
	rorl	$6,%r13d
	movl	%r10d,%r8d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r8d
	addl	%r12d,%eax
	addl	%r12d,%r8d

	leaq	20(%rbp),%rbp
	movl	20(%rsp),%r13d
	movl	8(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r8d
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	52(%rsp),%r12d

	addl	16(%rsp),%r12d
	movl	%eax,%r13d
	addl	%r15d,%r12d
	movl	%r8d,%r14d
	rorl	$14,%r13d
	movl	%ebx,%r15d

	xorl	%eax,%r13d
	rorl	$9,%r14d
	xorl	%ecx,%r15d

	movl	%r12d,16(%rsp)
	xorl	%r8d,%r14d
	andl	%eax,%r15d

	rorl	$5,%r13d
	addl	%edx,%r12d
	xorl	%ecx,%r15d

	rorl	$11,%r14d
	xorl	%eax,%r13d
	addl	%r15d,%r12d

	movl	%r8d,%r15d
	addl	(%rbp),%r12d
	xorl	%r8d,%r14d

	xorl	%r9d,%r15d
	rorl	$6,%r13d
	movl	%r9d,%edx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%edx
	addl	%r12d,%r11d
	addl	%r12d,%edx

	leaq	4(%rbp),%rbp
	movl	24(%rsp),%r13d
	movl	12(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%edx
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	56(%rsp),%r12d

	addl	20(%rsp),%r12d
	movl	%r11d,%r13d
	addl	%edi,%r12d
	movl	%edx,%r14d
	rorl	$14,%r13d
	movl	%eax,%edi

	xorl	%r11d,%r13d
	rorl	$9,%r14d
	xorl	%ebx,%edi

	movl	%r12d,20(%rsp)
	xorl	%edx,%r14d
	andl	%r11d,%edi

	rorl	$5,%r13d
	addl	%ecx,%r12d
	xorl	%ebx,%edi

	rorl	$11,%r14d
	xorl	%r11d,%r13d
	addl	%edi,%r12d

	movl	%edx,%edi
	addl	(%rbp),%r12d
	xorl	%edx,%r14d

	xorl	%r8d,%edi
	rorl	$6,%r13d
	movl	%r8d,%ecx

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%ecx
	addl	%r12d,%r10d
	addl	%r12d,%ecx

	leaq	4(%rbp),%rbp
	movl	28(%rsp),%r13d
	movl	16(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%ecx
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	60(%rsp),%r12d

	addl	24(%rsp),%r12d
	movl	%r10d,%r13d
	addl	%r15d,%r12d
	movl	%ecx,%r14d
	rorl	$14,%r13d
	movl	%r11d,%r15d

	xorl	%r10d,%r13d
	rorl	$9,%r14d
	xorl	%eax,%r15d

	movl	%r12d,24(%rsp)
	xorl	%ecx,%r14d
	andl	%r10d,%r15d

	rorl	$5,%r13d
	addl	%ebx,%r12d
	xorl	%eax,%r15d

	rorl	$11,%r14d
	xorl	%r10d,%r13d
	addl	%r15d,%r12d

	movl	%ecx,%r15d
	addl	(%rbp),%r12d
	xorl	%ecx,%r14d

	xorl	%edx,%r15d
	rorl	$6,%r13d
	movl	%edx,%ebx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%ebx
	addl	%r12d,%r9d
	addl	%r12d,%ebx

	leaq	4(%rbp),%rbp
	movl	32(%rsp),%r13d
	movl	20(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%ebx
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	0(%rsp),%r12d

	addl	28(%rsp),%r12d
	movl	%r9d,%r13d
	addl	%edi,%r12d
	movl	%ebx,%r14d
	rorl	$14,%r13d
	movl	%r10d,%edi

	xorl	%r9d,%r13d
	rorl	$9,%r14d
	xorl	%r11d,%edi

	movl	%r12d,28(%rsp)
	xorl	%ebx,%r14d
	andl	%r9d,%edi

	rorl	$5,%r13d
	addl	%eax,%r12d
	xorl	%r11d,%edi

	rorl	$11,%r14d
	xorl	%r9d,%r13d
	addl	%edi,%r12d

	movl	%ebx,%edi
	addl	(%rbp),%r12d
	xorl	%ebx,%r14d

	xorl	%ecx,%edi
	rorl	$6,%r13d
	movl	%ecx,%eax

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%eax
	addl	%r12d,%r8d
	addl	%r12d,%eax

	leaq	20(%rbp),%rbp
	movl	36(%rsp),%r13d
	movl	24(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%eax
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	4(%rsp),%r12d

	addl	32(%rsp),%r12d
	movl	%r8d,%r13d
	addl	%r15d,%r12d
	movl	%eax,%r14d
	rorl	$14,%r13d
	movl	%r9d,%r15d

	xorl	%r8d,%r13d
	rorl	$9,%r14d
	xorl	%r10d,%r15d

	movl	%r12d,32(%rsp)
	xorl	%eax,%r14d
	andl	%r8d,%r15d

	rorl	$5,%r13d
	addl	%r11d,%r12d
	xorl	%r10d,%r15d

	rorl	$11,%r14d
	xorl	%r8d,%r13d
	addl	%r15d,%r12d

	movl	%eax,%r15d
	addl	(%rbp),%r12d
	xorl	%eax,%r14d

	xorl	%ebx,%r15d
	rorl	$6,%r13d
	movl	%ebx,%r11d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r11d
	addl	%r12d,%edx
	addl	%r12d,%r11d

	leaq	4(%rbp),%rbp
	movl	40(%rsp),%r13d
	movl	28(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r11d
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	8(%rsp),%r12d

	addl	36(%rsp),%r12d
	movl	%edx,%r13d
	addl	%edi,%r12d
	movl	%r11d,%r14d
	rorl	$14,%r13d
	movl	%r8d,%edi

	xorl	%edx,%r13d
	rorl	$9,%r14d
	xorl	%r9d,%edi

	movl	%r12d,36(%rsp)
	xorl	%r11d,%r14d
	andl	%edx,%edi

	rorl	$5,%r13d
	addl	%r10d,%r12d
	xorl	%r9d,%edi

	rorl	$11,%r14d
	xorl	%edx,%r13d
	addl	%edi,%r12d

	movl	%r11d,%edi
	addl	(%rbp),%r12d
	xorl	%r11d,%r14d

	xorl	%eax,%edi
	rorl	$6,%r13d
	movl	%eax,%r10d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r10d
	addl	%r12d,%ecx
	addl	%r12d,%r10d

	leaq	4(%rbp),%rbp
	movl	44(%rsp),%r13d
	movl	32(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r10d
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	12(%rsp),%r12d

	addl	40(%rsp),%r12d
	movl	%ecx,%r13d
	addl	%r15d,%r12d
	movl	%r10d,%r14d
	rorl	$14,%r13d
	movl	%edx,%r15d

	xorl	%ecx,%r13d
	rorl	$9,%r14d
	xorl	%r8d,%r15d

	movl	%r12d,40(%rsp)
	xorl	%r10d,%r14d
	andl	%ecx,%r15d

	rorl	$5,%r13d
	addl	%r9d,%r12d
	xorl	%r8d,%r15d

	rorl	$11,%r14d
	xorl	%ecx,%r13d
	addl	%r15d,%r12d

	movl	%r10d,%r15d
	addl	(%rbp),%r12d
	xorl	%r10d,%r14d

	xorl	%r11d,%r15d
	rorl	$6,%r13d
	movl	%r11d,%r9d

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%r9d
	addl	%r12d,%ebx
	addl	%r12d,%r9d

	leaq	4(%rbp),%rbp
	movl	48(%rsp),%r13d
	movl	36(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r9d
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	16(%rsp),%r12d

	addl	44(%rsp),%r12d
	movl	%ebx,%r13d
	addl	%edi,%r12d
	movl	%r9d,%r14d
	rorl	$14,%r13d
	movl	%ecx,%edi

	xorl	%ebx,%r13d
	rorl	$9,%r14d
	xorl	%edx,%edi

	movl	%r12d,44(%rsp)
	xorl	%r9d,%r14d
	andl	%ebx,%edi

	rorl	$5,%r13d
	addl	%r8d,%r12d
	xorl	%edx,%edi

	rorl	$11,%r14d
	xorl	%ebx,%r13d
	addl	%edi,%r12d

	movl	%r9d,%edi
	addl	(%rbp),%r12d
	xorl	%r9d,%r14d

	xorl	%r10d,%edi
	rorl	$6,%r13d
	movl	%r10d,%r8d

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%r8d
	addl	%r12d,%eax
	addl	%r12d,%r8d

	leaq	20(%rbp),%rbp
	movl	52(%rsp),%r13d
	movl	40(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%r8d
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	20(%rsp),%r12d

	addl	48(%rsp),%r12d
	movl	%eax,%r13d
	addl	%r15d,%r12d
	movl	%r8d,%r14d
	rorl	$14,%r13d
	movl	%ebx,%r15d

	xorl	%eax,%r13d
	rorl	$9,%r14d
	xorl	%ecx,%r15d

	movl	%r12d,48(%rsp)
	xorl	%r8d,%r14d
	andl	%eax,%r15d

	rorl	$5,%r13d
	addl	%edx,%r12d
	xorl	%ecx,%r15d

	rorl	$11,%r14d
	xorl	%eax,%r13d
	addl	%r15d,%r12d

	movl	%r8d,%r15d
	addl	(%rbp),%r12d
	xorl	%r8d,%r14d

	xorl	%r9d,%r15d
	rorl	$6,%r13d
	movl	%r9d,%edx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%edx
	addl	%r12d,%r11d
	addl	%r12d,%edx

	leaq	4(%rbp),%rbp
	movl	56(%rsp),%r13d
	movl	44(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%edx
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	24(%rsp),%r12d

	addl	52(%rsp),%r12d
	movl	%r11d,%r13d
	addl	%edi,%r12d
	movl	%edx,%r14d
	rorl	$14,%r13d
	movl	%eax,%edi

	xorl	%r11d,%r13d
	rorl	$9,%r14d
	xorl	%ebx,%edi

	movl	%r12d,52(%rsp)
	xorl	%edx,%r14d
	andl	%r11d,%edi

	rorl	$5,%r13d
	addl	%ecx,%r12d
	xorl	%ebx,%edi

	rorl	$11,%r14d
	xorl	%r11d,%r13d
	addl	%edi,%r12d

	movl	%edx,%edi
	addl	(%rbp),%r12d
	xorl	%edx,%r14d

	xorl	%r8d,%edi
	rorl	$6,%r13d
	movl	%r8d,%ecx

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%ecx
	addl	%r12d,%r10d
	addl	%r12d,%ecx

	leaq	4(%rbp),%rbp
	movl	60(%rsp),%r13d
	movl	48(%rsp),%r15d

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%ecx
	movl	%r15d,%r14d
	rorl	$2,%r15d

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%r15d
	shrl	$10,%r14d

	rorl	$17,%r15d
	xorl	%r13d,%r12d
	xorl	%r14d,%r15d
	addl	28(%rsp),%r12d

	addl	56(%rsp),%r12d
	movl	%r10d,%r13d
	addl	%r15d,%r12d
	movl	%ecx,%r14d
	rorl	$14,%r13d
	movl	%r11d,%r15d

	xorl	%r10d,%r13d
	rorl	$9,%r14d
	xorl	%eax,%r15d

	movl	%r12d,56(%rsp)
	xorl	%ecx,%r14d
	andl	%r10d,%r15d

	rorl	$5,%r13d
	addl	%ebx,%r12d
	xorl	%eax,%r15d

	rorl	$11,%r14d
	xorl	%r10d,%r13d
	addl	%r15d,%r12d

	movl	%ecx,%r15d
	addl	(%rbp),%r12d
	xorl	%ecx,%r14d

	xorl	%edx,%r15d
	rorl	$6,%r13d
	movl	%edx,%ebx

	andl	%r15d,%edi
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%edi,%ebx
	addl	%r12d,%r9d
	addl	%r12d,%ebx

	leaq	4(%rbp),%rbp
	movl	0(%rsp),%r13d
	movl	52(%rsp),%edi

	movl	%r13d,%r12d
	rorl	$11,%r13d
	addl	%r14d,%ebx
	movl	%edi,%r14d
	rorl	$2,%edi

	xorl	%r12d,%r13d
	shrl	$3,%r12d
	rorl	$7,%r13d
	xorl	%r14d,%edi
	shrl	$10,%r14d

	rorl	$17,%edi
	xorl	%r13d,%r12d
	xorl	%r14d,%edi
	addl	32(%rsp),%r12d

	addl	60(%rsp),%r12d
	movl	%r9d,%r13d
	addl	%edi,%r12d
	movl	%ebx,%r14d
	rorl	$14,%r13d
	movl	%r10d,%edi

	xorl	%r9d,%r13d
	rorl	$9,%r14d
	xorl	%r11d,%edi

	movl	%r12d,60(%rsp)
	xorl	%ebx,%r14d
	andl	%r9d,%edi

	rorl	$5,%r13d
	addl	%eax,%r12d
	xorl	%r11d,%edi

	rorl	$11,%r14d
	xorl	%r9d,%r13d
	addl	%edi,%r12d

	movl	%ebx,%edi
	addl	(%rbp),%r12d
	xorl	%ebx,%r14d

	xorl	%ecx,%edi
	rorl	$6,%r13d
	movl	%ecx,%eax

	andl	%edi,%r15d
	rorl	$2,%r14d
	addl	%r13d,%r12d

	xorl	%r15d,%eax
	addl	%r12d,%r8d
	addl	%r12d,%eax

	leaq	20(%rbp),%rbp
	cmpb	$0,3(%rbp)
	jnz	.Lrounds_16_xx

	movq	64+0(%rsp),%rdi
	addl	%r14d,%eax
	leaq	64(%rsi),%rsi

	addl	0(%rdi),%eax
	addl	4(%rdi),%ebx
	addl	8(%rdi),%ecx
	addl	12(%rdi),%edx
	addl	16(%rdi),%r8d
	addl	20(%rdi),%r9d
	addl	24(%rdi),%r10d
	addl	28(%rdi),%r11d

	cmpq	64+16(%rsp),%rsi

	movl	%eax,0(%rdi)
	movl	%ebx,4(%rdi)
	movl	%ecx,8(%rdi)
	movl	%edx,12(%rdi)
	movl	%r8d,16(%rdi)
	movl	%r9d,20(%rdi)
	movl	%r10d,24(%rdi)
	movl	%r11d,28(%rdi)
	jb	.Lloop

	movq	88(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_block_data_order,.-sha256_block_data_order
.align	64
.type	K256,@object
K256:
.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
.long	0x428a2f98,0x71374491,0xb5c0fbcf,0xe9b5dba5
.long	0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5
.long	0x3956c25b,0x59f111f1,0x923f82a4,0xab1c5ed5
.long	0xd807aa98,0x12835b01,0x243185be,0x550c7dc3
.long	0xd807aa98,0x12835b01,0x243185be,0x550c7dc3
.long	0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174
.long	0x72be5d74,0x80deb1fe,0x9bdc06a7,0xc19bf174
.long	0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc
.long	0xe49b69c1,0xefbe4786,0x0fc19dc6,0x240ca1cc
.long	0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da
.long	0x2de92c6f,0x4a7484aa,0x5cb0a9dc,0x76f988da
.long	0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7
.long	0x983e5152,0xa831c66d,0xb00327c8,0xbf597fc7
.long	0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967
.long	0xc6e00bf3,0xd5a79147,0x06ca6351,0x14292967
.long	0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13
.long	0x27b70a85,0x2e1b2138,0x4d2c6dfc,0x53380d13
.long	0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85
.long	0x650a7354,0x766a0abb,0x81c2c92e,0x92722c85
.long	0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3
.long	0xa2bfe8a1,0xa81a664b,0xc24b8b70,0xc76c51a3
.long	0xd192e819,0xd6990624,0xf40e3585,0x106aa070
.long	0xd192e819,0xd6990624,0xf40e3585,0x106aa070
.long	0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5
.long	0x19a4c116,0x1e376c08,0x2748774c,0x34b0bcb5
.long	0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3
.long	0x391c0cb3,0x4ed8aa4a,0x5b9cca4f,0x682e6ff3
.long	0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208
.long	0x748f82ee,0x78a5636f,0x84c87814,0x8cc70208
.long	0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2
.long	0x90befffa,0xa4506ceb,0xbef9a3f7,0xc67178f2

.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.long	0x00010203,0x04050607,0x08090a0b,0x0c0d0e0f
.long	0x03020100,0x0b0a0908,0xffffffff,0xffffffff
.long	0x03020100,0x0b0a0908,0xffffffff,0xffffffff
.long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
.long	0xffffffff,0xffffffff,0x03020100,0x0b0a0908
.byte	83,72,65,50,53,54,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.type	sha256_block_data_order_shaext,@function
.align	64
sha256_block_data_order_shaext:
_shaext_shortcut:
.cfi_startproc	
	leaq	K256+128(%rip),%rcx
	movdqu	(%rdi),%xmm1
	movdqu	16(%rdi),%xmm2
	movdqa	512-128(%rcx),%xmm7

	pshufd	$0x1b,%xmm1,%xmm0
	pshufd	$0xb1,%xmm1,%xmm1
	pshufd	$0x1b,%xmm2,%xmm2
	movdqa	%xmm7,%xmm8
.byte	102,15,58,15,202,8
	punpcklqdq	%xmm0,%xmm2
	jmp	.Loop_shaext

.align	16
.Loop_shaext:
	movdqu	(%rsi),%xmm3
	movdqu	16(%rsi),%xmm4
	movdqu	32(%rsi),%xmm5
.byte	102,15,56,0,223
	movdqu	48(%rsi),%xmm6

	movdqa	0-128(%rcx),%xmm0
	paddd	%xmm3,%xmm0
.byte	102,15,56,0,231
	movdqa	%xmm2,%xmm10
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	nop
	movdqa	%xmm1,%xmm9
.byte	15,56,203,202

	movdqa	32-128(%rcx),%xmm0
	paddd	%xmm4,%xmm0
.byte	102,15,56,0,239
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	leaq	64(%rsi),%rsi
.byte	15,56,204,220
.byte	15,56,203,202

	movdqa	64-128(%rcx),%xmm0
	paddd	%xmm5,%xmm0
.byte	102,15,56,0,247
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
.byte	102,15,58,15,253,4
	nop
	paddd	%xmm7,%xmm3
.byte	15,56,204,229
.byte	15,56,203,202

	movdqa	96-128(%rcx),%xmm0
	paddd	%xmm6,%xmm0
.byte	15,56,205,222
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
.byte	102,15,58,15,254,4
	nop
	paddd	%xmm7,%xmm4
.byte	15,56,204,238
.byte	15,56,203,202
	movdqa	128-128(%rcx),%xmm0
	paddd	%xmm3,%xmm0
.byte	15,56,205,227
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
.byte	102,15,58,15,251,4
	nop
	paddd	%xmm7,%xmm5
.byte	15,56,204,243
.byte	15,56,203,202
	movdqa	160-128(%rcx),%xmm0
	paddd	%xmm4,%xmm0
.byte	15,56,205,236
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
.byte	102,15,58,15,252,4
	nop
	paddd	%xmm7,%xmm6
.byte	15,56,204,220
.byte	15,56,203,202
	movdqa	192-128(%rcx),%xmm0
	paddd	%xmm5,%xmm0
.byte	15,56,205,245
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
.byte	102,15,58,15,253,4
	nop
	paddd	%xmm7,%xmm3
.byte	15,56,204,229
.byte	15,56,203,202
	movdqa	224-128(%rcx),%xmm0
	paddd	%xmm6,%xmm0
.byte	15,56,205,222
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
.byte	102,15,58,15,254,4
	nop
	paddd	%xmm7,%xmm4
.byte	15,56,204,238
.byte	15,56,203,202
	movdqa	256-128(%rcx),%xmm0
	paddd	%xmm3,%xmm0
.byte	15,56,205,227
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
.byte	102,15,58,15,251,4
	nop
	paddd	%xmm7,%xmm5
.byte	15,56,204,243
.byte	15,56,203,202
	movdqa	288-128(%rcx),%xmm0
	paddd	%xmm4,%xmm0
.byte	15,56,205,236
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
.byte	102,15,58,15,252,4
	nop
	paddd	%xmm7,%xmm6
.byte	15,56,204,220
.byte	15,56,203,202
	movdqa	320-128(%rcx),%xmm0
	paddd	%xmm5,%xmm0
.byte	15,56,205,245
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm6,%xmm7
.byte	102,15,58,15,253,4
	nop
	paddd	%xmm7,%xmm3
.byte	15,56,204,229
.byte	15,56,203,202
	movdqa	352-128(%rcx),%xmm0
	paddd	%xmm6,%xmm0
.byte	15,56,205,222
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm3,%xmm7
.byte	102,15,58,15,254,4
	nop
	paddd	%xmm7,%xmm4
.byte	15,56,204,238
.byte	15,56,203,202
	movdqa	384-128(%rcx),%xmm0
	paddd	%xmm3,%xmm0
.byte	15,56,205,227
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm4,%xmm7
.byte	102,15,58,15,251,4
	nop
	paddd	%xmm7,%xmm5
.byte	15,56,204,243
.byte	15,56,203,202
	movdqa	416-128(%rcx),%xmm0
	paddd	%xmm4,%xmm0
.byte	15,56,205,236
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	movdqa	%xmm5,%xmm7
.byte	102,15,58,15,252,4
.byte	15,56,203,202
	paddd	%xmm7,%xmm6

	movdqa	448-128(%rcx),%xmm0
	paddd	%xmm5,%xmm0
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
.byte	15,56,205,245
	movdqa	%xmm8,%xmm7
.byte	15,56,203,202

	movdqa	480-128(%rcx),%xmm0
	paddd	%xmm6,%xmm0
	nop
.byte	15,56,203,209
	pshufd	$0x0e,%xmm0,%xmm0
	decq	%rdx
	nop
.byte	15,56,203,202

	paddd	%xmm10,%xmm2
	paddd	%xmm9,%xmm1
	jnz	.Loop_shaext

	pshufd	$0xb1,%xmm2,%xmm2
	pshufd	$0x1b,%xmm1,%xmm7
	pshufd	$0xb1,%xmm1,%xmm1
	punpckhqdq	%xmm2,%xmm1
.byte	102,15,58,15,215,8

	movdqu	%xmm1,(%rdi)
	movdqu	%xmm2,16(%rdi)
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_block_data_order_shaext,.-sha256_block_data_order_shaext
.type	sha256_block_data_order_ssse3,@function
.align	64
sha256_block_data_order_ssse3:
.cfi_startproc	
.Lssse3_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$96,%rsp
	leaq	(%rsi,%rdx,4),%rdx
	andq	$-64,%rsp
	movq	%rdi,64+0(%rsp)
	movq	%rsi,64+8(%rsp)
	movq	%rdx,64+16(%rsp)
	movq	%rax,88(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
.Lprologue_ssse3:

	movl	0(%rdi),%eax
	movl	4(%rdi),%ebx
	movl	8(%rdi),%ecx
	movl	12(%rdi),%edx
	movl	16(%rdi),%r8d
	movl	20(%rdi),%r9d
	movl	24(%rdi),%r10d
	movl	28(%rdi),%r11d


	jmp	.Lloop_ssse3
.align	16
.Lloop_ssse3:
	movdqa	K256+512(%rip),%xmm7
	movdqu	0(%rsi),%xmm0
	movdqu	16(%rsi),%xmm1
	movdqu	32(%rsi),%xmm2
.byte	102,15,56,0,199
	movdqu	48(%rsi),%xmm3
	leaq	K256(%rip),%rbp
.byte	102,15,56,0,207
	movdqa	0(%rbp),%xmm4
	movdqa	32(%rbp),%xmm5
.byte	102,15,56,0,215
	paddd	%xmm0,%xmm4
	movdqa	64(%rbp),%xmm6
.byte	102,15,56,0,223
	movdqa	96(%rbp),%xmm7
	paddd	%xmm1,%xmm5
	paddd	%xmm2,%xmm6
	paddd	%xmm3,%xmm7
	movdqa	%xmm4,0(%rsp)
	movl	%eax,%r14d
	movdqa	%xmm5,16(%rsp)
	movl	%ebx,%edi
	movdqa	%xmm6,32(%rsp)
	xorl	%ecx,%edi
	movdqa	%xmm7,48(%rsp)
	movl	%r8d,%r13d
	jmp	.Lssse3_00_47

.align	16
.Lssse3_00_47:
	subq	$-128,%rbp
	rorl	$14,%r13d
	movdqa	%xmm1,%xmm4
	movl	%r14d,%eax
	movl	%r9d,%r12d
	movdqa	%xmm3,%xmm7
	rorl	$9,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	rorl	$5,%r13d
	xorl	%eax,%r14d
.byte	102,15,58,15,224,4
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
.byte	102,15,58,15,250,4
	addl	0(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	rorl	$11,%r14d
	movdqa	%xmm4,%xmm5
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	movdqa	%xmm4,%xmm6
	rorl	$6,%r13d
	andl	%r15d,%edi
	psrld	$3,%xmm4
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	paddd	%xmm7,%xmm0
	rorl	$2,%r14d
	addl	%r11d,%edx
	psrld	$7,%xmm6
	addl	%edi,%r11d
	movl	%edx,%r13d
	pshufd	$250,%xmm3,%xmm7
	addl	%r11d,%r14d
	rorl	$14,%r13d
	pslld	$14,%xmm5
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	pxor	%xmm6,%xmm4
	rorl	$9,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	rorl	$5,%r13d
	psrld	$11,%xmm6
	xorl	%r11d,%r14d
	pxor	%xmm5,%xmm4
	andl	%edx,%r12d
	xorl	%edx,%r13d
	pslld	$11,%xmm5
	addl	4(%rsp),%r10d
	movl	%r11d,%edi
	pxor	%xmm6,%xmm4
	xorl	%r9d,%r12d
	rorl	$11,%r14d
	movdqa	%xmm7,%xmm6
	xorl	%eax,%edi
	addl	%r12d,%r10d
	pxor	%xmm5,%xmm4
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	psrld	$10,%xmm7
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	paddd	%xmm4,%xmm0
	rorl	$2,%r14d
	addl	%r10d,%ecx
	psrlq	$17,%xmm6
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	pxor	%xmm6,%xmm7
	rorl	$14,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	rorl	$9,%r14d
	psrlq	$2,%xmm6
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	pxor	%xmm6,%xmm7
	rorl	$5,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	pshufd	$128,%xmm7,%xmm7
	xorl	%ecx,%r13d
	addl	8(%rsp),%r9d
	movl	%r10d,%r15d
	psrldq	$8,%xmm7
	xorl	%r8d,%r12d
	rorl	$11,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	rorl	$6,%r13d
	paddd	%xmm7,%xmm0
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	pshufd	$80,%xmm0,%xmm7
	xorl	%r11d,%edi
	rorl	$2,%r14d
	addl	%r9d,%ebx
	movdqa	%xmm7,%xmm6
	addl	%edi,%r9d
	movl	%ebx,%r13d
	psrld	$10,%xmm7
	addl	%r9d,%r14d
	rorl	$14,%r13d
	psrlq	$17,%xmm6
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	pxor	%xmm6,%xmm7
	rorl	$9,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	rorl	$5,%r13d
	xorl	%r9d,%r14d
	psrlq	$2,%xmm6
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	12(%rsp),%r8d
	pxor	%xmm6,%xmm7
	movl	%r9d,%edi
	xorl	%edx,%r12d
	rorl	$11,%r14d
	pshufd	$8,%xmm7,%xmm7
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	movdqa	0(%rbp),%xmm6
	rorl	$6,%r13d
	andl	%edi,%r15d
	pslldq	$8,%xmm7
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	paddd	%xmm7,%xmm0
	rorl	$2,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	paddd	%xmm0,%xmm6
	movl	%eax,%r13d
	addl	%r8d,%r14d
	movdqa	%xmm6,0(%rsp)
	rorl	$14,%r13d
	movdqa	%xmm2,%xmm4
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	movdqa	%xmm0,%xmm7
	rorl	$9,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	rorl	$5,%r13d
	xorl	%r8d,%r14d
.byte	102,15,58,15,225,4
	andl	%eax,%r12d
	xorl	%eax,%r13d
.byte	102,15,58,15,251,4
	addl	16(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	rorl	$11,%r14d
	movdqa	%xmm4,%xmm5
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	movdqa	%xmm4,%xmm6
	rorl	$6,%r13d
	andl	%r15d,%edi
	psrld	$3,%xmm4
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	paddd	%xmm7,%xmm1
	rorl	$2,%r14d
	addl	%edx,%r11d
	psrld	$7,%xmm6
	addl	%edi,%edx
	movl	%r11d,%r13d
	pshufd	$250,%xmm0,%xmm7
	addl	%edx,%r14d
	rorl	$14,%r13d
	pslld	$14,%xmm5
	movl	%r14d,%edx
	movl	%eax,%r12d
	pxor	%xmm6,%xmm4
	rorl	$9,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	rorl	$5,%r13d
	psrld	$11,%xmm6
	xorl	%edx,%r14d
	pxor	%xmm5,%xmm4
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	pslld	$11,%xmm5
	addl	20(%rsp),%ecx
	movl	%edx,%edi
	pxor	%xmm6,%xmm4
	xorl	%ebx,%r12d
	rorl	$11,%r14d
	movdqa	%xmm7,%xmm6
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	pxor	%xmm5,%xmm4
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	psrld	$10,%xmm7
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	paddd	%xmm4,%xmm1
	rorl	$2,%r14d
	addl	%ecx,%r10d
	psrlq	$17,%xmm6
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	pxor	%xmm6,%xmm7
	rorl	$14,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	rorl	$9,%r14d
	psrlq	$2,%xmm6
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	pxor	%xmm6,%xmm7
	rorl	$5,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	pshufd	$128,%xmm7,%xmm7
	xorl	%r10d,%r13d
	addl	24(%rsp),%ebx
	movl	%ecx,%r15d
	psrldq	$8,%xmm7
	xorl	%eax,%r12d
	rorl	$11,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	rorl	$6,%r13d
	paddd	%xmm7,%xmm1
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	pshufd	$80,%xmm1,%xmm7
	xorl	%edx,%edi
	rorl	$2,%r14d
	addl	%ebx,%r9d
	movdqa	%xmm7,%xmm6
	addl	%edi,%ebx
	movl	%r9d,%r13d
	psrld	$10,%xmm7
	addl	%ebx,%r14d
	rorl	$14,%r13d
	psrlq	$17,%xmm6
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	pxor	%xmm6,%xmm7
	rorl	$9,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	rorl	$5,%r13d
	xorl	%ebx,%r14d
	psrlq	$2,%xmm6
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	28(%rsp),%eax
	pxor	%xmm6,%xmm7
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	rorl	$11,%r14d
	pshufd	$8,%xmm7,%xmm7
	xorl	%ecx,%edi
	addl	%r12d,%eax
	movdqa	32(%rbp),%xmm6
	rorl	$6,%r13d
	andl	%edi,%r15d
	pslldq	$8,%xmm7
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	paddd	%xmm7,%xmm1
	rorl	$2,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	paddd	%xmm1,%xmm6
	movl	%r8d,%r13d
	addl	%eax,%r14d
	movdqa	%xmm6,16(%rsp)
	rorl	$14,%r13d
	movdqa	%xmm3,%xmm4
	movl	%r14d,%eax
	movl	%r9d,%r12d
	movdqa	%xmm1,%xmm7
	rorl	$9,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	rorl	$5,%r13d
	xorl	%eax,%r14d
.byte	102,15,58,15,226,4
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
.byte	102,15,58,15,248,4
	addl	32(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	rorl	$11,%r14d
	movdqa	%xmm4,%xmm5
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	movdqa	%xmm4,%xmm6
	rorl	$6,%r13d
	andl	%r15d,%edi
	psrld	$3,%xmm4
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	paddd	%xmm7,%xmm2
	rorl	$2,%r14d
	addl	%r11d,%edx
	psrld	$7,%xmm6
	addl	%edi,%r11d
	movl	%edx,%r13d
	pshufd	$250,%xmm1,%xmm7
	addl	%r11d,%r14d
	rorl	$14,%r13d
	pslld	$14,%xmm5
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	pxor	%xmm6,%xmm4
	rorl	$9,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	rorl	$5,%r13d
	psrld	$11,%xmm6
	xorl	%r11d,%r14d
	pxor	%xmm5,%xmm4
	andl	%edx,%r12d
	xorl	%edx,%r13d
	pslld	$11,%xmm5
	addl	36(%rsp),%r10d
	movl	%r11d,%edi
	pxor	%xmm6,%xmm4
	xorl	%r9d,%r12d
	rorl	$11,%r14d
	movdqa	%xmm7,%xmm6
	xorl	%eax,%edi
	addl	%r12d,%r10d
	pxor	%xmm5,%xmm4
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	psrld	$10,%xmm7
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	paddd	%xmm4,%xmm2
	rorl	$2,%r14d
	addl	%r10d,%ecx
	psrlq	$17,%xmm6
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	pxor	%xmm6,%xmm7
	rorl	$14,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	rorl	$9,%r14d
	psrlq	$2,%xmm6
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	pxor	%xmm6,%xmm7
	rorl	$5,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	pshufd	$128,%xmm7,%xmm7
	xorl	%ecx,%r13d
	addl	40(%rsp),%r9d
	movl	%r10d,%r15d
	psrldq	$8,%xmm7
	xorl	%r8d,%r12d
	rorl	$11,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	rorl	$6,%r13d
	paddd	%xmm7,%xmm2
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	pshufd	$80,%xmm2,%xmm7
	xorl	%r11d,%edi
	rorl	$2,%r14d
	addl	%r9d,%ebx
	movdqa	%xmm7,%xmm6
	addl	%edi,%r9d
	movl	%ebx,%r13d
	psrld	$10,%xmm7
	addl	%r9d,%r14d
	rorl	$14,%r13d
	psrlq	$17,%xmm6
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	pxor	%xmm6,%xmm7
	rorl	$9,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	rorl	$5,%r13d
	xorl	%r9d,%r14d
	psrlq	$2,%xmm6
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	44(%rsp),%r8d
	pxor	%xmm6,%xmm7
	movl	%r9d,%edi
	xorl	%edx,%r12d
	rorl	$11,%r14d
	pshufd	$8,%xmm7,%xmm7
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	movdqa	64(%rbp),%xmm6
	rorl	$6,%r13d
	andl	%edi,%r15d
	pslldq	$8,%xmm7
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	paddd	%xmm7,%xmm2
	rorl	$2,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	paddd	%xmm2,%xmm6
	movl	%eax,%r13d
	addl	%r8d,%r14d
	movdqa	%xmm6,32(%rsp)
	rorl	$14,%r13d
	movdqa	%xmm0,%xmm4
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	movdqa	%xmm2,%xmm7
	rorl	$9,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	rorl	$5,%r13d
	xorl	%r8d,%r14d
.byte	102,15,58,15,227,4
	andl	%eax,%r12d
	xorl	%eax,%r13d
.byte	102,15,58,15,249,4
	addl	48(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	rorl	$11,%r14d
	movdqa	%xmm4,%xmm5
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	movdqa	%xmm4,%xmm6
	rorl	$6,%r13d
	andl	%r15d,%edi
	psrld	$3,%xmm4
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	paddd	%xmm7,%xmm3
	rorl	$2,%r14d
	addl	%edx,%r11d
	psrld	$7,%xmm6
	addl	%edi,%edx
	movl	%r11d,%r13d
	pshufd	$250,%xmm2,%xmm7
	addl	%edx,%r14d
	rorl	$14,%r13d
	pslld	$14,%xmm5
	movl	%r14d,%edx
	movl	%eax,%r12d
	pxor	%xmm6,%xmm4
	rorl	$9,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	rorl	$5,%r13d
	psrld	$11,%xmm6
	xorl	%edx,%r14d
	pxor	%xmm5,%xmm4
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	pslld	$11,%xmm5
	addl	52(%rsp),%ecx
	movl	%edx,%edi
	pxor	%xmm6,%xmm4
	xorl	%ebx,%r12d
	rorl	$11,%r14d
	movdqa	%xmm7,%xmm6
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	pxor	%xmm5,%xmm4
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	psrld	$10,%xmm7
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	paddd	%xmm4,%xmm3
	rorl	$2,%r14d
	addl	%ecx,%r10d
	psrlq	$17,%xmm6
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	pxor	%xmm6,%xmm7
	rorl	$14,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	rorl	$9,%r14d
	psrlq	$2,%xmm6
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	pxor	%xmm6,%xmm7
	rorl	$5,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	pshufd	$128,%xmm7,%xmm7
	xorl	%r10d,%r13d
	addl	56(%rsp),%ebx
	movl	%ecx,%r15d
	psrldq	$8,%xmm7
	xorl	%eax,%r12d
	rorl	$11,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	rorl	$6,%r13d
	paddd	%xmm7,%xmm3
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	pshufd	$80,%xmm3,%xmm7
	xorl	%edx,%edi
	rorl	$2,%r14d
	addl	%ebx,%r9d
	movdqa	%xmm7,%xmm6
	addl	%edi,%ebx
	movl	%r9d,%r13d
	psrld	$10,%xmm7
	addl	%ebx,%r14d
	rorl	$14,%r13d
	psrlq	$17,%xmm6
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	pxor	%xmm6,%xmm7
	rorl	$9,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	rorl	$5,%r13d
	xorl	%ebx,%r14d
	psrlq	$2,%xmm6
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	60(%rsp),%eax
	pxor	%xmm6,%xmm7
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	rorl	$11,%r14d
	pshufd	$8,%xmm7,%xmm7
	xorl	%ecx,%edi
	addl	%r12d,%eax
	movdqa	96(%rbp),%xmm6
	rorl	$6,%r13d
	andl	%edi,%r15d
	pslldq	$8,%xmm7
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	paddd	%xmm7,%xmm3
	rorl	$2,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	paddd	%xmm3,%xmm6
	movl	%r8d,%r13d
	addl	%eax,%r14d
	movdqa	%xmm6,48(%rsp)
	cmpb	$0,131(%rbp)
	jne	.Lssse3_00_47
	rorl	$14,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	rorl	$9,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	rorl	$5,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
	addl	0(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	rorl	$11,%r14d
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	rorl	$2,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	movl	%edx,%r13d
	addl	%r11d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	rorl	$9,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	rorl	$5,%r13d
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	addl	4(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	rorl	$11,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	rorl	$2,%r14d
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	rorl	$9,%r14d
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	rorl	$5,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	8(%rsp),%r9d
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	rorl	$11,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	xorl	%r11d,%edi
	rorl	$2,%r14d
	addl	%r9d,%ebx
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	rorl	$9,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	rorl	$5,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	12(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	rorl	$11,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	rorl	$2,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	rorl	$9,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	rorl	$5,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	xorl	%eax,%r13d
	addl	16(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	rorl	$11,%r14d
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	rorl	$2,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	movl	%r11d,%r13d
	addl	%edx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%edx
	movl	%eax,%r12d
	rorl	$9,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	rorl	$5,%r13d
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	addl	20(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	rorl	$11,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	rorl	$2,%r14d
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	rorl	$9,%r14d
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	rorl	$5,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	24(%rsp),%ebx
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	rorl	$11,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	xorl	%edx,%edi
	rorl	$2,%r14d
	addl	%ebx,%r9d
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	rorl	$9,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	rorl	$5,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	28(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	rorl	$11,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	rorl	$2,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	rorl	$14,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	rorl	$9,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	rorl	$5,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
	addl	32(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	rorl	$11,%r14d
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	rorl	$2,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	movl	%edx,%r13d
	addl	%r11d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	rorl	$9,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	rorl	$5,%r13d
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	addl	36(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	rorl	$11,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	rorl	$2,%r14d
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	rorl	$9,%r14d
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	rorl	$5,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	40(%rsp),%r9d
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	rorl	$11,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	xorl	%r11d,%edi
	rorl	$2,%r14d
	addl	%r9d,%ebx
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	rorl	$9,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	rorl	$5,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	44(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	rorl	$11,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	rorl	$2,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	rorl	$14,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	rorl	$9,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	rorl	$5,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	xorl	%eax,%r13d
	addl	48(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	rorl	$11,%r14d
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	rorl	$2,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	movl	%r11d,%r13d
	addl	%edx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%edx
	movl	%eax,%r12d
	rorl	$9,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	rorl	$5,%r13d
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	addl	52(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	rorl	$11,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	rorl	$2,%r14d
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	rorl	$9,%r14d
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	rorl	$5,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	56(%rsp),%ebx
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	rorl	$11,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	rorl	$6,%r13d
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	xorl	%edx,%edi
	rorl	$2,%r14d
	addl	%ebx,%r9d
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	rorl	$14,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	rorl	$9,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	rorl	$5,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	60(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	rorl	$11,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	rorl	$6,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	rorl	$2,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	movq	64+0(%rsp),%rdi
	movl	%r14d,%eax

	addl	0(%rdi),%eax
	leaq	64(%rsi),%rsi
	addl	4(%rdi),%ebx
	addl	8(%rdi),%ecx
	addl	12(%rdi),%edx
	addl	16(%rdi),%r8d
	addl	20(%rdi),%r9d
	addl	24(%rdi),%r10d
	addl	28(%rdi),%r11d

	cmpq	64+16(%rsp),%rsi

	movl	%eax,0(%rdi)
	movl	%ebx,4(%rdi)
	movl	%ecx,8(%rdi)
	movl	%edx,12(%rdi)
	movl	%r8d,16(%rdi)
	movl	%r9d,20(%rdi)
	movl	%r10d,24(%rdi)
	movl	%r11d,28(%rdi)
	jb	.Lloop_ssse3

	movq	88(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_ssse3:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_block_data_order_ssse3,.-sha256_block_data_order_ssse3
.type	sha256_block_data_order_avx,@function
.align	64
sha256_block_data_order_avx:
.cfi_startproc	
.Lavx_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$96,%rsp
	leaq	(%rsi,%rdx,4),%rdx
	andq	$-64,%rsp
	movq	%rdi,64+0(%rsp)
	movq	%rsi,64+8(%rsp)
	movq	%rdx,64+16(%rsp)
	movq	%rax,88(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
.Lprologue_avx:

	vzeroupper
	movl	0(%rdi),%eax
	movl	4(%rdi),%ebx
	movl	8(%rdi),%ecx
	movl	12(%rdi),%edx
	movl	16(%rdi),%r8d
	movl	20(%rdi),%r9d
	movl	24(%rdi),%r10d
	movl	28(%rdi),%r11d
	vmovdqa	K256+512+32(%rip),%xmm8
	vmovdqa	K256+512+64(%rip),%xmm9
	jmp	.Lloop_avx
.align	16
.Lloop_avx:
	vmovdqa	K256+512(%rip),%xmm7
	vmovdqu	0(%rsi),%xmm0
	vmovdqu	16(%rsi),%xmm1
	vmovdqu	32(%rsi),%xmm2
	vmovdqu	48(%rsi),%xmm3
	vpshufb	%xmm7,%xmm0,%xmm0
	leaq	K256(%rip),%rbp
	vpshufb	%xmm7,%xmm1,%xmm1
	vpshufb	%xmm7,%xmm2,%xmm2
	vpaddd	0(%rbp),%xmm0,%xmm4
	vpshufb	%xmm7,%xmm3,%xmm3
	vpaddd	32(%rbp),%xmm1,%xmm5
	vpaddd	64(%rbp),%xmm2,%xmm6
	vpaddd	96(%rbp),%xmm3,%xmm7
	vmovdqa	%xmm4,0(%rsp)
	movl	%eax,%r14d
	vmovdqa	%xmm5,16(%rsp)
	movl	%ebx,%edi
	vmovdqa	%xmm6,32(%rsp)
	xorl	%ecx,%edi
	vmovdqa	%xmm7,48(%rsp)
	movl	%r8d,%r13d
	jmp	.Lavx_00_47

.align	16
.Lavx_00_47:
	subq	$-128,%rbp
	vpalignr	$4,%xmm0,%xmm1,%xmm4
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	vpalignr	$4,%xmm2,%xmm3,%xmm7
	shrdl	$9,%r14d,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	vpsrld	$7,%xmm4,%xmm6
	shrdl	$5,%r13d,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	vpaddd	%xmm7,%xmm0,%xmm0
	xorl	%r8d,%r13d
	addl	0(%rsp),%r11d
	movl	%eax,%r15d
	vpsrld	$3,%xmm4,%xmm7
	xorl	%r10d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ebx,%r15d
	vpslld	$14,%xmm4,%xmm5
	addl	%r12d,%r11d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	vpxor	%xmm6,%xmm7,%xmm4
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	vpshufd	$250,%xmm3,%xmm7
	shrdl	$2,%r14d,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	vpsrld	$11,%xmm6,%xmm6
	movl	%edx,%r13d
	addl	%r11d,%r14d
	shrdl	$14,%r13d,%r13d
	vpxor	%xmm5,%xmm4,%xmm4
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	shrdl	$9,%r14d,%r14d
	vpslld	$11,%xmm5,%xmm5
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	shrdl	$5,%r13d,%r13d
	vpxor	%xmm6,%xmm4,%xmm4
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	vpsrld	$10,%xmm7,%xmm6
	addl	4(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	vpxor	%xmm5,%xmm4,%xmm4
	shrdl	$11,%r14d,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	vpsrlq	$17,%xmm7,%xmm7
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	vpaddd	%xmm4,%xmm0,%xmm0
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	shrdl	$2,%r14d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%r10d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r10d
	vpxor	%xmm7,%xmm6,%xmm6
	movl	%edx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ecx,%r13d
	vpshufb	%xmm8,%xmm6,%xmm6
	xorl	%r8d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r10d,%r14d
	vpaddd	%xmm6,%xmm0,%xmm0
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	8(%rsp),%r9d
	vpshufd	$80,%xmm0,%xmm7
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	shrdl	$11,%r14d,%r14d
	vpsrld	$10,%xmm7,%xmm6
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	shrdl	$6,%r13d,%r13d
	vpsrlq	$17,%xmm7,%xmm7
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	vpxor	%xmm7,%xmm6,%xmm6
	xorl	%r11d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r9d,%ebx
	vpsrlq	$2,%xmm7,%xmm7
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	vpshufb	%xmm9,%xmm6,%xmm6
	shrdl	$9,%r14d,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	vpaddd	%xmm6,%xmm0,%xmm0
	shrdl	$5,%r13d,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	vpaddd	0(%rbp),%xmm0,%xmm6
	xorl	%ebx,%r13d
	addl	12(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	vmovdqa	%xmm6,0(%rsp)
	vpalignr	$4,%xmm1,%xmm2,%xmm4
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	vpalignr	$4,%xmm3,%xmm0,%xmm7
	shrdl	$9,%r14d,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	vpsrld	$7,%xmm4,%xmm6
	shrdl	$5,%r13d,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	vpaddd	%xmm7,%xmm1,%xmm1
	xorl	%eax,%r13d
	addl	16(%rsp),%edx
	movl	%r8d,%r15d
	vpsrld	$3,%xmm4,%xmm7
	xorl	%ecx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r9d,%r15d
	vpslld	$14,%xmm4,%xmm5
	addl	%r12d,%edx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	vpxor	%xmm6,%xmm7,%xmm4
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	vpshufd	$250,%xmm0,%xmm7
	shrdl	$2,%r14d,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	vpsrld	$11,%xmm6,%xmm6
	movl	%r11d,%r13d
	addl	%edx,%r14d
	shrdl	$14,%r13d,%r13d
	vpxor	%xmm5,%xmm4,%xmm4
	movl	%r14d,%edx
	movl	%eax,%r12d
	shrdl	$9,%r14d,%r14d
	vpslld	$11,%xmm5,%xmm5
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	shrdl	$5,%r13d,%r13d
	vpxor	%xmm6,%xmm4,%xmm4
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	vpsrld	$10,%xmm7,%xmm6
	addl	20(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	vpxor	%xmm5,%xmm4,%xmm4
	shrdl	$11,%r14d,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	vpsrlq	$17,%xmm7,%xmm7
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	vpaddd	%xmm4,%xmm1,%xmm1
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	shrdl	$2,%r14d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%ecx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ecx
	vpxor	%xmm7,%xmm6,%xmm6
	movl	%r11d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r10d,%r13d
	vpshufb	%xmm8,%xmm6,%xmm6
	xorl	%eax,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ecx,%r14d
	vpaddd	%xmm6,%xmm1,%xmm1
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	24(%rsp),%ebx
	vpshufd	$80,%xmm1,%xmm7
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	shrdl	$11,%r14d,%r14d
	vpsrld	$10,%xmm7,%xmm6
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	shrdl	$6,%r13d,%r13d
	vpsrlq	$17,%xmm7,%xmm7
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	vpxor	%xmm7,%xmm6,%xmm6
	xorl	%edx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%ebx,%r9d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	vpshufb	%xmm9,%xmm6,%xmm6
	shrdl	$9,%r14d,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	vpaddd	%xmm6,%xmm1,%xmm1
	shrdl	$5,%r13d,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	vpaddd	32(%rbp),%xmm1,%xmm6
	xorl	%r9d,%r13d
	addl	28(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	vmovdqa	%xmm6,16(%rsp)
	vpalignr	$4,%xmm2,%xmm3,%xmm4
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	vpalignr	$4,%xmm0,%xmm1,%xmm7
	shrdl	$9,%r14d,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	vpsrld	$7,%xmm4,%xmm6
	shrdl	$5,%r13d,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	vpaddd	%xmm7,%xmm2,%xmm2
	xorl	%r8d,%r13d
	addl	32(%rsp),%r11d
	movl	%eax,%r15d
	vpsrld	$3,%xmm4,%xmm7
	xorl	%r10d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ebx,%r15d
	vpslld	$14,%xmm4,%xmm5
	addl	%r12d,%r11d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	vpxor	%xmm6,%xmm7,%xmm4
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	vpshufd	$250,%xmm1,%xmm7
	shrdl	$2,%r14d,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	vpsrld	$11,%xmm6,%xmm6
	movl	%edx,%r13d
	addl	%r11d,%r14d
	shrdl	$14,%r13d,%r13d
	vpxor	%xmm5,%xmm4,%xmm4
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	shrdl	$9,%r14d,%r14d
	vpslld	$11,%xmm5,%xmm5
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	shrdl	$5,%r13d,%r13d
	vpxor	%xmm6,%xmm4,%xmm4
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	vpsrld	$10,%xmm7,%xmm6
	addl	36(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	vpxor	%xmm5,%xmm4,%xmm4
	shrdl	$11,%r14d,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	vpsrlq	$17,%xmm7,%xmm7
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	vpaddd	%xmm4,%xmm2,%xmm2
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	shrdl	$2,%r14d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%r10d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r10d
	vpxor	%xmm7,%xmm6,%xmm6
	movl	%edx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ecx,%r13d
	vpshufb	%xmm8,%xmm6,%xmm6
	xorl	%r8d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r10d,%r14d
	vpaddd	%xmm6,%xmm2,%xmm2
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	40(%rsp),%r9d
	vpshufd	$80,%xmm2,%xmm7
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	shrdl	$11,%r14d,%r14d
	vpsrld	$10,%xmm7,%xmm6
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	shrdl	$6,%r13d,%r13d
	vpsrlq	$17,%xmm7,%xmm7
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	vpxor	%xmm7,%xmm6,%xmm6
	xorl	%r11d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r9d,%ebx
	vpsrlq	$2,%xmm7,%xmm7
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	vpshufb	%xmm9,%xmm6,%xmm6
	shrdl	$9,%r14d,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	vpaddd	%xmm6,%xmm2,%xmm2
	shrdl	$5,%r13d,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	vpaddd	64(%rbp),%xmm2,%xmm6
	xorl	%ebx,%r13d
	addl	44(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	vmovdqa	%xmm6,32(%rsp)
	vpalignr	$4,%xmm3,%xmm0,%xmm4
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	vpalignr	$4,%xmm1,%xmm2,%xmm7
	shrdl	$9,%r14d,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	vpsrld	$7,%xmm4,%xmm6
	shrdl	$5,%r13d,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	vpaddd	%xmm7,%xmm3,%xmm3
	xorl	%eax,%r13d
	addl	48(%rsp),%edx
	movl	%r8d,%r15d
	vpsrld	$3,%xmm4,%xmm7
	xorl	%ecx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r9d,%r15d
	vpslld	$14,%xmm4,%xmm5
	addl	%r12d,%edx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	vpxor	%xmm6,%xmm7,%xmm4
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	vpshufd	$250,%xmm2,%xmm7
	shrdl	$2,%r14d,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	vpsrld	$11,%xmm6,%xmm6
	movl	%r11d,%r13d
	addl	%edx,%r14d
	shrdl	$14,%r13d,%r13d
	vpxor	%xmm5,%xmm4,%xmm4
	movl	%r14d,%edx
	movl	%eax,%r12d
	shrdl	$9,%r14d,%r14d
	vpslld	$11,%xmm5,%xmm5
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	shrdl	$5,%r13d,%r13d
	vpxor	%xmm6,%xmm4,%xmm4
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	vpsrld	$10,%xmm7,%xmm6
	addl	52(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	vpxor	%xmm5,%xmm4,%xmm4
	shrdl	$11,%r14d,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	vpsrlq	$17,%xmm7,%xmm7
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	vpaddd	%xmm4,%xmm3,%xmm3
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	shrdl	$2,%r14d,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%ecx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ecx
	vpxor	%xmm7,%xmm6,%xmm6
	movl	%r11d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r10d,%r13d
	vpshufb	%xmm8,%xmm6,%xmm6
	xorl	%eax,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ecx,%r14d
	vpaddd	%xmm6,%xmm3,%xmm3
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	56(%rsp),%ebx
	vpshufd	$80,%xmm3,%xmm7
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	shrdl	$11,%r14d,%r14d
	vpsrld	$10,%xmm7,%xmm6
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	shrdl	$6,%r13d,%r13d
	vpsrlq	$17,%xmm7,%xmm7
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	vpxor	%xmm7,%xmm6,%xmm6
	xorl	%edx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%ebx,%r9d
	vpsrlq	$2,%xmm7,%xmm7
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	vpxor	%xmm7,%xmm6,%xmm6
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	vpshufb	%xmm9,%xmm6,%xmm6
	shrdl	$9,%r14d,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	vpaddd	%xmm6,%xmm3,%xmm3
	shrdl	$5,%r13d,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	vpaddd	96(%rbp),%xmm3,%xmm6
	xorl	%r9d,%r13d
	addl	60(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	vmovdqa	%xmm6,48(%rsp)
	cmpb	$0,131(%rbp)
	jne	.Lavx_00_47
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
	addl	0(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	movl	%edx,%r13d
	addl	%r11d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	addl	4(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	8(%rsp),%r9d
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	xorl	%r11d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r9d,%ebx
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	12(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	xorl	%eax,%r13d
	addl	16(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	movl	%r11d,%r13d
	addl	%edx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%edx
	movl	%eax,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	addl	20(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	24(%rsp),%ebx
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	xorl	%edx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%ebx,%r9d
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	28(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%eax
	movl	%r9d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r8d,%r13d
	xorl	%r10d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%eax,%r14d
	andl	%r8d,%r12d
	xorl	%r8d,%r13d
	addl	32(%rsp),%r11d
	movl	%eax,%r15d
	xorl	%r10d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ebx,%r15d
	addl	%r12d,%r11d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%eax,%r14d
	addl	%r13d,%r11d
	xorl	%ebx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r11d,%edx
	addl	%edi,%r11d
	movl	%edx,%r13d
	addl	%r11d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r11d
	movl	%r8d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%edx,%r13d
	xorl	%r9d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r11d,%r14d
	andl	%edx,%r12d
	xorl	%edx,%r13d
	addl	36(%rsp),%r10d
	movl	%r11d,%edi
	xorl	%r9d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%eax,%edi
	addl	%r12d,%r10d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r11d,%r14d
	addl	%r13d,%r10d
	xorl	%eax,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r10d,%ecx
	addl	%r15d,%r10d
	movl	%ecx,%r13d
	addl	%r10d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r10d
	movl	%edx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ecx,%r13d
	xorl	%r8d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r10d,%r14d
	andl	%ecx,%r12d
	xorl	%ecx,%r13d
	addl	40(%rsp),%r9d
	movl	%r10d,%r15d
	xorl	%r8d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r11d,%r15d
	addl	%r12d,%r9d
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%r10d,%r14d
	addl	%r13d,%r9d
	xorl	%r11d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%r9d,%ebx
	addl	%edi,%r9d
	movl	%ebx,%r13d
	addl	%r9d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r9d
	movl	%ecx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%ebx,%r13d
	xorl	%edx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r9d,%r14d
	andl	%ebx,%r12d
	xorl	%ebx,%r13d
	addl	44(%rsp),%r8d
	movl	%r9d,%edi
	xorl	%edx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r10d,%edi
	addl	%r12d,%r8d
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%r9d,%r14d
	addl	%r13d,%r8d
	xorl	%r10d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%r8d,%eax
	addl	%r15d,%r8d
	movl	%eax,%r13d
	addl	%r8d,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%r8d
	movl	%ebx,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%eax,%r13d
	xorl	%ecx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%r8d,%r14d
	andl	%eax,%r12d
	xorl	%eax,%r13d
	addl	48(%rsp),%edx
	movl	%r8d,%r15d
	xorl	%ecx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r9d,%r15d
	addl	%r12d,%edx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%r8d,%r14d
	addl	%r13d,%edx
	xorl	%r9d,%edi
	shrdl	$2,%r14d,%r14d
	addl	%edx,%r11d
	addl	%edi,%edx
	movl	%r11d,%r13d
	addl	%edx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%edx
	movl	%eax,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r11d,%r13d
	xorl	%ebx,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%edx,%r14d
	andl	%r11d,%r12d
	xorl	%r11d,%r13d
	addl	52(%rsp),%ecx
	movl	%edx,%edi
	xorl	%ebx,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%r8d,%edi
	addl	%r12d,%ecx
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%edx,%r14d
	addl	%r13d,%ecx
	xorl	%r8d,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%ecx,%r10d
	addl	%r15d,%ecx
	movl	%r10d,%r13d
	addl	%ecx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ecx
	movl	%r11d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r10d,%r13d
	xorl	%eax,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ecx,%r14d
	andl	%r10d,%r12d
	xorl	%r10d,%r13d
	addl	56(%rsp),%ebx
	movl	%ecx,%r15d
	xorl	%eax,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%edx,%r15d
	addl	%r12d,%ebx
	shrdl	$6,%r13d,%r13d
	andl	%r15d,%edi
	xorl	%ecx,%r14d
	addl	%r13d,%ebx
	xorl	%edx,%edi
	shrdl	$2,%r14d,%r14d
	addl	%ebx,%r9d
	addl	%edi,%ebx
	movl	%r9d,%r13d
	addl	%ebx,%r14d
	shrdl	$14,%r13d,%r13d
	movl	%r14d,%ebx
	movl	%r10d,%r12d
	shrdl	$9,%r14d,%r14d
	xorl	%r9d,%r13d
	xorl	%r11d,%r12d
	shrdl	$5,%r13d,%r13d
	xorl	%ebx,%r14d
	andl	%r9d,%r12d
	xorl	%r9d,%r13d
	addl	60(%rsp),%eax
	movl	%ebx,%edi
	xorl	%r11d,%r12d
	shrdl	$11,%r14d,%r14d
	xorl	%ecx,%edi
	addl	%r12d,%eax
	shrdl	$6,%r13d,%r13d
	andl	%edi,%r15d
	xorl	%ebx,%r14d
	addl	%r13d,%eax
	xorl	%ecx,%r15d
	shrdl	$2,%r14d,%r14d
	addl	%eax,%r8d
	addl	%r15d,%eax
	movl	%r8d,%r13d
	addl	%eax,%r14d
	movq	64+0(%rsp),%rdi
	movl	%r14d,%eax

	addl	0(%rdi),%eax
	leaq	64(%rsi),%rsi
	addl	4(%rdi),%ebx
	addl	8(%rdi),%ecx
	addl	12(%rdi),%edx
	addl	16(%rdi),%r8d
	addl	20(%rdi),%r9d
	addl	24(%rdi),%r10d
	addl	28(%rdi),%r11d

	cmpq	64+16(%rsp),%rsi

	movl	%eax,0(%rdi)
	movl	%ebx,4(%rdi)
	movl	%ecx,8(%rdi)
	movl	%edx,12(%rdi)
	movl	%r8d,16(%rdi)
	movl	%r9d,20(%rdi)
	movl	%r10d,24(%rdi)
	movl	%r11d,28(%rdi)
	jb	.Lloop_avx

	movq	88(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	vzeroupper
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_block_data_order_avx,.-sha256_block_data_order_avx
.type	sha256_block_data_order_avx2,@function
.align	64
sha256_block_data_order_avx2:
.cfi_startproc	
.Lavx2_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	subq	$544,%rsp
	shlq	$4,%rdx
	andq	$-1024,%rsp
	leaq	(%rsi,%rdx,4),%rdx
	addq	$448,%rsp
	movq	%rdi,64+0(%rsp)
	movq	%rsi,64+8(%rsp)
	movq	%rdx,64+16(%rsp)
	movq	%rax,88(%rsp)
.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
.Lprologue_avx2:

	vzeroupper
	subq	$-64,%rsi
	movl	0(%rdi),%eax
	movq	%rsi,%r12
	movl	4(%rdi),%ebx
	cmpq	%rdx,%rsi
	movl	8(%rdi),%ecx
	cmoveq	%rsp,%r12
	movl	12(%rdi),%edx
	movl	16(%rdi),%r8d
	movl	20(%rdi),%r9d
	movl	24(%rdi),%r10d
	movl	28(%rdi),%r11d
	vmovdqa	K256+512+32(%rip),%ymm8
	vmovdqa	K256+512+64(%rip),%ymm9
	jmp	.Loop_avx2
.align	16
.Loop_avx2:
	vmovdqa	K256+512(%rip),%ymm7
	vmovdqu	-64+0(%rsi),%xmm0
	vmovdqu	-64+16(%rsi),%xmm1
	vmovdqu	-64+32(%rsi),%xmm2
	vmovdqu	-64+48(%rsi),%xmm3

	vinserti128	$1,(%r12),%ymm0,%ymm0
	vinserti128	$1,16(%r12),%ymm1,%ymm1
	vpshufb	%ymm7,%ymm0,%ymm0
	vinserti128	$1,32(%r12),%ymm2,%ymm2
	vpshufb	%ymm7,%ymm1,%ymm1
	vinserti128	$1,48(%r12),%ymm3,%ymm3

	leaq	K256(%rip),%rbp
	vpshufb	%ymm7,%ymm2,%ymm2
	vpaddd	0(%rbp),%ymm0,%ymm4
	vpshufb	%ymm7,%ymm3,%ymm3
	vpaddd	32(%rbp),%ymm1,%ymm5
	vpaddd	64(%rbp),%ymm2,%ymm6
	vpaddd	96(%rbp),%ymm3,%ymm7
	vmovdqa	%ymm4,0(%rsp)
	xorl	%r14d,%r14d
	vmovdqa	%ymm5,32(%rsp)

	movq	88(%rsp),%rdi
.cfi_def_cfa	%rdi,8
	leaq	-64(%rsp),%rsp



	movq	%rdi,-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	movl	%ebx,%edi
	vmovdqa	%ymm6,0(%rsp)
	xorl	%ecx,%edi
	vmovdqa	%ymm7,32(%rsp)
	movl	%r9d,%r12d
	subq	$-32*4,%rbp
	jmp	.Lavx2_00_47

.align	16
.Lavx2_00_47:
	leaq	-64(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x38,0x06,0x23,0x08

	pushq	64-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x00,0x06,0x23,0x08
	leaq	8(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	vpalignr	$4,%ymm0,%ymm1,%ymm4
	addl	0+128(%rsp),%r11d
	andl	%r8d,%r12d
	rorxl	$25,%r8d,%r13d
	vpalignr	$4,%ymm2,%ymm3,%ymm7
	rorxl	$11,%r8d,%r15d
	leal	(%rax,%r14,1),%eax
	leal	(%r11,%r12,1),%r11d
	vpsrld	$7,%ymm4,%ymm6
	andnl	%r10d,%r8d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r8d,%r14d
	vpaddd	%ymm7,%ymm0,%ymm0
	leal	(%r11,%r12,1),%r11d
	xorl	%r14d,%r13d
	movl	%eax,%r15d
	vpsrld	$3,%ymm4,%ymm7
	rorxl	$22,%eax,%r12d
	leal	(%r11,%r13,1),%r11d
	xorl	%ebx,%r15d
	vpslld	$14,%ymm4,%ymm5
	rorxl	$13,%eax,%r14d
	rorxl	$2,%eax,%r13d
	leal	(%rdx,%r11,1),%edx
	vpxor	%ymm6,%ymm7,%ymm4
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%ebx,%edi
	vpshufd	$250,%ymm3,%ymm7
	xorl	%r13d,%r14d
	leal	(%r11,%rdi,1),%r11d
	movl	%r8d,%r12d
	vpsrld	$11,%ymm6,%ymm6
	addl	4+128(%rsp),%r10d
	andl	%edx,%r12d
	rorxl	$25,%edx,%r13d
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$11,%edx,%edi
	leal	(%r11,%r14,1),%r11d
	leal	(%r10,%r12,1),%r10d
	vpslld	$11,%ymm5,%ymm5
	andnl	%r9d,%edx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%edx,%r14d
	vpxor	%ymm6,%ymm4,%ymm4
	leal	(%r10,%r12,1),%r10d
	xorl	%r14d,%r13d
	movl	%r11d,%edi
	vpsrld	$10,%ymm7,%ymm6
	rorxl	$22,%r11d,%r12d
	leal	(%r10,%r13,1),%r10d
	xorl	%eax,%edi
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$13,%r11d,%r14d
	rorxl	$2,%r11d,%r13d
	leal	(%rcx,%r10,1),%ecx
	vpsrlq	$17,%ymm7,%ymm7
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%eax,%r15d
	vpaddd	%ymm4,%ymm0,%ymm0
	xorl	%r13d,%r14d
	leal	(%r10,%r15,1),%r10d
	movl	%edx,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	8+128(%rsp),%r9d
	andl	%ecx,%r12d
	rorxl	$25,%ecx,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%ecx,%r15d
	leal	(%r10,%r14,1),%r10d
	leal	(%r9,%r12,1),%r9d
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%r8d,%ecx,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%ecx,%r14d
	vpshufb	%ymm8,%ymm6,%ymm6
	leal	(%r9,%r12,1),%r9d
	xorl	%r14d,%r13d
	movl	%r10d,%r15d
	vpaddd	%ymm6,%ymm0,%ymm0
	rorxl	$22,%r10d,%r12d
	leal	(%r9,%r13,1),%r9d
	xorl	%r11d,%r15d
	vpshufd	$80,%ymm0,%ymm7
	rorxl	$13,%r10d,%r14d
	rorxl	$2,%r10d,%r13d
	leal	(%rbx,%r9,1),%ebx
	vpsrld	$10,%ymm7,%ymm6
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r11d,%edi
	vpsrlq	$17,%ymm7,%ymm7
	xorl	%r13d,%r14d
	leal	(%r9,%rdi,1),%r9d
	movl	%ecx,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	12+128(%rsp),%r8d
	andl	%ebx,%r12d
	rorxl	$25,%ebx,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%ebx,%edi
	leal	(%r9,%r14,1),%r9d
	leal	(%r8,%r12,1),%r8d
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%edx,%ebx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%ebx,%r14d
	vpshufb	%ymm9,%ymm6,%ymm6
	leal	(%r8,%r12,1),%r8d
	xorl	%r14d,%r13d
	movl	%r9d,%edi
	vpaddd	%ymm6,%ymm0,%ymm0
	rorxl	$22,%r9d,%r12d
	leal	(%r8,%r13,1),%r8d
	xorl	%r10d,%edi
	vpaddd	0(%rbp),%ymm0,%ymm6
	rorxl	$13,%r9d,%r14d
	rorxl	$2,%r9d,%r13d
	leal	(%rax,%r8,1),%eax
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r10d,%r15d
	xorl	%r13d,%r14d
	leal	(%r8,%r15,1),%r8d
	movl	%ebx,%r12d
	vmovdqa	%ymm6,0(%rsp)
	vpalignr	$4,%ymm1,%ymm2,%ymm4
	addl	32+128(%rsp),%edx
	andl	%eax,%r12d
	rorxl	$25,%eax,%r13d
	vpalignr	$4,%ymm3,%ymm0,%ymm7
	rorxl	$11,%eax,%r15d
	leal	(%r8,%r14,1),%r8d
	leal	(%rdx,%r12,1),%edx
	vpsrld	$7,%ymm4,%ymm6
	andnl	%ecx,%eax,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%eax,%r14d
	vpaddd	%ymm7,%ymm1,%ymm1
	leal	(%rdx,%r12,1),%edx
	xorl	%r14d,%r13d
	movl	%r8d,%r15d
	vpsrld	$3,%ymm4,%ymm7
	rorxl	$22,%r8d,%r12d
	leal	(%rdx,%r13,1),%edx
	xorl	%r9d,%r15d
	vpslld	$14,%ymm4,%ymm5
	rorxl	$13,%r8d,%r14d
	rorxl	$2,%r8d,%r13d
	leal	(%r11,%rdx,1),%r11d
	vpxor	%ymm6,%ymm7,%ymm4
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r9d,%edi
	vpshufd	$250,%ymm0,%ymm7
	xorl	%r13d,%r14d
	leal	(%rdx,%rdi,1),%edx
	movl	%eax,%r12d
	vpsrld	$11,%ymm6,%ymm6
	addl	36+128(%rsp),%ecx
	andl	%r11d,%r12d
	rorxl	$25,%r11d,%r13d
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$11,%r11d,%edi
	leal	(%rdx,%r14,1),%edx
	leal	(%rcx,%r12,1),%ecx
	vpslld	$11,%ymm5,%ymm5
	andnl	%ebx,%r11d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r11d,%r14d
	vpxor	%ymm6,%ymm4,%ymm4
	leal	(%rcx,%r12,1),%ecx
	xorl	%r14d,%r13d
	movl	%edx,%edi
	vpsrld	$10,%ymm7,%ymm6
	rorxl	$22,%edx,%r12d
	leal	(%rcx,%r13,1),%ecx
	xorl	%r8d,%edi
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$13,%edx,%r14d
	rorxl	$2,%edx,%r13d
	leal	(%r10,%rcx,1),%r10d
	vpsrlq	$17,%ymm7,%ymm7
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r8d,%r15d
	vpaddd	%ymm4,%ymm1,%ymm1
	xorl	%r13d,%r14d
	leal	(%rcx,%r15,1),%ecx
	movl	%r11d,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	40+128(%rsp),%ebx
	andl	%r10d,%r12d
	rorxl	$25,%r10d,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%r10d,%r15d
	leal	(%rcx,%r14,1),%ecx
	leal	(%rbx,%r12,1),%ebx
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%eax,%r10d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r10d,%r14d
	vpshufb	%ymm8,%ymm6,%ymm6
	leal	(%rbx,%r12,1),%ebx
	xorl	%r14d,%r13d
	movl	%ecx,%r15d
	vpaddd	%ymm6,%ymm1,%ymm1
	rorxl	$22,%ecx,%r12d
	leal	(%rbx,%r13,1),%ebx
	xorl	%edx,%r15d
	vpshufd	$80,%ymm1,%ymm7
	rorxl	$13,%ecx,%r14d
	rorxl	$2,%ecx,%r13d
	leal	(%r9,%rbx,1),%r9d
	vpsrld	$10,%ymm7,%ymm6
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%edx,%edi
	vpsrlq	$17,%ymm7,%ymm7
	xorl	%r13d,%r14d
	leal	(%rbx,%rdi,1),%ebx
	movl	%r10d,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	44+128(%rsp),%eax
	andl	%r9d,%r12d
	rorxl	$25,%r9d,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%r9d,%edi
	leal	(%rbx,%r14,1),%ebx
	leal	(%rax,%r12,1),%eax
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%r11d,%r9d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r9d,%r14d
	vpshufb	%ymm9,%ymm6,%ymm6
	leal	(%rax,%r12,1),%eax
	xorl	%r14d,%r13d
	movl	%ebx,%edi
	vpaddd	%ymm6,%ymm1,%ymm1
	rorxl	$22,%ebx,%r12d
	leal	(%rax,%r13,1),%eax
	xorl	%ecx,%edi
	vpaddd	32(%rbp),%ymm1,%ymm6
	rorxl	$13,%ebx,%r14d
	rorxl	$2,%ebx,%r13d
	leal	(%r8,%rax,1),%r8d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%ecx,%r15d
	xorl	%r13d,%r14d
	leal	(%rax,%r15,1),%eax
	movl	%r9d,%r12d
	vmovdqa	%ymm6,32(%rsp)
	leaq	-64(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x38,0x06,0x23,0x08

	pushq	64-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x00,0x06,0x23,0x08
	leaq	8(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	vpalignr	$4,%ymm2,%ymm3,%ymm4
	addl	0+128(%rsp),%r11d
	andl	%r8d,%r12d
	rorxl	$25,%r8d,%r13d
	vpalignr	$4,%ymm0,%ymm1,%ymm7
	rorxl	$11,%r8d,%r15d
	leal	(%rax,%r14,1),%eax
	leal	(%r11,%r12,1),%r11d
	vpsrld	$7,%ymm4,%ymm6
	andnl	%r10d,%r8d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r8d,%r14d
	vpaddd	%ymm7,%ymm2,%ymm2
	leal	(%r11,%r12,1),%r11d
	xorl	%r14d,%r13d
	movl	%eax,%r15d
	vpsrld	$3,%ymm4,%ymm7
	rorxl	$22,%eax,%r12d
	leal	(%r11,%r13,1),%r11d
	xorl	%ebx,%r15d
	vpslld	$14,%ymm4,%ymm5
	rorxl	$13,%eax,%r14d
	rorxl	$2,%eax,%r13d
	leal	(%rdx,%r11,1),%edx
	vpxor	%ymm6,%ymm7,%ymm4
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%ebx,%edi
	vpshufd	$250,%ymm1,%ymm7
	xorl	%r13d,%r14d
	leal	(%r11,%rdi,1),%r11d
	movl	%r8d,%r12d
	vpsrld	$11,%ymm6,%ymm6
	addl	4+128(%rsp),%r10d
	andl	%edx,%r12d
	rorxl	$25,%edx,%r13d
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$11,%edx,%edi
	leal	(%r11,%r14,1),%r11d
	leal	(%r10,%r12,1),%r10d
	vpslld	$11,%ymm5,%ymm5
	andnl	%r9d,%edx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%edx,%r14d
	vpxor	%ymm6,%ymm4,%ymm4
	leal	(%r10,%r12,1),%r10d
	xorl	%r14d,%r13d
	movl	%r11d,%edi
	vpsrld	$10,%ymm7,%ymm6
	rorxl	$22,%r11d,%r12d
	leal	(%r10,%r13,1),%r10d
	xorl	%eax,%edi
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$13,%r11d,%r14d
	rorxl	$2,%r11d,%r13d
	leal	(%rcx,%r10,1),%ecx
	vpsrlq	$17,%ymm7,%ymm7
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%eax,%r15d
	vpaddd	%ymm4,%ymm2,%ymm2
	xorl	%r13d,%r14d
	leal	(%r10,%r15,1),%r10d
	movl	%edx,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	8+128(%rsp),%r9d
	andl	%ecx,%r12d
	rorxl	$25,%ecx,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%ecx,%r15d
	leal	(%r10,%r14,1),%r10d
	leal	(%r9,%r12,1),%r9d
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%r8d,%ecx,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%ecx,%r14d
	vpshufb	%ymm8,%ymm6,%ymm6
	leal	(%r9,%r12,1),%r9d
	xorl	%r14d,%r13d
	movl	%r10d,%r15d
	vpaddd	%ymm6,%ymm2,%ymm2
	rorxl	$22,%r10d,%r12d
	leal	(%r9,%r13,1),%r9d
	xorl	%r11d,%r15d
	vpshufd	$80,%ymm2,%ymm7
	rorxl	$13,%r10d,%r14d
	rorxl	$2,%r10d,%r13d
	leal	(%rbx,%r9,1),%ebx
	vpsrld	$10,%ymm7,%ymm6
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r11d,%edi
	vpsrlq	$17,%ymm7,%ymm7
	xorl	%r13d,%r14d
	leal	(%r9,%rdi,1),%r9d
	movl	%ecx,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	12+128(%rsp),%r8d
	andl	%ebx,%r12d
	rorxl	$25,%ebx,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%ebx,%edi
	leal	(%r9,%r14,1),%r9d
	leal	(%r8,%r12,1),%r8d
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%edx,%ebx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%ebx,%r14d
	vpshufb	%ymm9,%ymm6,%ymm6
	leal	(%r8,%r12,1),%r8d
	xorl	%r14d,%r13d
	movl	%r9d,%edi
	vpaddd	%ymm6,%ymm2,%ymm2
	rorxl	$22,%r9d,%r12d
	leal	(%r8,%r13,1),%r8d
	xorl	%r10d,%edi
	vpaddd	64(%rbp),%ymm2,%ymm6
	rorxl	$13,%r9d,%r14d
	rorxl	$2,%r9d,%r13d
	leal	(%rax,%r8,1),%eax
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r10d,%r15d
	xorl	%r13d,%r14d
	leal	(%r8,%r15,1),%r8d
	movl	%ebx,%r12d
	vmovdqa	%ymm6,0(%rsp)
	vpalignr	$4,%ymm3,%ymm0,%ymm4
	addl	32+128(%rsp),%edx
	andl	%eax,%r12d
	rorxl	$25,%eax,%r13d
	vpalignr	$4,%ymm1,%ymm2,%ymm7
	rorxl	$11,%eax,%r15d
	leal	(%r8,%r14,1),%r8d
	leal	(%rdx,%r12,1),%edx
	vpsrld	$7,%ymm4,%ymm6
	andnl	%ecx,%eax,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%eax,%r14d
	vpaddd	%ymm7,%ymm3,%ymm3
	leal	(%rdx,%r12,1),%edx
	xorl	%r14d,%r13d
	movl	%r8d,%r15d
	vpsrld	$3,%ymm4,%ymm7
	rorxl	$22,%r8d,%r12d
	leal	(%rdx,%r13,1),%edx
	xorl	%r9d,%r15d
	vpslld	$14,%ymm4,%ymm5
	rorxl	$13,%r8d,%r14d
	rorxl	$2,%r8d,%r13d
	leal	(%r11,%rdx,1),%r11d
	vpxor	%ymm6,%ymm7,%ymm4
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r9d,%edi
	vpshufd	$250,%ymm2,%ymm7
	xorl	%r13d,%r14d
	leal	(%rdx,%rdi,1),%edx
	movl	%eax,%r12d
	vpsrld	$11,%ymm6,%ymm6
	addl	36+128(%rsp),%ecx
	andl	%r11d,%r12d
	rorxl	$25,%r11d,%r13d
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$11,%r11d,%edi
	leal	(%rdx,%r14,1),%edx
	leal	(%rcx,%r12,1),%ecx
	vpslld	$11,%ymm5,%ymm5
	andnl	%ebx,%r11d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r11d,%r14d
	vpxor	%ymm6,%ymm4,%ymm4
	leal	(%rcx,%r12,1),%ecx
	xorl	%r14d,%r13d
	movl	%edx,%edi
	vpsrld	$10,%ymm7,%ymm6
	rorxl	$22,%edx,%r12d
	leal	(%rcx,%r13,1),%ecx
	xorl	%r8d,%edi
	vpxor	%ymm5,%ymm4,%ymm4
	rorxl	$13,%edx,%r14d
	rorxl	$2,%edx,%r13d
	leal	(%r10,%rcx,1),%r10d
	vpsrlq	$17,%ymm7,%ymm7
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r8d,%r15d
	vpaddd	%ymm4,%ymm3,%ymm3
	xorl	%r13d,%r14d
	leal	(%rcx,%r15,1),%ecx
	movl	%r11d,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	40+128(%rsp),%ebx
	andl	%r10d,%r12d
	rorxl	$25,%r10d,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%r10d,%r15d
	leal	(%rcx,%r14,1),%ecx
	leal	(%rbx,%r12,1),%ebx
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%eax,%r10d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r10d,%r14d
	vpshufb	%ymm8,%ymm6,%ymm6
	leal	(%rbx,%r12,1),%ebx
	xorl	%r14d,%r13d
	movl	%ecx,%r15d
	vpaddd	%ymm6,%ymm3,%ymm3
	rorxl	$22,%ecx,%r12d
	leal	(%rbx,%r13,1),%ebx
	xorl	%edx,%r15d
	vpshufd	$80,%ymm3,%ymm7
	rorxl	$13,%ecx,%r14d
	rorxl	$2,%ecx,%r13d
	leal	(%r9,%rbx,1),%r9d
	vpsrld	$10,%ymm7,%ymm6
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%edx,%edi
	vpsrlq	$17,%ymm7,%ymm7
	xorl	%r13d,%r14d
	leal	(%rbx,%rdi,1),%ebx
	movl	%r10d,%r12d
	vpxor	%ymm7,%ymm6,%ymm6
	addl	44+128(%rsp),%eax
	andl	%r9d,%r12d
	rorxl	$25,%r9d,%r13d
	vpsrlq	$2,%ymm7,%ymm7
	rorxl	$11,%r9d,%edi
	leal	(%rbx,%r14,1),%ebx
	leal	(%rax,%r12,1),%eax
	vpxor	%ymm7,%ymm6,%ymm6
	andnl	%r11d,%r9d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r9d,%r14d
	vpshufb	%ymm9,%ymm6,%ymm6
	leal	(%rax,%r12,1),%eax
	xorl	%r14d,%r13d
	movl	%ebx,%edi
	vpaddd	%ymm6,%ymm3,%ymm3
	rorxl	$22,%ebx,%r12d
	leal	(%rax,%r13,1),%eax
	xorl	%ecx,%edi
	vpaddd	96(%rbp),%ymm3,%ymm6
	rorxl	$13,%ebx,%r14d
	rorxl	$2,%ebx,%r13d
	leal	(%r8,%rax,1),%r8d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%ecx,%r15d
	xorl	%r13d,%r14d
	leal	(%rax,%r15,1),%eax
	movl	%r9d,%r12d
	vmovdqa	%ymm6,32(%rsp)
	leaq	128(%rbp),%rbp
	cmpb	$0,3(%rbp)
	jne	.Lavx2_00_47
	addl	0+64(%rsp),%r11d
	andl	%r8d,%r12d
	rorxl	$25,%r8d,%r13d
	rorxl	$11,%r8d,%r15d
	leal	(%rax,%r14,1),%eax
	leal	(%r11,%r12,1),%r11d
	andnl	%r10d,%r8d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r8d,%r14d
	leal	(%r11,%r12,1),%r11d
	xorl	%r14d,%r13d
	movl	%eax,%r15d
	rorxl	$22,%eax,%r12d
	leal	(%r11,%r13,1),%r11d
	xorl	%ebx,%r15d
	rorxl	$13,%eax,%r14d
	rorxl	$2,%eax,%r13d
	leal	(%rdx,%r11,1),%edx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%ebx,%edi
	xorl	%r13d,%r14d
	leal	(%r11,%rdi,1),%r11d
	movl	%r8d,%r12d
	addl	4+64(%rsp),%r10d
	andl	%edx,%r12d
	rorxl	$25,%edx,%r13d
	rorxl	$11,%edx,%edi
	leal	(%r11,%r14,1),%r11d
	leal	(%r10,%r12,1),%r10d
	andnl	%r9d,%edx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%edx,%r14d
	leal	(%r10,%r12,1),%r10d
	xorl	%r14d,%r13d
	movl	%r11d,%edi
	rorxl	$22,%r11d,%r12d
	leal	(%r10,%r13,1),%r10d
	xorl	%eax,%edi
	rorxl	$13,%r11d,%r14d
	rorxl	$2,%r11d,%r13d
	leal	(%rcx,%r10,1),%ecx
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%eax,%r15d
	xorl	%r13d,%r14d
	leal	(%r10,%r15,1),%r10d
	movl	%edx,%r12d
	addl	8+64(%rsp),%r9d
	andl	%ecx,%r12d
	rorxl	$25,%ecx,%r13d
	rorxl	$11,%ecx,%r15d
	leal	(%r10,%r14,1),%r10d
	leal	(%r9,%r12,1),%r9d
	andnl	%r8d,%ecx,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%ecx,%r14d
	leal	(%r9,%r12,1),%r9d
	xorl	%r14d,%r13d
	movl	%r10d,%r15d
	rorxl	$22,%r10d,%r12d
	leal	(%r9,%r13,1),%r9d
	xorl	%r11d,%r15d
	rorxl	$13,%r10d,%r14d
	rorxl	$2,%r10d,%r13d
	leal	(%rbx,%r9,1),%ebx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r11d,%edi
	xorl	%r13d,%r14d
	leal	(%r9,%rdi,1),%r9d
	movl	%ecx,%r12d
	addl	12+64(%rsp),%r8d
	andl	%ebx,%r12d
	rorxl	$25,%ebx,%r13d
	rorxl	$11,%ebx,%edi
	leal	(%r9,%r14,1),%r9d
	leal	(%r8,%r12,1),%r8d
	andnl	%edx,%ebx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%ebx,%r14d
	leal	(%r8,%r12,1),%r8d
	xorl	%r14d,%r13d
	movl	%r9d,%edi
	rorxl	$22,%r9d,%r12d
	leal	(%r8,%r13,1),%r8d
	xorl	%r10d,%edi
	rorxl	$13,%r9d,%r14d
	rorxl	$2,%r9d,%r13d
	leal	(%rax,%r8,1),%eax
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r10d,%r15d
	xorl	%r13d,%r14d
	leal	(%r8,%r15,1),%r8d
	movl	%ebx,%r12d
	addl	32+64(%rsp),%edx
	andl	%eax,%r12d
	rorxl	$25,%eax,%r13d
	rorxl	$11,%eax,%r15d
	leal	(%r8,%r14,1),%r8d
	leal	(%rdx,%r12,1),%edx
	andnl	%ecx,%eax,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%eax,%r14d
	leal	(%rdx,%r12,1),%edx
	xorl	%r14d,%r13d
	movl	%r8d,%r15d
	rorxl	$22,%r8d,%r12d
	leal	(%rdx,%r13,1),%edx
	xorl	%r9d,%r15d
	rorxl	$13,%r8d,%r14d
	rorxl	$2,%r8d,%r13d
	leal	(%r11,%rdx,1),%r11d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r9d,%edi
	xorl	%r13d,%r14d
	leal	(%rdx,%rdi,1),%edx
	movl	%eax,%r12d
	addl	36+64(%rsp),%ecx
	andl	%r11d,%r12d
	rorxl	$25,%r11d,%r13d
	rorxl	$11,%r11d,%edi
	leal	(%rdx,%r14,1),%edx
	leal	(%rcx,%r12,1),%ecx
	andnl	%ebx,%r11d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r11d,%r14d
	leal	(%rcx,%r12,1),%ecx
	xorl	%r14d,%r13d
	movl	%edx,%edi
	rorxl	$22,%edx,%r12d
	leal	(%rcx,%r13,1),%ecx
	xorl	%r8d,%edi
	rorxl	$13,%edx,%r14d
	rorxl	$2,%edx,%r13d
	leal	(%r10,%rcx,1),%r10d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r8d,%r15d
	xorl	%r13d,%r14d
	leal	(%rcx,%r15,1),%ecx
	movl	%r11d,%r12d
	addl	40+64(%rsp),%ebx
	andl	%r10d,%r12d
	rorxl	$25,%r10d,%r13d
	rorxl	$11,%r10d,%r15d
	leal	(%rcx,%r14,1),%ecx
	leal	(%rbx,%r12,1),%ebx
	andnl	%eax,%r10d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r10d,%r14d
	leal	(%rbx,%r12,1),%ebx
	xorl	%r14d,%r13d
	movl	%ecx,%r15d
	rorxl	$22,%ecx,%r12d
	leal	(%rbx,%r13,1),%ebx
	xorl	%edx,%r15d
	rorxl	$13,%ecx,%r14d
	rorxl	$2,%ecx,%r13d
	leal	(%r9,%rbx,1),%r9d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%edx,%edi
	xorl	%r13d,%r14d
	leal	(%rbx,%rdi,1),%ebx
	movl	%r10d,%r12d
	addl	44+64(%rsp),%eax
	andl	%r9d,%r12d
	rorxl	$25,%r9d,%r13d
	rorxl	$11,%r9d,%edi
	leal	(%rbx,%r14,1),%ebx
	leal	(%rax,%r12,1),%eax
	andnl	%r11d,%r9d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r9d,%r14d
	leal	(%rax,%r12,1),%eax
	xorl	%r14d,%r13d
	movl	%ebx,%edi
	rorxl	$22,%ebx,%r12d
	leal	(%rax,%r13,1),%eax
	xorl	%ecx,%edi
	rorxl	$13,%ebx,%r14d
	rorxl	$2,%ebx,%r13d
	leal	(%r8,%rax,1),%r8d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%ecx,%r15d
	xorl	%r13d,%r14d
	leal	(%rax,%r15,1),%eax
	movl	%r9d,%r12d
	addl	0(%rsp),%r11d
	andl	%r8d,%r12d
	rorxl	$25,%r8d,%r13d
	rorxl	$11,%r8d,%r15d
	leal	(%rax,%r14,1),%eax
	leal	(%r11,%r12,1),%r11d
	andnl	%r10d,%r8d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r8d,%r14d
	leal	(%r11,%r12,1),%r11d
	xorl	%r14d,%r13d
	movl	%eax,%r15d
	rorxl	$22,%eax,%r12d
	leal	(%r11,%r13,1),%r11d
	xorl	%ebx,%r15d
	rorxl	$13,%eax,%r14d
	rorxl	$2,%eax,%r13d
	leal	(%rdx,%r11,1),%edx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%ebx,%edi
	xorl	%r13d,%r14d
	leal	(%r11,%rdi,1),%r11d
	movl	%r8d,%r12d
	addl	4(%rsp),%r10d
	andl	%edx,%r12d
	rorxl	$25,%edx,%r13d
	rorxl	$11,%edx,%edi
	leal	(%r11,%r14,1),%r11d
	leal	(%r10,%r12,1),%r10d
	andnl	%r9d,%edx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%edx,%r14d
	leal	(%r10,%r12,1),%r10d
	xorl	%r14d,%r13d
	movl	%r11d,%edi
	rorxl	$22,%r11d,%r12d
	leal	(%r10,%r13,1),%r10d
	xorl	%eax,%edi
	rorxl	$13,%r11d,%r14d
	rorxl	$2,%r11d,%r13d
	leal	(%rcx,%r10,1),%ecx
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%eax,%r15d
	xorl	%r13d,%r14d
	leal	(%r10,%r15,1),%r10d
	movl	%edx,%r12d
	addl	8(%rsp),%r9d
	andl	%ecx,%r12d
	rorxl	$25,%ecx,%r13d
	rorxl	$11,%ecx,%r15d
	leal	(%r10,%r14,1),%r10d
	leal	(%r9,%r12,1),%r9d
	andnl	%r8d,%ecx,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%ecx,%r14d
	leal	(%r9,%r12,1),%r9d
	xorl	%r14d,%r13d
	movl	%r10d,%r15d
	rorxl	$22,%r10d,%r12d
	leal	(%r9,%r13,1),%r9d
	xorl	%r11d,%r15d
	rorxl	$13,%r10d,%r14d
	rorxl	$2,%r10d,%r13d
	leal	(%rbx,%r9,1),%ebx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r11d,%edi
	xorl	%r13d,%r14d
	leal	(%r9,%rdi,1),%r9d
	movl	%ecx,%r12d
	addl	12(%rsp),%r8d
	andl	%ebx,%r12d
	rorxl	$25,%ebx,%r13d
	rorxl	$11,%ebx,%edi
	leal	(%r9,%r14,1),%r9d
	leal	(%r8,%r12,1),%r8d
	andnl	%edx,%ebx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%ebx,%r14d
	leal	(%r8,%r12,1),%r8d
	xorl	%r14d,%r13d
	movl	%r9d,%edi
	rorxl	$22,%r9d,%r12d
	leal	(%r8,%r13,1),%r8d
	xorl	%r10d,%edi
	rorxl	$13,%r9d,%r14d
	rorxl	$2,%r9d,%r13d
	leal	(%rax,%r8,1),%eax
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r10d,%r15d
	xorl	%r13d,%r14d
	leal	(%r8,%r15,1),%r8d
	movl	%ebx,%r12d
	addl	32(%rsp),%edx
	andl	%eax,%r12d
	rorxl	$25,%eax,%r13d
	rorxl	$11,%eax,%r15d
	leal	(%r8,%r14,1),%r8d
	leal	(%rdx,%r12,1),%edx
	andnl	%ecx,%eax,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%eax,%r14d
	leal	(%rdx,%r12,1),%edx
	xorl	%r14d,%r13d
	movl	%r8d,%r15d
	rorxl	$22,%r8d,%r12d
	leal	(%rdx,%r13,1),%edx
	xorl	%r9d,%r15d
	rorxl	$13,%r8d,%r14d
	rorxl	$2,%r8d,%r13d
	leal	(%r11,%rdx,1),%r11d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r9d,%edi
	xorl	%r13d,%r14d
	leal	(%rdx,%rdi,1),%edx
	movl	%eax,%r12d
	addl	36(%rsp),%ecx
	andl	%r11d,%r12d
	rorxl	$25,%r11d,%r13d
	rorxl	$11,%r11d,%edi
	leal	(%rdx,%r14,1),%edx
	leal	(%rcx,%r12,1),%ecx
	andnl	%ebx,%r11d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r11d,%r14d
	leal	(%rcx,%r12,1),%ecx
	xorl	%r14d,%r13d
	movl	%edx,%edi
	rorxl	$22,%edx,%r12d
	leal	(%rcx,%r13,1),%ecx
	xorl	%r8d,%edi
	rorxl	$13,%edx,%r14d
	rorxl	$2,%edx,%r13d
	leal	(%r10,%rcx,1),%r10d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r8d,%r15d
	xorl	%r13d,%r14d
	leal	(%rcx,%r15,1),%ecx
	movl	%r11d,%r12d
	addl	40(%rsp),%ebx
	andl	%r10d,%r12d
	rorxl	$25,%r10d,%r13d
	rorxl	$11,%r10d,%r15d
	leal	(%rcx,%r14,1),%ecx
	leal	(%rbx,%r12,1),%ebx
	andnl	%eax,%r10d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r10d,%r14d
	leal	(%rbx,%r12,1),%ebx
	xorl	%r14d,%r13d
	movl	%ecx,%r15d
	rorxl	$22,%ecx,%r12d
	leal	(%rbx,%r13,1),%ebx
	xorl	%edx,%r15d
	rorxl	$13,%ecx,%r14d
	rorxl	$2,%ecx,%r13d
	leal	(%r9,%rbx,1),%r9d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%edx,%edi
	xorl	%r13d,%r14d
	leal	(%rbx,%rdi,1),%ebx
	movl	%r10d,%r12d
	addl	44(%rsp),%eax
	andl	%r9d,%r12d
	rorxl	$25,%r9d,%r13d
	rorxl	$11,%r9d,%edi
	leal	(%rbx,%r14,1),%ebx
	leal	(%rax,%r12,1),%eax
	andnl	%r11d,%r9d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r9d,%r14d
	leal	(%rax,%r12,1),%eax
	xorl	%r14d,%r13d
	movl	%ebx,%edi
	rorxl	$22,%ebx,%r12d
	leal	(%rax,%r13,1),%eax
	xorl	%ecx,%edi
	rorxl	$13,%ebx,%r14d
	rorxl	$2,%ebx,%r13d
	leal	(%r8,%rax,1),%r8d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%ecx,%r15d
	xorl	%r13d,%r14d
	leal	(%rax,%r15,1),%eax
	movl	%r9d,%r12d
	movq	512(%rsp),%rdi
	addl	%r14d,%eax

	leaq	448(%rsp),%rbp

	addl	0(%rdi),%eax
	addl	4(%rdi),%ebx
	addl	8(%rdi),%ecx
	addl	12(%rdi),%edx
	addl	16(%rdi),%r8d
	addl	20(%rdi),%r9d
	addl	24(%rdi),%r10d
	addl	28(%rdi),%r11d

	movl	%eax,0(%rdi)
	movl	%ebx,4(%rdi)
	movl	%ecx,8(%rdi)
	movl	%edx,12(%rdi)
	movl	%r8d,16(%rdi)
	movl	%r9d,20(%rdi)
	movl	%r10d,24(%rdi)
	movl	%r11d,28(%rdi)

	cmpq	80(%rbp),%rsi
	je	.Ldone_avx2

	xorl	%r14d,%r14d
	movl	%ebx,%edi
	xorl	%ecx,%edi
	movl	%r9d,%r12d
	jmp	.Lower_avx2
.align	16
.Lower_avx2:
	addl	0+16(%rbp),%r11d
	andl	%r8d,%r12d
	rorxl	$25,%r8d,%r13d
	rorxl	$11,%r8d,%r15d
	leal	(%rax,%r14,1),%eax
	leal	(%r11,%r12,1),%r11d
	andnl	%r10d,%r8d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r8d,%r14d
	leal	(%r11,%r12,1),%r11d
	xorl	%r14d,%r13d
	movl	%eax,%r15d
	rorxl	$22,%eax,%r12d
	leal	(%r11,%r13,1),%r11d
	xorl	%ebx,%r15d
	rorxl	$13,%eax,%r14d
	rorxl	$2,%eax,%r13d
	leal	(%rdx,%r11,1),%edx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%ebx,%edi
	xorl	%r13d,%r14d
	leal	(%r11,%rdi,1),%r11d
	movl	%r8d,%r12d
	addl	4+16(%rbp),%r10d
	andl	%edx,%r12d
	rorxl	$25,%edx,%r13d
	rorxl	$11,%edx,%edi
	leal	(%r11,%r14,1),%r11d
	leal	(%r10,%r12,1),%r10d
	andnl	%r9d,%edx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%edx,%r14d
	leal	(%r10,%r12,1),%r10d
	xorl	%r14d,%r13d
	movl	%r11d,%edi
	rorxl	$22,%r11d,%r12d
	leal	(%r10,%r13,1),%r10d
	xorl	%eax,%edi
	rorxl	$13,%r11d,%r14d
	rorxl	$2,%r11d,%r13d
	leal	(%rcx,%r10,1),%ecx
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%eax,%r15d
	xorl	%r13d,%r14d
	leal	(%r10,%r15,1),%r10d
	movl	%edx,%r12d
	addl	8+16(%rbp),%r9d
	andl	%ecx,%r12d
	rorxl	$25,%ecx,%r13d
	rorxl	$11,%ecx,%r15d
	leal	(%r10,%r14,1),%r10d
	leal	(%r9,%r12,1),%r9d
	andnl	%r8d,%ecx,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%ecx,%r14d
	leal	(%r9,%r12,1),%r9d
	xorl	%r14d,%r13d
	movl	%r10d,%r15d
	rorxl	$22,%r10d,%r12d
	leal	(%r9,%r13,1),%r9d
	xorl	%r11d,%r15d
	rorxl	$13,%r10d,%r14d
	rorxl	$2,%r10d,%r13d
	leal	(%rbx,%r9,1),%ebx
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r11d,%edi
	xorl	%r13d,%r14d
	leal	(%r9,%rdi,1),%r9d
	movl	%ecx,%r12d
	addl	12+16(%rbp),%r8d
	andl	%ebx,%r12d
	rorxl	$25,%ebx,%r13d
	rorxl	$11,%ebx,%edi
	leal	(%r9,%r14,1),%r9d
	leal	(%r8,%r12,1),%r8d
	andnl	%edx,%ebx,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%ebx,%r14d
	leal	(%r8,%r12,1),%r8d
	xorl	%r14d,%r13d
	movl	%r9d,%edi
	rorxl	$22,%r9d,%r12d
	leal	(%r8,%r13,1),%r8d
	xorl	%r10d,%edi
	rorxl	$13,%r9d,%r14d
	rorxl	$2,%r9d,%r13d
	leal	(%rax,%r8,1),%eax
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r10d,%r15d
	xorl	%r13d,%r14d
	leal	(%r8,%r15,1),%r8d
	movl	%ebx,%r12d
	addl	32+16(%rbp),%edx
	andl	%eax,%r12d
	rorxl	$25,%eax,%r13d
	rorxl	$11,%eax,%r15d
	leal	(%r8,%r14,1),%r8d
	leal	(%rdx,%r12,1),%edx
	andnl	%ecx,%eax,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%eax,%r14d
	leal	(%rdx,%r12,1),%edx
	xorl	%r14d,%r13d
	movl	%r8d,%r15d
	rorxl	$22,%r8d,%r12d
	leal	(%rdx,%r13,1),%edx
	xorl	%r9d,%r15d
	rorxl	$13,%r8d,%r14d
	rorxl	$2,%r8d,%r13d
	leal	(%r11,%rdx,1),%r11d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%r9d,%edi
	xorl	%r13d,%r14d
	leal	(%rdx,%rdi,1),%edx
	movl	%eax,%r12d
	addl	36+16(%rbp),%ecx
	andl	%r11d,%r12d
	rorxl	$25,%r11d,%r13d
	rorxl	$11,%r11d,%edi
	leal	(%rdx,%r14,1),%edx
	leal	(%rcx,%r12,1),%ecx
	andnl	%ebx,%r11d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r11d,%r14d
	leal	(%rcx,%r12,1),%ecx
	xorl	%r14d,%r13d
	movl	%edx,%edi
	rorxl	$22,%edx,%r12d
	leal	(%rcx,%r13,1),%ecx
	xorl	%r8d,%edi
	rorxl	$13,%edx,%r14d
	rorxl	$2,%edx,%r13d
	leal	(%r10,%rcx,1),%r10d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%r8d,%r15d
	xorl	%r13d,%r14d
	leal	(%rcx,%r15,1),%ecx
	movl	%r11d,%r12d
	addl	40+16(%rbp),%ebx
	andl	%r10d,%r12d
	rorxl	$25,%r10d,%r13d
	rorxl	$11,%r10d,%r15d
	leal	(%rcx,%r14,1),%ecx
	leal	(%rbx,%r12,1),%ebx
	andnl	%eax,%r10d,%r12d
	xorl	%r15d,%r13d
	rorxl	$6,%r10d,%r14d
	leal	(%rbx,%r12,1),%ebx
	xorl	%r14d,%r13d
	movl	%ecx,%r15d
	rorxl	$22,%ecx,%r12d
	leal	(%rbx,%r13,1),%ebx
	xorl	%edx,%r15d
	rorxl	$13,%ecx,%r14d
	rorxl	$2,%ecx,%r13d
	leal	(%r9,%rbx,1),%r9d
	andl	%r15d,%edi
	xorl	%r12d,%r14d
	xorl	%edx,%edi
	xorl	%r13d,%r14d
	leal	(%rbx,%rdi,1),%ebx
	movl	%r10d,%r12d
	addl	44+16(%rbp),%eax
	andl	%r9d,%r12d
	rorxl	$25,%r9d,%r13d
	rorxl	$11,%r9d,%edi
	leal	(%rbx,%r14,1),%ebx
	leal	(%rax,%r12,1),%eax
	andnl	%r11d,%r9d,%r12d
	xorl	%edi,%r13d
	rorxl	$6,%r9d,%r14d
	leal	(%rax,%r12,1),%eax
	xorl	%r14d,%r13d
	movl	%ebx,%edi
	rorxl	$22,%ebx,%r12d
	leal	(%rax,%r13,1),%eax
	xorl	%ecx,%edi
	rorxl	$13,%ebx,%r14d
	rorxl	$2,%ebx,%r13d
	leal	(%r8,%rax,1),%r8d
	andl	%edi,%r15d
	xorl	%r12d,%r14d
	xorl	%ecx,%r15d
	xorl	%r13d,%r14d
	leal	(%rax,%r15,1),%eax
	movl	%r9d,%r12d
	leaq	-64(%rbp),%rbp
	cmpq	%rsp,%rbp
	jae	.Lower_avx2

	movq	512(%rsp),%rdi
	addl	%r14d,%eax

	leaq	448(%rsp),%rsp

.cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08

	addl	0(%rdi),%eax
	addl	4(%rdi),%ebx
	addl	8(%rdi),%ecx
	addl	12(%rdi),%edx
	addl	16(%rdi),%r8d
	addl	20(%rdi),%r9d
	leaq	128(%rsi),%rsi
	addl	24(%rdi),%r10d
	movq	%rsi,%r12
	addl	28(%rdi),%r11d
	cmpq	64+16(%rsp),%rsi

	movl	%eax,0(%rdi)
	cmoveq	%rsp,%r12
	movl	%ebx,4(%rdi)
	movl	%ecx,8(%rdi)
	movl	%edx,12(%rdi)
	movl	%r8d,16(%rdi)
	movl	%r9d,20(%rdi)
	movl	%r10d,24(%rdi)
	movl	%r11d,28(%rdi)

	jbe	.Loop_avx2
	leaq	(%rsp),%rbp


.cfi_escape	0x0f,0x06,0x76,0xd8,0x00,0x06,0x23,0x08

.Ldone_avx2:
	movq	88(%rbp),%rsi
.cfi_def_cfa	%rsi,8
	vzeroupper
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx2:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha256_block_data_order_avx2,.-sha256_block_data_order_avx2
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/sha/sha512-x86_64.s           0000664 0000000 0000000 00000275246 14746647661 0030072 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	


.globl	sha512_block_data_order
.type	sha512_block_data_order,@function
.align	16
sha512_block_data_order:
.cfi_startproc	
	leaq	OPENSSL_ia32cap_P(%rip),%r11
	movl	0(%r11),%r9d
	movl	4(%r11),%r10d
	movl	8(%r11),%r11d
	testl	$2048,%r10d
	jnz	.Lxop_shortcut
	andl	$296,%r11d
	cmpl	$296,%r11d
	je	.Lavx2_shortcut
	andl	$1073741824,%r9d
	andl	$268435968,%r10d
	orl	%r9d,%r10d
	cmpl	$1342177792,%r10d
	je	.Lavx_shortcut
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$128+32,%rsp
	leaq	(%rsi,%rdx,8),%rdx
	andq	$-64,%rsp
	movq	%rdi,128+0(%rsp)
	movq	%rsi,128+8(%rsp)
	movq	%rdx,128+16(%rsp)
	movq	%rax,152(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
.Lprologue:

	movq	0(%rdi),%rax
	movq	8(%rdi),%rbx
	movq	16(%rdi),%rcx
	movq	24(%rdi),%rdx
	movq	32(%rdi),%r8
	movq	40(%rdi),%r9
	movq	48(%rdi),%r10
	movq	56(%rdi),%r11
	jmp	.Lloop

.align	16
.Lloop:
	movq	%rbx,%rdi
	leaq	K512(%rip),%rbp
	xorq	%rcx,%rdi
	movq	0(%rsi),%r12
	movq	%r8,%r13
	movq	%rax,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r9,%r15

	xorq	%r8,%r13
	rorq	$5,%r14
	xorq	%r10,%r15

	movq	%r12,0(%rsp)
	xorq	%rax,%r14
	andq	%r8,%r15

	rorq	$4,%r13
	addq	%r11,%r12
	xorq	%r10,%r15

	rorq	$6,%r14
	xorq	%r8,%r13
	addq	%r15,%r12

	movq	%rax,%r15
	addq	(%rbp),%r12
	xorq	%rax,%r14

	xorq	%rbx,%r15
	rorq	$14,%r13
	movq	%rbx,%r11

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r11
	addq	%r12,%rdx
	addq	%r12,%r11

	leaq	8(%rbp),%rbp
	addq	%r14,%r11
	movq	8(%rsi),%r12
	movq	%rdx,%r13
	movq	%r11,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r8,%rdi

	xorq	%rdx,%r13
	rorq	$5,%r14
	xorq	%r9,%rdi

	movq	%r12,8(%rsp)
	xorq	%r11,%r14
	andq	%rdx,%rdi

	rorq	$4,%r13
	addq	%r10,%r12
	xorq	%r9,%rdi

	rorq	$6,%r14
	xorq	%rdx,%r13
	addq	%rdi,%r12

	movq	%r11,%rdi
	addq	(%rbp),%r12
	xorq	%r11,%r14

	xorq	%rax,%rdi
	rorq	$14,%r13
	movq	%rax,%r10

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r10
	addq	%r12,%rcx
	addq	%r12,%r10

	leaq	24(%rbp),%rbp
	addq	%r14,%r10
	movq	16(%rsi),%r12
	movq	%rcx,%r13
	movq	%r10,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rdx,%r15

	xorq	%rcx,%r13
	rorq	$5,%r14
	xorq	%r8,%r15

	movq	%r12,16(%rsp)
	xorq	%r10,%r14
	andq	%rcx,%r15

	rorq	$4,%r13
	addq	%r9,%r12
	xorq	%r8,%r15

	rorq	$6,%r14
	xorq	%rcx,%r13
	addq	%r15,%r12

	movq	%r10,%r15
	addq	(%rbp),%r12
	xorq	%r10,%r14

	xorq	%r11,%r15
	rorq	$14,%r13
	movq	%r11,%r9

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r9
	addq	%r12,%rbx
	addq	%r12,%r9

	leaq	8(%rbp),%rbp
	addq	%r14,%r9
	movq	24(%rsi),%r12
	movq	%rbx,%r13
	movq	%r9,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rcx,%rdi

	xorq	%rbx,%r13
	rorq	$5,%r14
	xorq	%rdx,%rdi

	movq	%r12,24(%rsp)
	xorq	%r9,%r14
	andq	%rbx,%rdi

	rorq	$4,%r13
	addq	%r8,%r12
	xorq	%rdx,%rdi

	rorq	$6,%r14
	xorq	%rbx,%r13
	addq	%rdi,%r12

	movq	%r9,%rdi
	addq	(%rbp),%r12
	xorq	%r9,%r14

	xorq	%r10,%rdi
	rorq	$14,%r13
	movq	%r10,%r8

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r8
	addq	%r12,%rax
	addq	%r12,%r8

	leaq	24(%rbp),%rbp
	addq	%r14,%r8
	movq	32(%rsi),%r12
	movq	%rax,%r13
	movq	%r8,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rbx,%r15

	xorq	%rax,%r13
	rorq	$5,%r14
	xorq	%rcx,%r15

	movq	%r12,32(%rsp)
	xorq	%r8,%r14
	andq	%rax,%r15

	rorq	$4,%r13
	addq	%rdx,%r12
	xorq	%rcx,%r15

	rorq	$6,%r14
	xorq	%rax,%r13
	addq	%r15,%r12

	movq	%r8,%r15
	addq	(%rbp),%r12
	xorq	%r8,%r14

	xorq	%r9,%r15
	rorq	$14,%r13
	movq	%r9,%rdx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rdx
	addq	%r12,%r11
	addq	%r12,%rdx

	leaq	8(%rbp),%rbp
	addq	%r14,%rdx
	movq	40(%rsi),%r12
	movq	%r11,%r13
	movq	%rdx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rax,%rdi

	xorq	%r11,%r13
	rorq	$5,%r14
	xorq	%rbx,%rdi

	movq	%r12,40(%rsp)
	xorq	%rdx,%r14
	andq	%r11,%rdi

	rorq	$4,%r13
	addq	%rcx,%r12
	xorq	%rbx,%rdi

	rorq	$6,%r14
	xorq	%r11,%r13
	addq	%rdi,%r12

	movq	%rdx,%rdi
	addq	(%rbp),%r12
	xorq	%rdx,%r14

	xorq	%r8,%rdi
	rorq	$14,%r13
	movq	%r8,%rcx

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rcx
	addq	%r12,%r10
	addq	%r12,%rcx

	leaq	24(%rbp),%rbp
	addq	%r14,%rcx
	movq	48(%rsi),%r12
	movq	%r10,%r13
	movq	%rcx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r11,%r15

	xorq	%r10,%r13
	rorq	$5,%r14
	xorq	%rax,%r15

	movq	%r12,48(%rsp)
	xorq	%rcx,%r14
	andq	%r10,%r15

	rorq	$4,%r13
	addq	%rbx,%r12
	xorq	%rax,%r15

	rorq	$6,%r14
	xorq	%r10,%r13
	addq	%r15,%r12

	movq	%rcx,%r15
	addq	(%rbp),%r12
	xorq	%rcx,%r14

	xorq	%rdx,%r15
	rorq	$14,%r13
	movq	%rdx,%rbx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rbx
	addq	%r12,%r9
	addq	%r12,%rbx

	leaq	8(%rbp),%rbp
	addq	%r14,%rbx
	movq	56(%rsi),%r12
	movq	%r9,%r13
	movq	%rbx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r10,%rdi

	xorq	%r9,%r13
	rorq	$5,%r14
	xorq	%r11,%rdi

	movq	%r12,56(%rsp)
	xorq	%rbx,%r14
	andq	%r9,%rdi

	rorq	$4,%r13
	addq	%rax,%r12
	xorq	%r11,%rdi

	rorq	$6,%r14
	xorq	%r9,%r13
	addq	%rdi,%r12

	movq	%rbx,%rdi
	addq	(%rbp),%r12
	xorq	%rbx,%r14

	xorq	%rcx,%rdi
	rorq	$14,%r13
	movq	%rcx,%rax

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rax
	addq	%r12,%r8
	addq	%r12,%rax

	leaq	24(%rbp),%rbp
	addq	%r14,%rax
	movq	64(%rsi),%r12
	movq	%r8,%r13
	movq	%rax,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r9,%r15

	xorq	%r8,%r13
	rorq	$5,%r14
	xorq	%r10,%r15

	movq	%r12,64(%rsp)
	xorq	%rax,%r14
	andq	%r8,%r15

	rorq	$4,%r13
	addq	%r11,%r12
	xorq	%r10,%r15

	rorq	$6,%r14
	xorq	%r8,%r13
	addq	%r15,%r12

	movq	%rax,%r15
	addq	(%rbp),%r12
	xorq	%rax,%r14

	xorq	%rbx,%r15
	rorq	$14,%r13
	movq	%rbx,%r11

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r11
	addq	%r12,%rdx
	addq	%r12,%r11

	leaq	8(%rbp),%rbp
	addq	%r14,%r11
	movq	72(%rsi),%r12
	movq	%rdx,%r13
	movq	%r11,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r8,%rdi

	xorq	%rdx,%r13
	rorq	$5,%r14
	xorq	%r9,%rdi

	movq	%r12,72(%rsp)
	xorq	%r11,%r14
	andq	%rdx,%rdi

	rorq	$4,%r13
	addq	%r10,%r12
	xorq	%r9,%rdi

	rorq	$6,%r14
	xorq	%rdx,%r13
	addq	%rdi,%r12

	movq	%r11,%rdi
	addq	(%rbp),%r12
	xorq	%r11,%r14

	xorq	%rax,%rdi
	rorq	$14,%r13
	movq	%rax,%r10

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r10
	addq	%r12,%rcx
	addq	%r12,%r10

	leaq	24(%rbp),%rbp
	addq	%r14,%r10
	movq	80(%rsi),%r12
	movq	%rcx,%r13
	movq	%r10,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rdx,%r15

	xorq	%rcx,%r13
	rorq	$5,%r14
	xorq	%r8,%r15

	movq	%r12,80(%rsp)
	xorq	%r10,%r14
	andq	%rcx,%r15

	rorq	$4,%r13
	addq	%r9,%r12
	xorq	%r8,%r15

	rorq	$6,%r14
	xorq	%rcx,%r13
	addq	%r15,%r12

	movq	%r10,%r15
	addq	(%rbp),%r12
	xorq	%r10,%r14

	xorq	%r11,%r15
	rorq	$14,%r13
	movq	%r11,%r9

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r9
	addq	%r12,%rbx
	addq	%r12,%r9

	leaq	8(%rbp),%rbp
	addq	%r14,%r9
	movq	88(%rsi),%r12
	movq	%rbx,%r13
	movq	%r9,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rcx,%rdi

	xorq	%rbx,%r13
	rorq	$5,%r14
	xorq	%rdx,%rdi

	movq	%r12,88(%rsp)
	xorq	%r9,%r14
	andq	%rbx,%rdi

	rorq	$4,%r13
	addq	%r8,%r12
	xorq	%rdx,%rdi

	rorq	$6,%r14
	xorq	%rbx,%r13
	addq	%rdi,%r12

	movq	%r9,%rdi
	addq	(%rbp),%r12
	xorq	%r9,%r14

	xorq	%r10,%rdi
	rorq	$14,%r13
	movq	%r10,%r8

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r8
	addq	%r12,%rax
	addq	%r12,%r8

	leaq	24(%rbp),%rbp
	addq	%r14,%r8
	movq	96(%rsi),%r12
	movq	%rax,%r13
	movq	%r8,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rbx,%r15

	xorq	%rax,%r13
	rorq	$5,%r14
	xorq	%rcx,%r15

	movq	%r12,96(%rsp)
	xorq	%r8,%r14
	andq	%rax,%r15

	rorq	$4,%r13
	addq	%rdx,%r12
	xorq	%rcx,%r15

	rorq	$6,%r14
	xorq	%rax,%r13
	addq	%r15,%r12

	movq	%r8,%r15
	addq	(%rbp),%r12
	xorq	%r8,%r14

	xorq	%r9,%r15
	rorq	$14,%r13
	movq	%r9,%rdx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rdx
	addq	%r12,%r11
	addq	%r12,%rdx

	leaq	8(%rbp),%rbp
	addq	%r14,%rdx
	movq	104(%rsi),%r12
	movq	%r11,%r13
	movq	%rdx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%rax,%rdi

	xorq	%r11,%r13
	rorq	$5,%r14
	xorq	%rbx,%rdi

	movq	%r12,104(%rsp)
	xorq	%rdx,%r14
	andq	%r11,%rdi

	rorq	$4,%r13
	addq	%rcx,%r12
	xorq	%rbx,%rdi

	rorq	$6,%r14
	xorq	%r11,%r13
	addq	%rdi,%r12

	movq	%rdx,%rdi
	addq	(%rbp),%r12
	xorq	%rdx,%r14

	xorq	%r8,%rdi
	rorq	$14,%r13
	movq	%r8,%rcx

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rcx
	addq	%r12,%r10
	addq	%r12,%rcx

	leaq	24(%rbp),%rbp
	addq	%r14,%rcx
	movq	112(%rsi),%r12
	movq	%r10,%r13
	movq	%rcx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r11,%r15

	xorq	%r10,%r13
	rorq	$5,%r14
	xorq	%rax,%r15

	movq	%r12,112(%rsp)
	xorq	%rcx,%r14
	andq	%r10,%r15

	rorq	$4,%r13
	addq	%rbx,%r12
	xorq	%rax,%r15

	rorq	$6,%r14
	xorq	%r10,%r13
	addq	%r15,%r12

	movq	%rcx,%r15
	addq	(%rbp),%r12
	xorq	%rcx,%r14

	xorq	%rdx,%r15
	rorq	$14,%r13
	movq	%rdx,%rbx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rbx
	addq	%r12,%r9
	addq	%r12,%rbx

	leaq	8(%rbp),%rbp
	addq	%r14,%rbx
	movq	120(%rsi),%r12
	movq	%r9,%r13
	movq	%rbx,%r14
	bswapq	%r12
	rorq	$23,%r13
	movq	%r10,%rdi

	xorq	%r9,%r13
	rorq	$5,%r14
	xorq	%r11,%rdi

	movq	%r12,120(%rsp)
	xorq	%rbx,%r14
	andq	%r9,%rdi

	rorq	$4,%r13
	addq	%rax,%r12
	xorq	%r11,%rdi

	rorq	$6,%r14
	xorq	%r9,%r13
	addq	%rdi,%r12

	movq	%rbx,%rdi
	addq	(%rbp),%r12
	xorq	%rbx,%r14

	xorq	%rcx,%rdi
	rorq	$14,%r13
	movq	%rcx,%rax

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rax
	addq	%r12,%r8
	addq	%r12,%rax

	leaq	24(%rbp),%rbp
	jmp	.Lrounds_16_xx
.align	16
.Lrounds_16_xx:
	movq	8(%rsp),%r13
	movq	112(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rax
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	72(%rsp),%r12

	addq	0(%rsp),%r12
	movq	%r8,%r13
	addq	%r15,%r12
	movq	%rax,%r14
	rorq	$23,%r13
	movq	%r9,%r15

	xorq	%r8,%r13
	rorq	$5,%r14
	xorq	%r10,%r15

	movq	%r12,0(%rsp)
	xorq	%rax,%r14
	andq	%r8,%r15

	rorq	$4,%r13
	addq	%r11,%r12
	xorq	%r10,%r15

	rorq	$6,%r14
	xorq	%r8,%r13
	addq	%r15,%r12

	movq	%rax,%r15
	addq	(%rbp),%r12
	xorq	%rax,%r14

	xorq	%rbx,%r15
	rorq	$14,%r13
	movq	%rbx,%r11

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r11
	addq	%r12,%rdx
	addq	%r12,%r11

	leaq	8(%rbp),%rbp
	movq	16(%rsp),%r13
	movq	120(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r11
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	80(%rsp),%r12

	addq	8(%rsp),%r12
	movq	%rdx,%r13
	addq	%rdi,%r12
	movq	%r11,%r14
	rorq	$23,%r13
	movq	%r8,%rdi

	xorq	%rdx,%r13
	rorq	$5,%r14
	xorq	%r9,%rdi

	movq	%r12,8(%rsp)
	xorq	%r11,%r14
	andq	%rdx,%rdi

	rorq	$4,%r13
	addq	%r10,%r12
	xorq	%r9,%rdi

	rorq	$6,%r14
	xorq	%rdx,%r13
	addq	%rdi,%r12

	movq	%r11,%rdi
	addq	(%rbp),%r12
	xorq	%r11,%r14

	xorq	%rax,%rdi
	rorq	$14,%r13
	movq	%rax,%r10

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r10
	addq	%r12,%rcx
	addq	%r12,%r10

	leaq	24(%rbp),%rbp
	movq	24(%rsp),%r13
	movq	0(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r10
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	88(%rsp),%r12

	addq	16(%rsp),%r12
	movq	%rcx,%r13
	addq	%r15,%r12
	movq	%r10,%r14
	rorq	$23,%r13
	movq	%rdx,%r15

	xorq	%rcx,%r13
	rorq	$5,%r14
	xorq	%r8,%r15

	movq	%r12,16(%rsp)
	xorq	%r10,%r14
	andq	%rcx,%r15

	rorq	$4,%r13
	addq	%r9,%r12
	xorq	%r8,%r15

	rorq	$6,%r14
	xorq	%rcx,%r13
	addq	%r15,%r12

	movq	%r10,%r15
	addq	(%rbp),%r12
	xorq	%r10,%r14

	xorq	%r11,%r15
	rorq	$14,%r13
	movq	%r11,%r9

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r9
	addq	%r12,%rbx
	addq	%r12,%r9

	leaq	8(%rbp),%rbp
	movq	32(%rsp),%r13
	movq	8(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r9
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	96(%rsp),%r12

	addq	24(%rsp),%r12
	movq	%rbx,%r13
	addq	%rdi,%r12
	movq	%r9,%r14
	rorq	$23,%r13
	movq	%rcx,%rdi

	xorq	%rbx,%r13
	rorq	$5,%r14
	xorq	%rdx,%rdi

	movq	%r12,24(%rsp)
	xorq	%r9,%r14
	andq	%rbx,%rdi

	rorq	$4,%r13
	addq	%r8,%r12
	xorq	%rdx,%rdi

	rorq	$6,%r14
	xorq	%rbx,%r13
	addq	%rdi,%r12

	movq	%r9,%rdi
	addq	(%rbp),%r12
	xorq	%r9,%r14

	xorq	%r10,%rdi
	rorq	$14,%r13
	movq	%r10,%r8

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r8
	addq	%r12,%rax
	addq	%r12,%r8

	leaq	24(%rbp),%rbp
	movq	40(%rsp),%r13
	movq	16(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r8
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	104(%rsp),%r12

	addq	32(%rsp),%r12
	movq	%rax,%r13
	addq	%r15,%r12
	movq	%r8,%r14
	rorq	$23,%r13
	movq	%rbx,%r15

	xorq	%rax,%r13
	rorq	$5,%r14
	xorq	%rcx,%r15

	movq	%r12,32(%rsp)
	xorq	%r8,%r14
	andq	%rax,%r15

	rorq	$4,%r13
	addq	%rdx,%r12
	xorq	%rcx,%r15

	rorq	$6,%r14
	xorq	%rax,%r13
	addq	%r15,%r12

	movq	%r8,%r15
	addq	(%rbp),%r12
	xorq	%r8,%r14

	xorq	%r9,%r15
	rorq	$14,%r13
	movq	%r9,%rdx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rdx
	addq	%r12,%r11
	addq	%r12,%rdx

	leaq	8(%rbp),%rbp
	movq	48(%rsp),%r13
	movq	24(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rdx
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	112(%rsp),%r12

	addq	40(%rsp),%r12
	movq	%r11,%r13
	addq	%rdi,%r12
	movq	%rdx,%r14
	rorq	$23,%r13
	movq	%rax,%rdi

	xorq	%r11,%r13
	rorq	$5,%r14
	xorq	%rbx,%rdi

	movq	%r12,40(%rsp)
	xorq	%rdx,%r14
	andq	%r11,%rdi

	rorq	$4,%r13
	addq	%rcx,%r12
	xorq	%rbx,%rdi

	rorq	$6,%r14
	xorq	%r11,%r13
	addq	%rdi,%r12

	movq	%rdx,%rdi
	addq	(%rbp),%r12
	xorq	%rdx,%r14

	xorq	%r8,%rdi
	rorq	$14,%r13
	movq	%r8,%rcx

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rcx
	addq	%r12,%r10
	addq	%r12,%rcx

	leaq	24(%rbp),%rbp
	movq	56(%rsp),%r13
	movq	32(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rcx
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	120(%rsp),%r12

	addq	48(%rsp),%r12
	movq	%r10,%r13
	addq	%r15,%r12
	movq	%rcx,%r14
	rorq	$23,%r13
	movq	%r11,%r15

	xorq	%r10,%r13
	rorq	$5,%r14
	xorq	%rax,%r15

	movq	%r12,48(%rsp)
	xorq	%rcx,%r14
	andq	%r10,%r15

	rorq	$4,%r13
	addq	%rbx,%r12
	xorq	%rax,%r15

	rorq	$6,%r14
	xorq	%r10,%r13
	addq	%r15,%r12

	movq	%rcx,%r15
	addq	(%rbp),%r12
	xorq	%rcx,%r14

	xorq	%rdx,%r15
	rorq	$14,%r13
	movq	%rdx,%rbx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rbx
	addq	%r12,%r9
	addq	%r12,%rbx

	leaq	8(%rbp),%rbp
	movq	64(%rsp),%r13
	movq	40(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rbx
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	0(%rsp),%r12

	addq	56(%rsp),%r12
	movq	%r9,%r13
	addq	%rdi,%r12
	movq	%rbx,%r14
	rorq	$23,%r13
	movq	%r10,%rdi

	xorq	%r9,%r13
	rorq	$5,%r14
	xorq	%r11,%rdi

	movq	%r12,56(%rsp)
	xorq	%rbx,%r14
	andq	%r9,%rdi

	rorq	$4,%r13
	addq	%rax,%r12
	xorq	%r11,%rdi

	rorq	$6,%r14
	xorq	%r9,%r13
	addq	%rdi,%r12

	movq	%rbx,%rdi
	addq	(%rbp),%r12
	xorq	%rbx,%r14

	xorq	%rcx,%rdi
	rorq	$14,%r13
	movq	%rcx,%rax

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rax
	addq	%r12,%r8
	addq	%r12,%rax

	leaq	24(%rbp),%rbp
	movq	72(%rsp),%r13
	movq	48(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rax
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	8(%rsp),%r12

	addq	64(%rsp),%r12
	movq	%r8,%r13
	addq	%r15,%r12
	movq	%rax,%r14
	rorq	$23,%r13
	movq	%r9,%r15

	xorq	%r8,%r13
	rorq	$5,%r14
	xorq	%r10,%r15

	movq	%r12,64(%rsp)
	xorq	%rax,%r14
	andq	%r8,%r15

	rorq	$4,%r13
	addq	%r11,%r12
	xorq	%r10,%r15

	rorq	$6,%r14
	xorq	%r8,%r13
	addq	%r15,%r12

	movq	%rax,%r15
	addq	(%rbp),%r12
	xorq	%rax,%r14

	xorq	%rbx,%r15
	rorq	$14,%r13
	movq	%rbx,%r11

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r11
	addq	%r12,%rdx
	addq	%r12,%r11

	leaq	8(%rbp),%rbp
	movq	80(%rsp),%r13
	movq	56(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r11
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	16(%rsp),%r12

	addq	72(%rsp),%r12
	movq	%rdx,%r13
	addq	%rdi,%r12
	movq	%r11,%r14
	rorq	$23,%r13
	movq	%r8,%rdi

	xorq	%rdx,%r13
	rorq	$5,%r14
	xorq	%r9,%rdi

	movq	%r12,72(%rsp)
	xorq	%r11,%r14
	andq	%rdx,%rdi

	rorq	$4,%r13
	addq	%r10,%r12
	xorq	%r9,%rdi

	rorq	$6,%r14
	xorq	%rdx,%r13
	addq	%rdi,%r12

	movq	%r11,%rdi
	addq	(%rbp),%r12
	xorq	%r11,%r14

	xorq	%rax,%rdi
	rorq	$14,%r13
	movq	%rax,%r10

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r10
	addq	%r12,%rcx
	addq	%r12,%r10

	leaq	24(%rbp),%rbp
	movq	88(%rsp),%r13
	movq	64(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r10
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	24(%rsp),%r12

	addq	80(%rsp),%r12
	movq	%rcx,%r13
	addq	%r15,%r12
	movq	%r10,%r14
	rorq	$23,%r13
	movq	%rdx,%r15

	xorq	%rcx,%r13
	rorq	$5,%r14
	xorq	%r8,%r15

	movq	%r12,80(%rsp)
	xorq	%r10,%r14
	andq	%rcx,%r15

	rorq	$4,%r13
	addq	%r9,%r12
	xorq	%r8,%r15

	rorq	$6,%r14
	xorq	%rcx,%r13
	addq	%r15,%r12

	movq	%r10,%r15
	addq	(%rbp),%r12
	xorq	%r10,%r14

	xorq	%r11,%r15
	rorq	$14,%r13
	movq	%r11,%r9

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%r9
	addq	%r12,%rbx
	addq	%r12,%r9

	leaq	8(%rbp),%rbp
	movq	96(%rsp),%r13
	movq	72(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r9
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	32(%rsp),%r12

	addq	88(%rsp),%r12
	movq	%rbx,%r13
	addq	%rdi,%r12
	movq	%r9,%r14
	rorq	$23,%r13
	movq	%rcx,%rdi

	xorq	%rbx,%r13
	rorq	$5,%r14
	xorq	%rdx,%rdi

	movq	%r12,88(%rsp)
	xorq	%r9,%r14
	andq	%rbx,%rdi

	rorq	$4,%r13
	addq	%r8,%r12
	xorq	%rdx,%rdi

	rorq	$6,%r14
	xorq	%rbx,%r13
	addq	%rdi,%r12

	movq	%r9,%rdi
	addq	(%rbp),%r12
	xorq	%r9,%r14

	xorq	%r10,%rdi
	rorq	$14,%r13
	movq	%r10,%r8

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%r8
	addq	%r12,%rax
	addq	%r12,%r8

	leaq	24(%rbp),%rbp
	movq	104(%rsp),%r13
	movq	80(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%r8
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	40(%rsp),%r12

	addq	96(%rsp),%r12
	movq	%rax,%r13
	addq	%r15,%r12
	movq	%r8,%r14
	rorq	$23,%r13
	movq	%rbx,%r15

	xorq	%rax,%r13
	rorq	$5,%r14
	xorq	%rcx,%r15

	movq	%r12,96(%rsp)
	xorq	%r8,%r14
	andq	%rax,%r15

	rorq	$4,%r13
	addq	%rdx,%r12
	xorq	%rcx,%r15

	rorq	$6,%r14
	xorq	%rax,%r13
	addq	%r15,%r12

	movq	%r8,%r15
	addq	(%rbp),%r12
	xorq	%r8,%r14

	xorq	%r9,%r15
	rorq	$14,%r13
	movq	%r9,%rdx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rdx
	addq	%r12,%r11
	addq	%r12,%rdx

	leaq	8(%rbp),%rbp
	movq	112(%rsp),%r13
	movq	88(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rdx
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	48(%rsp),%r12

	addq	104(%rsp),%r12
	movq	%r11,%r13
	addq	%rdi,%r12
	movq	%rdx,%r14
	rorq	$23,%r13
	movq	%rax,%rdi

	xorq	%r11,%r13
	rorq	$5,%r14
	xorq	%rbx,%rdi

	movq	%r12,104(%rsp)
	xorq	%rdx,%r14
	andq	%r11,%rdi

	rorq	$4,%r13
	addq	%rcx,%r12
	xorq	%rbx,%rdi

	rorq	$6,%r14
	xorq	%r11,%r13
	addq	%rdi,%r12

	movq	%rdx,%rdi
	addq	(%rbp),%r12
	xorq	%rdx,%r14

	xorq	%r8,%rdi
	rorq	$14,%r13
	movq	%r8,%rcx

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rcx
	addq	%r12,%r10
	addq	%r12,%rcx

	leaq	24(%rbp),%rbp
	movq	120(%rsp),%r13
	movq	96(%rsp),%r15

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rcx
	movq	%r15,%r14
	rorq	$42,%r15

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%r15
	shrq	$6,%r14

	rorq	$19,%r15
	xorq	%r13,%r12
	xorq	%r14,%r15
	addq	56(%rsp),%r12

	addq	112(%rsp),%r12
	movq	%r10,%r13
	addq	%r15,%r12
	movq	%rcx,%r14
	rorq	$23,%r13
	movq	%r11,%r15

	xorq	%r10,%r13
	rorq	$5,%r14
	xorq	%rax,%r15

	movq	%r12,112(%rsp)
	xorq	%rcx,%r14
	andq	%r10,%r15

	rorq	$4,%r13
	addq	%rbx,%r12
	xorq	%rax,%r15

	rorq	$6,%r14
	xorq	%r10,%r13
	addq	%r15,%r12

	movq	%rcx,%r15
	addq	(%rbp),%r12
	xorq	%rcx,%r14

	xorq	%rdx,%r15
	rorq	$14,%r13
	movq	%rdx,%rbx

	andq	%r15,%rdi
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%rdi,%rbx
	addq	%r12,%r9
	addq	%r12,%rbx

	leaq	8(%rbp),%rbp
	movq	0(%rsp),%r13
	movq	104(%rsp),%rdi

	movq	%r13,%r12
	rorq	$7,%r13
	addq	%r14,%rbx
	movq	%rdi,%r14
	rorq	$42,%rdi

	xorq	%r12,%r13
	shrq	$7,%r12
	rorq	$1,%r13
	xorq	%r14,%rdi
	shrq	$6,%r14

	rorq	$19,%rdi
	xorq	%r13,%r12
	xorq	%r14,%rdi
	addq	64(%rsp),%r12

	addq	120(%rsp),%r12
	movq	%r9,%r13
	addq	%rdi,%r12
	movq	%rbx,%r14
	rorq	$23,%r13
	movq	%r10,%rdi

	xorq	%r9,%r13
	rorq	$5,%r14
	xorq	%r11,%rdi

	movq	%r12,120(%rsp)
	xorq	%rbx,%r14
	andq	%r9,%rdi

	rorq	$4,%r13
	addq	%rax,%r12
	xorq	%r11,%rdi

	rorq	$6,%r14
	xorq	%r9,%r13
	addq	%rdi,%r12

	movq	%rbx,%rdi
	addq	(%rbp),%r12
	xorq	%rbx,%r14

	xorq	%rcx,%rdi
	rorq	$14,%r13
	movq	%rcx,%rax

	andq	%rdi,%r15
	rorq	$28,%r14
	addq	%r13,%r12

	xorq	%r15,%rax
	addq	%r12,%r8
	addq	%r12,%rax

	leaq	24(%rbp),%rbp
	cmpb	$0,7(%rbp)
	jnz	.Lrounds_16_xx

	movq	128+0(%rsp),%rdi
	addq	%r14,%rax
	leaq	128(%rsi),%rsi

	addq	0(%rdi),%rax
	addq	8(%rdi),%rbx
	addq	16(%rdi),%rcx
	addq	24(%rdi),%rdx
	addq	32(%rdi),%r8
	addq	40(%rdi),%r9
	addq	48(%rdi),%r10
	addq	56(%rdi),%r11

	cmpq	128+16(%rsp),%rsi

	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)
	movq	%r8,32(%rdi)
	movq	%r9,40(%rdi)
	movq	%r10,48(%rdi)
	movq	%r11,56(%rdi)
	jb	.Lloop

	movq	152(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha512_block_data_order,.-sha512_block_data_order
.align	64
.type	K512,@object
K512:
.quad	0x428a2f98d728ae22,0x7137449123ef65cd
.quad	0x428a2f98d728ae22,0x7137449123ef65cd
.quad	0xb5c0fbcfec4d3b2f,0xe9b5dba58189dbbc
.quad	0xb5c0fbcfec4d3b2f,0xe9b5dba58189dbbc
.quad	0x3956c25bf348b538,0x59f111f1b605d019
.quad	0x3956c25bf348b538,0x59f111f1b605d019
.quad	0x923f82a4af194f9b,0xab1c5ed5da6d8118
.quad	0x923f82a4af194f9b,0xab1c5ed5da6d8118
.quad	0xd807aa98a3030242,0x12835b0145706fbe
.quad	0xd807aa98a3030242,0x12835b0145706fbe
.quad	0x243185be4ee4b28c,0x550c7dc3d5ffb4e2
.quad	0x243185be4ee4b28c,0x550c7dc3d5ffb4e2
.quad	0x72be5d74f27b896f,0x80deb1fe3b1696b1
.quad	0x72be5d74f27b896f,0x80deb1fe3b1696b1
.quad	0x9bdc06a725c71235,0xc19bf174cf692694
.quad	0x9bdc06a725c71235,0xc19bf174cf692694
.quad	0xe49b69c19ef14ad2,0xefbe4786384f25e3
.quad	0xe49b69c19ef14ad2,0xefbe4786384f25e3
.quad	0x0fc19dc68b8cd5b5,0x240ca1cc77ac9c65
.quad	0x0fc19dc68b8cd5b5,0x240ca1cc77ac9c65
.quad	0x2de92c6f592b0275,0x4a7484aa6ea6e483
.quad	0x2de92c6f592b0275,0x4a7484aa6ea6e483
.quad	0x5cb0a9dcbd41fbd4,0x76f988da831153b5
.quad	0x5cb0a9dcbd41fbd4,0x76f988da831153b5
.quad	0x983e5152ee66dfab,0xa831c66d2db43210
.quad	0x983e5152ee66dfab,0xa831c66d2db43210
.quad	0xb00327c898fb213f,0xbf597fc7beef0ee4
.quad	0xb00327c898fb213f,0xbf597fc7beef0ee4
.quad	0xc6e00bf33da88fc2,0xd5a79147930aa725
.quad	0xc6e00bf33da88fc2,0xd5a79147930aa725
.quad	0x06ca6351e003826f,0x142929670a0e6e70
.quad	0x06ca6351e003826f,0x142929670a0e6e70
.quad	0x27b70a8546d22ffc,0x2e1b21385c26c926
.quad	0x27b70a8546d22ffc,0x2e1b21385c26c926
.quad	0x4d2c6dfc5ac42aed,0x53380d139d95b3df
.quad	0x4d2c6dfc5ac42aed,0x53380d139d95b3df
.quad	0x650a73548baf63de,0x766a0abb3c77b2a8
.quad	0x650a73548baf63de,0x766a0abb3c77b2a8
.quad	0x81c2c92e47edaee6,0x92722c851482353b
.quad	0x81c2c92e47edaee6,0x92722c851482353b
.quad	0xa2bfe8a14cf10364,0xa81a664bbc423001
.quad	0xa2bfe8a14cf10364,0xa81a664bbc423001
.quad	0xc24b8b70d0f89791,0xc76c51a30654be30
.quad	0xc24b8b70d0f89791,0xc76c51a30654be30
.quad	0xd192e819d6ef5218,0xd69906245565a910
.quad	0xd192e819d6ef5218,0xd69906245565a910
.quad	0xf40e35855771202a,0x106aa07032bbd1b8
.quad	0xf40e35855771202a,0x106aa07032bbd1b8
.quad	0x19a4c116b8d2d0c8,0x1e376c085141ab53
.quad	0x19a4c116b8d2d0c8,0x1e376c085141ab53
.quad	0x2748774cdf8eeb99,0x34b0bcb5e19b48a8
.quad	0x2748774cdf8eeb99,0x34b0bcb5e19b48a8
.quad	0x391c0cb3c5c95a63,0x4ed8aa4ae3418acb
.quad	0x391c0cb3c5c95a63,0x4ed8aa4ae3418acb
.quad	0x5b9cca4f7763e373,0x682e6ff3d6b2b8a3
.quad	0x5b9cca4f7763e373,0x682e6ff3d6b2b8a3
.quad	0x748f82ee5defb2fc,0x78a5636f43172f60
.quad	0x748f82ee5defb2fc,0x78a5636f43172f60
.quad	0x84c87814a1f0ab72,0x8cc702081a6439ec
.quad	0x84c87814a1f0ab72,0x8cc702081a6439ec
.quad	0x90befffa23631e28,0xa4506cebde82bde9
.quad	0x90befffa23631e28,0xa4506cebde82bde9
.quad	0xbef9a3f7b2c67915,0xc67178f2e372532b
.quad	0xbef9a3f7b2c67915,0xc67178f2e372532b
.quad	0xca273eceea26619c,0xd186b8c721c0c207
.quad	0xca273eceea26619c,0xd186b8c721c0c207
.quad	0xeada7dd6cde0eb1e,0xf57d4f7fee6ed178
.quad	0xeada7dd6cde0eb1e,0xf57d4f7fee6ed178
.quad	0x06f067aa72176fba,0x0a637dc5a2c898a6
.quad	0x06f067aa72176fba,0x0a637dc5a2c898a6
.quad	0x113f9804bef90dae,0x1b710b35131c471b
.quad	0x113f9804bef90dae,0x1b710b35131c471b
.quad	0x28db77f523047d84,0x32caab7b40c72493
.quad	0x28db77f523047d84,0x32caab7b40c72493
.quad	0x3c9ebe0a15c9bebc,0x431d67c49c100d4c
.quad	0x3c9ebe0a15c9bebc,0x431d67c49c100d4c
.quad	0x4cc5d4becb3e42b6,0x597f299cfc657e2a
.quad	0x4cc5d4becb3e42b6,0x597f299cfc657e2a
.quad	0x5fcb6fab3ad6faec,0x6c44198c4a475817
.quad	0x5fcb6fab3ad6faec,0x6c44198c4a475817

.quad	0x0001020304050607,0x08090a0b0c0d0e0f
.quad	0x0001020304050607,0x08090a0b0c0d0e0f
.byte	83,72,65,53,49,50,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.type	sha512_block_data_order_xop,@function
.align	64
sha512_block_data_order_xop:
.cfi_startproc	
.Lxop_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$160,%rsp
	leaq	(%rsi,%rdx,8),%rdx
	andq	$-64,%rsp
	movq	%rdi,128+0(%rsp)
	movq	%rsi,128+8(%rsp)
	movq	%rdx,128+16(%rsp)
	movq	%rax,152(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
.Lprologue_xop:

	vzeroupper
	movq	0(%rdi),%rax
	movq	8(%rdi),%rbx
	movq	16(%rdi),%rcx
	movq	24(%rdi),%rdx
	movq	32(%rdi),%r8
	movq	40(%rdi),%r9
	movq	48(%rdi),%r10
	movq	56(%rdi),%r11
	jmp	.Lloop_xop
.align	16
.Lloop_xop:
	vmovdqa	K512+1280(%rip),%xmm11
	vmovdqu	0(%rsi),%xmm0
	leaq	K512+128(%rip),%rbp
	vmovdqu	16(%rsi),%xmm1
	vmovdqu	32(%rsi),%xmm2
	vpshufb	%xmm11,%xmm0,%xmm0
	vmovdqu	48(%rsi),%xmm3
	vpshufb	%xmm11,%xmm1,%xmm1
	vmovdqu	64(%rsi),%xmm4
	vpshufb	%xmm11,%xmm2,%xmm2
	vmovdqu	80(%rsi),%xmm5
	vpshufb	%xmm11,%xmm3,%xmm3
	vmovdqu	96(%rsi),%xmm6
	vpshufb	%xmm11,%xmm4,%xmm4
	vmovdqu	112(%rsi),%xmm7
	vpshufb	%xmm11,%xmm5,%xmm5
	vpaddq	-128(%rbp),%xmm0,%xmm8
	vpshufb	%xmm11,%xmm6,%xmm6
	vpaddq	-96(%rbp),%xmm1,%xmm9
	vpshufb	%xmm11,%xmm7,%xmm7
	vpaddq	-64(%rbp),%xmm2,%xmm10
	vpaddq	-32(%rbp),%xmm3,%xmm11
	vmovdqa	%xmm8,0(%rsp)
	vpaddq	0(%rbp),%xmm4,%xmm8
	vmovdqa	%xmm9,16(%rsp)
	vpaddq	32(%rbp),%xmm5,%xmm9
	vmovdqa	%xmm10,32(%rsp)
	vpaddq	64(%rbp),%xmm6,%xmm10
	vmovdqa	%xmm11,48(%rsp)
	vpaddq	96(%rbp),%xmm7,%xmm11
	vmovdqa	%xmm8,64(%rsp)
	movq	%rax,%r14
	vmovdqa	%xmm9,80(%rsp)
	movq	%rbx,%rdi
	vmovdqa	%xmm10,96(%rsp)
	xorq	%rcx,%rdi
	vmovdqa	%xmm11,112(%rsp)
	movq	%r8,%r13
	jmp	.Lxop_00_47

.align	16
.Lxop_00_47:
	addq	$256,%rbp
	vpalignr	$8,%xmm0,%xmm1,%xmm8
	rorq	$23,%r13
	movq	%r14,%rax
	vpalignr	$8,%xmm4,%xmm5,%xmm11
	movq	%r9,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%r8,%r13
	xorq	%r10,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%rax,%r14
	vpaddq	%xmm11,%xmm0,%xmm0
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	0(%rsp),%r11
	movq	%rax,%r15
.byte	143,72,120,195,209,7
	xorq	%r10,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%rbx,%r15
	addq	%r12,%r11
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,223,3
	xorq	%rax,%r14
	addq	%r13,%r11
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rbx,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm7,%xmm10
	addq	%r11,%rdx
	addq	%rdi,%r11
	vpaddq	%xmm8,%xmm0,%xmm0
	movq	%rdx,%r13
	addq	%r11,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%r11
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%r8,%r12
	rorq	$5,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	vpaddq	%xmm11,%xmm0,%xmm0
	addq	8(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	rorq	$6,%r14
	vpaddq	-128(%rbp),%xmm0,%xmm10
	xorq	%rax,%rdi
	addq	%r12,%r10
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	rorq	$28,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	vmovdqa	%xmm10,0(%rsp)
	vpalignr	$8,%xmm1,%xmm2,%xmm8
	rorq	$23,%r13
	movq	%r14,%r10
	vpalignr	$8,%xmm5,%xmm6,%xmm11
	movq	%rdx,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%rcx,%r13
	xorq	%r8,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%r10,%r14
	vpaddq	%xmm11,%xmm1,%xmm1
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	16(%rsp),%r9
	movq	%r10,%r15
.byte	143,72,120,195,209,7
	xorq	%r8,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%r11,%r15
	addq	%r12,%r9
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,216,3
	xorq	%r10,%r14
	addq	%r13,%r9
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r11,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm0,%xmm10
	addq	%r9,%rbx
	addq	%rdi,%r9
	vpaddq	%xmm8,%xmm1,%xmm1
	movq	%rbx,%r13
	addq	%r9,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%r9
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%rcx,%r12
	rorq	$5,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	vpaddq	%xmm11,%xmm1,%xmm1
	addq	24(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	rorq	$6,%r14
	vpaddq	-96(%rbp),%xmm1,%xmm10
	xorq	%r10,%rdi
	addq	%r12,%r8
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	rorq	$28,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	vmovdqa	%xmm10,16(%rsp)
	vpalignr	$8,%xmm2,%xmm3,%xmm8
	rorq	$23,%r13
	movq	%r14,%r8
	vpalignr	$8,%xmm6,%xmm7,%xmm11
	movq	%rbx,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%rax,%r13
	xorq	%rcx,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%r8,%r14
	vpaddq	%xmm11,%xmm2,%xmm2
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	32(%rsp),%rdx
	movq	%r8,%r15
.byte	143,72,120,195,209,7
	xorq	%rcx,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%r9,%r15
	addq	%r12,%rdx
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,217,3
	xorq	%r8,%r14
	addq	%r13,%rdx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r9,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm1,%xmm10
	addq	%rdx,%r11
	addq	%rdi,%rdx
	vpaddq	%xmm8,%xmm2,%xmm2
	movq	%r11,%r13
	addq	%rdx,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%rdx
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%rax,%r12
	rorq	$5,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	vpaddq	%xmm11,%xmm2,%xmm2
	addq	40(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	rorq	$6,%r14
	vpaddq	-64(%rbp),%xmm2,%xmm10
	xorq	%r8,%rdi
	addq	%r12,%rcx
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	rorq	$28,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	vmovdqa	%xmm10,32(%rsp)
	vpalignr	$8,%xmm3,%xmm4,%xmm8
	rorq	$23,%r13
	movq	%r14,%rcx
	vpalignr	$8,%xmm7,%xmm0,%xmm11
	movq	%r11,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%r10,%r13
	xorq	%rax,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%rcx,%r14
	vpaddq	%xmm11,%xmm3,%xmm3
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	48(%rsp),%rbx
	movq	%rcx,%r15
.byte	143,72,120,195,209,7
	xorq	%rax,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%rdx,%r15
	addq	%r12,%rbx
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,218,3
	xorq	%rcx,%r14
	addq	%r13,%rbx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rdx,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm2,%xmm10
	addq	%rbx,%r9
	addq	%rdi,%rbx
	vpaddq	%xmm8,%xmm3,%xmm3
	movq	%r9,%r13
	addq	%rbx,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%rbx
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%r10,%r12
	rorq	$5,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	vpaddq	%xmm11,%xmm3,%xmm3
	addq	56(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	rorq	$6,%r14
	vpaddq	-32(%rbp),%xmm3,%xmm10
	xorq	%rcx,%rdi
	addq	%r12,%rax
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	rorq	$28,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	vmovdqa	%xmm10,48(%rsp)
	vpalignr	$8,%xmm4,%xmm5,%xmm8
	rorq	$23,%r13
	movq	%r14,%rax
	vpalignr	$8,%xmm0,%xmm1,%xmm11
	movq	%r9,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%r8,%r13
	xorq	%r10,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%rax,%r14
	vpaddq	%xmm11,%xmm4,%xmm4
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	64(%rsp),%r11
	movq	%rax,%r15
.byte	143,72,120,195,209,7
	xorq	%r10,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%rbx,%r15
	addq	%r12,%r11
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,219,3
	xorq	%rax,%r14
	addq	%r13,%r11
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rbx,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm3,%xmm10
	addq	%r11,%rdx
	addq	%rdi,%r11
	vpaddq	%xmm8,%xmm4,%xmm4
	movq	%rdx,%r13
	addq	%r11,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%r11
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%r8,%r12
	rorq	$5,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	vpaddq	%xmm11,%xmm4,%xmm4
	addq	72(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	rorq	$6,%r14
	vpaddq	0(%rbp),%xmm4,%xmm10
	xorq	%rax,%rdi
	addq	%r12,%r10
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	rorq	$28,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	vmovdqa	%xmm10,64(%rsp)
	vpalignr	$8,%xmm5,%xmm6,%xmm8
	rorq	$23,%r13
	movq	%r14,%r10
	vpalignr	$8,%xmm1,%xmm2,%xmm11
	movq	%rdx,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%rcx,%r13
	xorq	%r8,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%r10,%r14
	vpaddq	%xmm11,%xmm5,%xmm5
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	80(%rsp),%r9
	movq	%r10,%r15
.byte	143,72,120,195,209,7
	xorq	%r8,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%r11,%r15
	addq	%r12,%r9
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,220,3
	xorq	%r10,%r14
	addq	%r13,%r9
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r11,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm4,%xmm10
	addq	%r9,%rbx
	addq	%rdi,%r9
	vpaddq	%xmm8,%xmm5,%xmm5
	movq	%rbx,%r13
	addq	%r9,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%r9
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%rcx,%r12
	rorq	$5,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	vpaddq	%xmm11,%xmm5,%xmm5
	addq	88(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	rorq	$6,%r14
	vpaddq	32(%rbp),%xmm5,%xmm10
	xorq	%r10,%rdi
	addq	%r12,%r8
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	rorq	$28,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	vmovdqa	%xmm10,80(%rsp)
	vpalignr	$8,%xmm6,%xmm7,%xmm8
	rorq	$23,%r13
	movq	%r14,%r8
	vpalignr	$8,%xmm2,%xmm3,%xmm11
	movq	%rbx,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%rax,%r13
	xorq	%rcx,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%r8,%r14
	vpaddq	%xmm11,%xmm6,%xmm6
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	96(%rsp),%rdx
	movq	%r8,%r15
.byte	143,72,120,195,209,7
	xorq	%rcx,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%r9,%r15
	addq	%r12,%rdx
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,221,3
	xorq	%r8,%r14
	addq	%r13,%rdx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r9,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm5,%xmm10
	addq	%rdx,%r11
	addq	%rdi,%rdx
	vpaddq	%xmm8,%xmm6,%xmm6
	movq	%r11,%r13
	addq	%rdx,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%rdx
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%rax,%r12
	rorq	$5,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	vpaddq	%xmm11,%xmm6,%xmm6
	addq	104(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	rorq	$6,%r14
	vpaddq	64(%rbp),%xmm6,%xmm10
	xorq	%r8,%rdi
	addq	%r12,%rcx
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	rorq	$28,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	vmovdqa	%xmm10,96(%rsp)
	vpalignr	$8,%xmm7,%xmm0,%xmm8
	rorq	$23,%r13
	movq	%r14,%rcx
	vpalignr	$8,%xmm3,%xmm4,%xmm11
	movq	%r11,%r12
	rorq	$5,%r14
.byte	143,72,120,195,200,56
	xorq	%r10,%r13
	xorq	%rax,%r12
	vpsrlq	$7,%xmm8,%xmm8
	rorq	$4,%r13
	xorq	%rcx,%r14
	vpaddq	%xmm11,%xmm7,%xmm7
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	112(%rsp),%rbx
	movq	%rcx,%r15
.byte	143,72,120,195,209,7
	xorq	%rax,%r12
	rorq	$6,%r14
	vpxor	%xmm9,%xmm8,%xmm8
	xorq	%rdx,%r15
	addq	%r12,%rbx
	rorq	$14,%r13
	andq	%r15,%rdi
.byte	143,104,120,195,222,3
	xorq	%rcx,%r14
	addq	%r13,%rbx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rdx,%rdi
	rorq	$28,%r14
	vpsrlq	$6,%xmm6,%xmm10
	addq	%rbx,%r9
	addq	%rdi,%rbx
	vpaddq	%xmm8,%xmm7,%xmm7
	movq	%r9,%r13
	addq	%rbx,%r14
.byte	143,72,120,195,203,42
	rorq	$23,%r13
	movq	%r14,%rbx
	vpxor	%xmm10,%xmm11,%xmm11
	movq	%r10,%r12
	rorq	$5,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	vpxor	%xmm9,%xmm11,%xmm11
	rorq	$4,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	vpaddq	%xmm11,%xmm7,%xmm7
	addq	120(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	rorq	$6,%r14
	vpaddq	96(%rbp),%xmm7,%xmm10
	xorq	%rcx,%rdi
	addq	%r12,%rax
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	rorq	$28,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	vmovdqa	%xmm10,112(%rsp)
	cmpb	$0,135(%rbp)
	jne	.Lxop_00_47
	rorq	$23,%r13
	movq	%r14,%rax
	movq	%r9,%r12
	rorq	$5,%r14
	xorq	%r8,%r13
	xorq	%r10,%r12
	rorq	$4,%r13
	xorq	%rax,%r14
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	0(%rsp),%r11
	movq	%rax,%r15
	xorq	%r10,%r12
	rorq	$6,%r14
	xorq	%rbx,%r15
	addq	%r12,%r11
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%rax,%r14
	addq	%r13,%r11
	xorq	%rbx,%rdi
	rorq	$28,%r14
	addq	%r11,%rdx
	addq	%rdi,%r11
	movq	%rdx,%r13
	addq	%r11,%r14
	rorq	$23,%r13
	movq	%r14,%r11
	movq	%r8,%r12
	rorq	$5,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	rorq	$4,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	addq	8(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	rorq	$6,%r14
	xorq	%rax,%rdi
	addq	%r12,%r10
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	rorq	$28,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	rorq	$23,%r13
	movq	%r14,%r10
	movq	%rdx,%r12
	rorq	$5,%r14
	xorq	%rcx,%r13
	xorq	%r8,%r12
	rorq	$4,%r13
	xorq	%r10,%r14
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	16(%rsp),%r9
	movq	%r10,%r15
	xorq	%r8,%r12
	rorq	$6,%r14
	xorq	%r11,%r15
	addq	%r12,%r9
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%r10,%r14
	addq	%r13,%r9
	xorq	%r11,%rdi
	rorq	$28,%r14
	addq	%r9,%rbx
	addq	%rdi,%r9
	movq	%rbx,%r13
	addq	%r9,%r14
	rorq	$23,%r13
	movq	%r14,%r9
	movq	%rcx,%r12
	rorq	$5,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	rorq	$4,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	addq	24(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	rorq	$6,%r14
	xorq	%r10,%rdi
	addq	%r12,%r8
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	rorq	$28,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	rorq	$23,%r13
	movq	%r14,%r8
	movq	%rbx,%r12
	rorq	$5,%r14
	xorq	%rax,%r13
	xorq	%rcx,%r12
	rorq	$4,%r13
	xorq	%r8,%r14
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	32(%rsp),%rdx
	movq	%r8,%r15
	xorq	%rcx,%r12
	rorq	$6,%r14
	xorq	%r9,%r15
	addq	%r12,%rdx
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%r8,%r14
	addq	%r13,%rdx
	xorq	%r9,%rdi
	rorq	$28,%r14
	addq	%rdx,%r11
	addq	%rdi,%rdx
	movq	%r11,%r13
	addq	%rdx,%r14
	rorq	$23,%r13
	movq	%r14,%rdx
	movq	%rax,%r12
	rorq	$5,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	rorq	$4,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	addq	40(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	rorq	$6,%r14
	xorq	%r8,%rdi
	addq	%r12,%rcx
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	rorq	$28,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	rorq	$23,%r13
	movq	%r14,%rcx
	movq	%r11,%r12
	rorq	$5,%r14
	xorq	%r10,%r13
	xorq	%rax,%r12
	rorq	$4,%r13
	xorq	%rcx,%r14
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	48(%rsp),%rbx
	movq	%rcx,%r15
	xorq	%rax,%r12
	rorq	$6,%r14
	xorq	%rdx,%r15
	addq	%r12,%rbx
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%rcx,%r14
	addq	%r13,%rbx
	xorq	%rdx,%rdi
	rorq	$28,%r14
	addq	%rbx,%r9
	addq	%rdi,%rbx
	movq	%r9,%r13
	addq	%rbx,%r14
	rorq	$23,%r13
	movq	%r14,%rbx
	movq	%r10,%r12
	rorq	$5,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	rorq	$4,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	addq	56(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	rorq	$6,%r14
	xorq	%rcx,%rdi
	addq	%r12,%rax
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	rorq	$28,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	rorq	$23,%r13
	movq	%r14,%rax
	movq	%r9,%r12
	rorq	$5,%r14
	xorq	%r8,%r13
	xorq	%r10,%r12
	rorq	$4,%r13
	xorq	%rax,%r14
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	64(%rsp),%r11
	movq	%rax,%r15
	xorq	%r10,%r12
	rorq	$6,%r14
	xorq	%rbx,%r15
	addq	%r12,%r11
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%rax,%r14
	addq	%r13,%r11
	xorq	%rbx,%rdi
	rorq	$28,%r14
	addq	%r11,%rdx
	addq	%rdi,%r11
	movq	%rdx,%r13
	addq	%r11,%r14
	rorq	$23,%r13
	movq	%r14,%r11
	movq	%r8,%r12
	rorq	$5,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	rorq	$4,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	addq	72(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	rorq	$6,%r14
	xorq	%rax,%rdi
	addq	%r12,%r10
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	rorq	$28,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	rorq	$23,%r13
	movq	%r14,%r10
	movq	%rdx,%r12
	rorq	$5,%r14
	xorq	%rcx,%r13
	xorq	%r8,%r12
	rorq	$4,%r13
	xorq	%r10,%r14
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	80(%rsp),%r9
	movq	%r10,%r15
	xorq	%r8,%r12
	rorq	$6,%r14
	xorq	%r11,%r15
	addq	%r12,%r9
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%r10,%r14
	addq	%r13,%r9
	xorq	%r11,%rdi
	rorq	$28,%r14
	addq	%r9,%rbx
	addq	%rdi,%r9
	movq	%rbx,%r13
	addq	%r9,%r14
	rorq	$23,%r13
	movq	%r14,%r9
	movq	%rcx,%r12
	rorq	$5,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	rorq	$4,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	addq	88(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	rorq	$6,%r14
	xorq	%r10,%rdi
	addq	%r12,%r8
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	rorq	$28,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	rorq	$23,%r13
	movq	%r14,%r8
	movq	%rbx,%r12
	rorq	$5,%r14
	xorq	%rax,%r13
	xorq	%rcx,%r12
	rorq	$4,%r13
	xorq	%r8,%r14
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	96(%rsp),%rdx
	movq	%r8,%r15
	xorq	%rcx,%r12
	rorq	$6,%r14
	xorq	%r9,%r15
	addq	%r12,%rdx
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%r8,%r14
	addq	%r13,%rdx
	xorq	%r9,%rdi
	rorq	$28,%r14
	addq	%rdx,%r11
	addq	%rdi,%rdx
	movq	%r11,%r13
	addq	%rdx,%r14
	rorq	$23,%r13
	movq	%r14,%rdx
	movq	%rax,%r12
	rorq	$5,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	rorq	$4,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	addq	104(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	rorq	$6,%r14
	xorq	%r8,%rdi
	addq	%r12,%rcx
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	rorq	$28,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	rorq	$23,%r13
	movq	%r14,%rcx
	movq	%r11,%r12
	rorq	$5,%r14
	xorq	%r10,%r13
	xorq	%rax,%r12
	rorq	$4,%r13
	xorq	%rcx,%r14
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	112(%rsp),%rbx
	movq	%rcx,%r15
	xorq	%rax,%r12
	rorq	$6,%r14
	xorq	%rdx,%r15
	addq	%r12,%rbx
	rorq	$14,%r13
	andq	%r15,%rdi
	xorq	%rcx,%r14
	addq	%r13,%rbx
	xorq	%rdx,%rdi
	rorq	$28,%r14
	addq	%rbx,%r9
	addq	%rdi,%rbx
	movq	%r9,%r13
	addq	%rbx,%r14
	rorq	$23,%r13
	movq	%r14,%rbx
	movq	%r10,%r12
	rorq	$5,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	rorq	$4,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	addq	120(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	rorq	$6,%r14
	xorq	%rcx,%rdi
	addq	%r12,%rax
	rorq	$14,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	rorq	$28,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	movq	128+0(%rsp),%rdi
	movq	%r14,%rax

	addq	0(%rdi),%rax
	leaq	128(%rsi),%rsi
	addq	8(%rdi),%rbx
	addq	16(%rdi),%rcx
	addq	24(%rdi),%rdx
	addq	32(%rdi),%r8
	addq	40(%rdi),%r9
	addq	48(%rdi),%r10
	addq	56(%rdi),%r11

	cmpq	128+16(%rsp),%rsi

	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)
	movq	%r8,32(%rdi)
	movq	%r9,40(%rdi)
	movq	%r10,48(%rdi)
	movq	%r11,56(%rdi)
	jb	.Lloop_xop

	movq	152(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	vzeroupper
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_xop:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha512_block_data_order_xop,.-sha512_block_data_order_xop
.type	sha512_block_data_order_avx,@function
.align	64
sha512_block_data_order_avx:
.cfi_startproc	
.Lavx_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	shlq	$4,%rdx
	subq	$160,%rsp
	leaq	(%rsi,%rdx,8),%rdx
	andq	$-64,%rsp
	movq	%rdi,128+0(%rsp)
	movq	%rsi,128+8(%rsp)
	movq	%rdx,128+16(%rsp)
	movq	%rax,152(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
.Lprologue_avx:

	vzeroupper
	movq	0(%rdi),%rax
	movq	8(%rdi),%rbx
	movq	16(%rdi),%rcx
	movq	24(%rdi),%rdx
	movq	32(%rdi),%r8
	movq	40(%rdi),%r9
	movq	48(%rdi),%r10
	movq	56(%rdi),%r11
	jmp	.Lloop_avx
.align	16
.Lloop_avx:
	vmovdqa	K512+1280(%rip),%xmm11
	vmovdqu	0(%rsi),%xmm0
	leaq	K512+128(%rip),%rbp
	vmovdqu	16(%rsi),%xmm1
	vmovdqu	32(%rsi),%xmm2
	vpshufb	%xmm11,%xmm0,%xmm0
	vmovdqu	48(%rsi),%xmm3
	vpshufb	%xmm11,%xmm1,%xmm1
	vmovdqu	64(%rsi),%xmm4
	vpshufb	%xmm11,%xmm2,%xmm2
	vmovdqu	80(%rsi),%xmm5
	vpshufb	%xmm11,%xmm3,%xmm3
	vmovdqu	96(%rsi),%xmm6
	vpshufb	%xmm11,%xmm4,%xmm4
	vmovdqu	112(%rsi),%xmm7
	vpshufb	%xmm11,%xmm5,%xmm5
	vpaddq	-128(%rbp),%xmm0,%xmm8
	vpshufb	%xmm11,%xmm6,%xmm6
	vpaddq	-96(%rbp),%xmm1,%xmm9
	vpshufb	%xmm11,%xmm7,%xmm7
	vpaddq	-64(%rbp),%xmm2,%xmm10
	vpaddq	-32(%rbp),%xmm3,%xmm11
	vmovdqa	%xmm8,0(%rsp)
	vpaddq	0(%rbp),%xmm4,%xmm8
	vmovdqa	%xmm9,16(%rsp)
	vpaddq	32(%rbp),%xmm5,%xmm9
	vmovdqa	%xmm10,32(%rsp)
	vpaddq	64(%rbp),%xmm6,%xmm10
	vmovdqa	%xmm11,48(%rsp)
	vpaddq	96(%rbp),%xmm7,%xmm11
	vmovdqa	%xmm8,64(%rsp)
	movq	%rax,%r14
	vmovdqa	%xmm9,80(%rsp)
	movq	%rbx,%rdi
	vmovdqa	%xmm10,96(%rsp)
	xorq	%rcx,%rdi
	vmovdqa	%xmm11,112(%rsp)
	movq	%r8,%r13
	jmp	.Lavx_00_47

.align	16
.Lavx_00_47:
	addq	$256,%rbp
	vpalignr	$8,%xmm0,%xmm1,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%rax
	vpalignr	$8,%xmm4,%xmm5,%xmm11
	movq	%r9,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%r8,%r13
	xorq	%r10,%r12
	vpaddq	%xmm11,%xmm0,%xmm0
	shrdq	$4,%r13,%r13
	xorq	%rax,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%r8,%r12
	xorq	%r8,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	0(%rsp),%r11
	movq	%rax,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%r10,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%rbx,%r15
	addq	%r12,%r11
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%rax,%r14
	addq	%r13,%r11
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rbx,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm7,%xmm11
	addq	%r11,%rdx
	addq	%rdi,%r11
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%rdx,%r13
	addq	%r11,%r14
	vpsllq	$3,%xmm7,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%r11
	vpaddq	%xmm8,%xmm0,%xmm0
	movq	%r8,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm7,%xmm9
	xorq	%rdx,%r13
	xorq	%r9,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%r11,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%rdx,%r12
	xorq	%rdx,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	8(%rsp),%r10
	movq	%r11,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%r9,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%rax,%rdi
	addq	%r12,%r10
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm0,%xmm0
	xorq	%r11,%r14
	addq	%r13,%r10
	vpaddq	-128(%rbp),%xmm0,%xmm10
	xorq	%rax,%r15
	shrdq	$28,%r14,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	vmovdqa	%xmm10,0(%rsp)
	vpalignr	$8,%xmm1,%xmm2,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%r10
	vpalignr	$8,%xmm5,%xmm6,%xmm11
	movq	%rdx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%rcx,%r13
	xorq	%r8,%r12
	vpaddq	%xmm11,%xmm1,%xmm1
	shrdq	$4,%r13,%r13
	xorq	%r10,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%rcx,%r12
	xorq	%rcx,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	16(%rsp),%r9
	movq	%r10,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%r8,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%r11,%r15
	addq	%r12,%r9
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%r10,%r14
	addq	%r13,%r9
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r11,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm0,%xmm11
	addq	%r9,%rbx
	addq	%rdi,%r9
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%rbx,%r13
	addq	%r9,%r14
	vpsllq	$3,%xmm0,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%r9
	vpaddq	%xmm8,%xmm1,%xmm1
	movq	%rcx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm0,%xmm9
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%r9,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%rbx,%r12
	xorq	%rbx,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	24(%rsp),%r8
	movq	%r9,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%rdx,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%r10,%rdi
	addq	%r12,%r8
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm1,%xmm1
	xorq	%r9,%r14
	addq	%r13,%r8
	vpaddq	-96(%rbp),%xmm1,%xmm10
	xorq	%r10,%r15
	shrdq	$28,%r14,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	vmovdqa	%xmm10,16(%rsp)
	vpalignr	$8,%xmm2,%xmm3,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%r8
	vpalignr	$8,%xmm6,%xmm7,%xmm11
	movq	%rbx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%rax,%r13
	xorq	%rcx,%r12
	vpaddq	%xmm11,%xmm2,%xmm2
	shrdq	$4,%r13,%r13
	xorq	%r8,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%rax,%r12
	xorq	%rax,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	32(%rsp),%rdx
	movq	%r8,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%rcx,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%r9,%r15
	addq	%r12,%rdx
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%r8,%r14
	addq	%r13,%rdx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r9,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm1,%xmm11
	addq	%rdx,%r11
	addq	%rdi,%rdx
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%r11,%r13
	addq	%rdx,%r14
	vpsllq	$3,%xmm1,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%rdx
	vpaddq	%xmm8,%xmm2,%xmm2
	movq	%rax,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm1,%xmm9
	xorq	%r11,%r13
	xorq	%rbx,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%rdx,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%r11,%r12
	xorq	%r11,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	40(%rsp),%rcx
	movq	%rdx,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%rbx,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%r8,%rdi
	addq	%r12,%rcx
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm2,%xmm2
	xorq	%rdx,%r14
	addq	%r13,%rcx
	vpaddq	-64(%rbp),%xmm2,%xmm10
	xorq	%r8,%r15
	shrdq	$28,%r14,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	vmovdqa	%xmm10,32(%rsp)
	vpalignr	$8,%xmm3,%xmm4,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%rcx
	vpalignr	$8,%xmm7,%xmm0,%xmm11
	movq	%r11,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%r10,%r13
	xorq	%rax,%r12
	vpaddq	%xmm11,%xmm3,%xmm3
	shrdq	$4,%r13,%r13
	xorq	%rcx,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%r10,%r12
	xorq	%r10,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	48(%rsp),%rbx
	movq	%rcx,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%rax,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%rdx,%r15
	addq	%r12,%rbx
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%rcx,%r14
	addq	%r13,%rbx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rdx,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm2,%xmm11
	addq	%rbx,%r9
	addq	%rdi,%rbx
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%r9,%r13
	addq	%rbx,%r14
	vpsllq	$3,%xmm2,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%rbx
	vpaddq	%xmm8,%xmm3,%xmm3
	movq	%r10,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm2,%xmm9
	xorq	%r9,%r13
	xorq	%r11,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%rbx,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%r9,%r12
	xorq	%r9,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	56(%rsp),%rax
	movq	%rbx,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%r11,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%rcx,%rdi
	addq	%r12,%rax
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm3,%xmm3
	xorq	%rbx,%r14
	addq	%r13,%rax
	vpaddq	-32(%rbp),%xmm3,%xmm10
	xorq	%rcx,%r15
	shrdq	$28,%r14,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	vmovdqa	%xmm10,48(%rsp)
	vpalignr	$8,%xmm4,%xmm5,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%rax
	vpalignr	$8,%xmm0,%xmm1,%xmm11
	movq	%r9,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%r8,%r13
	xorq	%r10,%r12
	vpaddq	%xmm11,%xmm4,%xmm4
	shrdq	$4,%r13,%r13
	xorq	%rax,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%r8,%r12
	xorq	%r8,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	64(%rsp),%r11
	movq	%rax,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%r10,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%rbx,%r15
	addq	%r12,%r11
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%rax,%r14
	addq	%r13,%r11
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rbx,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm3,%xmm11
	addq	%r11,%rdx
	addq	%rdi,%r11
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%rdx,%r13
	addq	%r11,%r14
	vpsllq	$3,%xmm3,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%r11
	vpaddq	%xmm8,%xmm4,%xmm4
	movq	%r8,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm3,%xmm9
	xorq	%rdx,%r13
	xorq	%r9,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%r11,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%rdx,%r12
	xorq	%rdx,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	72(%rsp),%r10
	movq	%r11,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%r9,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%rax,%rdi
	addq	%r12,%r10
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm4,%xmm4
	xorq	%r11,%r14
	addq	%r13,%r10
	vpaddq	0(%rbp),%xmm4,%xmm10
	xorq	%rax,%r15
	shrdq	$28,%r14,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	vmovdqa	%xmm10,64(%rsp)
	vpalignr	$8,%xmm5,%xmm6,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%r10
	vpalignr	$8,%xmm1,%xmm2,%xmm11
	movq	%rdx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%rcx,%r13
	xorq	%r8,%r12
	vpaddq	%xmm11,%xmm5,%xmm5
	shrdq	$4,%r13,%r13
	xorq	%r10,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%rcx,%r12
	xorq	%rcx,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	80(%rsp),%r9
	movq	%r10,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%r8,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%r11,%r15
	addq	%r12,%r9
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%r10,%r14
	addq	%r13,%r9
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r11,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm4,%xmm11
	addq	%r9,%rbx
	addq	%rdi,%r9
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%rbx,%r13
	addq	%r9,%r14
	vpsllq	$3,%xmm4,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%r9
	vpaddq	%xmm8,%xmm5,%xmm5
	movq	%rcx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm4,%xmm9
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%r9,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%rbx,%r12
	xorq	%rbx,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	88(%rsp),%r8
	movq	%r9,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%rdx,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%r10,%rdi
	addq	%r12,%r8
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm5,%xmm5
	xorq	%r9,%r14
	addq	%r13,%r8
	vpaddq	32(%rbp),%xmm5,%xmm10
	xorq	%r10,%r15
	shrdq	$28,%r14,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	vmovdqa	%xmm10,80(%rsp)
	vpalignr	$8,%xmm6,%xmm7,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%r8
	vpalignr	$8,%xmm2,%xmm3,%xmm11
	movq	%rbx,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%rax,%r13
	xorq	%rcx,%r12
	vpaddq	%xmm11,%xmm6,%xmm6
	shrdq	$4,%r13,%r13
	xorq	%r8,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%rax,%r12
	xorq	%rax,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	96(%rsp),%rdx
	movq	%r8,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%rcx,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%r9,%r15
	addq	%r12,%rdx
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%r8,%r14
	addq	%r13,%rdx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%r9,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm5,%xmm11
	addq	%rdx,%r11
	addq	%rdi,%rdx
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%r11,%r13
	addq	%rdx,%r14
	vpsllq	$3,%xmm5,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%rdx
	vpaddq	%xmm8,%xmm6,%xmm6
	movq	%rax,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm5,%xmm9
	xorq	%r11,%r13
	xorq	%rbx,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%rdx,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%r11,%r12
	xorq	%r11,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	104(%rsp),%rcx
	movq	%rdx,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%rbx,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%r8,%rdi
	addq	%r12,%rcx
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm6,%xmm6
	xorq	%rdx,%r14
	addq	%r13,%rcx
	vpaddq	64(%rbp),%xmm6,%xmm10
	xorq	%r8,%r15
	shrdq	$28,%r14,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	vmovdqa	%xmm10,96(%rsp)
	vpalignr	$8,%xmm7,%xmm0,%xmm8
	shrdq	$23,%r13,%r13
	movq	%r14,%rcx
	vpalignr	$8,%xmm3,%xmm4,%xmm11
	movq	%r11,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$1,%xmm8,%xmm10
	xorq	%r10,%r13
	xorq	%rax,%r12
	vpaddq	%xmm11,%xmm7,%xmm7
	shrdq	$4,%r13,%r13
	xorq	%rcx,%r14
	vpsrlq	$7,%xmm8,%xmm11
	andq	%r10,%r12
	xorq	%r10,%r13
	vpsllq	$56,%xmm8,%xmm9
	addq	112(%rsp),%rbx
	movq	%rcx,%r15
	vpxor	%xmm10,%xmm11,%xmm8
	xorq	%rax,%r12
	shrdq	$6,%r14,%r14
	vpsrlq	$7,%xmm10,%xmm10
	xorq	%rdx,%r15
	addq	%r12,%rbx
	vpxor	%xmm9,%xmm8,%xmm8
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	vpsllq	$7,%xmm9,%xmm9
	xorq	%rcx,%r14
	addq	%r13,%rbx
	vpxor	%xmm10,%xmm8,%xmm8
	xorq	%rdx,%rdi
	shrdq	$28,%r14,%r14
	vpsrlq	$6,%xmm6,%xmm11
	addq	%rbx,%r9
	addq	%rdi,%rbx
	vpxor	%xmm9,%xmm8,%xmm8
	movq	%r9,%r13
	addq	%rbx,%r14
	vpsllq	$3,%xmm6,%xmm10
	shrdq	$23,%r13,%r13
	movq	%r14,%rbx
	vpaddq	%xmm8,%xmm7,%xmm7
	movq	%r10,%r12
	shrdq	$5,%r14,%r14
	vpsrlq	$19,%xmm6,%xmm9
	xorq	%r9,%r13
	xorq	%r11,%r12
	vpxor	%xmm10,%xmm11,%xmm11
	shrdq	$4,%r13,%r13
	xorq	%rbx,%r14
	vpsllq	$42,%xmm10,%xmm10
	andq	%r9,%r12
	xorq	%r9,%r13
	vpxor	%xmm9,%xmm11,%xmm11
	addq	120(%rsp),%rax
	movq	%rbx,%rdi
	vpsrlq	$42,%xmm9,%xmm9
	xorq	%r11,%r12
	shrdq	$6,%r14,%r14
	vpxor	%xmm10,%xmm11,%xmm11
	xorq	%rcx,%rdi
	addq	%r12,%rax
	vpxor	%xmm9,%xmm11,%xmm11
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	vpaddq	%xmm11,%xmm7,%xmm7
	xorq	%rbx,%r14
	addq	%r13,%rax
	vpaddq	96(%rbp),%xmm7,%xmm10
	xorq	%rcx,%r15
	shrdq	$28,%r14,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	vmovdqa	%xmm10,112(%rsp)
	cmpb	$0,135(%rbp)
	jne	.Lavx_00_47
	shrdq	$23,%r13,%r13
	movq	%r14,%rax
	movq	%r9,%r12
	shrdq	$5,%r14,%r14
	xorq	%r8,%r13
	xorq	%r10,%r12
	shrdq	$4,%r13,%r13
	xorq	%rax,%r14
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	0(%rsp),%r11
	movq	%rax,%r15
	xorq	%r10,%r12
	shrdq	$6,%r14,%r14
	xorq	%rbx,%r15
	addq	%r12,%r11
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%rax,%r14
	addq	%r13,%r11
	xorq	%rbx,%rdi
	shrdq	$28,%r14,%r14
	addq	%r11,%rdx
	addq	%rdi,%r11
	movq	%rdx,%r13
	addq	%r11,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r11
	movq	%r8,%r12
	shrdq	$5,%r14,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	shrdq	$4,%r13,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	addq	8(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	shrdq	$6,%r14,%r14
	xorq	%rax,%rdi
	addq	%r12,%r10
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	shrdq	$28,%r14,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r10
	movq	%rdx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rcx,%r13
	xorq	%r8,%r12
	shrdq	$4,%r13,%r13
	xorq	%r10,%r14
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	16(%rsp),%r9
	movq	%r10,%r15
	xorq	%r8,%r12
	shrdq	$6,%r14,%r14
	xorq	%r11,%r15
	addq	%r12,%r9
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%r10,%r14
	addq	%r13,%r9
	xorq	%r11,%rdi
	shrdq	$28,%r14,%r14
	addq	%r9,%rbx
	addq	%rdi,%r9
	movq	%rbx,%r13
	addq	%r9,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r9
	movq	%rcx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	shrdq	$4,%r13,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	addq	24(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r10,%rdi
	addq	%r12,%r8
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	shrdq	$28,%r14,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r8
	movq	%rbx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rax,%r13
	xorq	%rcx,%r12
	shrdq	$4,%r13,%r13
	xorq	%r8,%r14
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	32(%rsp),%rdx
	movq	%r8,%r15
	xorq	%rcx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r9,%r15
	addq	%r12,%rdx
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%r8,%r14
	addq	%r13,%rdx
	xorq	%r9,%rdi
	shrdq	$28,%r14,%r14
	addq	%rdx,%r11
	addq	%rdi,%rdx
	movq	%r11,%r13
	addq	%rdx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rdx
	movq	%rax,%r12
	shrdq	$5,%r14,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	shrdq	$4,%r13,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	addq	40(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r8,%rdi
	addq	%r12,%rcx
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	shrdq	$28,%r14,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rcx
	movq	%r11,%r12
	shrdq	$5,%r14,%r14
	xorq	%r10,%r13
	xorq	%rax,%r12
	shrdq	$4,%r13,%r13
	xorq	%rcx,%r14
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	48(%rsp),%rbx
	movq	%rcx,%r15
	xorq	%rax,%r12
	shrdq	$6,%r14,%r14
	xorq	%rdx,%r15
	addq	%r12,%rbx
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%rcx,%r14
	addq	%r13,%rbx
	xorq	%rdx,%rdi
	shrdq	$28,%r14,%r14
	addq	%rbx,%r9
	addq	%rdi,%rbx
	movq	%r9,%r13
	addq	%rbx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rbx
	movq	%r10,%r12
	shrdq	$5,%r14,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	shrdq	$4,%r13,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	addq	56(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	shrdq	$6,%r14,%r14
	xorq	%rcx,%rdi
	addq	%r12,%rax
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	shrdq	$28,%r14,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rax
	movq	%r9,%r12
	shrdq	$5,%r14,%r14
	xorq	%r8,%r13
	xorq	%r10,%r12
	shrdq	$4,%r13,%r13
	xorq	%rax,%r14
	andq	%r8,%r12
	xorq	%r8,%r13
	addq	64(%rsp),%r11
	movq	%rax,%r15
	xorq	%r10,%r12
	shrdq	$6,%r14,%r14
	xorq	%rbx,%r15
	addq	%r12,%r11
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%rax,%r14
	addq	%r13,%r11
	xorq	%rbx,%rdi
	shrdq	$28,%r14,%r14
	addq	%r11,%rdx
	addq	%rdi,%r11
	movq	%rdx,%r13
	addq	%r11,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r11
	movq	%r8,%r12
	shrdq	$5,%r14,%r14
	xorq	%rdx,%r13
	xorq	%r9,%r12
	shrdq	$4,%r13,%r13
	xorq	%r11,%r14
	andq	%rdx,%r12
	xorq	%rdx,%r13
	addq	72(%rsp),%r10
	movq	%r11,%rdi
	xorq	%r9,%r12
	shrdq	$6,%r14,%r14
	xorq	%rax,%rdi
	addq	%r12,%r10
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%r11,%r14
	addq	%r13,%r10
	xorq	%rax,%r15
	shrdq	$28,%r14,%r14
	addq	%r10,%rcx
	addq	%r15,%r10
	movq	%rcx,%r13
	addq	%r10,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r10
	movq	%rdx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rcx,%r13
	xorq	%r8,%r12
	shrdq	$4,%r13,%r13
	xorq	%r10,%r14
	andq	%rcx,%r12
	xorq	%rcx,%r13
	addq	80(%rsp),%r9
	movq	%r10,%r15
	xorq	%r8,%r12
	shrdq	$6,%r14,%r14
	xorq	%r11,%r15
	addq	%r12,%r9
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%r10,%r14
	addq	%r13,%r9
	xorq	%r11,%rdi
	shrdq	$28,%r14,%r14
	addq	%r9,%rbx
	addq	%rdi,%r9
	movq	%rbx,%r13
	addq	%r9,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r9
	movq	%rcx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rbx,%r13
	xorq	%rdx,%r12
	shrdq	$4,%r13,%r13
	xorq	%r9,%r14
	andq	%rbx,%r12
	xorq	%rbx,%r13
	addq	88(%rsp),%r8
	movq	%r9,%rdi
	xorq	%rdx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r10,%rdi
	addq	%r12,%r8
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%r9,%r14
	addq	%r13,%r8
	xorq	%r10,%r15
	shrdq	$28,%r14,%r14
	addq	%r8,%rax
	addq	%r15,%r8
	movq	%rax,%r13
	addq	%r8,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%r8
	movq	%rbx,%r12
	shrdq	$5,%r14,%r14
	xorq	%rax,%r13
	xorq	%rcx,%r12
	shrdq	$4,%r13,%r13
	xorq	%r8,%r14
	andq	%rax,%r12
	xorq	%rax,%r13
	addq	96(%rsp),%rdx
	movq	%r8,%r15
	xorq	%rcx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r9,%r15
	addq	%r12,%rdx
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%r8,%r14
	addq	%r13,%rdx
	xorq	%r9,%rdi
	shrdq	$28,%r14,%r14
	addq	%rdx,%r11
	addq	%rdi,%rdx
	movq	%r11,%r13
	addq	%rdx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rdx
	movq	%rax,%r12
	shrdq	$5,%r14,%r14
	xorq	%r11,%r13
	xorq	%rbx,%r12
	shrdq	$4,%r13,%r13
	xorq	%rdx,%r14
	andq	%r11,%r12
	xorq	%r11,%r13
	addq	104(%rsp),%rcx
	movq	%rdx,%rdi
	xorq	%rbx,%r12
	shrdq	$6,%r14,%r14
	xorq	%r8,%rdi
	addq	%r12,%rcx
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%rdx,%r14
	addq	%r13,%rcx
	xorq	%r8,%r15
	shrdq	$28,%r14,%r14
	addq	%rcx,%r10
	addq	%r15,%rcx
	movq	%r10,%r13
	addq	%rcx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rcx
	movq	%r11,%r12
	shrdq	$5,%r14,%r14
	xorq	%r10,%r13
	xorq	%rax,%r12
	shrdq	$4,%r13,%r13
	xorq	%rcx,%r14
	andq	%r10,%r12
	xorq	%r10,%r13
	addq	112(%rsp),%rbx
	movq	%rcx,%r15
	xorq	%rax,%r12
	shrdq	$6,%r14,%r14
	xorq	%rdx,%r15
	addq	%r12,%rbx
	shrdq	$14,%r13,%r13
	andq	%r15,%rdi
	xorq	%rcx,%r14
	addq	%r13,%rbx
	xorq	%rdx,%rdi
	shrdq	$28,%r14,%r14
	addq	%rbx,%r9
	addq	%rdi,%rbx
	movq	%r9,%r13
	addq	%rbx,%r14
	shrdq	$23,%r13,%r13
	movq	%r14,%rbx
	movq	%r10,%r12
	shrdq	$5,%r14,%r14
	xorq	%r9,%r13
	xorq	%r11,%r12
	shrdq	$4,%r13,%r13
	xorq	%rbx,%r14
	andq	%r9,%r12
	xorq	%r9,%r13
	addq	120(%rsp),%rax
	movq	%rbx,%rdi
	xorq	%r11,%r12
	shrdq	$6,%r14,%r14
	xorq	%rcx,%rdi
	addq	%r12,%rax
	shrdq	$14,%r13,%r13
	andq	%rdi,%r15
	xorq	%rbx,%r14
	addq	%r13,%rax
	xorq	%rcx,%r15
	shrdq	$28,%r14,%r14
	addq	%rax,%r8
	addq	%r15,%rax
	movq	%r8,%r13
	addq	%rax,%r14
	movq	128+0(%rsp),%rdi
	movq	%r14,%rax

	addq	0(%rdi),%rax
	leaq	128(%rsi),%rsi
	addq	8(%rdi),%rbx
	addq	16(%rdi),%rcx
	addq	24(%rdi),%rdx
	addq	32(%rdi),%r8
	addq	40(%rdi),%r9
	addq	48(%rdi),%r10
	addq	56(%rdi),%r11

	cmpq	128+16(%rsp),%rsi

	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)
	movq	%r8,32(%rdi)
	movq	%r9,40(%rdi)
	movq	%r10,48(%rdi)
	movq	%r11,56(%rdi)
	jb	.Lloop_avx

	movq	152(%rsp),%rsi
.cfi_def_cfa	%rsi,8
	vzeroupper
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha512_block_data_order_avx,.-sha512_block_data_order_avx
.type	sha512_block_data_order_avx2,@function
.align	64
sha512_block_data_order_avx2:
.cfi_startproc	
.Lavx2_shortcut:
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56
	subq	$1312,%rsp
	shlq	$4,%rdx
	andq	$-2048,%rsp
	leaq	(%rsi,%rdx,8),%rdx
	addq	$1152,%rsp
	movq	%rdi,128+0(%rsp)
	movq	%rsi,128+8(%rsp)
	movq	%rdx,128+16(%rsp)
	movq	%rax,152(%rsp)
.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
.Lprologue_avx2:

	vzeroupper
	subq	$-128,%rsi
	movq	0(%rdi),%rax
	movq	%rsi,%r12
	movq	8(%rdi),%rbx
	cmpq	%rdx,%rsi
	movq	16(%rdi),%rcx
	cmoveq	%rsp,%r12
	movq	24(%rdi),%rdx
	movq	32(%rdi),%r8
	movq	40(%rdi),%r9
	movq	48(%rdi),%r10
	movq	56(%rdi),%r11
	jmp	.Loop_avx2
.align	16
.Loop_avx2:
	vmovdqu	-128(%rsi),%xmm0
	vmovdqu	-128+16(%rsi),%xmm1
	vmovdqu	-128+32(%rsi),%xmm2
	leaq	K512+128(%rip),%rbp
	vmovdqu	-128+48(%rsi),%xmm3
	vmovdqu	-128+64(%rsi),%xmm4
	vmovdqu	-128+80(%rsi),%xmm5
	vmovdqu	-128+96(%rsi),%xmm6
	vmovdqu	-128+112(%rsi),%xmm7

	vmovdqa	1152(%rbp),%ymm10
	vinserti128	$1,(%r12),%ymm0,%ymm0
	vinserti128	$1,16(%r12),%ymm1,%ymm1
	vpshufb	%ymm10,%ymm0,%ymm0
	vinserti128	$1,32(%r12),%ymm2,%ymm2
	vpshufb	%ymm10,%ymm1,%ymm1
	vinserti128	$1,48(%r12),%ymm3,%ymm3
	vpshufb	%ymm10,%ymm2,%ymm2
	vinserti128	$1,64(%r12),%ymm4,%ymm4
	vpshufb	%ymm10,%ymm3,%ymm3
	vinserti128	$1,80(%r12),%ymm5,%ymm5
	vpshufb	%ymm10,%ymm4,%ymm4
	vinserti128	$1,96(%r12),%ymm6,%ymm6
	vpshufb	%ymm10,%ymm5,%ymm5
	vinserti128	$1,112(%r12),%ymm7,%ymm7

	vpaddq	-128(%rbp),%ymm0,%ymm8
	vpshufb	%ymm10,%ymm6,%ymm6
	vpaddq	-96(%rbp),%ymm1,%ymm9
	vpshufb	%ymm10,%ymm7,%ymm7
	vpaddq	-64(%rbp),%ymm2,%ymm10
	vpaddq	-32(%rbp),%ymm3,%ymm11
	vmovdqa	%ymm8,0(%rsp)
	vpaddq	0(%rbp),%ymm4,%ymm8
	vmovdqa	%ymm9,32(%rsp)
	vpaddq	32(%rbp),%ymm5,%ymm9
	vmovdqa	%ymm10,64(%rsp)
	vpaddq	64(%rbp),%ymm6,%ymm10
	vmovdqa	%ymm11,96(%rsp)

	movq	152(%rsp),%rdi
.cfi_def_cfa	%rdi,8
	leaq	-128(%rsp),%rsp



	movq	%rdi,-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	vpaddq	96(%rbp),%ymm7,%ymm11
	vmovdqa	%ymm8,0(%rsp)
	xorq	%r14,%r14
	vmovdqa	%ymm9,32(%rsp)
	movq	%rbx,%rdi
	vmovdqa	%ymm10,64(%rsp)
	xorq	%rcx,%rdi
	vmovdqa	%ymm11,96(%rsp)
	movq	%r9,%r12
	addq	$32*8,%rbp
	jmp	.Lavx2_00_47

.align	16
.Lavx2_00_47:
	leaq	-128(%rsp),%rsp
.cfi_escape	0x0f,0x06,0x77,0xf8,0x00,0x06,0x23,0x08

	pushq	128-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x00,0x06,0x23,0x08
	leaq	8(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	vpalignr	$8,%ymm0,%ymm1,%ymm8
	addq	0+256(%rsp),%r11
	andq	%r8,%r12
	rorxq	$41,%r8,%r13
	vpalignr	$8,%ymm4,%ymm5,%ymm11
	rorxq	$18,%r8,%r15
	leaq	(%rax,%r14,1),%rax
	leaq	(%r11,%r12,1),%r11
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%r10,%r8,%r12
	xorq	%r15,%r13
	rorxq	$14,%r8,%r14
	vpaddq	%ymm11,%ymm0,%ymm0
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%r11,%r12,1),%r11
	xorq	%r14,%r13
	movq	%rax,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%rax,%r12
	leaq	(%r11,%r13,1),%r11
	xorq	%rbx,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%rax,%r14
	rorxq	$28,%rax,%r13
	leaq	(%rdx,%r11,1),%rdx
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rbx,%rdi
	vpsrlq	$6,%ymm7,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%r11,%rdi,1),%r11
	movq	%r8,%r12
	vpsllq	$3,%ymm7,%ymm10
	vpaddq	%ymm8,%ymm0,%ymm0
	addq	8+256(%rsp),%r10
	andq	%rdx,%r12
	rorxq	$41,%rdx,%r13
	vpsrlq	$19,%ymm7,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%rdx,%rdi
	leaq	(%r11,%r14,1),%r11
	leaq	(%r10,%r12,1),%r10
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%r9,%rdx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rdx,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%r10,%r12,1),%r10
	xorq	%r14,%r13
	movq	%r11,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%r11,%r12
	leaq	(%r10,%r13,1),%r10
	xorq	%rax,%rdi
	vpaddq	%ymm11,%ymm0,%ymm0
	rorxq	$34,%r11,%r14
	rorxq	$28,%r11,%r13
	leaq	(%rcx,%r10,1),%rcx
	vpaddq	-128(%rbp),%ymm0,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rax,%r15
	xorq	%r13,%r14
	leaq	(%r10,%r15,1),%r10
	movq	%rdx,%r12
	vmovdqa	%ymm10,0(%rsp)
	vpalignr	$8,%ymm1,%ymm2,%ymm8
	addq	32+256(%rsp),%r9
	andq	%rcx,%r12
	rorxq	$41,%rcx,%r13
	vpalignr	$8,%ymm5,%ymm6,%ymm11
	rorxq	$18,%rcx,%r15
	leaq	(%r10,%r14,1),%r10
	leaq	(%r9,%r12,1),%r9
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%r8,%rcx,%r12
	xorq	%r15,%r13
	rorxq	$14,%rcx,%r14
	vpaddq	%ymm11,%ymm1,%ymm1
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%r9,%r12,1),%r9
	xorq	%r14,%r13
	movq	%r10,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%r10,%r12
	leaq	(%r9,%r13,1),%r9
	xorq	%r11,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%r10,%r14
	rorxq	$28,%r10,%r13
	leaq	(%rbx,%r9,1),%rbx
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r11,%rdi
	vpsrlq	$6,%ymm0,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%r9,%rdi,1),%r9
	movq	%rcx,%r12
	vpsllq	$3,%ymm0,%ymm10
	vpaddq	%ymm8,%ymm1,%ymm1
	addq	40+256(%rsp),%r8
	andq	%rbx,%r12
	rorxq	$41,%rbx,%r13
	vpsrlq	$19,%ymm0,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%rbx,%rdi
	leaq	(%r9,%r14,1),%r9
	leaq	(%r8,%r12,1),%r8
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%rdx,%rbx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rbx,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%r8,%r12,1),%r8
	xorq	%r14,%r13
	movq	%r9,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%r9,%r12
	leaq	(%r8,%r13,1),%r8
	xorq	%r10,%rdi
	vpaddq	%ymm11,%ymm1,%ymm1
	rorxq	$34,%r9,%r14
	rorxq	$28,%r9,%r13
	leaq	(%rax,%r8,1),%rax
	vpaddq	-96(%rbp),%ymm1,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r10,%r15
	xorq	%r13,%r14
	leaq	(%r8,%r15,1),%r8
	movq	%rbx,%r12
	vmovdqa	%ymm10,32(%rsp)
	vpalignr	$8,%ymm2,%ymm3,%ymm8
	addq	64+256(%rsp),%rdx
	andq	%rax,%r12
	rorxq	$41,%rax,%r13
	vpalignr	$8,%ymm6,%ymm7,%ymm11
	rorxq	$18,%rax,%r15
	leaq	(%r8,%r14,1),%r8
	leaq	(%rdx,%r12,1),%rdx
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%rcx,%rax,%r12
	xorq	%r15,%r13
	rorxq	$14,%rax,%r14
	vpaddq	%ymm11,%ymm2,%ymm2
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%rdx,%r12,1),%rdx
	xorq	%r14,%r13
	movq	%r8,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%r8,%r12
	leaq	(%rdx,%r13,1),%rdx
	xorq	%r9,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%r8,%r14
	rorxq	$28,%r8,%r13
	leaq	(%r11,%rdx,1),%r11
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r9,%rdi
	vpsrlq	$6,%ymm1,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%rdx,%rdi,1),%rdx
	movq	%rax,%r12
	vpsllq	$3,%ymm1,%ymm10
	vpaddq	%ymm8,%ymm2,%ymm2
	addq	72+256(%rsp),%rcx
	andq	%r11,%r12
	rorxq	$41,%r11,%r13
	vpsrlq	$19,%ymm1,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%r11,%rdi
	leaq	(%rdx,%r14,1),%rdx
	leaq	(%rcx,%r12,1),%rcx
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%rbx,%r11,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r11,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%rcx,%r12,1),%rcx
	xorq	%r14,%r13
	movq	%rdx,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%rdx,%r12
	leaq	(%rcx,%r13,1),%rcx
	xorq	%r8,%rdi
	vpaddq	%ymm11,%ymm2,%ymm2
	rorxq	$34,%rdx,%r14
	rorxq	$28,%rdx,%r13
	leaq	(%r10,%rcx,1),%r10
	vpaddq	-64(%rbp),%ymm2,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r8,%r15
	xorq	%r13,%r14
	leaq	(%rcx,%r15,1),%rcx
	movq	%r11,%r12
	vmovdqa	%ymm10,64(%rsp)
	vpalignr	$8,%ymm3,%ymm4,%ymm8
	addq	96+256(%rsp),%rbx
	andq	%r10,%r12
	rorxq	$41,%r10,%r13
	vpalignr	$8,%ymm7,%ymm0,%ymm11
	rorxq	$18,%r10,%r15
	leaq	(%rcx,%r14,1),%rcx
	leaq	(%rbx,%r12,1),%rbx
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%rax,%r10,%r12
	xorq	%r15,%r13
	rorxq	$14,%r10,%r14
	vpaddq	%ymm11,%ymm3,%ymm3
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%rbx,%r12,1),%rbx
	xorq	%r14,%r13
	movq	%rcx,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%rcx,%r12
	leaq	(%rbx,%r13,1),%rbx
	xorq	%rdx,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%rcx,%r14
	rorxq	$28,%rcx,%r13
	leaq	(%r9,%rbx,1),%r9
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rdx,%rdi
	vpsrlq	$6,%ymm2,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%rbx,%rdi,1),%rbx
	movq	%r10,%r12
	vpsllq	$3,%ymm2,%ymm10
	vpaddq	%ymm8,%ymm3,%ymm3
	addq	104+256(%rsp),%rax
	andq	%r9,%r12
	rorxq	$41,%r9,%r13
	vpsrlq	$19,%ymm2,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%r9,%rdi
	leaq	(%rbx,%r14,1),%rbx
	leaq	(%rax,%r12,1),%rax
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%r11,%r9,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r9,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%rax,%r12,1),%rax
	xorq	%r14,%r13
	movq	%rbx,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%rbx,%r12
	leaq	(%rax,%r13,1),%rax
	xorq	%rcx,%rdi
	vpaddq	%ymm11,%ymm3,%ymm3
	rorxq	$34,%rbx,%r14
	rorxq	$28,%rbx,%r13
	leaq	(%r8,%rax,1),%r8
	vpaddq	-32(%rbp),%ymm3,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rcx,%r15
	xorq	%r13,%r14
	leaq	(%rax,%r15,1),%rax
	movq	%r9,%r12
	vmovdqa	%ymm10,96(%rsp)
	leaq	-128(%rsp),%rsp
.cfi_escape	0x0f,0x06,0x77,0xf8,0x00,0x06,0x23,0x08

	pushq	128-8(%rsp)
.cfi_escape	0x0f,0x05,0x77,0x00,0x06,0x23,0x08
	leaq	8(%rsp),%rsp
.cfi_escape	0x0f,0x05,0x77,0x78,0x06,0x23,0x08
	vpalignr	$8,%ymm4,%ymm5,%ymm8
	addq	0+256(%rsp),%r11
	andq	%r8,%r12
	rorxq	$41,%r8,%r13
	vpalignr	$8,%ymm0,%ymm1,%ymm11
	rorxq	$18,%r8,%r15
	leaq	(%rax,%r14,1),%rax
	leaq	(%r11,%r12,1),%r11
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%r10,%r8,%r12
	xorq	%r15,%r13
	rorxq	$14,%r8,%r14
	vpaddq	%ymm11,%ymm4,%ymm4
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%r11,%r12,1),%r11
	xorq	%r14,%r13
	movq	%rax,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%rax,%r12
	leaq	(%r11,%r13,1),%r11
	xorq	%rbx,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%rax,%r14
	rorxq	$28,%rax,%r13
	leaq	(%rdx,%r11,1),%rdx
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rbx,%rdi
	vpsrlq	$6,%ymm3,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%r11,%rdi,1),%r11
	movq	%r8,%r12
	vpsllq	$3,%ymm3,%ymm10
	vpaddq	%ymm8,%ymm4,%ymm4
	addq	8+256(%rsp),%r10
	andq	%rdx,%r12
	rorxq	$41,%rdx,%r13
	vpsrlq	$19,%ymm3,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%rdx,%rdi
	leaq	(%r11,%r14,1),%r11
	leaq	(%r10,%r12,1),%r10
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%r9,%rdx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rdx,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%r10,%r12,1),%r10
	xorq	%r14,%r13
	movq	%r11,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%r11,%r12
	leaq	(%r10,%r13,1),%r10
	xorq	%rax,%rdi
	vpaddq	%ymm11,%ymm4,%ymm4
	rorxq	$34,%r11,%r14
	rorxq	$28,%r11,%r13
	leaq	(%rcx,%r10,1),%rcx
	vpaddq	0(%rbp),%ymm4,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rax,%r15
	xorq	%r13,%r14
	leaq	(%r10,%r15,1),%r10
	movq	%rdx,%r12
	vmovdqa	%ymm10,0(%rsp)
	vpalignr	$8,%ymm5,%ymm6,%ymm8
	addq	32+256(%rsp),%r9
	andq	%rcx,%r12
	rorxq	$41,%rcx,%r13
	vpalignr	$8,%ymm1,%ymm2,%ymm11
	rorxq	$18,%rcx,%r15
	leaq	(%r10,%r14,1),%r10
	leaq	(%r9,%r12,1),%r9
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%r8,%rcx,%r12
	xorq	%r15,%r13
	rorxq	$14,%rcx,%r14
	vpaddq	%ymm11,%ymm5,%ymm5
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%r9,%r12,1),%r9
	xorq	%r14,%r13
	movq	%r10,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%r10,%r12
	leaq	(%r9,%r13,1),%r9
	xorq	%r11,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%r10,%r14
	rorxq	$28,%r10,%r13
	leaq	(%rbx,%r9,1),%rbx
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r11,%rdi
	vpsrlq	$6,%ymm4,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%r9,%rdi,1),%r9
	movq	%rcx,%r12
	vpsllq	$3,%ymm4,%ymm10
	vpaddq	%ymm8,%ymm5,%ymm5
	addq	40+256(%rsp),%r8
	andq	%rbx,%r12
	rorxq	$41,%rbx,%r13
	vpsrlq	$19,%ymm4,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%rbx,%rdi
	leaq	(%r9,%r14,1),%r9
	leaq	(%r8,%r12,1),%r8
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%rdx,%rbx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rbx,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%r8,%r12,1),%r8
	xorq	%r14,%r13
	movq	%r9,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%r9,%r12
	leaq	(%r8,%r13,1),%r8
	xorq	%r10,%rdi
	vpaddq	%ymm11,%ymm5,%ymm5
	rorxq	$34,%r9,%r14
	rorxq	$28,%r9,%r13
	leaq	(%rax,%r8,1),%rax
	vpaddq	32(%rbp),%ymm5,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r10,%r15
	xorq	%r13,%r14
	leaq	(%r8,%r15,1),%r8
	movq	%rbx,%r12
	vmovdqa	%ymm10,32(%rsp)
	vpalignr	$8,%ymm6,%ymm7,%ymm8
	addq	64+256(%rsp),%rdx
	andq	%rax,%r12
	rorxq	$41,%rax,%r13
	vpalignr	$8,%ymm2,%ymm3,%ymm11
	rorxq	$18,%rax,%r15
	leaq	(%r8,%r14,1),%r8
	leaq	(%rdx,%r12,1),%rdx
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%rcx,%rax,%r12
	xorq	%r15,%r13
	rorxq	$14,%rax,%r14
	vpaddq	%ymm11,%ymm6,%ymm6
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%rdx,%r12,1),%rdx
	xorq	%r14,%r13
	movq	%r8,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%r8,%r12
	leaq	(%rdx,%r13,1),%rdx
	xorq	%r9,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%r8,%r14
	rorxq	$28,%r8,%r13
	leaq	(%r11,%rdx,1),%r11
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r9,%rdi
	vpsrlq	$6,%ymm5,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%rdx,%rdi,1),%rdx
	movq	%rax,%r12
	vpsllq	$3,%ymm5,%ymm10
	vpaddq	%ymm8,%ymm6,%ymm6
	addq	72+256(%rsp),%rcx
	andq	%r11,%r12
	rorxq	$41,%r11,%r13
	vpsrlq	$19,%ymm5,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%r11,%rdi
	leaq	(%rdx,%r14,1),%rdx
	leaq	(%rcx,%r12,1),%rcx
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%rbx,%r11,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r11,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%rcx,%r12,1),%rcx
	xorq	%r14,%r13
	movq	%rdx,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%rdx,%r12
	leaq	(%rcx,%r13,1),%rcx
	xorq	%r8,%rdi
	vpaddq	%ymm11,%ymm6,%ymm6
	rorxq	$34,%rdx,%r14
	rorxq	$28,%rdx,%r13
	leaq	(%r10,%rcx,1),%r10
	vpaddq	64(%rbp),%ymm6,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r8,%r15
	xorq	%r13,%r14
	leaq	(%rcx,%r15,1),%rcx
	movq	%r11,%r12
	vmovdqa	%ymm10,64(%rsp)
	vpalignr	$8,%ymm7,%ymm0,%ymm8
	addq	96+256(%rsp),%rbx
	andq	%r10,%r12
	rorxq	$41,%r10,%r13
	vpalignr	$8,%ymm3,%ymm4,%ymm11
	rorxq	$18,%r10,%r15
	leaq	(%rcx,%r14,1),%rcx
	leaq	(%rbx,%r12,1),%rbx
	vpsrlq	$1,%ymm8,%ymm10
	andnq	%rax,%r10,%r12
	xorq	%r15,%r13
	rorxq	$14,%r10,%r14
	vpaddq	%ymm11,%ymm7,%ymm7
	vpsrlq	$7,%ymm8,%ymm11
	leaq	(%rbx,%r12,1),%rbx
	xorq	%r14,%r13
	movq	%rcx,%r15
	vpsllq	$56,%ymm8,%ymm9
	vpxor	%ymm10,%ymm11,%ymm8
	rorxq	$39,%rcx,%r12
	leaq	(%rbx,%r13,1),%rbx
	xorq	%rdx,%r15
	vpsrlq	$7,%ymm10,%ymm10
	vpxor	%ymm9,%ymm8,%ymm8
	rorxq	$34,%rcx,%r14
	rorxq	$28,%rcx,%r13
	leaq	(%r9,%rbx,1),%r9
	vpsllq	$7,%ymm9,%ymm9
	vpxor	%ymm10,%ymm8,%ymm8
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rdx,%rdi
	vpsrlq	$6,%ymm6,%ymm11
	vpxor	%ymm9,%ymm8,%ymm8
	xorq	%r13,%r14
	leaq	(%rbx,%rdi,1),%rbx
	movq	%r10,%r12
	vpsllq	$3,%ymm6,%ymm10
	vpaddq	%ymm8,%ymm7,%ymm7
	addq	104+256(%rsp),%rax
	andq	%r9,%r12
	rorxq	$41,%r9,%r13
	vpsrlq	$19,%ymm6,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	rorxq	$18,%r9,%rdi
	leaq	(%rbx,%r14,1),%rbx
	leaq	(%rax,%r12,1),%rax
	vpsllq	$42,%ymm10,%ymm10
	vpxor	%ymm9,%ymm11,%ymm11
	andnq	%r11,%r9,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r9,%r14
	vpsrlq	$42,%ymm9,%ymm9
	vpxor	%ymm10,%ymm11,%ymm11
	leaq	(%rax,%r12,1),%rax
	xorq	%r14,%r13
	movq	%rbx,%rdi
	vpxor	%ymm9,%ymm11,%ymm11
	rorxq	$39,%rbx,%r12
	leaq	(%rax,%r13,1),%rax
	xorq	%rcx,%rdi
	vpaddq	%ymm11,%ymm7,%ymm7
	rorxq	$34,%rbx,%r14
	rorxq	$28,%rbx,%r13
	leaq	(%r8,%rax,1),%r8
	vpaddq	96(%rbp),%ymm7,%ymm10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rcx,%r15
	xorq	%r13,%r14
	leaq	(%rax,%r15,1),%rax
	movq	%r9,%r12
	vmovdqa	%ymm10,96(%rsp)
	leaq	256(%rbp),%rbp
	cmpb	$0,-121(%rbp)
	jne	.Lavx2_00_47
	addq	0+128(%rsp),%r11
	andq	%r8,%r12
	rorxq	$41,%r8,%r13
	rorxq	$18,%r8,%r15
	leaq	(%rax,%r14,1),%rax
	leaq	(%r11,%r12,1),%r11
	andnq	%r10,%r8,%r12
	xorq	%r15,%r13
	rorxq	$14,%r8,%r14
	leaq	(%r11,%r12,1),%r11
	xorq	%r14,%r13
	movq	%rax,%r15
	rorxq	$39,%rax,%r12
	leaq	(%r11,%r13,1),%r11
	xorq	%rbx,%r15
	rorxq	$34,%rax,%r14
	rorxq	$28,%rax,%r13
	leaq	(%rdx,%r11,1),%rdx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rbx,%rdi
	xorq	%r13,%r14
	leaq	(%r11,%rdi,1),%r11
	movq	%r8,%r12
	addq	8+128(%rsp),%r10
	andq	%rdx,%r12
	rorxq	$41,%rdx,%r13
	rorxq	$18,%rdx,%rdi
	leaq	(%r11,%r14,1),%r11
	leaq	(%r10,%r12,1),%r10
	andnq	%r9,%rdx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rdx,%r14
	leaq	(%r10,%r12,1),%r10
	xorq	%r14,%r13
	movq	%r11,%rdi
	rorxq	$39,%r11,%r12
	leaq	(%r10,%r13,1),%r10
	xorq	%rax,%rdi
	rorxq	$34,%r11,%r14
	rorxq	$28,%r11,%r13
	leaq	(%rcx,%r10,1),%rcx
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rax,%r15
	xorq	%r13,%r14
	leaq	(%r10,%r15,1),%r10
	movq	%rdx,%r12
	addq	32+128(%rsp),%r9
	andq	%rcx,%r12
	rorxq	$41,%rcx,%r13
	rorxq	$18,%rcx,%r15
	leaq	(%r10,%r14,1),%r10
	leaq	(%r9,%r12,1),%r9
	andnq	%r8,%rcx,%r12
	xorq	%r15,%r13
	rorxq	$14,%rcx,%r14
	leaq	(%r9,%r12,1),%r9
	xorq	%r14,%r13
	movq	%r10,%r15
	rorxq	$39,%r10,%r12
	leaq	(%r9,%r13,1),%r9
	xorq	%r11,%r15
	rorxq	$34,%r10,%r14
	rorxq	$28,%r10,%r13
	leaq	(%rbx,%r9,1),%rbx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r11,%rdi
	xorq	%r13,%r14
	leaq	(%r9,%rdi,1),%r9
	movq	%rcx,%r12
	addq	40+128(%rsp),%r8
	andq	%rbx,%r12
	rorxq	$41,%rbx,%r13
	rorxq	$18,%rbx,%rdi
	leaq	(%r9,%r14,1),%r9
	leaq	(%r8,%r12,1),%r8
	andnq	%rdx,%rbx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rbx,%r14
	leaq	(%r8,%r12,1),%r8
	xorq	%r14,%r13
	movq	%r9,%rdi
	rorxq	$39,%r9,%r12
	leaq	(%r8,%r13,1),%r8
	xorq	%r10,%rdi
	rorxq	$34,%r9,%r14
	rorxq	$28,%r9,%r13
	leaq	(%rax,%r8,1),%rax
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r10,%r15
	xorq	%r13,%r14
	leaq	(%r8,%r15,1),%r8
	movq	%rbx,%r12
	addq	64+128(%rsp),%rdx
	andq	%rax,%r12
	rorxq	$41,%rax,%r13
	rorxq	$18,%rax,%r15
	leaq	(%r8,%r14,1),%r8
	leaq	(%rdx,%r12,1),%rdx
	andnq	%rcx,%rax,%r12
	xorq	%r15,%r13
	rorxq	$14,%rax,%r14
	leaq	(%rdx,%r12,1),%rdx
	xorq	%r14,%r13
	movq	%r8,%r15
	rorxq	$39,%r8,%r12
	leaq	(%rdx,%r13,1),%rdx
	xorq	%r9,%r15
	rorxq	$34,%r8,%r14
	rorxq	$28,%r8,%r13
	leaq	(%r11,%rdx,1),%r11
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r9,%rdi
	xorq	%r13,%r14
	leaq	(%rdx,%rdi,1),%rdx
	movq	%rax,%r12
	addq	72+128(%rsp),%rcx
	andq	%r11,%r12
	rorxq	$41,%r11,%r13
	rorxq	$18,%r11,%rdi
	leaq	(%rdx,%r14,1),%rdx
	leaq	(%rcx,%r12,1),%rcx
	andnq	%rbx,%r11,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r11,%r14
	leaq	(%rcx,%r12,1),%rcx
	xorq	%r14,%r13
	movq	%rdx,%rdi
	rorxq	$39,%rdx,%r12
	leaq	(%rcx,%r13,1),%rcx
	xorq	%r8,%rdi
	rorxq	$34,%rdx,%r14
	rorxq	$28,%rdx,%r13
	leaq	(%r10,%rcx,1),%r10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r8,%r15
	xorq	%r13,%r14
	leaq	(%rcx,%r15,1),%rcx
	movq	%r11,%r12
	addq	96+128(%rsp),%rbx
	andq	%r10,%r12
	rorxq	$41,%r10,%r13
	rorxq	$18,%r10,%r15
	leaq	(%rcx,%r14,1),%rcx
	leaq	(%rbx,%r12,1),%rbx
	andnq	%rax,%r10,%r12
	xorq	%r15,%r13
	rorxq	$14,%r10,%r14
	leaq	(%rbx,%r12,1),%rbx
	xorq	%r14,%r13
	movq	%rcx,%r15
	rorxq	$39,%rcx,%r12
	leaq	(%rbx,%r13,1),%rbx
	xorq	%rdx,%r15
	rorxq	$34,%rcx,%r14
	rorxq	$28,%rcx,%r13
	leaq	(%r9,%rbx,1),%r9
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rdx,%rdi
	xorq	%r13,%r14
	leaq	(%rbx,%rdi,1),%rbx
	movq	%r10,%r12
	addq	104+128(%rsp),%rax
	andq	%r9,%r12
	rorxq	$41,%r9,%r13
	rorxq	$18,%r9,%rdi
	leaq	(%rbx,%r14,1),%rbx
	leaq	(%rax,%r12,1),%rax
	andnq	%r11,%r9,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r9,%r14
	leaq	(%rax,%r12,1),%rax
	xorq	%r14,%r13
	movq	%rbx,%rdi
	rorxq	$39,%rbx,%r12
	leaq	(%rax,%r13,1),%rax
	xorq	%rcx,%rdi
	rorxq	$34,%rbx,%r14
	rorxq	$28,%rbx,%r13
	leaq	(%r8,%rax,1),%r8
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rcx,%r15
	xorq	%r13,%r14
	leaq	(%rax,%r15,1),%rax
	movq	%r9,%r12
	addq	0(%rsp),%r11
	andq	%r8,%r12
	rorxq	$41,%r8,%r13
	rorxq	$18,%r8,%r15
	leaq	(%rax,%r14,1),%rax
	leaq	(%r11,%r12,1),%r11
	andnq	%r10,%r8,%r12
	xorq	%r15,%r13
	rorxq	$14,%r8,%r14
	leaq	(%r11,%r12,1),%r11
	xorq	%r14,%r13
	movq	%rax,%r15
	rorxq	$39,%rax,%r12
	leaq	(%r11,%r13,1),%r11
	xorq	%rbx,%r15
	rorxq	$34,%rax,%r14
	rorxq	$28,%rax,%r13
	leaq	(%rdx,%r11,1),%rdx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rbx,%rdi
	xorq	%r13,%r14
	leaq	(%r11,%rdi,1),%r11
	movq	%r8,%r12
	addq	8(%rsp),%r10
	andq	%rdx,%r12
	rorxq	$41,%rdx,%r13
	rorxq	$18,%rdx,%rdi
	leaq	(%r11,%r14,1),%r11
	leaq	(%r10,%r12,1),%r10
	andnq	%r9,%rdx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rdx,%r14
	leaq	(%r10,%r12,1),%r10
	xorq	%r14,%r13
	movq	%r11,%rdi
	rorxq	$39,%r11,%r12
	leaq	(%r10,%r13,1),%r10
	xorq	%rax,%rdi
	rorxq	$34,%r11,%r14
	rorxq	$28,%r11,%r13
	leaq	(%rcx,%r10,1),%rcx
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rax,%r15
	xorq	%r13,%r14
	leaq	(%r10,%r15,1),%r10
	movq	%rdx,%r12
	addq	32(%rsp),%r9
	andq	%rcx,%r12
	rorxq	$41,%rcx,%r13
	rorxq	$18,%rcx,%r15
	leaq	(%r10,%r14,1),%r10
	leaq	(%r9,%r12,1),%r9
	andnq	%r8,%rcx,%r12
	xorq	%r15,%r13
	rorxq	$14,%rcx,%r14
	leaq	(%r9,%r12,1),%r9
	xorq	%r14,%r13
	movq	%r10,%r15
	rorxq	$39,%r10,%r12
	leaq	(%r9,%r13,1),%r9
	xorq	%r11,%r15
	rorxq	$34,%r10,%r14
	rorxq	$28,%r10,%r13
	leaq	(%rbx,%r9,1),%rbx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r11,%rdi
	xorq	%r13,%r14
	leaq	(%r9,%rdi,1),%r9
	movq	%rcx,%r12
	addq	40(%rsp),%r8
	andq	%rbx,%r12
	rorxq	$41,%rbx,%r13
	rorxq	$18,%rbx,%rdi
	leaq	(%r9,%r14,1),%r9
	leaq	(%r8,%r12,1),%r8
	andnq	%rdx,%rbx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rbx,%r14
	leaq	(%r8,%r12,1),%r8
	xorq	%r14,%r13
	movq	%r9,%rdi
	rorxq	$39,%r9,%r12
	leaq	(%r8,%r13,1),%r8
	xorq	%r10,%rdi
	rorxq	$34,%r9,%r14
	rorxq	$28,%r9,%r13
	leaq	(%rax,%r8,1),%rax
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r10,%r15
	xorq	%r13,%r14
	leaq	(%r8,%r15,1),%r8
	movq	%rbx,%r12
	addq	64(%rsp),%rdx
	andq	%rax,%r12
	rorxq	$41,%rax,%r13
	rorxq	$18,%rax,%r15
	leaq	(%r8,%r14,1),%r8
	leaq	(%rdx,%r12,1),%rdx
	andnq	%rcx,%rax,%r12
	xorq	%r15,%r13
	rorxq	$14,%rax,%r14
	leaq	(%rdx,%r12,1),%rdx
	xorq	%r14,%r13
	movq	%r8,%r15
	rorxq	$39,%r8,%r12
	leaq	(%rdx,%r13,1),%rdx
	xorq	%r9,%r15
	rorxq	$34,%r8,%r14
	rorxq	$28,%r8,%r13
	leaq	(%r11,%rdx,1),%r11
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r9,%rdi
	xorq	%r13,%r14
	leaq	(%rdx,%rdi,1),%rdx
	movq	%rax,%r12
	addq	72(%rsp),%rcx
	andq	%r11,%r12
	rorxq	$41,%r11,%r13
	rorxq	$18,%r11,%rdi
	leaq	(%rdx,%r14,1),%rdx
	leaq	(%rcx,%r12,1),%rcx
	andnq	%rbx,%r11,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r11,%r14
	leaq	(%rcx,%r12,1),%rcx
	xorq	%r14,%r13
	movq	%rdx,%rdi
	rorxq	$39,%rdx,%r12
	leaq	(%rcx,%r13,1),%rcx
	xorq	%r8,%rdi
	rorxq	$34,%rdx,%r14
	rorxq	$28,%rdx,%r13
	leaq	(%r10,%rcx,1),%r10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r8,%r15
	xorq	%r13,%r14
	leaq	(%rcx,%r15,1),%rcx
	movq	%r11,%r12
	addq	96(%rsp),%rbx
	andq	%r10,%r12
	rorxq	$41,%r10,%r13
	rorxq	$18,%r10,%r15
	leaq	(%rcx,%r14,1),%rcx
	leaq	(%rbx,%r12,1),%rbx
	andnq	%rax,%r10,%r12
	xorq	%r15,%r13
	rorxq	$14,%r10,%r14
	leaq	(%rbx,%r12,1),%rbx
	xorq	%r14,%r13
	movq	%rcx,%r15
	rorxq	$39,%rcx,%r12
	leaq	(%rbx,%r13,1),%rbx
	xorq	%rdx,%r15
	rorxq	$34,%rcx,%r14
	rorxq	$28,%rcx,%r13
	leaq	(%r9,%rbx,1),%r9
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rdx,%rdi
	xorq	%r13,%r14
	leaq	(%rbx,%rdi,1),%rbx
	movq	%r10,%r12
	addq	104(%rsp),%rax
	andq	%r9,%r12
	rorxq	$41,%r9,%r13
	rorxq	$18,%r9,%rdi
	leaq	(%rbx,%r14,1),%rbx
	leaq	(%rax,%r12,1),%rax
	andnq	%r11,%r9,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r9,%r14
	leaq	(%rax,%r12,1),%rax
	xorq	%r14,%r13
	movq	%rbx,%rdi
	rorxq	$39,%rbx,%r12
	leaq	(%rax,%r13,1),%rax
	xorq	%rcx,%rdi
	rorxq	$34,%rbx,%r14
	rorxq	$28,%rbx,%r13
	leaq	(%r8,%rax,1),%r8
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rcx,%r15
	xorq	%r13,%r14
	leaq	(%rax,%r15,1),%rax
	movq	%r9,%r12
	movq	1280(%rsp),%rdi
	addq	%r14,%rax

	leaq	1152(%rsp),%rbp

	addq	0(%rdi),%rax
	addq	8(%rdi),%rbx
	addq	16(%rdi),%rcx
	addq	24(%rdi),%rdx
	addq	32(%rdi),%r8
	addq	40(%rdi),%r9
	addq	48(%rdi),%r10
	addq	56(%rdi),%r11

	movq	%rax,0(%rdi)
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)
	movq	%r8,32(%rdi)
	movq	%r9,40(%rdi)
	movq	%r10,48(%rdi)
	movq	%r11,56(%rdi)

	cmpq	144(%rbp),%rsi
	je	.Ldone_avx2

	xorq	%r14,%r14
	movq	%rbx,%rdi
	xorq	%rcx,%rdi
	movq	%r9,%r12
	jmp	.Lower_avx2
.align	16
.Lower_avx2:
	addq	0+16(%rbp),%r11
	andq	%r8,%r12
	rorxq	$41,%r8,%r13
	rorxq	$18,%r8,%r15
	leaq	(%rax,%r14,1),%rax
	leaq	(%r11,%r12,1),%r11
	andnq	%r10,%r8,%r12
	xorq	%r15,%r13
	rorxq	$14,%r8,%r14
	leaq	(%r11,%r12,1),%r11
	xorq	%r14,%r13
	movq	%rax,%r15
	rorxq	$39,%rax,%r12
	leaq	(%r11,%r13,1),%r11
	xorq	%rbx,%r15
	rorxq	$34,%rax,%r14
	rorxq	$28,%rax,%r13
	leaq	(%rdx,%r11,1),%rdx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rbx,%rdi
	xorq	%r13,%r14
	leaq	(%r11,%rdi,1),%r11
	movq	%r8,%r12
	addq	8+16(%rbp),%r10
	andq	%rdx,%r12
	rorxq	$41,%rdx,%r13
	rorxq	$18,%rdx,%rdi
	leaq	(%r11,%r14,1),%r11
	leaq	(%r10,%r12,1),%r10
	andnq	%r9,%rdx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rdx,%r14
	leaq	(%r10,%r12,1),%r10
	xorq	%r14,%r13
	movq	%r11,%rdi
	rorxq	$39,%r11,%r12
	leaq	(%r10,%r13,1),%r10
	xorq	%rax,%rdi
	rorxq	$34,%r11,%r14
	rorxq	$28,%r11,%r13
	leaq	(%rcx,%r10,1),%rcx
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rax,%r15
	xorq	%r13,%r14
	leaq	(%r10,%r15,1),%r10
	movq	%rdx,%r12
	addq	32+16(%rbp),%r9
	andq	%rcx,%r12
	rorxq	$41,%rcx,%r13
	rorxq	$18,%rcx,%r15
	leaq	(%r10,%r14,1),%r10
	leaq	(%r9,%r12,1),%r9
	andnq	%r8,%rcx,%r12
	xorq	%r15,%r13
	rorxq	$14,%rcx,%r14
	leaq	(%r9,%r12,1),%r9
	xorq	%r14,%r13
	movq	%r10,%r15
	rorxq	$39,%r10,%r12
	leaq	(%r9,%r13,1),%r9
	xorq	%r11,%r15
	rorxq	$34,%r10,%r14
	rorxq	$28,%r10,%r13
	leaq	(%rbx,%r9,1),%rbx
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r11,%rdi
	xorq	%r13,%r14
	leaq	(%r9,%rdi,1),%r9
	movq	%rcx,%r12
	addq	40+16(%rbp),%r8
	andq	%rbx,%r12
	rorxq	$41,%rbx,%r13
	rorxq	$18,%rbx,%rdi
	leaq	(%r9,%r14,1),%r9
	leaq	(%r8,%r12,1),%r8
	andnq	%rdx,%rbx,%r12
	xorq	%rdi,%r13
	rorxq	$14,%rbx,%r14
	leaq	(%r8,%r12,1),%r8
	xorq	%r14,%r13
	movq	%r9,%rdi
	rorxq	$39,%r9,%r12
	leaq	(%r8,%r13,1),%r8
	xorq	%r10,%rdi
	rorxq	$34,%r9,%r14
	rorxq	$28,%r9,%r13
	leaq	(%rax,%r8,1),%rax
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r10,%r15
	xorq	%r13,%r14
	leaq	(%r8,%r15,1),%r8
	movq	%rbx,%r12
	addq	64+16(%rbp),%rdx
	andq	%rax,%r12
	rorxq	$41,%rax,%r13
	rorxq	$18,%rax,%r15
	leaq	(%r8,%r14,1),%r8
	leaq	(%rdx,%r12,1),%rdx
	andnq	%rcx,%rax,%r12
	xorq	%r15,%r13
	rorxq	$14,%rax,%r14
	leaq	(%rdx,%r12,1),%rdx
	xorq	%r14,%r13
	movq	%r8,%r15
	rorxq	$39,%r8,%r12
	leaq	(%rdx,%r13,1),%rdx
	xorq	%r9,%r15
	rorxq	$34,%r8,%r14
	rorxq	$28,%r8,%r13
	leaq	(%r11,%rdx,1),%r11
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%r9,%rdi
	xorq	%r13,%r14
	leaq	(%rdx,%rdi,1),%rdx
	movq	%rax,%r12
	addq	72+16(%rbp),%rcx
	andq	%r11,%r12
	rorxq	$41,%r11,%r13
	rorxq	$18,%r11,%rdi
	leaq	(%rdx,%r14,1),%rdx
	leaq	(%rcx,%r12,1),%rcx
	andnq	%rbx,%r11,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r11,%r14
	leaq	(%rcx,%r12,1),%rcx
	xorq	%r14,%r13
	movq	%rdx,%rdi
	rorxq	$39,%rdx,%r12
	leaq	(%rcx,%r13,1),%rcx
	xorq	%r8,%rdi
	rorxq	$34,%rdx,%r14
	rorxq	$28,%rdx,%r13
	leaq	(%r10,%rcx,1),%r10
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%r8,%r15
	xorq	%r13,%r14
	leaq	(%rcx,%r15,1),%rcx
	movq	%r11,%r12
	addq	96+16(%rbp),%rbx
	andq	%r10,%r12
	rorxq	$41,%r10,%r13
	rorxq	$18,%r10,%r15
	leaq	(%rcx,%r14,1),%rcx
	leaq	(%rbx,%r12,1),%rbx
	andnq	%rax,%r10,%r12
	xorq	%r15,%r13
	rorxq	$14,%r10,%r14
	leaq	(%rbx,%r12,1),%rbx
	xorq	%r14,%r13
	movq	%rcx,%r15
	rorxq	$39,%rcx,%r12
	leaq	(%rbx,%r13,1),%rbx
	xorq	%rdx,%r15
	rorxq	$34,%rcx,%r14
	rorxq	$28,%rcx,%r13
	leaq	(%r9,%rbx,1),%r9
	andq	%r15,%rdi
	xorq	%r12,%r14
	xorq	%rdx,%rdi
	xorq	%r13,%r14
	leaq	(%rbx,%rdi,1),%rbx
	movq	%r10,%r12
	addq	104+16(%rbp),%rax
	andq	%r9,%r12
	rorxq	$41,%r9,%r13
	rorxq	$18,%r9,%rdi
	leaq	(%rbx,%r14,1),%rbx
	leaq	(%rax,%r12,1),%rax
	andnq	%r11,%r9,%r12
	xorq	%rdi,%r13
	rorxq	$14,%r9,%r14
	leaq	(%rax,%r12,1),%rax
	xorq	%r14,%r13
	movq	%rbx,%rdi
	rorxq	$39,%rbx,%r12
	leaq	(%rax,%r13,1),%rax
	xorq	%rcx,%rdi
	rorxq	$34,%rbx,%r14
	rorxq	$28,%rbx,%r13
	leaq	(%r8,%rax,1),%r8
	andq	%rdi,%r15
	xorq	%r12,%r14
	xorq	%rcx,%r15
	xorq	%r13,%r14
	leaq	(%rax,%r15,1),%rax
	movq	%r9,%r12
	leaq	-128(%rbp),%rbp
	cmpq	%rsp,%rbp
	jae	.Lower_avx2

	movq	1280(%rsp),%rdi
	addq	%r14,%rax

	leaq	1152(%rsp),%rsp

.cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08

	addq	0(%rdi),%rax
	addq	8(%rdi),%rbx
	addq	16(%rdi),%rcx
	addq	24(%rdi),%rdx
	addq	32(%rdi),%r8
	addq	40(%rdi),%r9
	leaq	256(%rsi),%rsi
	addq	48(%rdi),%r10
	movq	%rsi,%r12
	addq	56(%rdi),%r11
	cmpq	128+16(%rsp),%rsi

	movq	%rax,0(%rdi)
	cmoveq	%rsp,%r12
	movq	%rbx,8(%rdi)
	movq	%rcx,16(%rdi)
	movq	%rdx,24(%rdi)
	movq	%r8,32(%rdi)
	movq	%r9,40(%rdi)
	movq	%r10,48(%rdi)
	movq	%r11,56(%rdi)

	jbe	.Loop_avx2
	leaq	(%rsp),%rbp


.cfi_escape	0x0f,0x06,0x76,0x98,0x01,0x06,0x23,0x08

.Ldone_avx2:
	movq	152(%rbp),%rsi
.cfi_def_cfa	%rsi,8
	vzeroupper
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue_avx2:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	sha512_block_data_order_avx2,.-sha512_block_data_order_avx2
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/whrlpool/                     0000775 0000000 0000000 00000000000 14746647661 0027022 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/whrlpool/wp-x86_64.s          0000664 0000000 0000000 00000071766 14746647661 0030611 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	

.globl	whirlpool_block
.type	whirlpool_block,@function
.align	16
whirlpool_block:
.cfi_startproc	
	movq	%rsp,%rax
.cfi_def_cfa_register	%rax
	pushq	%rbx
.cfi_offset	%rbx,-16
	pushq	%rbp
.cfi_offset	%rbp,-24
	pushq	%r12
.cfi_offset	%r12,-32
	pushq	%r13
.cfi_offset	%r13,-40
	pushq	%r14
.cfi_offset	%r14,-48
	pushq	%r15
.cfi_offset	%r15,-56

	subq	$128+40,%rsp
	andq	$-64,%rsp

	leaq	128(%rsp),%r10
	movq	%rdi,0(%r10)
	movq	%rsi,8(%r10)
	movq	%rdx,16(%r10)
	movq	%rax,32(%r10)
.cfi_escape	0x0f,0x06,0x77,0xa0,0x01,0x06,0x23,0x08
.Lprologue:

	movq	%r10,%rbx
	leaq	.Ltable(%rip),%rbp

	xorq	%rcx,%rcx
	xorq	%rdx,%rdx
	movq	0(%rdi),%r8
	movq	8(%rdi),%r9
	movq	16(%rdi),%r10
	movq	24(%rdi),%r11
	movq	32(%rdi),%r12
	movq	40(%rdi),%r13
	movq	48(%rdi),%r14
	movq	56(%rdi),%r15
.Louterloop:
	movq	%r8,0(%rsp)
	movq	%r9,8(%rsp)
	movq	%r10,16(%rsp)
	movq	%r11,24(%rsp)
	movq	%r12,32(%rsp)
	movq	%r13,40(%rsp)
	movq	%r14,48(%rsp)
	movq	%r15,56(%rsp)
	xorq	0(%rsi),%r8
	xorq	8(%rsi),%r9
	xorq	16(%rsi),%r10
	xorq	24(%rsi),%r11
	xorq	32(%rsi),%r12
	xorq	40(%rsi),%r13
	xorq	48(%rsi),%r14
	xorq	56(%rsi),%r15
	movq	%r8,64+0(%rsp)
	movq	%r9,64+8(%rsp)
	movq	%r10,64+16(%rsp)
	movq	%r11,64+24(%rsp)
	movq	%r12,64+32(%rsp)
	movq	%r13,64+40(%rsp)
	movq	%r14,64+48(%rsp)
	movq	%r15,64+56(%rsp)
	xorq	%rsi,%rsi
	movq	%rsi,24(%rbx)
	jmp	.Lround
.align	16
.Lround:
	movq	4096(%rbp,%rsi,8),%r8
	movl	0(%rsp),%eax
	movl	4(%rsp),%ebx
	movzbl	%al,%ecx
	movzbl	%ah,%edx
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r8
	movq	7(%rbp,%rdi,8),%r9
	movl	0+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	movq	6(%rbp,%rsi,8),%r10
	movq	5(%rbp,%rdi,8),%r11
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	movq	4(%rbp,%rsi,8),%r12
	movq	3(%rbp,%rdi,8),%r13
	movl	0+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	movq	2(%rbp,%rsi,8),%r14
	movq	1(%rbp,%rdi,8),%r15
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r9
	xorq	7(%rbp,%rdi,8),%r10
	movl	8+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r11
	xorq	5(%rbp,%rdi,8),%r12
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r13
	xorq	3(%rbp,%rdi,8),%r14
	movl	8+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r15
	xorq	1(%rbp,%rdi,8),%r8
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r10
	xorq	7(%rbp,%rdi,8),%r11
	movl	16+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r12
	xorq	5(%rbp,%rdi,8),%r13
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r14
	xorq	3(%rbp,%rdi,8),%r15
	movl	16+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r8
	xorq	1(%rbp,%rdi,8),%r9
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r11
	xorq	7(%rbp,%rdi,8),%r12
	movl	24+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r13
	xorq	5(%rbp,%rdi,8),%r14
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r15
	xorq	3(%rbp,%rdi,8),%r8
	movl	24+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r9
	xorq	1(%rbp,%rdi,8),%r10
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r12
	xorq	7(%rbp,%rdi,8),%r13
	movl	32+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r14
	xorq	5(%rbp,%rdi,8),%r15
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r8
	xorq	3(%rbp,%rdi,8),%r9
	movl	32+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r10
	xorq	1(%rbp,%rdi,8),%r11
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r13
	xorq	7(%rbp,%rdi,8),%r14
	movl	40+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r15
	xorq	5(%rbp,%rdi,8),%r8
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r9
	xorq	3(%rbp,%rdi,8),%r10
	movl	40+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r11
	xorq	1(%rbp,%rdi,8),%r12
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r14
	xorq	7(%rbp,%rdi,8),%r15
	movl	48+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r8
	xorq	5(%rbp,%rdi,8),%r9
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r10
	xorq	3(%rbp,%rdi,8),%r11
	movl	48+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r12
	xorq	1(%rbp,%rdi,8),%r13
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r15
	xorq	7(%rbp,%rdi,8),%r8
	movl	56+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r9
	xorq	5(%rbp,%rdi,8),%r10
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r11
	xorq	3(%rbp,%rdi,8),%r12
	movl	56+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r13
	xorq	1(%rbp,%rdi,8),%r14
	movq	%r8,0(%rsp)
	movq	%r9,8(%rsp)
	movq	%r10,16(%rsp)
	movq	%r11,24(%rsp)
	movq	%r12,32(%rsp)
	movq	%r13,40(%rsp)
	movq	%r14,48(%rsp)
	movq	%r15,56(%rsp)
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r8
	xorq	7(%rbp,%rdi,8),%r9
	movl	64+0+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r10
	xorq	5(%rbp,%rdi,8),%r11
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r12
	xorq	3(%rbp,%rdi,8),%r13
	movl	64+0+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r14
	xorq	1(%rbp,%rdi,8),%r15
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r9
	xorq	7(%rbp,%rdi,8),%r10
	movl	64+8+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r11
	xorq	5(%rbp,%rdi,8),%r12
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r13
	xorq	3(%rbp,%rdi,8),%r14
	movl	64+8+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r15
	xorq	1(%rbp,%rdi,8),%r8
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r10
	xorq	7(%rbp,%rdi,8),%r11
	movl	64+16+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r12
	xorq	5(%rbp,%rdi,8),%r13
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r14
	xorq	3(%rbp,%rdi,8),%r15
	movl	64+16+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r8
	xorq	1(%rbp,%rdi,8),%r9
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r11
	xorq	7(%rbp,%rdi,8),%r12
	movl	64+24+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r13
	xorq	5(%rbp,%rdi,8),%r14
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r15
	xorq	3(%rbp,%rdi,8),%r8
	movl	64+24+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r9
	xorq	1(%rbp,%rdi,8),%r10
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r12
	xorq	7(%rbp,%rdi,8),%r13
	movl	64+32+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r14
	xorq	5(%rbp,%rdi,8),%r15
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r8
	xorq	3(%rbp,%rdi,8),%r9
	movl	64+32+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r10
	xorq	1(%rbp,%rdi,8),%r11
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r13
	xorq	7(%rbp,%rdi,8),%r14
	movl	64+40+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r15
	xorq	5(%rbp,%rdi,8),%r8
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r9
	xorq	3(%rbp,%rdi,8),%r10
	movl	64+40+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r11
	xorq	1(%rbp,%rdi,8),%r12
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r14
	xorq	7(%rbp,%rdi,8),%r15
	movl	64+48+8(%rsp),%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r8
	xorq	5(%rbp,%rdi,8),%r9
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r10
	xorq	3(%rbp,%rdi,8),%r11
	movl	64+48+8+4(%rsp),%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r12
	xorq	1(%rbp,%rdi,8),%r13
	shrl	$16,%eax
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	0(%rbp,%rsi,8),%r15
	xorq	7(%rbp,%rdi,8),%r8

	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	6(%rbp,%rsi,8),%r9
	xorq	5(%rbp,%rdi,8),%r10
	shrl	$16,%ebx
	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%bl,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%bh,%edx
	xorq	4(%rbp,%rsi,8),%r11
	xorq	3(%rbp,%rdi,8),%r12

	leaq	(%rcx,%rcx,1),%rsi
	movzbl	%al,%ecx
	leaq	(%rdx,%rdx,1),%rdi
	movzbl	%ah,%edx
	xorq	2(%rbp,%rsi,8),%r13
	xorq	1(%rbp,%rdi,8),%r14
	leaq	128(%rsp),%rbx
	movq	24(%rbx),%rsi
	addq	$1,%rsi
	cmpq	$10,%rsi
	je	.Lroundsdone

	movq	%rsi,24(%rbx)
	movq	%r8,64+0(%rsp)
	movq	%r9,64+8(%rsp)
	movq	%r10,64+16(%rsp)
	movq	%r11,64+24(%rsp)
	movq	%r12,64+32(%rsp)
	movq	%r13,64+40(%rsp)
	movq	%r14,64+48(%rsp)
	movq	%r15,64+56(%rsp)
	jmp	.Lround
.align	16
.Lroundsdone:
	movq	0(%rbx),%rdi
	movq	8(%rbx),%rsi
	movq	16(%rbx),%rax
	xorq	0(%rsi),%r8
	xorq	8(%rsi),%r9
	xorq	16(%rsi),%r10
	xorq	24(%rsi),%r11
	xorq	32(%rsi),%r12
	xorq	40(%rsi),%r13
	xorq	48(%rsi),%r14
	xorq	56(%rsi),%r15
	xorq	0(%rdi),%r8
	xorq	8(%rdi),%r9
	xorq	16(%rdi),%r10
	xorq	24(%rdi),%r11
	xorq	32(%rdi),%r12
	xorq	40(%rdi),%r13
	xorq	48(%rdi),%r14
	xorq	56(%rdi),%r15
	movq	%r8,0(%rdi)
	movq	%r9,8(%rdi)
	movq	%r10,16(%rdi)
	movq	%r11,24(%rdi)
	movq	%r12,32(%rdi)
	movq	%r13,40(%rdi)
	movq	%r14,48(%rdi)
	movq	%r15,56(%rdi)
	leaq	64(%rsi),%rsi
	subq	$1,%rax
	jz	.Lalldone
	movq	%rsi,8(%rbx)
	movq	%rax,16(%rbx)
	jmp	.Louterloop
.Lalldone:
	movq	32(%rbx),%rsi
.cfi_def_cfa	%rsi,8
	movq	-48(%rsi),%r15
.cfi_restore	%r15
	movq	-40(%rsi),%r14
.cfi_restore	%r14
	movq	-32(%rsi),%r13
.cfi_restore	%r13
	movq	-24(%rsi),%r12
.cfi_restore	%r12
	movq	-16(%rsi),%rbp
.cfi_restore	%rbp
	movq	-8(%rsi),%rbx
.cfi_restore	%rbx
	leaq	(%rsi),%rsp
.cfi_def_cfa_register	%rsp
.Lepilogue:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	whirlpool_block,.-whirlpool_block

.align	64
.type	.Ltable,@object
.Ltable:
.byte	24,24,96,24,192,120,48,216,24,24,96,24,192,120,48,216
.byte	35,35,140,35,5,175,70,38,35,35,140,35,5,175,70,38
.byte	198,198,63,198,126,249,145,184,198,198,63,198,126,249,145,184
.byte	232,232,135,232,19,111,205,251,232,232,135,232,19,111,205,251
.byte	135,135,38,135,76,161,19,203,135,135,38,135,76,161,19,203
.byte	184,184,218,184,169,98,109,17,184,184,218,184,169,98,109,17
.byte	1,1,4,1,8,5,2,9,1,1,4,1,8,5,2,9
.byte	79,79,33,79,66,110,158,13,79,79,33,79,66,110,158,13
.byte	54,54,216,54,173,238,108,155,54,54,216,54,173,238,108,155
.byte	166,166,162,166,89,4,81,255,166,166,162,166,89,4,81,255
.byte	210,210,111,210,222,189,185,12,210,210,111,210,222,189,185,12
.byte	245,245,243,245,251,6,247,14,245,245,243,245,251,6,247,14
.byte	121,121,249,121,239,128,242,150,121,121,249,121,239,128,242,150
.byte	111,111,161,111,95,206,222,48,111,111,161,111,95,206,222,48
.byte	145,145,126,145,252,239,63,109,145,145,126,145,252,239,63,109
.byte	82,82,85,82,170,7,164,248,82,82,85,82,170,7,164,248
.byte	96,96,157,96,39,253,192,71,96,96,157,96,39,253,192,71
.byte	188,188,202,188,137,118,101,53,188,188,202,188,137,118,101,53
.byte	155,155,86,155,172,205,43,55,155,155,86,155,172,205,43,55
.byte	142,142,2,142,4,140,1,138,142,142,2,142,4,140,1,138
.byte	163,163,182,163,113,21,91,210,163,163,182,163,113,21,91,210
.byte	12,12,48,12,96,60,24,108,12,12,48,12,96,60,24,108
.byte	123,123,241,123,255,138,246,132,123,123,241,123,255,138,246,132
.byte	53,53,212,53,181,225,106,128,53,53,212,53,181,225,106,128
.byte	29,29,116,29,232,105,58,245,29,29,116,29,232,105,58,245
.byte	224,224,167,224,83,71,221,179,224,224,167,224,83,71,221,179
.byte	215,215,123,215,246,172,179,33,215,215,123,215,246,172,179,33
.byte	194,194,47,194,94,237,153,156,194,194,47,194,94,237,153,156
.byte	46,46,184,46,109,150,92,67,46,46,184,46,109,150,92,67
.byte	75,75,49,75,98,122,150,41,75,75,49,75,98,122,150,41
.byte	254,254,223,254,163,33,225,93,254,254,223,254,163,33,225,93
.byte	87,87,65,87,130,22,174,213,87,87,65,87,130,22,174,213
.byte	21,21,84,21,168,65,42,189,21,21,84,21,168,65,42,189
.byte	119,119,193,119,159,182,238,232,119,119,193,119,159,182,238,232
.byte	55,55,220,55,165,235,110,146,55,55,220,55,165,235,110,146
.byte	229,229,179,229,123,86,215,158,229,229,179,229,123,86,215,158
.byte	159,159,70,159,140,217,35,19,159,159,70,159,140,217,35,19
.byte	240,240,231,240,211,23,253,35,240,240,231,240,211,23,253,35
.byte	74,74,53,74,106,127,148,32,74,74,53,74,106,127,148,32
.byte	218,218,79,218,158,149,169,68,218,218,79,218,158,149,169,68
.byte	88,88,125,88,250,37,176,162,88,88,125,88,250,37,176,162
.byte	201,201,3,201,6,202,143,207,201,201,3,201,6,202,143,207
.byte	41,41,164,41,85,141,82,124,41,41,164,41,85,141,82,124
.byte	10,10,40,10,80,34,20,90,10,10,40,10,80,34,20,90
.byte	177,177,254,177,225,79,127,80,177,177,254,177,225,79,127,80
.byte	160,160,186,160,105,26,93,201,160,160,186,160,105,26,93,201
.byte	107,107,177,107,127,218,214,20,107,107,177,107,127,218,214,20
.byte	133,133,46,133,92,171,23,217,133,133,46,133,92,171,23,217
.byte	189,189,206,189,129,115,103,60,189,189,206,189,129,115,103,60
.byte	93,93,105,93,210,52,186,143,93,93,105,93,210,52,186,143
.byte	16,16,64,16,128,80,32,144,16,16,64,16,128,80,32,144
.byte	244,244,247,244,243,3,245,7,244,244,247,244,243,3,245,7
.byte	203,203,11,203,22,192,139,221,203,203,11,203,22,192,139,221
.byte	62,62,248,62,237,198,124,211,62,62,248,62,237,198,124,211
.byte	5,5,20,5,40,17,10,45,5,5,20,5,40,17,10,45
.byte	103,103,129,103,31,230,206,120,103,103,129,103,31,230,206,120
.byte	228,228,183,228,115,83,213,151,228,228,183,228,115,83,213,151
.byte	39,39,156,39,37,187,78,2,39,39,156,39,37,187,78,2
.byte	65,65,25,65,50,88,130,115,65,65,25,65,50,88,130,115
.byte	139,139,22,139,44,157,11,167,139,139,22,139,44,157,11,167
.byte	167,167,166,167,81,1,83,246,167,167,166,167,81,1,83,246
.byte	125,125,233,125,207,148,250,178,125,125,233,125,207,148,250,178
.byte	149,149,110,149,220,251,55,73,149,149,110,149,220,251,55,73
.byte	216,216,71,216,142,159,173,86,216,216,71,216,142,159,173,86
.byte	251,251,203,251,139,48,235,112,251,251,203,251,139,48,235,112
.byte	238,238,159,238,35,113,193,205,238,238,159,238,35,113,193,205
.byte	124,124,237,124,199,145,248,187,124,124,237,124,199,145,248,187
.byte	102,102,133,102,23,227,204,113,102,102,133,102,23,227,204,113
.byte	221,221,83,221,166,142,167,123,221,221,83,221,166,142,167,123
.byte	23,23,92,23,184,75,46,175,23,23,92,23,184,75,46,175
.byte	71,71,1,71,2,70,142,69,71,71,1,71,2,70,142,69
.byte	158,158,66,158,132,220,33,26,158,158,66,158,132,220,33,26
.byte	202,202,15,202,30,197,137,212,202,202,15,202,30,197,137,212
.byte	45,45,180,45,117,153,90,88,45,45,180,45,117,153,90,88
.byte	191,191,198,191,145,121,99,46,191,191,198,191,145,121,99,46
.byte	7,7,28,7,56,27,14,63,7,7,28,7,56,27,14,63
.byte	173,173,142,173,1,35,71,172,173,173,142,173,1,35,71,172
.byte	90,90,117,90,234,47,180,176,90,90,117,90,234,47,180,176
.byte	131,131,54,131,108,181,27,239,131,131,54,131,108,181,27,239
.byte	51,51,204,51,133,255,102,182,51,51,204,51,133,255,102,182
.byte	99,99,145,99,63,242,198,92,99,99,145,99,63,242,198,92
.byte	2,2,8,2,16,10,4,18,2,2,8,2,16,10,4,18
.byte	170,170,146,170,57,56,73,147,170,170,146,170,57,56,73,147
.byte	113,113,217,113,175,168,226,222,113,113,217,113,175,168,226,222
.byte	200,200,7,200,14,207,141,198,200,200,7,200,14,207,141,198
.byte	25,25,100,25,200,125,50,209,25,25,100,25,200,125,50,209
.byte	73,73,57,73,114,112,146,59,73,73,57,73,114,112,146,59
.byte	217,217,67,217,134,154,175,95,217,217,67,217,134,154,175,95
.byte	242,242,239,242,195,29,249,49,242,242,239,242,195,29,249,49
.byte	227,227,171,227,75,72,219,168,227,227,171,227,75,72,219,168
.byte	91,91,113,91,226,42,182,185,91,91,113,91,226,42,182,185
.byte	136,136,26,136,52,146,13,188,136,136,26,136,52,146,13,188
.byte	154,154,82,154,164,200,41,62,154,154,82,154,164,200,41,62
.byte	38,38,152,38,45,190,76,11,38,38,152,38,45,190,76,11
.byte	50,50,200,50,141,250,100,191,50,50,200,50,141,250,100,191
.byte	176,176,250,176,233,74,125,89,176,176,250,176,233,74,125,89
.byte	233,233,131,233,27,106,207,242,233,233,131,233,27,106,207,242
.byte	15,15,60,15,120,51,30,119,15,15,60,15,120,51,30,119
.byte	213,213,115,213,230,166,183,51,213,213,115,213,230,166,183,51
.byte	128,128,58,128,116,186,29,244,128,128,58,128,116,186,29,244
.byte	190,190,194,190,153,124,97,39,190,190,194,190,153,124,97,39
.byte	205,205,19,205,38,222,135,235,205,205,19,205,38,222,135,235
.byte	52,52,208,52,189,228,104,137,52,52,208,52,189,228,104,137
.byte	72,72,61,72,122,117,144,50,72,72,61,72,122,117,144,50
.byte	255,255,219,255,171,36,227,84,255,255,219,255,171,36,227,84
.byte	122,122,245,122,247,143,244,141,122,122,245,122,247,143,244,141
.byte	144,144,122,144,244,234,61,100,144,144,122,144,244,234,61,100
.byte	95,95,97,95,194,62,190,157,95,95,97,95,194,62,190,157
.byte	32,32,128,32,29,160,64,61,32,32,128,32,29,160,64,61
.byte	104,104,189,104,103,213,208,15,104,104,189,104,103,213,208,15
.byte	26,26,104,26,208,114,52,202,26,26,104,26,208,114,52,202
.byte	174,174,130,174,25,44,65,183,174,174,130,174,25,44,65,183
.byte	180,180,234,180,201,94,117,125,180,180,234,180,201,94,117,125
.byte	84,84,77,84,154,25,168,206,84,84,77,84,154,25,168,206
.byte	147,147,118,147,236,229,59,127,147,147,118,147,236,229,59,127
.byte	34,34,136,34,13,170,68,47,34,34,136,34,13,170,68,47
.byte	100,100,141,100,7,233,200,99,100,100,141,100,7,233,200,99
.byte	241,241,227,241,219,18,255,42,241,241,227,241,219,18,255,42
.byte	115,115,209,115,191,162,230,204,115,115,209,115,191,162,230,204
.byte	18,18,72,18,144,90,36,130,18,18,72,18,144,90,36,130
.byte	64,64,29,64,58,93,128,122,64,64,29,64,58,93,128,122
.byte	8,8,32,8,64,40,16,72,8,8,32,8,64,40,16,72
.byte	195,195,43,195,86,232,155,149,195,195,43,195,86,232,155,149
.byte	236,236,151,236,51,123,197,223,236,236,151,236,51,123,197,223
.byte	219,219,75,219,150,144,171,77,219,219,75,219,150,144,171,77
.byte	161,161,190,161,97,31,95,192,161,161,190,161,97,31,95,192
.byte	141,141,14,141,28,131,7,145,141,141,14,141,28,131,7,145
.byte	61,61,244,61,245,201,122,200,61,61,244,61,245,201,122,200
.byte	151,151,102,151,204,241,51,91,151,151,102,151,204,241,51,91
.byte	0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
.byte	207,207,27,207,54,212,131,249,207,207,27,207,54,212,131,249
.byte	43,43,172,43,69,135,86,110,43,43,172,43,69,135,86,110
.byte	118,118,197,118,151,179,236,225,118,118,197,118,151,179,236,225
.byte	130,130,50,130,100,176,25,230,130,130,50,130,100,176,25,230
.byte	214,214,127,214,254,169,177,40,214,214,127,214,254,169,177,40
.byte	27,27,108,27,216,119,54,195,27,27,108,27,216,119,54,195
.byte	181,181,238,181,193,91,119,116,181,181,238,181,193,91,119,116
.byte	175,175,134,175,17,41,67,190,175,175,134,175,17,41,67,190
.byte	106,106,181,106,119,223,212,29,106,106,181,106,119,223,212,29
.byte	80,80,93,80,186,13,160,234,80,80,93,80,186,13,160,234
.byte	69,69,9,69,18,76,138,87,69,69,9,69,18,76,138,87
.byte	243,243,235,243,203,24,251,56,243,243,235,243,203,24,251,56
.byte	48,48,192,48,157,240,96,173,48,48,192,48,157,240,96,173
.byte	239,239,155,239,43,116,195,196,239,239,155,239,43,116,195,196
.byte	63,63,252,63,229,195,126,218,63,63,252,63,229,195,126,218
.byte	85,85,73,85,146,28,170,199,85,85,73,85,146,28,170,199
.byte	162,162,178,162,121,16,89,219,162,162,178,162,121,16,89,219
.byte	234,234,143,234,3,101,201,233,234,234,143,234,3,101,201,233
.byte	101,101,137,101,15,236,202,106,101,101,137,101,15,236,202,106
.byte	186,186,210,186,185,104,105,3,186,186,210,186,185,104,105,3
.byte	47,47,188,47,101,147,94,74,47,47,188,47,101,147,94,74
.byte	192,192,39,192,78,231,157,142,192,192,39,192,78,231,157,142
.byte	222,222,95,222,190,129,161,96,222,222,95,222,190,129,161,96
.byte	28,28,112,28,224,108,56,252,28,28,112,28,224,108,56,252
.byte	253,253,211,253,187,46,231,70,253,253,211,253,187,46,231,70
.byte	77,77,41,77,82,100,154,31,77,77,41,77,82,100,154,31
.byte	146,146,114,146,228,224,57,118,146,146,114,146,228,224,57,118
.byte	117,117,201,117,143,188,234,250,117,117,201,117,143,188,234,250
.byte	6,6,24,6,48,30,12,54,6,6,24,6,48,30,12,54
.byte	138,138,18,138,36,152,9,174,138,138,18,138,36,152,9,174
.byte	178,178,242,178,249,64,121,75,178,178,242,178,249,64,121,75
.byte	230,230,191,230,99,89,209,133,230,230,191,230,99,89,209,133
.byte	14,14,56,14,112,54,28,126,14,14,56,14,112,54,28,126
.byte	31,31,124,31,248,99,62,231,31,31,124,31,248,99,62,231
.byte	98,98,149,98,55,247,196,85,98,98,149,98,55,247,196,85
.byte	212,212,119,212,238,163,181,58,212,212,119,212,238,163,181,58
.byte	168,168,154,168,41,50,77,129,168,168,154,168,41,50,77,129
.byte	150,150,98,150,196,244,49,82,150,150,98,150,196,244,49,82
.byte	249,249,195,249,155,58,239,98,249,249,195,249,155,58,239,98
.byte	197,197,51,197,102,246,151,163,197,197,51,197,102,246,151,163
.byte	37,37,148,37,53,177,74,16,37,37,148,37,53,177,74,16
.byte	89,89,121,89,242,32,178,171,89,89,121,89,242,32,178,171
.byte	132,132,42,132,84,174,21,208,132,132,42,132,84,174,21,208
.byte	114,114,213,114,183,167,228,197,114,114,213,114,183,167,228,197
.byte	57,57,228,57,213,221,114,236,57,57,228,57,213,221,114,236
.byte	76,76,45,76,90,97,152,22,76,76,45,76,90,97,152,22
.byte	94,94,101,94,202,59,188,148,94,94,101,94,202,59,188,148
.byte	120,120,253,120,231,133,240,159,120,120,253,120,231,133,240,159
.byte	56,56,224,56,221,216,112,229,56,56,224,56,221,216,112,229
.byte	140,140,10,140,20,134,5,152,140,140,10,140,20,134,5,152
.byte	209,209,99,209,198,178,191,23,209,209,99,209,198,178,191,23
.byte	165,165,174,165,65,11,87,228,165,165,174,165,65,11,87,228
.byte	226,226,175,226,67,77,217,161,226,226,175,226,67,77,217,161
.byte	97,97,153,97,47,248,194,78,97,97,153,97,47,248,194,78
.byte	179,179,246,179,241,69,123,66,179,179,246,179,241,69,123,66
.byte	33,33,132,33,21,165,66,52,33,33,132,33,21,165,66,52
.byte	156,156,74,156,148,214,37,8,156,156,74,156,148,214,37,8
.byte	30,30,120,30,240,102,60,238,30,30,120,30,240,102,60,238
.byte	67,67,17,67,34,82,134,97,67,67,17,67,34,82,134,97
.byte	199,199,59,199,118,252,147,177,199,199,59,199,118,252,147,177
.byte	252,252,215,252,179,43,229,79,252,252,215,252,179,43,229,79
.byte	4,4,16,4,32,20,8,36,4,4,16,4,32,20,8,36
.byte	81,81,89,81,178,8,162,227,81,81,89,81,178,8,162,227
.byte	153,153,94,153,188,199,47,37,153,153,94,153,188,199,47,37
.byte	109,109,169,109,79,196,218,34,109,109,169,109,79,196,218,34
.byte	13,13,52,13,104,57,26,101,13,13,52,13,104,57,26,101
.byte	250,250,207,250,131,53,233,121,250,250,207,250,131,53,233,121
.byte	223,223,91,223,182,132,163,105,223,223,91,223,182,132,163,105
.byte	126,126,229,126,215,155,252,169,126,126,229,126,215,155,252,169
.byte	36,36,144,36,61,180,72,25,36,36,144,36,61,180,72,25
.byte	59,59,236,59,197,215,118,254,59,59,236,59,197,215,118,254
.byte	171,171,150,171,49,61,75,154,171,171,150,171,49,61,75,154
.byte	206,206,31,206,62,209,129,240,206,206,31,206,62,209,129,240
.byte	17,17,68,17,136,85,34,153,17,17,68,17,136,85,34,153
.byte	143,143,6,143,12,137,3,131,143,143,6,143,12,137,3,131
.byte	78,78,37,78,74,107,156,4,78,78,37,78,74,107,156,4
.byte	183,183,230,183,209,81,115,102,183,183,230,183,209,81,115,102
.byte	235,235,139,235,11,96,203,224,235,235,139,235,11,96,203,224
.byte	60,60,240,60,253,204,120,193,60,60,240,60,253,204,120,193
.byte	129,129,62,129,124,191,31,253,129,129,62,129,124,191,31,253
.byte	148,148,106,148,212,254,53,64,148,148,106,148,212,254,53,64
.byte	247,247,251,247,235,12,243,28,247,247,251,247,235,12,243,28
.byte	185,185,222,185,161,103,111,24,185,185,222,185,161,103,111,24
.byte	19,19,76,19,152,95,38,139,19,19,76,19,152,95,38,139
.byte	44,44,176,44,125,156,88,81,44,44,176,44,125,156,88,81
.byte	211,211,107,211,214,184,187,5,211,211,107,211,214,184,187,5
.byte	231,231,187,231,107,92,211,140,231,231,187,231,107,92,211,140
.byte	110,110,165,110,87,203,220,57,110,110,165,110,87,203,220,57
.byte	196,196,55,196,110,243,149,170,196,196,55,196,110,243,149,170
.byte	3,3,12,3,24,15,6,27,3,3,12,3,24,15,6,27
.byte	86,86,69,86,138,19,172,220,86,86,69,86,138,19,172,220
.byte	68,68,13,68,26,73,136,94,68,68,13,68,26,73,136,94
.byte	127,127,225,127,223,158,254,160,127,127,225,127,223,158,254,160
.byte	169,169,158,169,33,55,79,136,169,169,158,169,33,55,79,136
.byte	42,42,168,42,77,130,84,103,42,42,168,42,77,130,84,103
.byte	187,187,214,187,177,109,107,10,187,187,214,187,177,109,107,10
.byte	193,193,35,193,70,226,159,135,193,193,35,193,70,226,159,135
.byte	83,83,81,83,162,2,166,241,83,83,81,83,162,2,166,241
.byte	220,220,87,220,174,139,165,114,220,220,87,220,174,139,165,114
.byte	11,11,44,11,88,39,22,83,11,11,44,11,88,39,22,83
.byte	157,157,78,157,156,211,39,1,157,157,78,157,156,211,39,1
.byte	108,108,173,108,71,193,216,43,108,108,173,108,71,193,216,43
.byte	49,49,196,49,149,245,98,164,49,49,196,49,149,245,98,164
.byte	116,116,205,116,135,185,232,243,116,116,205,116,135,185,232,243
.byte	246,246,255,246,227,9,241,21,246,246,255,246,227,9,241,21
.byte	70,70,5,70,10,67,140,76,70,70,5,70,10,67,140,76
.byte	172,172,138,172,9,38,69,165,172,172,138,172,9,38,69,165
.byte	137,137,30,137,60,151,15,181,137,137,30,137,60,151,15,181
.byte	20,20,80,20,160,68,40,180,20,20,80,20,160,68,40,180
.byte	225,225,163,225,91,66,223,186,225,225,163,225,91,66,223,186
.byte	22,22,88,22,176,78,44,166,22,22,88,22,176,78,44,166
.byte	58,58,232,58,205,210,116,247,58,58,232,58,205,210,116,247
.byte	105,105,185,105,111,208,210,6,105,105,185,105,111,208,210,6
.byte	9,9,36,9,72,45,18,65,9,9,36,9,72,45,18,65
.byte	112,112,221,112,167,173,224,215,112,112,221,112,167,173,224,215
.byte	182,182,226,182,217,84,113,111,182,182,226,182,217,84,113,111
.byte	208,208,103,208,206,183,189,30,208,208,103,208,206,183,189,30
.byte	237,237,147,237,59,126,199,214,237,237,147,237,59,126,199,214
.byte	204,204,23,204,46,219,133,226,204,204,23,204,46,219,133,226
.byte	66,66,21,66,42,87,132,104,66,66,21,66,42,87,132,104
.byte	152,152,90,152,180,194,45,44,152,152,90,152,180,194,45,44
.byte	164,164,170,164,73,14,85,237,164,164,170,164,73,14,85,237
.byte	40,40,160,40,93,136,80,117,40,40,160,40,93,136,80,117
.byte	92,92,109,92,218,49,184,134,92,92,109,92,218,49,184,134
.byte	248,248,199,248,147,63,237,107,248,248,199,248,147,63,237,107
.byte	134,134,34,134,68,164,17,194,134,134,34,134,68,164,17,194
.byte	24,35,198,232,135,184,1,79
.byte	54,166,210,245,121,111,145,82
.byte	96,188,155,142,163,12,123,53
.byte	29,224,215,194,46,75,254,87
.byte	21,119,55,229,159,240,74,218
.byte	88,201,41,10,177,160,107,133
.byte	189,93,16,244,203,62,5,103
.byte	228,39,65,139,167,125,149,216
.byte	251,238,124,102,221,23,71,158
.byte	202,45,191,7,173,90,131,51
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
          node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/crypto/x86_64cpuid.s                 0000664 0000000 0000000 00000017761 14746647661 0027337 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        
.hidden	OPENSSL_cpuid_setup
.section	.init
	call	OPENSSL_cpuid_setup

.hidden	OPENSSL_ia32cap_P
.comm	OPENSSL_ia32cap_P,16,4

.text	

.globl	OPENSSL_atomic_add
.type	OPENSSL_atomic_add,@function
.align	16
OPENSSL_atomic_add:
.cfi_startproc	
.byte	243,15,30,250
	movl	(%rdi),%eax
.Lspin:	leaq	(%rsi,%rax,1),%r8
.byte	0xf0
	cmpxchgl	%r8d,(%rdi)
	jne	.Lspin
	movl	%r8d,%eax
.byte	0x48,0x98
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_atomic_add,.-OPENSSL_atomic_add

.globl	OPENSSL_rdtsc
.type	OPENSSL_rdtsc,@function
.align	16
OPENSSL_rdtsc:
.cfi_startproc	
.byte	243,15,30,250
	rdtsc
	shlq	$32,%rdx
	orq	%rdx,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_rdtsc,.-OPENSSL_rdtsc

.globl	OPENSSL_ia32_cpuid
.type	OPENSSL_ia32_cpuid,@function
.align	16
OPENSSL_ia32_cpuid:
.cfi_startproc	
.byte	243,15,30,250
	movq	%rbx,%r8
.cfi_register	%rbx,%r8

	xorl	%eax,%eax
	movq	%rax,8(%rdi)
	cpuid
	movl	%eax,%r11d

	xorl	%eax,%eax
	cmpl	$0x756e6547,%ebx
	setne	%al
	movl	%eax,%r9d
	cmpl	$0x49656e69,%edx
	setne	%al
	orl	%eax,%r9d
	cmpl	$0x6c65746e,%ecx
	setne	%al
	orl	%eax,%r9d
	jz	.Lintel

	cmpl	$0x68747541,%ebx
	setne	%al
	movl	%eax,%r10d
	cmpl	$0x69746E65,%edx
	setne	%al
	orl	%eax,%r10d
	cmpl	$0x444D4163,%ecx
	setne	%al
	orl	%eax,%r10d
	jnz	.Lintel


	movl	$0x80000000,%eax
	cpuid
	cmpl	$0x80000001,%eax
	jb	.Lintel
	movl	%eax,%r10d
	movl	$0x80000001,%eax
	cpuid
	orl	%ecx,%r9d
	andl	$0x00000801,%r9d

	cmpl	$0x80000008,%r10d
	jb	.Lintel

	movl	$0x80000008,%eax
	cpuid
	movzbq	%cl,%r10
	incq	%r10

	movl	$1,%eax
	cpuid
	btl	$28,%edx
	jnc	.Lgeneric
	shrl	$16,%ebx
	cmpb	%r10b,%bl
	ja	.Lgeneric
	andl	$0xefffffff,%edx
	jmp	.Lgeneric

.Lintel:
	cmpl	$4,%r11d
	movl	$-1,%r10d
	jb	.Lnocacheinfo

	movl	$4,%eax
	movl	$0,%ecx
	cpuid
	movl	%eax,%r10d
	shrl	$14,%r10d
	andl	$0xfff,%r10d

.Lnocacheinfo:
	movl	$1,%eax
	cpuid
	movd	%eax,%xmm0
	andl	$0xbfefffff,%edx
	cmpl	$0,%r9d
	jne	.Lnotintel
	orl	$0x40000000,%edx
	andb	$15,%ah
	cmpb	$15,%ah
	jne	.LnotP4
	orl	$0x00100000,%edx
.LnotP4:
	cmpb	$6,%ah
	jne	.Lnotintel
	andl	$0x0fff0ff0,%eax
	cmpl	$0x00050670,%eax
	je	.Lknights
	cmpl	$0x00080650,%eax
	jne	.Lnotintel
.Lknights:
	andl	$0xfbffffff,%ecx

.Lnotintel:
	btl	$28,%edx
	jnc	.Lgeneric
	andl	$0xefffffff,%edx
	cmpl	$0,%r10d
	je	.Lgeneric

	orl	$0x10000000,%edx
	shrl	$16,%ebx
	cmpb	$1,%bl
	ja	.Lgeneric
	andl	$0xefffffff,%edx
.Lgeneric:
	andl	$0x00000800,%r9d
	andl	$0xfffff7ff,%ecx
	orl	%ecx,%r9d

	movl	%edx,%r10d

	cmpl	$7,%r11d
	jb	.Lno_extended_info
	movl	$7,%eax
	xorl	%ecx,%ecx
	cpuid
	btl	$26,%r9d
	jc	.Lnotknights
	andl	$0xfff7ffff,%ebx
.Lnotknights:
	movd	%xmm0,%eax
	andl	$0x0fff0ff0,%eax
	cmpl	$0x00050650,%eax
	jne	.Lnotskylakex
	andl	$0xfffeffff,%ebx

.Lnotskylakex:
	movl	%ebx,8(%rdi)
	movl	%ecx,12(%rdi)
.Lno_extended_info:

	btl	$27,%r9d
	jnc	.Lclear_avx
	xorl	%ecx,%ecx
.byte	0x0f,0x01,0xd0
	andl	$0xe6,%eax
	cmpl	$0xe6,%eax
	je	.Ldone
	andl	$0x3fdeffff,8(%rdi)




	andl	$6,%eax
	cmpl	$6,%eax
	je	.Ldone
.Lclear_avx:
	movl	$0xefffe7ff,%eax
	andl	%eax,%r9d
	movl	$0x3fdeffdf,%eax
	andl	%eax,8(%rdi)
.Ldone:
	shlq	$32,%r9
	movl	%r10d,%eax
	movq	%r8,%rbx
.cfi_restore	%rbx
	orq	%r9,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_ia32_cpuid,.-OPENSSL_ia32_cpuid

.globl	OPENSSL_cleanse
.type	OPENSSL_cleanse,@function
.align	16
OPENSSL_cleanse:
.cfi_startproc	
.byte	243,15,30,250
	xorq	%rax,%rax
	cmpq	$15,%rsi
	jae	.Lot
	cmpq	$0,%rsi
	je	.Lret
.Little:
	movb	%al,(%rdi)
	subq	$1,%rsi
	leaq	1(%rdi),%rdi
	jnz	.Little
.Lret:
	.byte	0xf3,0xc3
.align	16
.Lot:
	testq	$7,%rdi
	jz	.Laligned
	movb	%al,(%rdi)
	leaq	-1(%rsi),%rsi
	leaq	1(%rdi),%rdi
	jmp	.Lot
.Laligned:
	movq	%rax,(%rdi)
	leaq	-8(%rsi),%rsi
	testq	$-8,%rsi
	leaq	8(%rdi),%rdi
	jnz	.Laligned
	cmpq	$0,%rsi
	jne	.Little
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_cleanse,.-OPENSSL_cleanse

.globl	CRYPTO_memcmp
.type	CRYPTO_memcmp,@function
.align	16
CRYPTO_memcmp:
.cfi_startproc	
.byte	243,15,30,250
	xorq	%rax,%rax
	xorq	%r10,%r10
	cmpq	$0,%rdx
	je	.Lno_data
	cmpq	$16,%rdx
	jne	.Loop_cmp
	movq	(%rdi),%r10
	movq	8(%rdi),%r11
	movq	$1,%rdx
	xorq	(%rsi),%r10
	xorq	8(%rsi),%r11
	orq	%r11,%r10
	cmovnzq	%rdx,%rax
	.byte	0xf3,0xc3

.align	16
.Loop_cmp:
	movb	(%rdi),%r10b
	leaq	1(%rdi),%rdi
	xorb	(%rsi),%r10b
	leaq	1(%rsi),%rsi
	orb	%r10b,%al
	decq	%rdx
	jnz	.Loop_cmp
	negq	%rax
	shrq	$63,%rax
.Lno_data:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	CRYPTO_memcmp,.-CRYPTO_memcmp
.globl	OPENSSL_wipe_cpu
.type	OPENSSL_wipe_cpu,@function
.align	16
OPENSSL_wipe_cpu:
.cfi_startproc	
.byte	243,15,30,250
	pxor	%xmm0,%xmm0
	pxor	%xmm1,%xmm1
	pxor	%xmm2,%xmm2
	pxor	%xmm3,%xmm3
	pxor	%xmm4,%xmm4
	pxor	%xmm5,%xmm5
	pxor	%xmm6,%xmm6
	pxor	%xmm7,%xmm7
	pxor	%xmm8,%xmm8
	pxor	%xmm9,%xmm9
	pxor	%xmm10,%xmm10
	pxor	%xmm11,%xmm11
	pxor	%xmm12,%xmm12
	pxor	%xmm13,%xmm13
	pxor	%xmm14,%xmm14
	pxor	%xmm15,%xmm15
	xorq	%rcx,%rcx
	xorq	%rdx,%rdx
	xorq	%rsi,%rsi
	xorq	%rdi,%rdi
	xorq	%r8,%r8
	xorq	%r9,%r9
	xorq	%r10,%r10
	xorq	%r11,%r11
	leaq	8(%rsp),%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_wipe_cpu,.-OPENSSL_wipe_cpu
.globl	OPENSSL_instrument_bus
.type	OPENSSL_instrument_bus,@function
.align	16
OPENSSL_instrument_bus:
.cfi_startproc	
.byte	243,15,30,250
	movq	%rdi,%r10
	movq	%rsi,%rcx
	movq	%rsi,%r11

	rdtsc
	movl	%eax,%r8d
	movl	$0,%r9d
	clflush	(%r10)
.byte	0xf0
	addl	%r9d,(%r10)
	jmp	.Loop
.align	16
.Loop:	rdtsc
	movl	%eax,%edx
	subl	%r8d,%eax
	movl	%edx,%r8d
	movl	%eax,%r9d
	clflush	(%r10)
.byte	0xf0
	addl	%eax,(%r10)
	leaq	4(%r10),%r10
	subq	$1,%rcx
	jnz	.Loop

	movq	%r11,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_instrument_bus,.-OPENSSL_instrument_bus

.globl	OPENSSL_instrument_bus2
.type	OPENSSL_instrument_bus2,@function
.align	16
OPENSSL_instrument_bus2:
.cfi_startproc	
.byte	243,15,30,250
	movq	%rdi,%r10
	movq	%rsi,%rcx
	movq	%rdx,%r11
	movq	%rcx,8(%rsp)

	rdtsc
	movl	%eax,%r8d
	movl	$0,%r9d

	clflush	(%r10)
.byte	0xf0
	addl	%r9d,(%r10)

	rdtsc
	movl	%eax,%edx
	subl	%r8d,%eax
	movl	%edx,%r8d
	movl	%eax,%r9d
.Loop2:
	clflush	(%r10)
.byte	0xf0
	addl	%eax,(%r10)

	subq	$1,%r11
	jz	.Ldone2

	rdtsc
	movl	%eax,%edx
	subl	%r8d,%eax
	movl	%edx,%r8d
	cmpl	%r9d,%eax
	movl	%eax,%r9d
	movl	$0,%edx
	setne	%dl
	subq	%rdx,%rcx
	leaq	(%r10,%rdx,4),%r10
	jnz	.Loop2

.Ldone2:
	movq	8(%rsp),%rax
	subq	%rcx,%rax
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_instrument_bus2,.-OPENSSL_instrument_bus2
.globl	OPENSSL_ia32_rdrand_bytes
.type	OPENSSL_ia32_rdrand_bytes,@function
.align	16
OPENSSL_ia32_rdrand_bytes:
.cfi_startproc	
.byte	243,15,30,250
	xorq	%rax,%rax
	cmpq	$0,%rsi
	je	.Ldone_rdrand_bytes

	movq	$8,%r11
.Loop_rdrand_bytes:
.byte	73,15,199,242
	jc	.Lbreak_rdrand_bytes
	decq	%r11
	jnz	.Loop_rdrand_bytes
	jmp	.Ldone_rdrand_bytes

.align	16
.Lbreak_rdrand_bytes:
	cmpq	$8,%rsi
	jb	.Ltail_rdrand_bytes
	movq	%r10,(%rdi)
	leaq	8(%rdi),%rdi
	addq	$8,%rax
	subq	$8,%rsi
	jz	.Ldone_rdrand_bytes
	movq	$8,%r11
	jmp	.Loop_rdrand_bytes

.align	16
.Ltail_rdrand_bytes:
	movb	%r10b,(%rdi)
	leaq	1(%rdi),%rdi
	incq	%rax
	shrq	$8,%r10
	decq	%rsi
	jnz	.Ltail_rdrand_bytes

.Ldone_rdrand_bytes:
	xorq	%r10,%r10
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_ia32_rdrand_bytes,.-OPENSSL_ia32_rdrand_bytes
.globl	OPENSSL_ia32_rdseed_bytes
.type	OPENSSL_ia32_rdseed_bytes,@function
.align	16
OPENSSL_ia32_rdseed_bytes:
.cfi_startproc	
.byte	243,15,30,250
	xorq	%rax,%rax
	cmpq	$0,%rsi
	je	.Ldone_rdseed_bytes

	movq	$8,%r11
.Loop_rdseed_bytes:
.byte	73,15,199,250
	jc	.Lbreak_rdseed_bytes
	decq	%r11
	jnz	.Loop_rdseed_bytes
	jmp	.Ldone_rdseed_bytes

.align	16
.Lbreak_rdseed_bytes:
	cmpq	$8,%rsi
	jb	.Ltail_rdseed_bytes
	movq	%r10,(%rdi)
	leaq	8(%rdi),%rdi
	addq	$8,%rax
	subq	$8,%rsi
	jz	.Ldone_rdseed_bytes
	movq	$8,%r11
	jmp	.Loop_rdseed_bytes

.align	16
.Ltail_rdseed_bytes:
	movb	%r10b,(%rdi)
	leaq	1(%rdi),%rdi
	incq	%rax
	shrq	$8,%r10
	decq	%rsi
	jnz	.Ltail_rdseed_bytes

.Ldone_rdseed_bytes:
	xorq	%r10,%r10
	.byte	0xf3,0xc3
.cfi_endproc	
.size	OPENSSL_ia32_rdseed_bytes,.-OPENSSL_ia32_rdseed_bytes
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
               node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/engines/                             0000775 0000000 0000000 00000000000 14746647661 0025264 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/engines/e_padlock-x86_64.s           0000664 0000000 0000000 00000044133 14746647661 0030332 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        .text	
.globl	padlock_capability
.type	padlock_capability,@function
.align	16
padlock_capability:
	movq	%rbx,%r8
	xorl	%eax,%eax
	cpuid
	xorl	%eax,%eax
	cmpl	$0x746e6543,%ebx
	jne	.Lzhaoxin
	cmpl	$0x48727561,%edx
	jne	.Lnoluck
	cmpl	$0x736c7561,%ecx
	jne	.Lnoluck
	jmp	.LzhaoxinEnd
.Lzhaoxin:
	cmpl	$0x68532020,%ebx
	jne	.Lnoluck
	cmpl	$0x68676e61,%edx
	jne	.Lnoluck
	cmpl	$0x20206961,%ecx
	jne	.Lnoluck
.LzhaoxinEnd:
	movl	$0xC0000000,%eax
	cpuid
	movl	%eax,%edx
	xorl	%eax,%eax
	cmpl	$0xC0000001,%edx
	jb	.Lnoluck
	movl	$0xC0000001,%eax
	cpuid
	movl	%edx,%eax
	andl	$0xffffffef,%eax
	orl	$0x10,%eax
.Lnoluck:
	movq	%r8,%rbx
	.byte	0xf3,0xc3
.size	padlock_capability,.-padlock_capability

.globl	padlock_key_bswap
.type	padlock_key_bswap,@function
.align	16
padlock_key_bswap:
	movl	240(%rdi),%edx
	incl	%edx
	shll	$2,%edx
.Lbswap_loop:
	movl	(%rdi),%eax
	bswapl	%eax
	movl	%eax,(%rdi)
	leaq	4(%rdi),%rdi
	subl	$1,%edx
	jnz	.Lbswap_loop
	.byte	0xf3,0xc3
.size	padlock_key_bswap,.-padlock_key_bswap

.globl	padlock_verify_context
.type	padlock_verify_context,@function
.align	16
padlock_verify_context:
	movq	%rdi,%rdx
	pushf
	leaq	.Lpadlock_saved_context(%rip),%rax
	call	_padlock_verify_ctx
	leaq	8(%rsp),%rsp
	.byte	0xf3,0xc3
.size	padlock_verify_context,.-padlock_verify_context

.type	_padlock_verify_ctx,@function
.align	16
_padlock_verify_ctx:
	movq	8(%rsp),%r8
	btq	$30,%r8
	jnc	.Lverified
	cmpq	(%rax),%rdx
	je	.Lverified
	pushf
	popf
.Lverified:
	movq	%rdx,(%rax)
	.byte	0xf3,0xc3
.size	_padlock_verify_ctx,.-_padlock_verify_ctx

.globl	padlock_reload_key
.type	padlock_reload_key,@function
.align	16
padlock_reload_key:
	pushf
	popf
	.byte	0xf3,0xc3
.size	padlock_reload_key,.-padlock_reload_key

.globl	padlock_aes_block
.type	padlock_aes_block,@function
.align	16
padlock_aes_block:
	movq	%rbx,%r8
	movq	$1,%rcx
	leaq	32(%rdx),%rbx
	leaq	16(%rdx),%rdx
.byte	0xf3,0x0f,0xa7,0xc8
	movq	%r8,%rbx
	.byte	0xf3,0xc3
.size	padlock_aes_block,.-padlock_aes_block

.globl	padlock_xstore
.type	padlock_xstore,@function
.align	16
padlock_xstore:
	movl	%esi,%edx
.byte	0x0f,0xa7,0xc0
	.byte	0xf3,0xc3
.size	padlock_xstore,.-padlock_xstore

.globl	padlock_sha1_oneshot
.type	padlock_sha1_oneshot,@function
.align	16
padlock_sha1_oneshot:
	movq	%rdx,%rcx
	movq	%rdi,%rdx
	movups	(%rdi),%xmm0
	subq	$128+8,%rsp
	movl	16(%rdi),%eax
	movaps	%xmm0,(%rsp)
	movq	%rsp,%rdi
	movl	%eax,16(%rsp)
	xorq	%rax,%rax
.byte	0xf3,0x0f,0xa6,0xc8
	movaps	(%rsp),%xmm0
	movl	16(%rsp),%eax
	addq	$128+8,%rsp
	movups	%xmm0,(%rdx)
	movl	%eax,16(%rdx)
	.byte	0xf3,0xc3
.size	padlock_sha1_oneshot,.-padlock_sha1_oneshot

.globl	padlock_sha1_blocks
.type	padlock_sha1_blocks,@function
.align	16
padlock_sha1_blocks:
	movq	%rdx,%rcx
	movq	%rdi,%rdx
	movups	(%rdi),%xmm0
	subq	$128+8,%rsp
	movl	16(%rdi),%eax
	movaps	%xmm0,(%rsp)
	movq	%rsp,%rdi
	movl	%eax,16(%rsp)
	movq	$-1,%rax
.byte	0xf3,0x0f,0xa6,0xc8
	movaps	(%rsp),%xmm0
	movl	16(%rsp),%eax
	addq	$128+8,%rsp
	movups	%xmm0,(%rdx)
	movl	%eax,16(%rdx)
	.byte	0xf3,0xc3
.size	padlock_sha1_blocks,.-padlock_sha1_blocks

.globl	padlock_sha256_oneshot
.type	padlock_sha256_oneshot,@function
.align	16
padlock_sha256_oneshot:
	movq	%rdx,%rcx
	movq	%rdi,%rdx
	movups	(%rdi),%xmm0
	subq	$128+8,%rsp
	movups	16(%rdi),%xmm1
	movaps	%xmm0,(%rsp)
	movq	%rsp,%rdi
	movaps	%xmm1,16(%rsp)
	xorq	%rax,%rax
.byte	0xf3,0x0f,0xa6,0xd0
	movaps	(%rsp),%xmm0
	movaps	16(%rsp),%xmm1
	addq	$128+8,%rsp
	movups	%xmm0,(%rdx)
	movups	%xmm1,16(%rdx)
	.byte	0xf3,0xc3
.size	padlock_sha256_oneshot,.-padlock_sha256_oneshot

.globl	padlock_sha256_blocks
.type	padlock_sha256_blocks,@function
.align	16
padlock_sha256_blocks:
	movq	%rdx,%rcx
	movq	%rdi,%rdx
	movups	(%rdi),%xmm0
	subq	$128+8,%rsp
	movups	16(%rdi),%xmm1
	movaps	%xmm0,(%rsp)
	movq	%rsp,%rdi
	movaps	%xmm1,16(%rsp)
	movq	$-1,%rax
.byte	0xf3,0x0f,0xa6,0xd0
	movaps	(%rsp),%xmm0
	movaps	16(%rsp),%xmm1
	addq	$128+8,%rsp
	movups	%xmm0,(%rdx)
	movups	%xmm1,16(%rdx)
	.byte	0xf3,0xc3
.size	padlock_sha256_blocks,.-padlock_sha256_blocks

.globl	padlock_sha512_blocks
.type	padlock_sha512_blocks,@function
.align	16
padlock_sha512_blocks:
	movq	%rdx,%rcx
	movq	%rdi,%rdx
	movups	(%rdi),%xmm0
	subq	$128+8,%rsp
	movups	16(%rdi),%xmm1
	movups	32(%rdi),%xmm2
	movups	48(%rdi),%xmm3
	movaps	%xmm0,(%rsp)
	movq	%rsp,%rdi
	movaps	%xmm1,16(%rsp)
	movaps	%xmm2,32(%rsp)
	movaps	%xmm3,48(%rsp)
.byte	0xf3,0x0f,0xa6,0xe0
	movaps	(%rsp),%xmm0
	movaps	16(%rsp),%xmm1
	movaps	32(%rsp),%xmm2
	movaps	48(%rsp),%xmm3
	addq	$128+8,%rsp
	movups	%xmm0,(%rdx)
	movups	%xmm1,16(%rdx)
	movups	%xmm2,32(%rdx)
	movups	%xmm3,48(%rdx)
	.byte	0xf3,0xc3
.size	padlock_sha512_blocks,.-padlock_sha512_blocks
.globl	padlock_ecb_encrypt
.type	padlock_ecb_encrypt,@function
.align	16
padlock_ecb_encrypt:
	pushq	%rbp
	pushq	%rbx

	xorl	%eax,%eax
	testq	$15,%rdx
	jnz	.Lecb_abort
	testq	$15,%rcx
	jnz	.Lecb_abort
	leaq	.Lpadlock_saved_context(%rip),%rax
	pushf
	cld
	call	_padlock_verify_ctx
	leaq	16(%rdx),%rdx
	xorl	%eax,%eax
	xorl	%ebx,%ebx
	testl	$32,(%rdx)
	jnz	.Lecb_aligned
	testq	$0x0f,%rdi
	setz	%al
	testq	$0x0f,%rsi
	setz	%bl
	testl	%ebx,%eax
	jnz	.Lecb_aligned
	negq	%rax
	movq	$512,%rbx
	notq	%rax
	leaq	(%rsp),%rbp
	cmpq	%rbx,%rcx
	cmovcq	%rcx,%rbx
	andq	%rbx,%rax
	movq	%rcx,%rbx
	negq	%rax
	andq	$512-1,%rbx
	leaq	(%rax,%rbp,1),%rsp
	movq	$512,%rax
	cmovzq	%rax,%rbx
	cmpq	%rbx,%rcx
	ja	.Lecb_loop
	movq	%rsi,%rax
	cmpq	%rsp,%rbp
	cmoveq	%rdi,%rax
	addq	%rcx,%rax
	negq	%rax
	andq	$0xfff,%rax
	cmpq	$128,%rax
	movq	$-128,%rax
	cmovaeq	%rbx,%rax
	andq	%rax,%rbx
	jz	.Lecb_unaligned_tail
	jmp	.Lecb_loop
.align	16
.Lecb_loop:
	cmpq	%rcx,%rbx
	cmovaq	%rcx,%rbx
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11
	testq	$0x0f,%rdi
	cmovnzq	%rsp,%rdi
	testq	$0x0f,%rsi
	jz	.Lecb_inp_aligned
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
	movq	%rbx,%rcx
	movq	%rdi,%rsi
.Lecb_inp_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,200
	movq	%r8,%rdi
	movq	%r11,%rbx
	testq	$0x0f,%rdi
	jz	.Lecb_out_aligned
	movq	%rbx,%rcx
	leaq	(%rsp),%rsi
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
.Lecb_out_aligned:
	movq	%r9,%rsi
	movq	%r10,%rcx
	addq	%rbx,%rdi
	addq	%rbx,%rsi
	subq	%rbx,%rcx
	movq	$512,%rbx
	jz	.Lecb_break
	cmpq	%rbx,%rcx
	jae	.Lecb_loop
.Lecb_unaligned_tail:
	xorl	%eax,%eax
	cmpq	%rsp,%rbp
	cmoveq	%rcx,%rax
	movq	%rdi,%r8
	movq	%rcx,%rbx
	subq	%rax,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	movq	%rsp,%rsi
	movq	%r8,%rdi
	movq	%rbx,%rcx
	jmp	.Lecb_loop
.align	16
.Lecb_break:
	cmpq	%rbp,%rsp
	je	.Lecb_done

	pxor	%xmm0,%xmm0
	leaq	(%rsp),%rax
.Lecb_bzero:
	movaps	%xmm0,(%rax)
	leaq	16(%rax),%rax
	cmpq	%rax,%rbp
	ja	.Lecb_bzero

.Lecb_done:
	leaq	(%rbp),%rsp
	jmp	.Lecb_exit

.align	16
.Lecb_aligned:
	leaq	(%rsi,%rcx,1),%rbp
	negq	%rbp
	andq	$0xfff,%rbp
	xorl	%eax,%eax
	cmpq	$128,%rbp
	movq	$128-1,%rbp
	cmovaeq	%rax,%rbp
	andq	%rcx,%rbp
	subq	%rbp,%rcx
	jz	.Lecb_aligned_tail
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,200
	testq	%rbp,%rbp
	jz	.Lecb_exit

.Lecb_aligned_tail:
	movq	%rdi,%r8
	movq	%rbp,%rbx
	movq	%rbp,%rcx
	leaq	(%rsp),%rbp
	subq	%rcx,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	leaq	(%r8),%rdi
	leaq	(%rsp),%rsi
	movq	%rbx,%rcx
	jmp	.Lecb_loop
.Lecb_exit:
	movl	$1,%eax
	leaq	8(%rsp),%rsp
.Lecb_abort:
	popq	%rbx
	popq	%rbp
	.byte	0xf3,0xc3
.size	padlock_ecb_encrypt,.-padlock_ecb_encrypt
.globl	padlock_cbc_encrypt
.type	padlock_cbc_encrypt,@function
.align	16
padlock_cbc_encrypt:
	pushq	%rbp
	pushq	%rbx

	xorl	%eax,%eax
	testq	$15,%rdx
	jnz	.Lcbc_abort
	testq	$15,%rcx
	jnz	.Lcbc_abort
	leaq	.Lpadlock_saved_context(%rip),%rax
	pushf
	cld
	call	_padlock_verify_ctx
	leaq	16(%rdx),%rdx
	xorl	%eax,%eax
	xorl	%ebx,%ebx
	testl	$32,(%rdx)
	jnz	.Lcbc_aligned
	testq	$0x0f,%rdi
	setz	%al
	testq	$0x0f,%rsi
	setz	%bl
	testl	%ebx,%eax
	jnz	.Lcbc_aligned
	negq	%rax
	movq	$512,%rbx
	notq	%rax
	leaq	(%rsp),%rbp
	cmpq	%rbx,%rcx
	cmovcq	%rcx,%rbx
	andq	%rbx,%rax
	movq	%rcx,%rbx
	negq	%rax
	andq	$512-1,%rbx
	leaq	(%rax,%rbp,1),%rsp
	movq	$512,%rax
	cmovzq	%rax,%rbx
	cmpq	%rbx,%rcx
	ja	.Lcbc_loop
	movq	%rsi,%rax
	cmpq	%rsp,%rbp
	cmoveq	%rdi,%rax
	addq	%rcx,%rax
	negq	%rax
	andq	$0xfff,%rax
	cmpq	$64,%rax
	movq	$-64,%rax
	cmovaeq	%rbx,%rax
	andq	%rax,%rbx
	jz	.Lcbc_unaligned_tail
	jmp	.Lcbc_loop
.align	16
.Lcbc_loop:
	cmpq	%rcx,%rbx
	cmovaq	%rcx,%rbx
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11
	testq	$0x0f,%rdi
	cmovnzq	%rsp,%rdi
	testq	$0x0f,%rsi
	jz	.Lcbc_inp_aligned
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
	movq	%rbx,%rcx
	movq	%rdi,%rsi
.Lcbc_inp_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,208
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
	movq	%r8,%rdi
	movq	%r11,%rbx
	testq	$0x0f,%rdi
	jz	.Lcbc_out_aligned
	movq	%rbx,%rcx
	leaq	(%rsp),%rsi
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
.Lcbc_out_aligned:
	movq	%r9,%rsi
	movq	%r10,%rcx
	addq	%rbx,%rdi
	addq	%rbx,%rsi
	subq	%rbx,%rcx
	movq	$512,%rbx
	jz	.Lcbc_break
	cmpq	%rbx,%rcx
	jae	.Lcbc_loop
.Lcbc_unaligned_tail:
	xorl	%eax,%eax
	cmpq	%rsp,%rbp
	cmoveq	%rcx,%rax
	movq	%rdi,%r8
	movq	%rcx,%rbx
	subq	%rax,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	movq	%rsp,%rsi
	movq	%r8,%rdi
	movq	%rbx,%rcx
	jmp	.Lcbc_loop
.align	16
.Lcbc_break:
	cmpq	%rbp,%rsp
	je	.Lcbc_done

	pxor	%xmm0,%xmm0
	leaq	(%rsp),%rax
.Lcbc_bzero:
	movaps	%xmm0,(%rax)
	leaq	16(%rax),%rax
	cmpq	%rax,%rbp
	ja	.Lcbc_bzero

.Lcbc_done:
	leaq	(%rbp),%rsp
	jmp	.Lcbc_exit

.align	16
.Lcbc_aligned:
	leaq	(%rsi,%rcx,1),%rbp
	negq	%rbp
	andq	$0xfff,%rbp
	xorl	%eax,%eax
	cmpq	$64,%rbp
	movq	$64-1,%rbp
	cmovaeq	%rax,%rbp
	andq	%rcx,%rbp
	subq	%rbp,%rcx
	jz	.Lcbc_aligned_tail
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,208
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
	testq	%rbp,%rbp
	jz	.Lcbc_exit

.Lcbc_aligned_tail:
	movq	%rdi,%r8
	movq	%rbp,%rbx
	movq	%rbp,%rcx
	leaq	(%rsp),%rbp
	subq	%rcx,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	leaq	(%r8),%rdi
	leaq	(%rsp),%rsi
	movq	%rbx,%rcx
	jmp	.Lcbc_loop
.Lcbc_exit:
	movl	$1,%eax
	leaq	8(%rsp),%rsp
.Lcbc_abort:
	popq	%rbx
	popq	%rbp
	.byte	0xf3,0xc3
.size	padlock_cbc_encrypt,.-padlock_cbc_encrypt
.globl	padlock_cfb_encrypt
.type	padlock_cfb_encrypt,@function
.align	16
padlock_cfb_encrypt:
	pushq	%rbp
	pushq	%rbx

	xorl	%eax,%eax
	testq	$15,%rdx
	jnz	.Lcfb_abort
	testq	$15,%rcx
	jnz	.Lcfb_abort
	leaq	.Lpadlock_saved_context(%rip),%rax
	pushf
	cld
	call	_padlock_verify_ctx
	leaq	16(%rdx),%rdx
	xorl	%eax,%eax
	xorl	%ebx,%ebx
	testl	$32,(%rdx)
	jnz	.Lcfb_aligned
	testq	$0x0f,%rdi
	setz	%al
	testq	$0x0f,%rsi
	setz	%bl
	testl	%ebx,%eax
	jnz	.Lcfb_aligned
	negq	%rax
	movq	$512,%rbx
	notq	%rax
	leaq	(%rsp),%rbp
	cmpq	%rbx,%rcx
	cmovcq	%rcx,%rbx
	andq	%rbx,%rax
	movq	%rcx,%rbx
	negq	%rax
	andq	$512-1,%rbx
	leaq	(%rax,%rbp,1),%rsp
	movq	$512,%rax
	cmovzq	%rax,%rbx
	jmp	.Lcfb_loop
.align	16
.Lcfb_loop:
	cmpq	%rcx,%rbx
	cmovaq	%rcx,%rbx
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11
	testq	$0x0f,%rdi
	cmovnzq	%rsp,%rdi
	testq	$0x0f,%rsi
	jz	.Lcfb_inp_aligned
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
	movq	%rbx,%rcx
	movq	%rdi,%rsi
.Lcfb_inp_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,224
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
	movq	%r8,%rdi
	movq	%r11,%rbx
	testq	$0x0f,%rdi
	jz	.Lcfb_out_aligned
	movq	%rbx,%rcx
	leaq	(%rsp),%rsi
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
.Lcfb_out_aligned:
	movq	%r9,%rsi
	movq	%r10,%rcx
	addq	%rbx,%rdi
	addq	%rbx,%rsi
	subq	%rbx,%rcx
	movq	$512,%rbx
	jnz	.Lcfb_loop
	cmpq	%rbp,%rsp
	je	.Lcfb_done

	pxor	%xmm0,%xmm0
	leaq	(%rsp),%rax
.Lcfb_bzero:
	movaps	%xmm0,(%rax)
	leaq	16(%rax),%rax
	cmpq	%rax,%rbp
	ja	.Lcfb_bzero

.Lcfb_done:
	leaq	(%rbp),%rsp
	jmp	.Lcfb_exit

.align	16
.Lcfb_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,224
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
.Lcfb_exit:
	movl	$1,%eax
	leaq	8(%rsp),%rsp
.Lcfb_abort:
	popq	%rbx
	popq	%rbp
	.byte	0xf3,0xc3
.size	padlock_cfb_encrypt,.-padlock_cfb_encrypt
.globl	padlock_ofb_encrypt
.type	padlock_ofb_encrypt,@function
.align	16
padlock_ofb_encrypt:
	pushq	%rbp
	pushq	%rbx

	xorl	%eax,%eax
	testq	$15,%rdx
	jnz	.Lofb_abort
	testq	$15,%rcx
	jnz	.Lofb_abort
	leaq	.Lpadlock_saved_context(%rip),%rax
	pushf
	cld
	call	_padlock_verify_ctx
	leaq	16(%rdx),%rdx
	xorl	%eax,%eax
	xorl	%ebx,%ebx
	testl	$32,(%rdx)
	jnz	.Lofb_aligned
	testq	$0x0f,%rdi
	setz	%al
	testq	$0x0f,%rsi
	setz	%bl
	testl	%ebx,%eax
	jnz	.Lofb_aligned
	negq	%rax
	movq	$512,%rbx
	notq	%rax
	leaq	(%rsp),%rbp
	cmpq	%rbx,%rcx
	cmovcq	%rcx,%rbx
	andq	%rbx,%rax
	movq	%rcx,%rbx
	negq	%rax
	andq	$512-1,%rbx
	leaq	(%rax,%rbp,1),%rsp
	movq	$512,%rax
	cmovzq	%rax,%rbx
	jmp	.Lofb_loop
.align	16
.Lofb_loop:
	cmpq	%rcx,%rbx
	cmovaq	%rcx,%rbx
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11
	testq	$0x0f,%rdi
	cmovnzq	%rsp,%rdi
	testq	$0x0f,%rsi
	jz	.Lofb_inp_aligned
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
	movq	%rbx,%rcx
	movq	%rdi,%rsi
.Lofb_inp_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,232
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
	movq	%r8,%rdi
	movq	%r11,%rbx
	testq	$0x0f,%rdi
	jz	.Lofb_out_aligned
	movq	%rbx,%rcx
	leaq	(%rsp),%rsi
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
.Lofb_out_aligned:
	movq	%r9,%rsi
	movq	%r10,%rcx
	addq	%rbx,%rdi
	addq	%rbx,%rsi
	subq	%rbx,%rcx
	movq	$512,%rbx
	jnz	.Lofb_loop
	cmpq	%rbp,%rsp
	je	.Lofb_done

	pxor	%xmm0,%xmm0
	leaq	(%rsp),%rax
.Lofb_bzero:
	movaps	%xmm0,(%rax)
	leaq	16(%rax),%rax
	cmpq	%rax,%rbp
	ja	.Lofb_bzero

.Lofb_done:
	leaq	(%rbp),%rsp
	jmp	.Lofb_exit

.align	16
.Lofb_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,232
	movdqa	(%rax),%xmm0
	movdqa	%xmm0,-16(%rdx)
.Lofb_exit:
	movl	$1,%eax
	leaq	8(%rsp),%rsp
.Lofb_abort:
	popq	%rbx
	popq	%rbp
	.byte	0xf3,0xc3
.size	padlock_ofb_encrypt,.-padlock_ofb_encrypt
.globl	padlock_ctr32_encrypt
.type	padlock_ctr32_encrypt,@function
.align	16
padlock_ctr32_encrypt:
	pushq	%rbp
	pushq	%rbx

	xorl	%eax,%eax
	testq	$15,%rdx
	jnz	.Lctr32_abort
	testq	$15,%rcx
	jnz	.Lctr32_abort
	leaq	.Lpadlock_saved_context(%rip),%rax
	pushf
	cld
	call	_padlock_verify_ctx
	leaq	16(%rdx),%rdx
	xorl	%eax,%eax
	xorl	%ebx,%ebx
	testl	$32,(%rdx)
	jnz	.Lctr32_aligned
	testq	$0x0f,%rdi
	setz	%al
	testq	$0x0f,%rsi
	setz	%bl
	testl	%ebx,%eax
	jnz	.Lctr32_aligned
	negq	%rax
	movq	$512,%rbx
	notq	%rax
	leaq	(%rsp),%rbp
	cmpq	%rbx,%rcx
	cmovcq	%rcx,%rbx
	andq	%rbx,%rax
	movq	%rcx,%rbx
	negq	%rax
	andq	$512-1,%rbx
	leaq	(%rax,%rbp,1),%rsp
	movq	$512,%rax
	cmovzq	%rax,%rbx
.Lctr32_reenter:
	movl	-4(%rdx),%eax
	bswapl	%eax
	negl	%eax
	andl	$31,%eax
	movq	$512,%rbx
	shll	$4,%eax
	cmovzq	%rbx,%rax
	cmpq	%rax,%rcx
	cmovaq	%rax,%rbx
	cmovbeq	%rcx,%rbx
	cmpq	%rbx,%rcx
	ja	.Lctr32_loop
	movq	%rsi,%rax
	cmpq	%rsp,%rbp
	cmoveq	%rdi,%rax
	addq	%rcx,%rax
	negq	%rax
	andq	$0xfff,%rax
	cmpq	$32,%rax
	movq	$-32,%rax
	cmovaeq	%rbx,%rax
	andq	%rax,%rbx
	jz	.Lctr32_unaligned_tail
	jmp	.Lctr32_loop
.align	16
.Lctr32_loop:
	cmpq	%rcx,%rbx
	cmovaq	%rcx,%rbx
	movq	%rdi,%r8
	movq	%rsi,%r9
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11
	testq	$0x0f,%rdi
	cmovnzq	%rsp,%rdi
	testq	$0x0f,%rsi
	jz	.Lctr32_inp_aligned
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
	movq	%rbx,%rcx
	movq	%rdi,%rsi
.Lctr32_inp_aligned:
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,216
	movl	-4(%rdx),%eax
	testl	$0xffff0000,%eax
	jnz	.Lctr32_no_carry
	bswapl	%eax
	addl	$0x10000,%eax
	bswapl	%eax
	movl	%eax,-4(%rdx)
.Lctr32_no_carry:
	movq	%r8,%rdi
	movq	%r11,%rbx
	testq	$0x0f,%rdi
	jz	.Lctr32_out_aligned
	movq	%rbx,%rcx
	leaq	(%rsp),%rsi
	shrq	$3,%rcx
.byte	0xf3,0x48,0xa5
	subq	%rbx,%rdi
.Lctr32_out_aligned:
	movq	%r9,%rsi
	movq	%r10,%rcx
	addq	%rbx,%rdi
	addq	%rbx,%rsi
	subq	%rbx,%rcx
	movq	$512,%rbx
	jz	.Lctr32_break
	cmpq	%rbx,%rcx
	jae	.Lctr32_loop
	movq	%rcx,%rbx
	movq	%rsi,%rax
	cmpq	%rsp,%rbp
	cmoveq	%rdi,%rax
	addq	%rcx,%rax
	negq	%rax
	andq	$0xfff,%rax
	cmpq	$32,%rax
	movq	$-32,%rax
	cmovaeq	%rbx,%rax
	andq	%rax,%rbx
	jnz	.Lctr32_loop
.Lctr32_unaligned_tail:
	xorl	%eax,%eax
	cmpq	%rsp,%rbp
	cmoveq	%rcx,%rax
	movq	%rdi,%r8
	movq	%rcx,%rbx
	subq	%rax,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	movq	%rsp,%rsi
	movq	%r8,%rdi
	movq	%rbx,%rcx
	jmp	.Lctr32_loop
.align	16
.Lctr32_break:
	cmpq	%rbp,%rsp
	je	.Lctr32_done

	pxor	%xmm0,%xmm0
	leaq	(%rsp),%rax
.Lctr32_bzero:
	movaps	%xmm0,(%rax)
	leaq	16(%rax),%rax
	cmpq	%rax,%rbp
	ja	.Lctr32_bzero

.Lctr32_done:
	leaq	(%rbp),%rsp
	jmp	.Lctr32_exit

.align	16
.Lctr32_aligned:
	movl	-4(%rdx),%eax
	bswapl	%eax
	negl	%eax
	andl	$0xffff,%eax
	movq	$1048576,%rbx
	shll	$4,%eax
	cmovzq	%rbx,%rax
	cmpq	%rax,%rcx
	cmovaq	%rax,%rbx
	cmovbeq	%rcx,%rbx
	jbe	.Lctr32_aligned_skip

.Lctr32_aligned_loop:
	movq	%rcx,%r10
	movq	%rbx,%rcx
	movq	%rbx,%r11

	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,216

	movl	-4(%rdx),%eax
	bswapl	%eax
	addl	$0x10000,%eax
	bswapl	%eax
	movl	%eax,-4(%rdx)

	movq	%r10,%rcx
	subq	%r11,%rcx
	movq	$1048576,%rbx
	jz	.Lctr32_exit
	cmpq	%rbx,%rcx
	jae	.Lctr32_aligned_loop

.Lctr32_aligned_skip:
	leaq	(%rsi,%rcx,1),%rbp
	negq	%rbp
	andq	$0xfff,%rbp
	xorl	%eax,%eax
	cmpq	$32,%rbp
	movq	$32-1,%rbp
	cmovaeq	%rax,%rbp
	andq	%rcx,%rbp
	subq	%rbp,%rcx
	jz	.Lctr32_aligned_tail
	leaq	-16(%rdx),%rax
	leaq	16(%rdx),%rbx
	shrq	$4,%rcx
.byte	0xf3,0x0f,0xa7,216
	testq	%rbp,%rbp
	jz	.Lctr32_exit

.Lctr32_aligned_tail:
	movq	%rdi,%r8
	movq	%rbp,%rbx
	movq	%rbp,%rcx
	leaq	(%rsp),%rbp
	subq	%rcx,%rsp
	shrq	$3,%rcx
	leaq	(%rsp),%rdi
.byte	0xf3,0x48,0xa5
	leaq	(%r8),%rdi
	leaq	(%rsp),%rsi
	movq	%rbx,%rcx
	jmp	.Lctr32_loop
.Lctr32_exit:
	movl	$1,%eax
	leaq	8(%rsp),%rsp
.Lctr32_abort:
	popq	%rbx
	popq	%rbp
	.byte	0xf3,0xc3
.size	padlock_ctr32_encrypt,.-padlock_ctr32_encrypt
.byte	86,73,65,32,80,97,100,108,111,99,107,32,120,56,54,95,54,52,32,109,111,100,117,108,101,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
.align	16
.data	
.align	8
.Lpadlock_saved_context:
.quad	0
	.section ".note.gnu.property", "a"
	.p2align 3
	.long 1f - 0f
	.long 4f - 1f
	.long 5
0:
	# "GNU" encoded with .byte, since .asciz isn't supported
	# on Solaris.
	.byte 0x47
	.byte 0x4e
	.byte 0x55
	.byte 0
1:
	.p2align 3
	.long 0xc0000002
	.long 3f - 2f
2:
	.long 3
3:
	.p2align 3
4:
                                                                                                                                                                                                                                                                                                                                                                                                                                     node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/                             0000775 0000000 0000000 00000000000 14746647661 0025257 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/crypto/                      0000775 0000000 0000000 00000000000 14746647661 0026577 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/crypto/bn_conf.h             0000664 0000000 0000000 00000001505 14746647661 0030355 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /* WARNING: do not edit! */
/* Generated by Makefile from include/crypto/bn_conf.h.in */
/*
 * Copyright 2016-2021 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */

#ifndef OSSL_CRYPTO_BN_CONF_H
# define OSSL_CRYPTO_BN_CONF_H
# pragma once

/*
 * The contents of this file are not used in the UEFI build, as
 * both 32-bit and 64-bit builds are supported from a single run
 * of the Configure script.
 */

/* Should we define BN_DIV2W here? */

/* Only one for the following should be defined */
#define SIXTY_FOUR_BIT_LONG
#undef SIXTY_FOUR_BIT
#undef THIRTY_TWO_BIT

#endif
                                                                                                                                                                                           node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/crypto/dso_conf.h            0000664 0000000 0000000 00000001106 14746647661 0030540 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /* WARNING: do not edit! */
/* Generated by Makefile from include/crypto/dso_conf.h.in */
/*
 * Copyright 2016-2021 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */

#ifndef OSSL_CRYPTO_DSO_CONF_H
# define OSSL_CRYPTO_DSO_CONF_H
# pragma once

# define DSO_DLFCN
# define HAVE_DLFCN_H
# define DSO_EXTENSION ".so"
#endif
                                                                                                                                                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/                     0000775 0000000 0000000 00000000000 14746647661 0026742 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/asn1.h               0000664 0000000 0000000 00000166762 14746647661 0027777 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /*
 * WARNING: do not edit!
 * Generated by Makefile from include/openssl/asn1.h.in
 *
 * Copyright 1995-2021 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */



#ifndef OPENSSL_ASN1_H
# define OPENSSL_ASN1_H
# pragma once

# include <openssl/macros.h>
# ifndef OPENSSL_NO_DEPRECATED_3_0
#  define HEADER_ASN1_H
# endif

# include <time.h>
# include <openssl/e_os2.h>
# include <openssl/opensslconf.h>
# include <openssl/bio.h>
# include <openssl/safestack.h>
# include <openssl/asn1err.h>
# include <openssl/symhacks.h>

# include <openssl/types.h>
# include <openssl/bn.h>

# ifdef OPENSSL_BUILD_SHLIBCRYPTO
#  undef OPENSSL_EXTERN
#  define OPENSSL_EXTERN OPENSSL_EXPORT
# endif

#ifdef  __cplusplus
extern "C" {
#endif

# define V_ASN1_UNIVERSAL                0x00
# define V_ASN1_APPLICATION              0x40
# define V_ASN1_CONTEXT_SPECIFIC         0x80
# define V_ASN1_PRIVATE                  0xc0

# define V_ASN1_CONSTRUCTED              0x20
# define V_ASN1_PRIMITIVE_TAG            0x1f
# define V_ASN1_PRIMATIVE_TAG /*compat*/ V_ASN1_PRIMITIVE_TAG

# define V_ASN1_APP_CHOOSE               -2/* let the recipient choose */
# define V_ASN1_OTHER                    -3/* used in ASN1_TYPE */
# define V_ASN1_ANY                      -4/* used in ASN1 template code */

# define V_ASN1_UNDEF                    -1
/* ASN.1 tag values */
# define V_ASN1_EOC                      0
# define V_ASN1_BOOLEAN                  1 /**/
# define V_ASN1_INTEGER                  2
# define V_ASN1_BIT_STRING               3
# define V_ASN1_OCTET_STRING             4
# define V_ASN1_NULL                     5
# define V_ASN1_OBJECT                   6
# define V_ASN1_OBJECT_DESCRIPTOR        7
# define V_ASN1_EXTERNAL                 8
# define V_ASN1_REAL                     9
# define V_ASN1_ENUMERATED               10
# define V_ASN1_UTF8STRING               12
# define V_ASN1_SEQUENCE                 16
# define V_ASN1_SET                      17
# define V_ASN1_NUMERICSTRING            18 /**/
# define V_ASN1_PRINTABLESTRING          19
# define V_ASN1_T61STRING                20
# define V_ASN1_TELETEXSTRING            20/* alias */
# define V_ASN1_VIDEOTEXSTRING           21 /**/
# define V_ASN1_IA5STRING                22
# define V_ASN1_UTCTIME                  23
# define V_ASN1_GENERALIZEDTIME          24 /**/
# define V_ASN1_GRAPHICSTRING            25 /**/
# define V_ASN1_ISO64STRING              26 /**/
# define V_ASN1_VISIBLESTRING            26/* alias */
# define V_ASN1_GENERALSTRING            27 /**/
# define V_ASN1_UNIVERSALSTRING          28 /**/
# define V_ASN1_BMPSTRING                30

/*
 * NB the constants below are used internally by ASN1_INTEGER
 * and ASN1_ENUMERATED to indicate the sign. They are *not* on
 * the wire tag values.
 */

# define V_ASN1_NEG                      0x100
# define V_ASN1_NEG_INTEGER              (2 | V_ASN1_NEG)
# define V_ASN1_NEG_ENUMERATED           (10 | V_ASN1_NEG)

/* For use with d2i_ASN1_type_bytes() */
# define B_ASN1_NUMERICSTRING    0x0001
# define B_ASN1_PRINTABLESTRING  0x0002
# define B_ASN1_T61STRING        0x0004
# define B_ASN1_TELETEXSTRING    0x0004
# define B_ASN1_VIDEOTEXSTRING   0x0008
# define B_ASN1_IA5STRING        0x0010
# define B_ASN1_GRAPHICSTRING    0x0020
# define B_ASN1_ISO64STRING      0x0040
# define B_ASN1_VISIBLESTRING    0x0040
# define B_ASN1_GENERALSTRING    0x0080
# define B_ASN1_UNIVERSALSTRING  0x0100
# define B_ASN1_OCTET_STRING     0x0200
# define B_ASN1_BIT_STRING       0x0400
# define B_ASN1_BMPSTRING        0x0800
# define B_ASN1_UNKNOWN          0x1000
# define B_ASN1_UTF8STRING       0x2000
# define B_ASN1_UTCTIME          0x4000
# define B_ASN1_GENERALIZEDTIME  0x8000
# define B_ASN1_SEQUENCE         0x10000
/* For use with ASN1_mbstring_copy() */
# define MBSTRING_FLAG           0x1000
# define MBSTRING_UTF8           (MBSTRING_FLAG)
# define MBSTRING_ASC            (MBSTRING_FLAG|1)
# define MBSTRING_BMP            (MBSTRING_FLAG|2)
# define MBSTRING_UNIV           (MBSTRING_FLAG|4)
# define SMIME_OLDMIME           0x400
# define SMIME_CRLFEOL           0x800
# define SMIME_STREAM            0x1000

/* Stacks for types not otherwise defined in this header */
SKM_DEFINE_STACK_OF_INTERNAL(X509_ALGOR, X509_ALGOR, X509_ALGOR)
#define sk_X509_ALGOR_num(sk) OPENSSL_sk_num(ossl_check_const_X509_ALGOR_sk_type(sk))
#define sk_X509_ALGOR_value(sk, idx) ((X509_ALGOR *)OPENSSL_sk_value(ossl_check_const_X509_ALGOR_sk_type(sk), (idx)))
#define sk_X509_ALGOR_new(cmp) ((STACK_OF(X509_ALGOR) *)OPENSSL_sk_new(ossl_check_X509_ALGOR_compfunc_type(cmp)))
#define sk_X509_ALGOR_new_null() ((STACK_OF(X509_ALGOR) *)OPENSSL_sk_new_null())
#define sk_X509_ALGOR_new_reserve(cmp, n) ((STACK_OF(X509_ALGOR) *)OPENSSL_sk_new_reserve(ossl_check_X509_ALGOR_compfunc_type(cmp), (n)))
#define sk_X509_ALGOR_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_X509_ALGOR_sk_type(sk), (n))
#define sk_X509_ALGOR_free(sk) OPENSSL_sk_free(ossl_check_X509_ALGOR_sk_type(sk))
#define sk_X509_ALGOR_zero(sk) OPENSSL_sk_zero(ossl_check_X509_ALGOR_sk_type(sk))
#define sk_X509_ALGOR_delete(sk, i) ((X509_ALGOR *)OPENSSL_sk_delete(ossl_check_X509_ALGOR_sk_type(sk), (i)))
#define sk_X509_ALGOR_delete_ptr(sk, ptr) ((X509_ALGOR *)OPENSSL_sk_delete_ptr(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr)))
#define sk_X509_ALGOR_push(sk, ptr) OPENSSL_sk_push(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr))
#define sk_X509_ALGOR_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr))
#define sk_X509_ALGOR_pop(sk) ((X509_ALGOR *)OPENSSL_sk_pop(ossl_check_X509_ALGOR_sk_type(sk)))
#define sk_X509_ALGOR_shift(sk) ((X509_ALGOR *)OPENSSL_sk_shift(ossl_check_X509_ALGOR_sk_type(sk)))
#define sk_X509_ALGOR_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_X509_ALGOR_sk_type(sk),ossl_check_X509_ALGOR_freefunc_type(freefunc))
#define sk_X509_ALGOR_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr), (idx))
#define sk_X509_ALGOR_set(sk, idx, ptr) ((X509_ALGOR *)OPENSSL_sk_set(ossl_check_X509_ALGOR_sk_type(sk), (idx), ossl_check_X509_ALGOR_type(ptr)))
#define sk_X509_ALGOR_find(sk, ptr) OPENSSL_sk_find(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr))
#define sk_X509_ALGOR_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr))
#define sk_X509_ALGOR_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_type(ptr), pnum)
#define sk_X509_ALGOR_sort(sk) OPENSSL_sk_sort(ossl_check_X509_ALGOR_sk_type(sk))
#define sk_X509_ALGOR_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_X509_ALGOR_sk_type(sk))
#define sk_X509_ALGOR_dup(sk) ((STACK_OF(X509_ALGOR) *)OPENSSL_sk_dup(ossl_check_const_X509_ALGOR_sk_type(sk)))
#define sk_X509_ALGOR_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(X509_ALGOR) *)OPENSSL_sk_deep_copy(ossl_check_const_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_copyfunc_type(copyfunc), ossl_check_X509_ALGOR_freefunc_type(freefunc)))
#define sk_X509_ALGOR_set_cmp_func(sk, cmp) ((sk_X509_ALGOR_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_X509_ALGOR_sk_type(sk), ossl_check_X509_ALGOR_compfunc_type(cmp)))



# define ASN1_STRING_FLAG_BITS_LEFT 0x08/* Set if 0x07 has bits left value */
/*
 * This indicates that the ASN1_STRING is not a real value but just a place
 * holder for the location where indefinite length constructed data should be
 * inserted in the memory buffer
 */
# define ASN1_STRING_FLAG_NDEF 0x010

/*
 * This flag is used by the CMS code to indicate that a string is not
 * complete and is a place holder for content when it had all been accessed.
 * The flag will be reset when content has been written to it.
 */

# define ASN1_STRING_FLAG_CONT 0x020
/*
 * This flag is used by ASN1 code to indicate an ASN1_STRING is an MSTRING
 * type.
 */
# define ASN1_STRING_FLAG_MSTRING 0x040
/* String is embedded and only content should be freed */
# define ASN1_STRING_FLAG_EMBED 0x080
/* String should be parsed in RFC 5280's time format */
# define ASN1_STRING_FLAG_X509_TIME 0x100
/* This is the base type that holds just about everything :-) */
struct asn1_string_st {
    int length;
    int type;
    unsigned char *data;
    /*
     * The value of the following field depends on the type being held.  It
     * is mostly being used for BIT_STRING so if the input data has a
     * non-zero 'unused bits' value, it will be handled correctly
     */
    long flags;
};

/*
 * ASN1_ENCODING structure: this is used to save the received encoding of an
 * ASN1 type. This is useful to get round problems with invalid encodings
 * which can break signatures.
 */

typedef struct ASN1_ENCODING_st {
    unsigned char *enc;         /* DER encoding */
    long len;                   /* Length of encoding */
    int modified;               /* set to 1 if 'enc' is invalid */
} ASN1_ENCODING;

/* Used with ASN1 LONG type: if a long is set to this it is omitted */
# define ASN1_LONG_UNDEF 0x7fffffffL

# define STABLE_FLAGS_MALLOC     0x01
/*
 * A zero passed to ASN1_STRING_TABLE_new_add for the flags is interpreted
 * as "don't change" and STABLE_FLAGS_MALLOC is always set. By setting
 * STABLE_FLAGS_MALLOC only we can clear the existing value. Use the alias
 * STABLE_FLAGS_CLEAR to reflect this.
 */
# define STABLE_FLAGS_CLEAR      STABLE_FLAGS_MALLOC
# define STABLE_NO_MASK          0x02
# define DIRSTRING_TYPE  \
 (B_ASN1_PRINTABLESTRING|B_ASN1_T61STRING|B_ASN1_BMPSTRING|B_ASN1_UTF8STRING)
# define PKCS9STRING_TYPE (DIRSTRING_TYPE|B_ASN1_IA5STRING)

struct asn1_string_table_st {
    int nid;
    long minsize;
    long maxsize;
    unsigned long mask;
    unsigned long flags;
};

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_STRING_TABLE, ASN1_STRING_TABLE, ASN1_STRING_TABLE)
#define sk_ASN1_STRING_TABLE_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_STRING_TABLE_sk_type(sk))
#define sk_ASN1_STRING_TABLE_value(sk, idx) ((ASN1_STRING_TABLE *)OPENSSL_sk_value(ossl_check_const_ASN1_STRING_TABLE_sk_type(sk), (idx)))
#define sk_ASN1_STRING_TABLE_new(cmp) ((STACK_OF(ASN1_STRING_TABLE) *)OPENSSL_sk_new(ossl_check_ASN1_STRING_TABLE_compfunc_type(cmp)))
#define sk_ASN1_STRING_TABLE_new_null() ((STACK_OF(ASN1_STRING_TABLE) *)OPENSSL_sk_new_null())
#define sk_ASN1_STRING_TABLE_new_reserve(cmp, n) ((STACK_OF(ASN1_STRING_TABLE) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_STRING_TABLE_compfunc_type(cmp), (n)))
#define sk_ASN1_STRING_TABLE_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_STRING_TABLE_sk_type(sk), (n))
#define sk_ASN1_STRING_TABLE_free(sk) OPENSSL_sk_free(ossl_check_ASN1_STRING_TABLE_sk_type(sk))
#define sk_ASN1_STRING_TABLE_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_STRING_TABLE_sk_type(sk))
#define sk_ASN1_STRING_TABLE_delete(sk, i) ((ASN1_STRING_TABLE *)OPENSSL_sk_delete(ossl_check_ASN1_STRING_TABLE_sk_type(sk), (i)))
#define sk_ASN1_STRING_TABLE_delete_ptr(sk, ptr) ((ASN1_STRING_TABLE *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr)))
#define sk_ASN1_STRING_TABLE_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr))
#define sk_ASN1_STRING_TABLE_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr))
#define sk_ASN1_STRING_TABLE_pop(sk) ((ASN1_STRING_TABLE *)OPENSSL_sk_pop(ossl_check_ASN1_STRING_TABLE_sk_type(sk)))
#define sk_ASN1_STRING_TABLE_shift(sk) ((ASN1_STRING_TABLE *)OPENSSL_sk_shift(ossl_check_ASN1_STRING_TABLE_sk_type(sk)))
#define sk_ASN1_STRING_TABLE_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_STRING_TABLE_sk_type(sk),ossl_check_ASN1_STRING_TABLE_freefunc_type(freefunc))
#define sk_ASN1_STRING_TABLE_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr), (idx))
#define sk_ASN1_STRING_TABLE_set(sk, idx, ptr) ((ASN1_STRING_TABLE *)OPENSSL_sk_set(ossl_check_ASN1_STRING_TABLE_sk_type(sk), (idx), ossl_check_ASN1_STRING_TABLE_type(ptr)))
#define sk_ASN1_STRING_TABLE_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr))
#define sk_ASN1_STRING_TABLE_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr))
#define sk_ASN1_STRING_TABLE_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_type(ptr), pnum)
#define sk_ASN1_STRING_TABLE_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_STRING_TABLE_sk_type(sk))
#define sk_ASN1_STRING_TABLE_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_STRING_TABLE_sk_type(sk))
#define sk_ASN1_STRING_TABLE_dup(sk) ((STACK_OF(ASN1_STRING_TABLE) *)OPENSSL_sk_dup(ossl_check_const_ASN1_STRING_TABLE_sk_type(sk)))
#define sk_ASN1_STRING_TABLE_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_STRING_TABLE) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_copyfunc_type(copyfunc), ossl_check_ASN1_STRING_TABLE_freefunc_type(freefunc)))
#define sk_ASN1_STRING_TABLE_set_cmp_func(sk, cmp) ((sk_ASN1_STRING_TABLE_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_STRING_TABLE_sk_type(sk), ossl_check_ASN1_STRING_TABLE_compfunc_type(cmp)))


/* size limits: this stuff is taken straight from RFC2459 */

# define ub_name                         32768
# define ub_common_name                  64
# define ub_locality_name                128
# define ub_state_name                   128
# define ub_organization_name            64
# define ub_organization_unit_name       64
# define ub_title                        64
# define ub_email_address                128

/*
 * Declarations for template structures: for full definitions see asn1t.h
 */
typedef struct ASN1_TEMPLATE_st ASN1_TEMPLATE;
typedef struct ASN1_TLC_st ASN1_TLC;
/* This is just an opaque pointer */
typedef struct ASN1_VALUE_st ASN1_VALUE;

/* Declare ASN1 functions: the implement macro in in asn1t.h */

/*
 * The mysterious 'extern' that's passed to some macros is innocuous,
 * and is there to quiet pre-C99 compilers that may complain about empty
 * arguments in macro calls.
 */

# define DECLARE_ASN1_FUNCTIONS_attr(attr, type)                            \
    DECLARE_ASN1_FUNCTIONS_name_attr(attr, type, type)
# define DECLARE_ASN1_FUNCTIONS(type)                                       \
    DECLARE_ASN1_FUNCTIONS_attr(extern, type)

# define DECLARE_ASN1_ALLOC_FUNCTIONS_attr(attr, type)                      \
    DECLARE_ASN1_ALLOC_FUNCTIONS_name_attr(attr, type, type)
# define DECLARE_ASN1_ALLOC_FUNCTIONS(type)                                 \
    DECLARE_ASN1_ALLOC_FUNCTIONS_attr(extern, type)

# define DECLARE_ASN1_FUNCTIONS_name_attr(attr, type, name)                 \
    DECLARE_ASN1_ALLOC_FUNCTIONS_name_attr(attr, type, name)                \
    DECLARE_ASN1_ENCODE_FUNCTIONS_name_attr(attr, type, name)
# define DECLARE_ASN1_FUNCTIONS_name(type, name)                            \
    DECLARE_ASN1_FUNCTIONS_name_attr(extern, type, name)

# define DECLARE_ASN1_ENCODE_FUNCTIONS_attr(attr, type, itname, name)       \
    DECLARE_ASN1_ENCODE_FUNCTIONS_only_attr(attr, type, name)               \
    DECLARE_ASN1_ITEM_attr(attr, itname)
# define DECLARE_ASN1_ENCODE_FUNCTIONS(type, itname, name)                  \
    DECLARE_ASN1_ENCODE_FUNCTIONS_attr(extern, type, itname, name)

# define DECLARE_ASN1_ENCODE_FUNCTIONS_name_attr(attr, type, name)          \
    DECLARE_ASN1_ENCODE_FUNCTIONS_attr(attr, type, name, name)
# define DECLARE_ASN1_ENCODE_FUNCTIONS_name(type, name) \
    DECLARE_ASN1_ENCODE_FUNCTIONS_name_attr(extern, type, name)

# define DECLARE_ASN1_ENCODE_FUNCTIONS_only_attr(attr, type, name)          \
    attr type *d2i_##name(type **a, const unsigned char **in, long len);    \
    attr int i2d_##name(const type *a, unsigned char **out);
# define DECLARE_ASN1_ENCODE_FUNCTIONS_only(type, name)                     \
    DECLARE_ASN1_ENCODE_FUNCTIONS_only_attr(extern, type, name)

# define DECLARE_ASN1_NDEF_FUNCTION_attr(attr, name)                        \
    attr int i2d_##name##_NDEF(const name *a, unsigned char **out);
# define DECLARE_ASN1_NDEF_FUNCTION(name)                                   \
    DECLARE_ASN1_NDEF_FUNCTION_attr(extern, name)

# define DECLARE_ASN1_ALLOC_FUNCTIONS_name_attr(attr, type, name)           \
    attr type *name##_new(void);                                            \
    attr void name##_free(type *a);
# define DECLARE_ASN1_ALLOC_FUNCTIONS_name(type, name)                      \
    DECLARE_ASN1_ALLOC_FUNCTIONS_name_attr(extern, type, name)

# define DECLARE_ASN1_DUP_FUNCTION_attr(attr, type)                         \
    DECLARE_ASN1_DUP_FUNCTION_name_attr(attr, type, type)
# define DECLARE_ASN1_DUP_FUNCTION(type)                                    \
    DECLARE_ASN1_DUP_FUNCTION_attr(extern, type)

# define DECLARE_ASN1_DUP_FUNCTION_name_attr(attr, type, name)              \
    attr type *name##_dup(const type *a);
# define DECLARE_ASN1_DUP_FUNCTION_name(type, name)                         \
    DECLARE_ASN1_DUP_FUNCTION_name_attr(extern, type, name)

# define DECLARE_ASN1_PRINT_FUNCTION_attr(attr, stname)                     \
    DECLARE_ASN1_PRINT_FUNCTION_fname_attr(attr, stname, stname)
# define DECLARE_ASN1_PRINT_FUNCTION(stname)                                \
    DECLARE_ASN1_PRINT_FUNCTION_attr(extern, stname)

# define DECLARE_ASN1_PRINT_FUNCTION_fname_attr(attr, stname, fname)        \
    attr int fname##_print_ctx(BIO *out, const stname *x, int indent,       \
                               const ASN1_PCTX *pctx);
# define DECLARE_ASN1_PRINT_FUNCTION_fname(stname, fname)                   \
    DECLARE_ASN1_PRINT_FUNCTION_fname_attr(extern, stname, fname)

# define D2I_OF(type) type *(*)(type **,const unsigned char **,long)
# define I2D_OF(type) int (*)(const type *,unsigned char **)

# define CHECKED_D2I_OF(type, d2i) \
    ((d2i_of_void*) (1 ? d2i : ((D2I_OF(type))0)))
# define CHECKED_I2D_OF(type, i2d) \
    ((i2d_of_void*) (1 ? i2d : ((I2D_OF(type))0)))
# define CHECKED_NEW_OF(type, xnew) \
    ((void *(*)(void)) (1 ? xnew : ((type *(*)(void))0)))
# define CHECKED_PTR_OF(type, p) \
    ((void*) (1 ? p : (type*)0))
# define CHECKED_PPTR_OF(type, p) \
    ((void**) (1 ? p : (type**)0))

# define TYPEDEF_D2I_OF(type) typedef type *d2i_of_##type(type **,const unsigned char **,long)
# define TYPEDEF_I2D_OF(type) typedef int i2d_of_##type(const type *,unsigned char **)
# define TYPEDEF_D2I2D_OF(type) TYPEDEF_D2I_OF(type); TYPEDEF_I2D_OF(type)

typedef void *d2i_of_void(void **, const unsigned char **, long);
typedef int i2d_of_void(const void *, unsigned char **);

/*-
 * The following macros and typedefs allow an ASN1_ITEM
 * to be embedded in a structure and referenced. Since
 * the ASN1_ITEM pointers need to be globally accessible
 * (possibly from shared libraries) they may exist in
 * different forms. On platforms that support it the
 * ASN1_ITEM structure itself will be globally exported.
 * Other platforms will export a function that returns
 * an ASN1_ITEM pointer.
 *
 * To handle both cases transparently the macros below
 * should be used instead of hard coding an ASN1_ITEM
 * pointer in a structure.
 *
 * The structure will look like this:
 *
 * typedef struct SOMETHING_st {
 *      ...
 *      ASN1_ITEM_EXP *iptr;
 *      ...
 * } SOMETHING;
 *
 * It would be initialised as e.g.:
 *
 * SOMETHING somevar = {...,ASN1_ITEM_ref(X509),...};
 *
 * and the actual pointer extracted with:
 *
 * const ASN1_ITEM *it = ASN1_ITEM_ptr(somevar.iptr);
 *
 * Finally an ASN1_ITEM pointer can be extracted from an
 * appropriate reference with: ASN1_ITEM_rptr(X509). This
 * would be used when a function takes an ASN1_ITEM * argument.
 *
 */


/*
 * Platforms that can't easily handle shared global variables are declared as
 * functions returning ASN1_ITEM pointers.
 */

/* ASN1_ITEM pointer exported type */
typedef const ASN1_ITEM *ASN1_ITEM_EXP (void);

/* Macro to obtain ASN1_ITEM pointer from exported type */
# define ASN1_ITEM_ptr(iptr) (iptr())

/* Macro to include ASN1_ITEM pointer from base type */
# define ASN1_ITEM_ref(iptr) (iptr##_it)

# define ASN1_ITEM_rptr(ref) (ref##_it())

# define DECLARE_ASN1_ITEM_attr(attr, name)                                 \
    attr const ASN1_ITEM * name##_it(void);
# define DECLARE_ASN1_ITEM(name)                                            \
    DECLARE_ASN1_ITEM_attr(extern, name)

/* Parameters used by ASN1_STRING_print_ex() */

/*
 * These determine which characters to escape: RFC2253 special characters,
 * control characters and MSB set characters
 */

# define ASN1_STRFLGS_ESC_2253           1
# define ASN1_STRFLGS_ESC_CTRL           2
# define ASN1_STRFLGS_ESC_MSB            4

/* Lower 8 bits are reserved as an output type specifier */
# define ASN1_DTFLGS_TYPE_MASK    0x0FUL
# define ASN1_DTFLGS_RFC822       0x00UL
# define ASN1_DTFLGS_ISO8601      0x01UL

/*
 * This flag determines how we do escaping: normally RC2253 backslash only,
 * set this to use backslash and quote.
 */

# define ASN1_STRFLGS_ESC_QUOTE          8

/* These three flags are internal use only. */

/* Character is a valid PrintableString character */
# define CHARTYPE_PRINTABLESTRING        0x10
/* Character needs escaping if it is the first character */
# define CHARTYPE_FIRST_ESC_2253         0x20
/* Character needs escaping if it is the last character */
# define CHARTYPE_LAST_ESC_2253          0x40

/*
 * NB the internal flags are safely reused below by flags handled at the top
 * level.
 */

/*
 * If this is set we convert all character strings to UTF8 first
 */

# define ASN1_STRFLGS_UTF8_CONVERT       0x10

/*
 * If this is set we don't attempt to interpret content: just assume all
 * strings are 1 byte per character. This will produce some pretty odd
 * looking output!
 */

# define ASN1_STRFLGS_IGNORE_TYPE        0x20

/* If this is set we include the string type in the output */
# define ASN1_STRFLGS_SHOW_TYPE          0x40

/*
 * This determines which strings to display and which to 'dump' (hex dump of
 * content octets or DER encoding). We can only dump non character strings or
 * everything. If we don't dump 'unknown' they are interpreted as character
 * strings with 1 octet per character and are subject to the usual escaping
 * options.
 */

# define ASN1_STRFLGS_DUMP_ALL           0x80
# define ASN1_STRFLGS_DUMP_UNKNOWN       0x100

/*
 * These determine what 'dumping' does, we can dump the content octets or the
 * DER encoding: both use the RFC2253 #XXXXX notation.
 */

# define ASN1_STRFLGS_DUMP_DER           0x200

/*
 * This flag specifies that RC2254 escaping shall be performed.
 */
#define ASN1_STRFLGS_ESC_2254           0x400

/*
 * All the string flags consistent with RFC2253, escaping control characters
 * isn't essential in RFC2253 but it is advisable anyway.
 */

# define ASN1_STRFLGS_RFC2253    (ASN1_STRFLGS_ESC_2253 | \
                                ASN1_STRFLGS_ESC_CTRL | \
                                ASN1_STRFLGS_ESC_MSB | \
                                ASN1_STRFLGS_UTF8_CONVERT | \
                                ASN1_STRFLGS_DUMP_UNKNOWN | \
                                ASN1_STRFLGS_DUMP_DER)


struct asn1_type_st {
    int type;
    union {
        char *ptr;
        ASN1_BOOLEAN boolean;
        ASN1_STRING *asn1_string;
        ASN1_OBJECT *object;
        ASN1_INTEGER *integer;
        ASN1_ENUMERATED *enumerated;
        ASN1_BIT_STRING *bit_string;
        ASN1_OCTET_STRING *octet_string;
        ASN1_PRINTABLESTRING *printablestring;
        ASN1_T61STRING *t61string;
        ASN1_IA5STRING *ia5string;
        ASN1_GENERALSTRING *generalstring;
        ASN1_BMPSTRING *bmpstring;
        ASN1_UNIVERSALSTRING *universalstring;
        ASN1_UTCTIME *utctime;
        ASN1_GENERALIZEDTIME *generalizedtime;
        ASN1_VISIBLESTRING *visiblestring;
        ASN1_UTF8STRING *utf8string;
        /*
         * set and sequence are left complete and still contain the set or
         * sequence bytes
         */
        ASN1_STRING *set;
        ASN1_STRING *sequence;
        ASN1_VALUE *asn1_value;
    } value;
};

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_TYPE, ASN1_TYPE, ASN1_TYPE)
#define sk_ASN1_TYPE_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_TYPE_sk_type(sk))
#define sk_ASN1_TYPE_value(sk, idx) ((ASN1_TYPE *)OPENSSL_sk_value(ossl_check_const_ASN1_TYPE_sk_type(sk), (idx)))
#define sk_ASN1_TYPE_new(cmp) ((STACK_OF(ASN1_TYPE) *)OPENSSL_sk_new(ossl_check_ASN1_TYPE_compfunc_type(cmp)))
#define sk_ASN1_TYPE_new_null() ((STACK_OF(ASN1_TYPE) *)OPENSSL_sk_new_null())
#define sk_ASN1_TYPE_new_reserve(cmp, n) ((STACK_OF(ASN1_TYPE) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_TYPE_compfunc_type(cmp), (n)))
#define sk_ASN1_TYPE_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_TYPE_sk_type(sk), (n))
#define sk_ASN1_TYPE_free(sk) OPENSSL_sk_free(ossl_check_ASN1_TYPE_sk_type(sk))
#define sk_ASN1_TYPE_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_TYPE_sk_type(sk))
#define sk_ASN1_TYPE_delete(sk, i) ((ASN1_TYPE *)OPENSSL_sk_delete(ossl_check_ASN1_TYPE_sk_type(sk), (i)))
#define sk_ASN1_TYPE_delete_ptr(sk, ptr) ((ASN1_TYPE *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr)))
#define sk_ASN1_TYPE_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr))
#define sk_ASN1_TYPE_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr))
#define sk_ASN1_TYPE_pop(sk) ((ASN1_TYPE *)OPENSSL_sk_pop(ossl_check_ASN1_TYPE_sk_type(sk)))
#define sk_ASN1_TYPE_shift(sk) ((ASN1_TYPE *)OPENSSL_sk_shift(ossl_check_ASN1_TYPE_sk_type(sk)))
#define sk_ASN1_TYPE_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_TYPE_sk_type(sk),ossl_check_ASN1_TYPE_freefunc_type(freefunc))
#define sk_ASN1_TYPE_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr), (idx))
#define sk_ASN1_TYPE_set(sk, idx, ptr) ((ASN1_TYPE *)OPENSSL_sk_set(ossl_check_ASN1_TYPE_sk_type(sk), (idx), ossl_check_ASN1_TYPE_type(ptr)))
#define sk_ASN1_TYPE_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr))
#define sk_ASN1_TYPE_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr))
#define sk_ASN1_TYPE_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_type(ptr), pnum)
#define sk_ASN1_TYPE_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_TYPE_sk_type(sk))
#define sk_ASN1_TYPE_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_TYPE_sk_type(sk))
#define sk_ASN1_TYPE_dup(sk) ((STACK_OF(ASN1_TYPE) *)OPENSSL_sk_dup(ossl_check_const_ASN1_TYPE_sk_type(sk)))
#define sk_ASN1_TYPE_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_TYPE) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_copyfunc_type(copyfunc), ossl_check_ASN1_TYPE_freefunc_type(freefunc)))
#define sk_ASN1_TYPE_set_cmp_func(sk, cmp) ((sk_ASN1_TYPE_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_TYPE_sk_type(sk), ossl_check_ASN1_TYPE_compfunc_type(cmp)))


typedef STACK_OF(ASN1_TYPE) ASN1_SEQUENCE_ANY;

DECLARE_ASN1_ENCODE_FUNCTIONS_name(ASN1_SEQUENCE_ANY, ASN1_SEQUENCE_ANY)
DECLARE_ASN1_ENCODE_FUNCTIONS_name(ASN1_SEQUENCE_ANY, ASN1_SET_ANY)

/* This is used to contain a list of bit names */
typedef struct BIT_STRING_BITNAME_st {
    int bitnum;
    const char *lname;
    const char *sname;
} BIT_STRING_BITNAME;

# define B_ASN1_TIME \
                        B_ASN1_UTCTIME | \
                        B_ASN1_GENERALIZEDTIME

# define B_ASN1_PRINTABLE \
                        B_ASN1_NUMERICSTRING| \
                        B_ASN1_PRINTABLESTRING| \
                        B_ASN1_T61STRING| \
                        B_ASN1_IA5STRING| \
                        B_ASN1_BIT_STRING| \
                        B_ASN1_UNIVERSALSTRING|\
                        B_ASN1_BMPSTRING|\
                        B_ASN1_UTF8STRING|\
                        B_ASN1_SEQUENCE|\
                        B_ASN1_UNKNOWN

# define B_ASN1_DIRECTORYSTRING \
                        B_ASN1_PRINTABLESTRING| \
                        B_ASN1_TELETEXSTRING|\
                        B_ASN1_BMPSTRING|\
                        B_ASN1_UNIVERSALSTRING|\
                        B_ASN1_UTF8STRING

# define B_ASN1_DISPLAYTEXT \
                        B_ASN1_IA5STRING| \
                        B_ASN1_VISIBLESTRING| \
                        B_ASN1_BMPSTRING|\
                        B_ASN1_UTF8STRING

DECLARE_ASN1_ALLOC_FUNCTIONS_name(ASN1_TYPE, ASN1_TYPE)
DECLARE_ASN1_ENCODE_FUNCTIONS(ASN1_TYPE, ASN1_ANY, ASN1_TYPE)

int ASN1_TYPE_get(const ASN1_TYPE *a);
void ASN1_TYPE_set(ASN1_TYPE *a, int type, void *value);
int ASN1_TYPE_set1(ASN1_TYPE *a, int type, const void *value);
int ASN1_TYPE_cmp(const ASN1_TYPE *a, const ASN1_TYPE *b);

ASN1_TYPE *ASN1_TYPE_pack_sequence(const ASN1_ITEM *it, void *s, ASN1_TYPE **t);
void *ASN1_TYPE_unpack_sequence(const ASN1_ITEM *it, const ASN1_TYPE *t);

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_OBJECT, ASN1_OBJECT, ASN1_OBJECT)
#define sk_ASN1_OBJECT_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_OBJECT_sk_type(sk))
#define sk_ASN1_OBJECT_value(sk, idx) ((ASN1_OBJECT *)OPENSSL_sk_value(ossl_check_const_ASN1_OBJECT_sk_type(sk), (idx)))
#define sk_ASN1_OBJECT_new(cmp) ((STACK_OF(ASN1_OBJECT) *)OPENSSL_sk_new(ossl_check_ASN1_OBJECT_compfunc_type(cmp)))
#define sk_ASN1_OBJECT_new_null() ((STACK_OF(ASN1_OBJECT) *)OPENSSL_sk_new_null())
#define sk_ASN1_OBJECT_new_reserve(cmp, n) ((STACK_OF(ASN1_OBJECT) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_OBJECT_compfunc_type(cmp), (n)))
#define sk_ASN1_OBJECT_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_OBJECT_sk_type(sk), (n))
#define sk_ASN1_OBJECT_free(sk) OPENSSL_sk_free(ossl_check_ASN1_OBJECT_sk_type(sk))
#define sk_ASN1_OBJECT_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_OBJECT_sk_type(sk))
#define sk_ASN1_OBJECT_delete(sk, i) ((ASN1_OBJECT *)OPENSSL_sk_delete(ossl_check_ASN1_OBJECT_sk_type(sk), (i)))
#define sk_ASN1_OBJECT_delete_ptr(sk, ptr) ((ASN1_OBJECT *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr)))
#define sk_ASN1_OBJECT_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr))
#define sk_ASN1_OBJECT_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr))
#define sk_ASN1_OBJECT_pop(sk) ((ASN1_OBJECT *)OPENSSL_sk_pop(ossl_check_ASN1_OBJECT_sk_type(sk)))
#define sk_ASN1_OBJECT_shift(sk) ((ASN1_OBJECT *)OPENSSL_sk_shift(ossl_check_ASN1_OBJECT_sk_type(sk)))
#define sk_ASN1_OBJECT_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_OBJECT_sk_type(sk),ossl_check_ASN1_OBJECT_freefunc_type(freefunc))
#define sk_ASN1_OBJECT_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr), (idx))
#define sk_ASN1_OBJECT_set(sk, idx, ptr) ((ASN1_OBJECT *)OPENSSL_sk_set(ossl_check_ASN1_OBJECT_sk_type(sk), (idx), ossl_check_ASN1_OBJECT_type(ptr)))
#define sk_ASN1_OBJECT_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr))
#define sk_ASN1_OBJECT_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr))
#define sk_ASN1_OBJECT_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_type(ptr), pnum)
#define sk_ASN1_OBJECT_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_OBJECT_sk_type(sk))
#define sk_ASN1_OBJECT_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_OBJECT_sk_type(sk))
#define sk_ASN1_OBJECT_dup(sk) ((STACK_OF(ASN1_OBJECT) *)OPENSSL_sk_dup(ossl_check_const_ASN1_OBJECT_sk_type(sk)))
#define sk_ASN1_OBJECT_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_OBJECT) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_copyfunc_type(copyfunc), ossl_check_ASN1_OBJECT_freefunc_type(freefunc)))
#define sk_ASN1_OBJECT_set_cmp_func(sk, cmp) ((sk_ASN1_OBJECT_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_OBJECT_sk_type(sk), ossl_check_ASN1_OBJECT_compfunc_type(cmp)))


DECLARE_ASN1_FUNCTIONS(ASN1_OBJECT)

ASN1_STRING *ASN1_STRING_new(void);
void ASN1_STRING_free(ASN1_STRING *a);
void ASN1_STRING_clear_free(ASN1_STRING *a);
int ASN1_STRING_copy(ASN1_STRING *dst, const ASN1_STRING *str);
DECLARE_ASN1_DUP_FUNCTION(ASN1_STRING)
ASN1_STRING *ASN1_STRING_type_new(int type);
int ASN1_STRING_cmp(const ASN1_STRING *a, const ASN1_STRING *b);
  /*
   * Since this is used to store all sorts of things, via macros, for now,
   * make its data void *
   */
int ASN1_STRING_set(ASN1_STRING *str, const void *data, int len);
void ASN1_STRING_set0(ASN1_STRING *str, void *data, int len);
int ASN1_STRING_length(const ASN1_STRING *x);
# ifndef OPENSSL_NO_DEPRECATED_3_0
OSSL_DEPRECATEDIN_3_0 void ASN1_STRING_length_set(ASN1_STRING *x, int n);
# endif
int ASN1_STRING_type(const ASN1_STRING *x);
# ifndef OPENSSL_NO_DEPRECATED_1_1_0
OSSL_DEPRECATEDIN_1_1_0 unsigned char *ASN1_STRING_data(ASN1_STRING *x);
# endif
const unsigned char *ASN1_STRING_get0_data(const ASN1_STRING *x);

DECLARE_ASN1_FUNCTIONS(ASN1_BIT_STRING)
int ASN1_BIT_STRING_set(ASN1_BIT_STRING *a, unsigned char *d, int length);
int ASN1_BIT_STRING_set_bit(ASN1_BIT_STRING *a, int n, int value);
int ASN1_BIT_STRING_get_bit(const ASN1_BIT_STRING *a, int n);
int ASN1_BIT_STRING_check(const ASN1_BIT_STRING *a,
                          const unsigned char *flags, int flags_len);

int ASN1_BIT_STRING_name_print(BIO *out, ASN1_BIT_STRING *bs,
                               BIT_STRING_BITNAME *tbl, int indent);
int ASN1_BIT_STRING_num_asc(const char *name, BIT_STRING_BITNAME *tbl);
int ASN1_BIT_STRING_set_asc(ASN1_BIT_STRING *bs, const char *name, int value,
                            BIT_STRING_BITNAME *tbl);

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_INTEGER, ASN1_INTEGER, ASN1_INTEGER)
#define sk_ASN1_INTEGER_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_INTEGER_sk_type(sk))
#define sk_ASN1_INTEGER_value(sk, idx) ((ASN1_INTEGER *)OPENSSL_sk_value(ossl_check_const_ASN1_INTEGER_sk_type(sk), (idx)))
#define sk_ASN1_INTEGER_new(cmp) ((STACK_OF(ASN1_INTEGER) *)OPENSSL_sk_new(ossl_check_ASN1_INTEGER_compfunc_type(cmp)))
#define sk_ASN1_INTEGER_new_null() ((STACK_OF(ASN1_INTEGER) *)OPENSSL_sk_new_null())
#define sk_ASN1_INTEGER_new_reserve(cmp, n) ((STACK_OF(ASN1_INTEGER) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_INTEGER_compfunc_type(cmp), (n)))
#define sk_ASN1_INTEGER_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_INTEGER_sk_type(sk), (n))
#define sk_ASN1_INTEGER_free(sk) OPENSSL_sk_free(ossl_check_ASN1_INTEGER_sk_type(sk))
#define sk_ASN1_INTEGER_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_INTEGER_sk_type(sk))
#define sk_ASN1_INTEGER_delete(sk, i) ((ASN1_INTEGER *)OPENSSL_sk_delete(ossl_check_ASN1_INTEGER_sk_type(sk), (i)))
#define sk_ASN1_INTEGER_delete_ptr(sk, ptr) ((ASN1_INTEGER *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr)))
#define sk_ASN1_INTEGER_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr))
#define sk_ASN1_INTEGER_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr))
#define sk_ASN1_INTEGER_pop(sk) ((ASN1_INTEGER *)OPENSSL_sk_pop(ossl_check_ASN1_INTEGER_sk_type(sk)))
#define sk_ASN1_INTEGER_shift(sk) ((ASN1_INTEGER *)OPENSSL_sk_shift(ossl_check_ASN1_INTEGER_sk_type(sk)))
#define sk_ASN1_INTEGER_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_INTEGER_sk_type(sk),ossl_check_ASN1_INTEGER_freefunc_type(freefunc))
#define sk_ASN1_INTEGER_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr), (idx))
#define sk_ASN1_INTEGER_set(sk, idx, ptr) ((ASN1_INTEGER *)OPENSSL_sk_set(ossl_check_ASN1_INTEGER_sk_type(sk), (idx), ossl_check_ASN1_INTEGER_type(ptr)))
#define sk_ASN1_INTEGER_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr))
#define sk_ASN1_INTEGER_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr))
#define sk_ASN1_INTEGER_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_type(ptr), pnum)
#define sk_ASN1_INTEGER_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_INTEGER_sk_type(sk))
#define sk_ASN1_INTEGER_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_INTEGER_sk_type(sk))
#define sk_ASN1_INTEGER_dup(sk) ((STACK_OF(ASN1_INTEGER) *)OPENSSL_sk_dup(ossl_check_const_ASN1_INTEGER_sk_type(sk)))
#define sk_ASN1_INTEGER_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_INTEGER) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_copyfunc_type(copyfunc), ossl_check_ASN1_INTEGER_freefunc_type(freefunc)))
#define sk_ASN1_INTEGER_set_cmp_func(sk, cmp) ((sk_ASN1_INTEGER_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_INTEGER_sk_type(sk), ossl_check_ASN1_INTEGER_compfunc_type(cmp)))



DECLARE_ASN1_FUNCTIONS(ASN1_INTEGER)
ASN1_INTEGER *d2i_ASN1_UINTEGER(ASN1_INTEGER **a, const unsigned char **pp,
                                long length);
DECLARE_ASN1_DUP_FUNCTION(ASN1_INTEGER)
int ASN1_INTEGER_cmp(const ASN1_INTEGER *x, const ASN1_INTEGER *y);

DECLARE_ASN1_FUNCTIONS(ASN1_ENUMERATED)

int ASN1_UTCTIME_check(const ASN1_UTCTIME *a);
ASN1_UTCTIME *ASN1_UTCTIME_set(ASN1_UTCTIME *s, time_t t);
ASN1_UTCTIME *ASN1_UTCTIME_adj(ASN1_UTCTIME *s, time_t t,
                               int offset_day, long offset_sec);
int ASN1_UTCTIME_set_string(ASN1_UTCTIME *s, const char *str);
int ASN1_UTCTIME_cmp_time_t(const ASN1_UTCTIME *s, time_t t);

int ASN1_GENERALIZEDTIME_check(const ASN1_GENERALIZEDTIME *a);
ASN1_GENERALIZEDTIME *ASN1_GENERALIZEDTIME_set(ASN1_GENERALIZEDTIME *s,
                                               time_t t);
ASN1_GENERALIZEDTIME *ASN1_GENERALIZEDTIME_adj(ASN1_GENERALIZEDTIME *s,
                                               time_t t, int offset_day,
                                               long offset_sec);
int ASN1_GENERALIZEDTIME_set_string(ASN1_GENERALIZEDTIME *s, const char *str);

int ASN1_TIME_diff(int *pday, int *psec,
                   const ASN1_TIME *from, const ASN1_TIME *to);

DECLARE_ASN1_FUNCTIONS(ASN1_OCTET_STRING)
DECLARE_ASN1_DUP_FUNCTION(ASN1_OCTET_STRING)
int ASN1_OCTET_STRING_cmp(const ASN1_OCTET_STRING *a,
                          const ASN1_OCTET_STRING *b);
int ASN1_OCTET_STRING_set(ASN1_OCTET_STRING *str, const unsigned char *data,
                          int len);

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_UTF8STRING, ASN1_UTF8STRING, ASN1_UTF8STRING)
#define sk_ASN1_UTF8STRING_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_UTF8STRING_sk_type(sk))
#define sk_ASN1_UTF8STRING_value(sk, idx) ((ASN1_UTF8STRING *)OPENSSL_sk_value(ossl_check_const_ASN1_UTF8STRING_sk_type(sk), (idx)))
#define sk_ASN1_UTF8STRING_new(cmp) ((STACK_OF(ASN1_UTF8STRING) *)OPENSSL_sk_new(ossl_check_ASN1_UTF8STRING_compfunc_type(cmp)))
#define sk_ASN1_UTF8STRING_new_null() ((STACK_OF(ASN1_UTF8STRING) *)OPENSSL_sk_new_null())
#define sk_ASN1_UTF8STRING_new_reserve(cmp, n) ((STACK_OF(ASN1_UTF8STRING) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_UTF8STRING_compfunc_type(cmp), (n)))
#define sk_ASN1_UTF8STRING_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_UTF8STRING_sk_type(sk), (n))
#define sk_ASN1_UTF8STRING_free(sk) OPENSSL_sk_free(ossl_check_ASN1_UTF8STRING_sk_type(sk))
#define sk_ASN1_UTF8STRING_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_UTF8STRING_sk_type(sk))
#define sk_ASN1_UTF8STRING_delete(sk, i) ((ASN1_UTF8STRING *)OPENSSL_sk_delete(ossl_check_ASN1_UTF8STRING_sk_type(sk), (i)))
#define sk_ASN1_UTF8STRING_delete_ptr(sk, ptr) ((ASN1_UTF8STRING *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr)))
#define sk_ASN1_UTF8STRING_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr))
#define sk_ASN1_UTF8STRING_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr))
#define sk_ASN1_UTF8STRING_pop(sk) ((ASN1_UTF8STRING *)OPENSSL_sk_pop(ossl_check_ASN1_UTF8STRING_sk_type(sk)))
#define sk_ASN1_UTF8STRING_shift(sk) ((ASN1_UTF8STRING *)OPENSSL_sk_shift(ossl_check_ASN1_UTF8STRING_sk_type(sk)))
#define sk_ASN1_UTF8STRING_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_UTF8STRING_sk_type(sk),ossl_check_ASN1_UTF8STRING_freefunc_type(freefunc))
#define sk_ASN1_UTF8STRING_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr), (idx))
#define sk_ASN1_UTF8STRING_set(sk, idx, ptr) ((ASN1_UTF8STRING *)OPENSSL_sk_set(ossl_check_ASN1_UTF8STRING_sk_type(sk), (idx), ossl_check_ASN1_UTF8STRING_type(ptr)))
#define sk_ASN1_UTF8STRING_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr))
#define sk_ASN1_UTF8STRING_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr))
#define sk_ASN1_UTF8STRING_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_type(ptr), pnum)
#define sk_ASN1_UTF8STRING_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_UTF8STRING_sk_type(sk))
#define sk_ASN1_UTF8STRING_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_UTF8STRING_sk_type(sk))
#define sk_ASN1_UTF8STRING_dup(sk) ((STACK_OF(ASN1_UTF8STRING) *)OPENSSL_sk_dup(ossl_check_const_ASN1_UTF8STRING_sk_type(sk)))
#define sk_ASN1_UTF8STRING_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_UTF8STRING) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_copyfunc_type(copyfunc), ossl_check_ASN1_UTF8STRING_freefunc_type(freefunc)))
#define sk_ASN1_UTF8STRING_set_cmp_func(sk, cmp) ((sk_ASN1_UTF8STRING_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_UTF8STRING_sk_type(sk), ossl_check_ASN1_UTF8STRING_compfunc_type(cmp)))


DECLARE_ASN1_FUNCTIONS(ASN1_VISIBLESTRING)
DECLARE_ASN1_FUNCTIONS(ASN1_UNIVERSALSTRING)
DECLARE_ASN1_FUNCTIONS(ASN1_UTF8STRING)
DECLARE_ASN1_FUNCTIONS(ASN1_NULL)
DECLARE_ASN1_FUNCTIONS(ASN1_BMPSTRING)

int UTF8_getc(const unsigned char *str, int len, unsigned long *val);
int UTF8_putc(unsigned char *str, int len, unsigned long value);

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_GENERALSTRING, ASN1_GENERALSTRING, ASN1_GENERALSTRING)
#define sk_ASN1_GENERALSTRING_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_GENERALSTRING_sk_type(sk))
#define sk_ASN1_GENERALSTRING_value(sk, idx) ((ASN1_GENERALSTRING *)OPENSSL_sk_value(ossl_check_const_ASN1_GENERALSTRING_sk_type(sk), (idx)))
#define sk_ASN1_GENERALSTRING_new(cmp) ((STACK_OF(ASN1_GENERALSTRING) *)OPENSSL_sk_new(ossl_check_ASN1_GENERALSTRING_compfunc_type(cmp)))
#define sk_ASN1_GENERALSTRING_new_null() ((STACK_OF(ASN1_GENERALSTRING) *)OPENSSL_sk_new_null())
#define sk_ASN1_GENERALSTRING_new_reserve(cmp, n) ((STACK_OF(ASN1_GENERALSTRING) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_GENERALSTRING_compfunc_type(cmp), (n)))
#define sk_ASN1_GENERALSTRING_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_GENERALSTRING_sk_type(sk), (n))
#define sk_ASN1_GENERALSTRING_free(sk) OPENSSL_sk_free(ossl_check_ASN1_GENERALSTRING_sk_type(sk))
#define sk_ASN1_GENERALSTRING_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_GENERALSTRING_sk_type(sk))
#define sk_ASN1_GENERALSTRING_delete(sk, i) ((ASN1_GENERALSTRING *)OPENSSL_sk_delete(ossl_check_ASN1_GENERALSTRING_sk_type(sk), (i)))
#define sk_ASN1_GENERALSTRING_delete_ptr(sk, ptr) ((ASN1_GENERALSTRING *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr)))
#define sk_ASN1_GENERALSTRING_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr))
#define sk_ASN1_GENERALSTRING_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr))
#define sk_ASN1_GENERALSTRING_pop(sk) ((ASN1_GENERALSTRING *)OPENSSL_sk_pop(ossl_check_ASN1_GENERALSTRING_sk_type(sk)))
#define sk_ASN1_GENERALSTRING_shift(sk) ((ASN1_GENERALSTRING *)OPENSSL_sk_shift(ossl_check_ASN1_GENERALSTRING_sk_type(sk)))
#define sk_ASN1_GENERALSTRING_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_GENERALSTRING_sk_type(sk),ossl_check_ASN1_GENERALSTRING_freefunc_type(freefunc))
#define sk_ASN1_GENERALSTRING_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr), (idx))
#define sk_ASN1_GENERALSTRING_set(sk, idx, ptr) ((ASN1_GENERALSTRING *)OPENSSL_sk_set(ossl_check_ASN1_GENERALSTRING_sk_type(sk), (idx), ossl_check_ASN1_GENERALSTRING_type(ptr)))
#define sk_ASN1_GENERALSTRING_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr))
#define sk_ASN1_GENERALSTRING_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr))
#define sk_ASN1_GENERALSTRING_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_type(ptr), pnum)
#define sk_ASN1_GENERALSTRING_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_GENERALSTRING_sk_type(sk))
#define sk_ASN1_GENERALSTRING_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_GENERALSTRING_sk_type(sk))
#define sk_ASN1_GENERALSTRING_dup(sk) ((STACK_OF(ASN1_GENERALSTRING) *)OPENSSL_sk_dup(ossl_check_const_ASN1_GENERALSTRING_sk_type(sk)))
#define sk_ASN1_GENERALSTRING_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_GENERALSTRING) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_copyfunc_type(copyfunc), ossl_check_ASN1_GENERALSTRING_freefunc_type(freefunc)))
#define sk_ASN1_GENERALSTRING_set_cmp_func(sk, cmp) ((sk_ASN1_GENERALSTRING_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_GENERALSTRING_sk_type(sk), ossl_check_ASN1_GENERALSTRING_compfunc_type(cmp)))


DECLARE_ASN1_FUNCTIONS_name(ASN1_STRING, ASN1_PRINTABLE)

DECLARE_ASN1_FUNCTIONS_name(ASN1_STRING, DIRECTORYSTRING)
DECLARE_ASN1_FUNCTIONS_name(ASN1_STRING, DISPLAYTEXT)
DECLARE_ASN1_FUNCTIONS(ASN1_PRINTABLESTRING)
DECLARE_ASN1_FUNCTIONS(ASN1_T61STRING)
DECLARE_ASN1_FUNCTIONS(ASN1_IA5STRING)
DECLARE_ASN1_FUNCTIONS(ASN1_GENERALSTRING)
DECLARE_ASN1_FUNCTIONS(ASN1_UTCTIME)
DECLARE_ASN1_FUNCTIONS(ASN1_GENERALIZEDTIME)
DECLARE_ASN1_FUNCTIONS(ASN1_TIME)

DECLARE_ASN1_DUP_FUNCTION(ASN1_TIME)
DECLARE_ASN1_DUP_FUNCTION(ASN1_UTCTIME)
DECLARE_ASN1_DUP_FUNCTION(ASN1_GENERALIZEDTIME)

DECLARE_ASN1_ITEM(ASN1_OCTET_STRING_NDEF)

ASN1_TIME *ASN1_TIME_set(ASN1_TIME *s, time_t t);
ASN1_TIME *ASN1_TIME_adj(ASN1_TIME *s, time_t t,
                         int offset_day, long offset_sec);
int ASN1_TIME_check(const ASN1_TIME *t);
ASN1_GENERALIZEDTIME *ASN1_TIME_to_generalizedtime(const ASN1_TIME *t,
                                                   ASN1_GENERALIZEDTIME **out);
int ASN1_TIME_set_string(ASN1_TIME *s, const char *str);
int ASN1_TIME_set_string_X509(ASN1_TIME *s, const char *str);
int ASN1_TIME_to_tm(const ASN1_TIME *s, struct tm *tm);
int ASN1_TIME_normalize(ASN1_TIME *s);
int ASN1_TIME_cmp_time_t(const ASN1_TIME *s, time_t t);
int ASN1_TIME_compare(const ASN1_TIME *a, const ASN1_TIME *b);

int i2a_ASN1_INTEGER(BIO *bp, const ASN1_INTEGER *a);
int a2i_ASN1_INTEGER(BIO *bp, ASN1_INTEGER *bs, char *buf, int size);
int i2a_ASN1_ENUMERATED(BIO *bp, const ASN1_ENUMERATED *a);
int a2i_ASN1_ENUMERATED(BIO *bp, ASN1_ENUMERATED *bs, char *buf, int size);
int i2a_ASN1_OBJECT(BIO *bp, const ASN1_OBJECT *a);
int a2i_ASN1_STRING(BIO *bp, ASN1_STRING *bs, char *buf, int size);
int i2a_ASN1_STRING(BIO *bp, const ASN1_STRING *a, int type);
int i2t_ASN1_OBJECT(char *buf, int buf_len, const ASN1_OBJECT *a);

int a2d_ASN1_OBJECT(unsigned char *out, int olen, const char *buf, int num);
ASN1_OBJECT *ASN1_OBJECT_create(int nid, unsigned char *data, int len,
                                const char *sn, const char *ln);

int ASN1_INTEGER_get_int64(int64_t *pr, const ASN1_INTEGER *a);
int ASN1_INTEGER_set_int64(ASN1_INTEGER *a, int64_t r);
int ASN1_INTEGER_get_uint64(uint64_t *pr, const ASN1_INTEGER *a);
int ASN1_INTEGER_set_uint64(ASN1_INTEGER *a, uint64_t r);

int ASN1_INTEGER_set(ASN1_INTEGER *a, long v);
long ASN1_INTEGER_get(const ASN1_INTEGER *a);
ASN1_INTEGER *BN_to_ASN1_INTEGER(const BIGNUM *bn, ASN1_INTEGER *ai);
BIGNUM *ASN1_INTEGER_to_BN(const ASN1_INTEGER *ai, BIGNUM *bn);

int ASN1_ENUMERATED_get_int64(int64_t *pr, const ASN1_ENUMERATED *a);
int ASN1_ENUMERATED_set_int64(ASN1_ENUMERATED *a, int64_t r);


int ASN1_ENUMERATED_set(ASN1_ENUMERATED *a, long v);
long ASN1_ENUMERATED_get(const ASN1_ENUMERATED *a);
ASN1_ENUMERATED *BN_to_ASN1_ENUMERATED(const BIGNUM *bn, ASN1_ENUMERATED *ai);
BIGNUM *ASN1_ENUMERATED_to_BN(const ASN1_ENUMERATED *ai, BIGNUM *bn);

/* General */
/* given a string, return the correct type, max is the maximum length */
int ASN1_PRINTABLE_type(const unsigned char *s, int max);

unsigned long ASN1_tag2bit(int tag);

/* SPECIALS */
int ASN1_get_object(const unsigned char **pp, long *plength, int *ptag,
                    int *pclass, long omax);
int ASN1_check_infinite_end(unsigned char **p, long len);
int ASN1_const_check_infinite_end(const unsigned char **p, long len);
void ASN1_put_object(unsigned char **pp, int constructed, int length,
                     int tag, int xclass);
int ASN1_put_eoc(unsigned char **pp);
int ASN1_object_size(int constructed, int length, int tag);

/* Used to implement other functions */
void *ASN1_dup(i2d_of_void *i2d, d2i_of_void *d2i, const void *x);

# define ASN1_dup_of(type,i2d,d2i,x) \
    ((type*)ASN1_dup(CHECKED_I2D_OF(type, i2d), \
                     CHECKED_D2I_OF(type, d2i), \
                     CHECKED_PTR_OF(const type, x)))

void *ASN1_item_dup(const ASN1_ITEM *it, const void *x);
int ASN1_item_sign_ex(const ASN1_ITEM *it, X509_ALGOR *algor1,
                      X509_ALGOR *algor2, ASN1_BIT_STRING *signature,
                      const void *data, const ASN1_OCTET_STRING *id,
                      EVP_PKEY *pkey, const EVP_MD *md, OSSL_LIB_CTX *libctx,
                      const char *propq);
int ASN1_item_verify_ex(const ASN1_ITEM *it, const X509_ALGOR *alg,
                        const ASN1_BIT_STRING *signature, const void *data,
                        const ASN1_OCTET_STRING *id, EVP_PKEY *pkey,
                        OSSL_LIB_CTX *libctx, const char *propq);

/* ASN1 alloc/free macros for when a type is only used internally */

# define M_ASN1_new_of(type) (type *)ASN1_item_new(ASN1_ITEM_rptr(type))
# define M_ASN1_free_of(x, type) \
                ASN1_item_free(CHECKED_PTR_OF(type, x), ASN1_ITEM_rptr(type))

# ifndef OPENSSL_NO_STDIO
void *ASN1_d2i_fp(void *(*xnew) (void), d2i_of_void *d2i, FILE *in, void **x);

#  define ASN1_d2i_fp_of(type,xnew,d2i,in,x) \
    ((type*)ASN1_d2i_fp(CHECKED_NEW_OF(type, xnew), \
                        CHECKED_D2I_OF(type, d2i), \
                        in, \
                        CHECKED_PPTR_OF(type, x)))

void *ASN1_item_d2i_fp_ex(const ASN1_ITEM *it, FILE *in, void *x,
                          OSSL_LIB_CTX *libctx, const char *propq);
void *ASN1_item_d2i_fp(const ASN1_ITEM *it, FILE *in, void *x);
int ASN1_i2d_fp(i2d_of_void *i2d, FILE *out, const void *x);

#  define ASN1_i2d_fp_of(type,i2d,out,x) \
    (ASN1_i2d_fp(CHECKED_I2D_OF(type, i2d), \
                 out, \
                 CHECKED_PTR_OF(const type, x)))

int ASN1_item_i2d_fp(const ASN1_ITEM *it, FILE *out, const void *x);
int ASN1_STRING_print_ex_fp(FILE *fp, const ASN1_STRING *str, unsigned long flags);
# endif

int ASN1_STRING_to_UTF8(unsigned char **out, const ASN1_STRING *in);

void *ASN1_d2i_bio(void *(*xnew) (void), d2i_of_void *d2i, BIO *in, void **x);

#  define ASN1_d2i_bio_of(type,xnew,d2i,in,x) \
    ((type*)ASN1_d2i_bio( CHECKED_NEW_OF(type, xnew), \
                          CHECKED_D2I_OF(type, d2i), \
                          in, \
                          CHECKED_PPTR_OF(type, x)))

void *ASN1_item_d2i_bio_ex(const ASN1_ITEM *it, BIO *in, void *pval,
                           OSSL_LIB_CTX *libctx, const char *propq);
void *ASN1_item_d2i_bio(const ASN1_ITEM *it, BIO *in, void *pval);
int ASN1_i2d_bio(i2d_of_void *i2d, BIO *out, const void *x);

#  define ASN1_i2d_bio_of(type,i2d,out,x) \
    (ASN1_i2d_bio(CHECKED_I2D_OF(type, i2d), \
                  out, \
                  CHECKED_PTR_OF(const type, x)))

int ASN1_item_i2d_bio(const ASN1_ITEM *it, BIO *out, const void *x);
BIO *ASN1_item_i2d_mem_bio(const ASN1_ITEM *it, const ASN1_VALUE *val);
int ASN1_UTCTIME_print(BIO *fp, const ASN1_UTCTIME *a);
int ASN1_GENERALIZEDTIME_print(BIO *fp, const ASN1_GENERALIZEDTIME *a);
int ASN1_TIME_print(BIO *bp, const ASN1_TIME *tm);
int ASN1_TIME_print_ex(BIO *bp, const ASN1_TIME *tm, unsigned long flags);
int ASN1_STRING_print(BIO *bp, const ASN1_STRING *v);
int ASN1_STRING_print_ex(BIO *out, const ASN1_STRING *str, unsigned long flags);
int ASN1_buf_print(BIO *bp, const unsigned char *buf, size_t buflen, int off);
int ASN1_bn_print(BIO *bp, const char *number, const BIGNUM *num,
                  unsigned char *buf, int off);
int ASN1_parse(BIO *bp, const unsigned char *pp, long len, int indent);
int ASN1_parse_dump(BIO *bp, const unsigned char *pp, long len, int indent,
                    int dump);
const char *ASN1_tag2str(int tag);

/* Used to load and write Netscape format cert */

int ASN1_UNIVERSALSTRING_to_string(ASN1_UNIVERSALSTRING *s);

int ASN1_TYPE_set_octetstring(ASN1_TYPE *a, unsigned char *data, int len);
int ASN1_TYPE_get_octetstring(const ASN1_TYPE *a, unsigned char *data, int max_len);
int ASN1_TYPE_set_int_octetstring(ASN1_TYPE *a, long num,
                                  unsigned char *data, int len);
int ASN1_TYPE_get_int_octetstring(const ASN1_TYPE *a, long *num,
                                  unsigned char *data, int max_len);

void *ASN1_item_unpack(const ASN1_STRING *oct, const ASN1_ITEM *it);

ASN1_STRING *ASN1_item_pack(void *obj, const ASN1_ITEM *it,
                            ASN1_OCTET_STRING **oct);

void ASN1_STRING_set_default_mask(unsigned long mask);
int ASN1_STRING_set_default_mask_asc(const char *p);
unsigned long ASN1_STRING_get_default_mask(void);
int ASN1_mbstring_copy(ASN1_STRING **out, const unsigned char *in, int len,
                       int inform, unsigned long mask);
int ASN1_mbstring_ncopy(ASN1_STRING **out, const unsigned char *in, int len,
                        int inform, unsigned long mask,
                        long minsize, long maxsize);

ASN1_STRING *ASN1_STRING_set_by_NID(ASN1_STRING **out,
                                    const unsigned char *in, int inlen,
                                    int inform, int nid);
ASN1_STRING_TABLE *ASN1_STRING_TABLE_get(int nid);
int ASN1_STRING_TABLE_add(int, long, long, unsigned long, unsigned long);
void ASN1_STRING_TABLE_cleanup(void);

/* ASN1 template functions */

/* Old API compatible functions */
ASN1_VALUE *ASN1_item_new(const ASN1_ITEM *it);
ASN1_VALUE *ASN1_item_new_ex(const ASN1_ITEM *it, OSSL_LIB_CTX *libctx,
                             const char *propq);
void ASN1_item_free(ASN1_VALUE *val, const ASN1_ITEM *it);
ASN1_VALUE *ASN1_item_d2i_ex(ASN1_VALUE **val, const unsigned char **in,
                             long len, const ASN1_ITEM *it,
                             OSSL_LIB_CTX *libctx, const char *propq);
ASN1_VALUE *ASN1_item_d2i(ASN1_VALUE **val, const unsigned char **in,
                          long len, const ASN1_ITEM *it);
int ASN1_item_i2d(const ASN1_VALUE *val, unsigned char **out, const ASN1_ITEM *it);
int ASN1_item_ndef_i2d(const ASN1_VALUE *val, unsigned char **out,
                       const ASN1_ITEM *it);

void ASN1_add_oid_module(void);
void ASN1_add_stable_module(void);

ASN1_TYPE *ASN1_generate_nconf(const char *str, CONF *nconf);
ASN1_TYPE *ASN1_generate_v3(const char *str, X509V3_CTX *cnf);
int ASN1_str2mask(const char *str, unsigned long *pmask);

/* ASN1 Print flags */

/* Indicate missing OPTIONAL fields */
# define ASN1_PCTX_FLAGS_SHOW_ABSENT             0x001
/* Mark start and end of SEQUENCE */
# define ASN1_PCTX_FLAGS_SHOW_SEQUENCE           0x002
/* Mark start and end of SEQUENCE/SET OF */
# define ASN1_PCTX_FLAGS_SHOW_SSOF               0x004
/* Show the ASN1 type of primitives */
# define ASN1_PCTX_FLAGS_SHOW_TYPE               0x008
/* Don't show ASN1 type of ANY */
# define ASN1_PCTX_FLAGS_NO_ANY_TYPE             0x010
/* Don't show ASN1 type of MSTRINGs */
# define ASN1_PCTX_FLAGS_NO_MSTRING_TYPE         0x020
/* Don't show field names in SEQUENCE */
# define ASN1_PCTX_FLAGS_NO_FIELD_NAME           0x040
/* Show structure names of each SEQUENCE field */
# define ASN1_PCTX_FLAGS_SHOW_FIELD_STRUCT_NAME  0x080
/* Don't show structure name even at top level */
# define ASN1_PCTX_FLAGS_NO_STRUCT_NAME          0x100

int ASN1_item_print(BIO *out, const ASN1_VALUE *ifld, int indent,
                    const ASN1_ITEM *it, const ASN1_PCTX *pctx);
ASN1_PCTX *ASN1_PCTX_new(void);
void ASN1_PCTX_free(ASN1_PCTX *p);
unsigned long ASN1_PCTX_get_flags(const ASN1_PCTX *p);
void ASN1_PCTX_set_flags(ASN1_PCTX *p, unsigned long flags);
unsigned long ASN1_PCTX_get_nm_flags(const ASN1_PCTX *p);
void ASN1_PCTX_set_nm_flags(ASN1_PCTX *p, unsigned long flags);
unsigned long ASN1_PCTX_get_cert_flags(const ASN1_PCTX *p);
void ASN1_PCTX_set_cert_flags(ASN1_PCTX *p, unsigned long flags);
unsigned long ASN1_PCTX_get_oid_flags(const ASN1_PCTX *p);
void ASN1_PCTX_set_oid_flags(ASN1_PCTX *p, unsigned long flags);
unsigned long ASN1_PCTX_get_str_flags(const ASN1_PCTX *p);
void ASN1_PCTX_set_str_flags(ASN1_PCTX *p, unsigned long flags);

ASN1_SCTX *ASN1_SCTX_new(int (*scan_cb) (ASN1_SCTX *ctx));
void ASN1_SCTX_free(ASN1_SCTX *p);
const ASN1_ITEM *ASN1_SCTX_get_item(ASN1_SCTX *p);
const ASN1_TEMPLATE *ASN1_SCTX_get_template(ASN1_SCTX *p);
unsigned long ASN1_SCTX_get_flags(ASN1_SCTX *p);
void ASN1_SCTX_set_app_data(ASN1_SCTX *p, void *data);
void *ASN1_SCTX_get_app_data(ASN1_SCTX *p);

const BIO_METHOD *BIO_f_asn1(void);

/* cannot constify val because of CMS_stream() */
BIO *BIO_new_NDEF(BIO *out, ASN1_VALUE *val, const ASN1_ITEM *it);

int i2d_ASN1_bio_stream(BIO *out, ASN1_VALUE *val, BIO *in, int flags,
                        const ASN1_ITEM *it);
int PEM_write_bio_ASN1_stream(BIO *out, ASN1_VALUE *val, BIO *in, int flags,
                              const char *hdr, const ASN1_ITEM *it);
/* cannot constify val because of CMS_dataFinal() */
int SMIME_write_ASN1(BIO *bio, ASN1_VALUE *val, BIO *data, int flags,
                     int ctype_nid, int econt_nid,
                     STACK_OF(X509_ALGOR) *mdalgs, const ASN1_ITEM *it);
int SMIME_write_ASN1_ex(BIO *bio, ASN1_VALUE *val, BIO *data, int flags,
                        int ctype_nid, int econt_nid,
                        STACK_OF(X509_ALGOR) *mdalgs, const ASN1_ITEM *it,
                        OSSL_LIB_CTX *libctx, const char *propq);
ASN1_VALUE *SMIME_read_ASN1(BIO *bio, BIO **bcont, const ASN1_ITEM *it);
ASN1_VALUE *SMIME_read_ASN1_ex(BIO *bio, int flags, BIO **bcont,
                               const ASN1_ITEM *it, ASN1_VALUE **x,
                               OSSL_LIB_CTX *libctx, const char *propq);
int SMIME_crlf_copy(BIO *in, BIO *out, int flags);
int SMIME_text(BIO *in, BIO *out);

const ASN1_ITEM *ASN1_ITEM_lookup(const char *name);
const ASN1_ITEM *ASN1_ITEM_get(size_t i);

/* Legacy compatibility */
# define DECLARE_ASN1_FUNCTIONS_fname(type, itname, name) \
         DECLARE_ASN1_ALLOC_FUNCTIONS_name(type, name) \
         DECLARE_ASN1_ENCODE_FUNCTIONS(type, itname, name)
# define DECLARE_ASN1_FUNCTIONS_const(type) DECLARE_ASN1_FUNCTIONS(type)
# define DECLARE_ASN1_ENCODE_FUNCTIONS_const(type, name) \
         DECLARE_ASN1_ENCODE_FUNCTIONS(type, name)
# define I2D_OF_const(type) I2D_OF(type)
# define ASN1_dup_of_const(type,i2d,d2i,x) ASN1_dup_of(type,i2d,d2i,x)
# define ASN1_i2d_fp_of_const(type,i2d,out,x) ASN1_i2d_fp_of(type,i2d,out,x)
# define ASN1_i2d_bio_of_const(type,i2d,out,x) ASN1_i2d_bio_of(type,i2d,out,x)

# ifdef  __cplusplus
}
# endif
#endif
              node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/asn1t.h              0000664 0000000 0000000 00000106141 14746647661 0030144 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /*
 * WARNING: do not edit!
 * Generated by Makefile from include/openssl/asn1t.h.in
 *
 * Copyright 2000-2021 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */



#ifndef OPENSSL_ASN1T_H
# define OPENSSL_ASN1T_H
# pragma once

# include <openssl/macros.h>
# ifndef OPENSSL_NO_DEPRECATED_3_0
#  define HEADER_ASN1T_H
# endif

# include <stddef.h>
# include <openssl/e_os2.h>
# include <openssl/asn1.h>

# ifdef OPENSSL_BUILD_SHLIBCRYPTO
#  undef OPENSSL_EXTERN
#  define OPENSSL_EXTERN OPENSSL_EXPORT
# endif

/* ASN1 template defines, structures and functions */

#ifdef  __cplusplus
extern "C" {
#endif

/*-
 * These are the possible values for the itype field of the
 * ASN1_ITEM structure and determine how it is interpreted.
 *
 * For PRIMITIVE types the underlying type
 * determines the behaviour if items is NULL.
 *
 * Otherwise templates must contain a single
 * template and the type is treated in the
 * same way as the type specified in the template.
 *
 * For SEQUENCE types the templates field points
 * to the members, the size field is the
 * structure size.
 *
 * For CHOICE types the templates field points
 * to each possible member (typically a union)
 * and the 'size' field is the offset of the
 * selector.
 *
 * The 'funcs' field is used for application-specific
 * data and functions.
 *
 * The EXTERN type uses a new style d2i/i2d.
 * The new style should be used where possible
 * because it avoids things like the d2i IMPLICIT
 * hack.
 *
 * MSTRING is a multiple string type, it is used
 * for a CHOICE of character strings where the
 * actual strings all occupy an ASN1_STRING
 * structure. In this case the 'utype' field
 * has a special meaning, it is used as a mask
 * of acceptable types using the B_ASN1 constants.
 *
 * NDEF_SEQUENCE is the same as SEQUENCE except
 * that it will use indefinite length constructed
 * encoding if requested.
 *
 */

# define ASN1_ITYPE_PRIMITIVE            0x0
# define ASN1_ITYPE_SEQUENCE             0x1
# define ASN1_ITYPE_CHOICE               0x2
/* unused value                          0x3 */
# define ASN1_ITYPE_EXTERN               0x4
# define ASN1_ITYPE_MSTRING              0x5
# define ASN1_ITYPE_NDEF_SEQUENCE        0x6

/* Macro to obtain ASN1_ADB pointer from a type (only used internally) */
# define ASN1_ADB_ptr(iptr) ((const ASN1_ADB *)((iptr)()))

/* Macros for start and end of ASN1_ITEM definition */

# define ASN1_ITEM_start(itname) \
        const ASN1_ITEM * itname##_it(void) \
        { \
                static const ASN1_ITEM local_it = {

# define static_ASN1_ITEM_start(itname) \
        static ASN1_ITEM_start(itname)

# define ASN1_ITEM_end(itname) \
                }; \
        return &local_it; \
        }

/* Macros to aid ASN1 template writing */

# define ASN1_ITEM_TEMPLATE(tname) \
        static const ASN1_TEMPLATE tname##_item_tt

# define ASN1_ITEM_TEMPLATE_END(tname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_PRIMITIVE,\
                -1,\
                &tname##_item_tt,\
                0,\
                NULL,\
                0,\
                #tname \
        ASN1_ITEM_end(tname)
# define static_ASN1_ITEM_TEMPLATE_END(tname) \
        ;\
        static_ASN1_ITEM_start(tname) \
                ASN1_ITYPE_PRIMITIVE,\
                -1,\
                &tname##_item_tt,\
                0,\
                NULL,\
                0,\
                #tname \
        ASN1_ITEM_end(tname)

/* This is a ASN1 type which just embeds a template */

/*-
 * This pair helps declare a SEQUENCE. We can do:
 *
 *      ASN1_SEQUENCE(stname) = {
 *              ... SEQUENCE components ...
 *      } ASN1_SEQUENCE_END(stname)
 *
 *      This will produce an ASN1_ITEM called stname_it
 *      for a structure called stname.
 *
 *      If you want the same structure but a different
 *      name then use:
 *
 *      ASN1_SEQUENCE(itname) = {
 *              ... SEQUENCE components ...
 *      } ASN1_SEQUENCE_END_name(stname, itname)
 *
 *      This will create an item called itname_it using
 *      a structure called stname.
 */

# define ASN1_SEQUENCE(tname) \
        static const ASN1_TEMPLATE tname##_seq_tt[]

# define ASN1_SEQUENCE_END(stname) ASN1_SEQUENCE_END_name(stname, stname)

# define static_ASN1_SEQUENCE_END(stname) static_ASN1_SEQUENCE_END_name(stname, stname)

# define ASN1_SEQUENCE_END_name(stname, tname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(stname),\
                #tname \
        ASN1_ITEM_end(tname)

# define static_ASN1_SEQUENCE_END_name(stname, tname) \
        ;\
        static_ASN1_ITEM_start(tname) \
                ASN1_ITYPE_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

# define ASN1_NDEF_SEQUENCE(tname) \
        ASN1_SEQUENCE(tname)

# define ASN1_NDEF_SEQUENCE_cb(tname, cb) \
        ASN1_SEQUENCE_cb(tname, cb)

# define ASN1_SEQUENCE_cb(tname, cb) \
        static const ASN1_AUX tname##_aux = {NULL, 0, 0, 0, cb, 0, NULL}; \
        ASN1_SEQUENCE(tname)

# define ASN1_SEQUENCE_const_cb(tname, const_cb) \
        static const ASN1_AUX tname##_aux = \
            {NULL, ASN1_AFLG_CONST_CB, 0, 0, NULL, 0, const_cb}; \
        ASN1_SEQUENCE(tname)

# define ASN1_SEQUENCE_cb_const_cb(tname, cb, const_cb) \
        static const ASN1_AUX tname##_aux = \
            {NULL, ASN1_AFLG_CONST_CB, 0, 0, cb, 0, const_cb}; \
        ASN1_SEQUENCE(tname)

# define ASN1_SEQUENCE_ref(tname, cb) \
        static const ASN1_AUX tname##_aux = {NULL, ASN1_AFLG_REFCOUNT, offsetof(tname, references), offsetof(tname, lock), cb, 0, NULL}; \
        ASN1_SEQUENCE(tname)

# define ASN1_SEQUENCE_enc(tname, enc, cb) \
        static const ASN1_AUX tname##_aux = {NULL, ASN1_AFLG_ENCODING, 0, 0, cb, offsetof(tname, enc), NULL}; \
        ASN1_SEQUENCE(tname)

# define ASN1_NDEF_SEQUENCE_END(tname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_NDEF_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(tname),\
                #tname \
        ASN1_ITEM_end(tname)
# define static_ASN1_NDEF_SEQUENCE_END(tname) \
        ;\
        static_ASN1_ITEM_start(tname) \
                ASN1_ITYPE_NDEF_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(tname),\
                #tname \
        ASN1_ITEM_end(tname)


# define ASN1_SEQUENCE_END_enc(stname, tname) ASN1_SEQUENCE_END_ref(stname, tname)

# define ASN1_SEQUENCE_END_cb(stname, tname) ASN1_SEQUENCE_END_ref(stname, tname)
# define static_ASN1_SEQUENCE_END_cb(stname, tname) static_ASN1_SEQUENCE_END_ref(stname, tname)

# define ASN1_SEQUENCE_END_ref(stname, tname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                &tname##_aux,\
                sizeof(stname),\
                #tname \
        ASN1_ITEM_end(tname)
# define static_ASN1_SEQUENCE_END_ref(stname, tname) \
        ;\
        static_ASN1_ITEM_start(tname) \
                ASN1_ITYPE_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                &tname##_aux,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

# define ASN1_NDEF_SEQUENCE_END_cb(stname, tname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_NDEF_SEQUENCE,\
                V_ASN1_SEQUENCE,\
                tname##_seq_tt,\
                sizeof(tname##_seq_tt) / sizeof(ASN1_TEMPLATE),\
                &tname##_aux,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

/*-
 * This pair helps declare a CHOICE type. We can do:
 *
 *      ASN1_CHOICE(chname) = {
 *              ... CHOICE options ...
 *      ASN1_CHOICE_END(chname)
 *
 *      This will produce an ASN1_ITEM called chname_it
 *      for a structure called chname. The structure
 *      definition must look like this:
 *      typedef struct {
 *              int type;
 *              union {
 *                      ASN1_SOMETHING *opt1;
 *                      ASN1_SOMEOTHER *opt2;
 *              } value;
 *      } chname;
 *
 *      the name of the selector must be 'type'.
 *      to use an alternative selector name use the
 *      ASN1_CHOICE_END_selector() version.
 */

# define ASN1_CHOICE(tname) \
        static const ASN1_TEMPLATE tname##_ch_tt[]

# define ASN1_CHOICE_cb(tname, cb) \
        static const ASN1_AUX tname##_aux = {NULL, 0, 0, 0, cb, 0, NULL}; \
        ASN1_CHOICE(tname)

# define ASN1_CHOICE_END(stname) ASN1_CHOICE_END_name(stname, stname)

# define static_ASN1_CHOICE_END(stname) static_ASN1_CHOICE_END_name(stname, stname)

# define ASN1_CHOICE_END_name(stname, tname) ASN1_CHOICE_END_selector(stname, tname, type)

# define static_ASN1_CHOICE_END_name(stname, tname) static_ASN1_CHOICE_END_selector(stname, tname, type)

# define ASN1_CHOICE_END_selector(stname, tname, selname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_CHOICE,\
                offsetof(stname,selname) ,\
                tname##_ch_tt,\
                sizeof(tname##_ch_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

# define static_ASN1_CHOICE_END_selector(stname, tname, selname) \
        ;\
        static_ASN1_ITEM_start(tname) \
                ASN1_ITYPE_CHOICE,\
                offsetof(stname,selname) ,\
                tname##_ch_tt,\
                sizeof(tname##_ch_tt) / sizeof(ASN1_TEMPLATE),\
                NULL,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

# define ASN1_CHOICE_END_cb(stname, tname, selname) \
        ;\
        ASN1_ITEM_start(tname) \
                ASN1_ITYPE_CHOICE,\
                offsetof(stname,selname) ,\
                tname##_ch_tt,\
                sizeof(tname##_ch_tt) / sizeof(ASN1_TEMPLATE),\
                &tname##_aux,\
                sizeof(stname),\
                #stname \
        ASN1_ITEM_end(tname)

/* This helps with the template wrapper form of ASN1_ITEM */

# define ASN1_EX_TEMPLATE_TYPE(flags, tag, name, type) { \
        (flags), (tag), 0,\
        #name, ASN1_ITEM_ref(type) }

/* These help with SEQUENCE or CHOICE components */

/* used to declare other types */

# define ASN1_EX_TYPE(flags, tag, stname, field, type) { \
        (flags), (tag), offsetof(stname, field),\
        #field, ASN1_ITEM_ref(type) }

/* implicit and explicit helper macros */

# define ASN1_IMP_EX(stname, field, type, tag, ex) \
         ASN1_EX_TYPE(ASN1_TFLG_IMPLICIT | (ex), tag, stname, field, type)

# define ASN1_EXP_EX(stname, field, type, tag, ex) \
         ASN1_EX_TYPE(ASN1_TFLG_EXPLICIT | (ex), tag, stname, field, type)

/* Any defined by macros: the field used is in the table itself */

# define ASN1_ADB_OBJECT(tblname) { ASN1_TFLG_ADB_OID, -1, 0, #tblname, tblname##_adb }
# define ASN1_ADB_INTEGER(tblname) { ASN1_TFLG_ADB_INT, -1, 0, #tblname, tblname##_adb }

/* Plain simple type */
# define ASN1_SIMPLE(stname, field, type) ASN1_EX_TYPE(0,0, stname, field, type)
/* Embedded simple type */
# define ASN1_EMBED(stname, field, type) ASN1_EX_TYPE(ASN1_TFLG_EMBED,0, stname, field, type)

/* OPTIONAL simple type */
# define ASN1_OPT(stname, field, type) ASN1_EX_TYPE(ASN1_TFLG_OPTIONAL, 0, stname, field, type)
# define ASN1_OPT_EMBED(stname, field, type) ASN1_EX_TYPE(ASN1_TFLG_OPTIONAL|ASN1_TFLG_EMBED, 0, stname, field, type)

/* IMPLICIT tagged simple type */
# define ASN1_IMP(stname, field, type, tag) ASN1_IMP_EX(stname, field, type, tag, 0)
# define ASN1_IMP_EMBED(stname, field, type, tag) ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_EMBED)

/* IMPLICIT tagged OPTIONAL simple type */
# define ASN1_IMP_OPT(stname, field, type, tag) ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_OPTIONAL)
# define ASN1_IMP_OPT_EMBED(stname, field, type, tag) ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_OPTIONAL|ASN1_TFLG_EMBED)

/* Same as above but EXPLICIT */

# define ASN1_EXP(stname, field, type, tag) ASN1_EXP_EX(stname, field, type, tag, 0)
# define ASN1_EXP_EMBED(stname, field, type, tag) ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_EMBED)
# define ASN1_EXP_OPT(stname, field, type, tag) ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_OPTIONAL)
# define ASN1_EXP_OPT_EMBED(stname, field, type, tag) ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_OPTIONAL|ASN1_TFLG_EMBED)

/* SEQUENCE OF type */
# define ASN1_SEQUENCE_OF(stname, field, type) \
                ASN1_EX_TYPE(ASN1_TFLG_SEQUENCE_OF, 0, stname, field, type)

/* OPTIONAL SEQUENCE OF */
# define ASN1_SEQUENCE_OF_OPT(stname, field, type) \
                ASN1_EX_TYPE(ASN1_TFLG_SEQUENCE_OF|ASN1_TFLG_OPTIONAL, 0, stname, field, type)

/* Same as above but for SET OF */

# define ASN1_SET_OF(stname, field, type) \
                ASN1_EX_TYPE(ASN1_TFLG_SET_OF, 0, stname, field, type)

# define ASN1_SET_OF_OPT(stname, field, type) \
                ASN1_EX_TYPE(ASN1_TFLG_SET_OF|ASN1_TFLG_OPTIONAL, 0, stname, field, type)

/* Finally compound types of SEQUENCE, SET, IMPLICIT, EXPLICIT and OPTIONAL */

# define ASN1_IMP_SET_OF(stname, field, type, tag) \
                        ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_SET_OF)

# define ASN1_EXP_SET_OF(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_SET_OF)

# define ASN1_IMP_SET_OF_OPT(stname, field, type, tag) \
                        ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_SET_OF|ASN1_TFLG_OPTIONAL)

# define ASN1_EXP_SET_OF_OPT(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_SET_OF|ASN1_TFLG_OPTIONAL)

# define ASN1_IMP_SEQUENCE_OF(stname, field, type, tag) \
                        ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_SEQUENCE_OF)

# define ASN1_IMP_SEQUENCE_OF_OPT(stname, field, type, tag) \
                        ASN1_IMP_EX(stname, field, type, tag, ASN1_TFLG_SEQUENCE_OF|ASN1_TFLG_OPTIONAL)

# define ASN1_EXP_SEQUENCE_OF(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_SEQUENCE_OF)

# define ASN1_EXP_SEQUENCE_OF_OPT(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_SEQUENCE_OF|ASN1_TFLG_OPTIONAL)

/* EXPLICIT using indefinite length constructed form */
# define ASN1_NDEF_EXP(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_NDEF)

/* EXPLICIT OPTIONAL using indefinite length constructed form */
# define ASN1_NDEF_EXP_OPT(stname, field, type, tag) \
                        ASN1_EXP_EX(stname, field, type, tag, ASN1_TFLG_OPTIONAL|ASN1_TFLG_NDEF)

/* Macros for the ASN1_ADB structure */

# define ASN1_ADB(name) \
        static const ASN1_ADB_TABLE name##_adbtbl[]

# define ASN1_ADB_END(name, flags, field, adb_cb, def, none) \
        ;\
        static const ASN1_ITEM *name##_adb(void) \
        { \
        static const ASN1_ADB internal_adb = \
                {\
                flags,\
                offsetof(name, field),\
                adb_cb,\
                name##_adbtbl,\
                sizeof(name##_adbtbl) / sizeof(ASN1_ADB_TABLE),\
                def,\
                none\
                }; \
                return (const ASN1_ITEM *) &internal_adb; \
        } \
        void dummy_function(void)

# define ADB_ENTRY(val, template) {val, template}

# define ASN1_ADB_TEMPLATE(name) \
        static const ASN1_TEMPLATE name##_tt

/*
 * This is the ASN1 template structure that defines a wrapper round the
 * actual type. It determines the actual position of the field in the value
 * structure, various flags such as OPTIONAL and the field name.
 */

struct ASN1_TEMPLATE_st {
    unsigned long flags;        /* Various flags */
    long tag;                   /* tag, not used if no tagging */
    unsigned long offset;       /* Offset of this field in structure */
    const char *field_name;     /* Field name */
    ASN1_ITEM_EXP *item;        /* Relevant ASN1_ITEM or ASN1_ADB */
};

/* Macro to extract ASN1_ITEM and ASN1_ADB pointer from ASN1_TEMPLATE */

# define ASN1_TEMPLATE_item(t) (t->item_ptr)
# define ASN1_TEMPLATE_adb(t) (t->item_ptr)

typedef struct ASN1_ADB_TABLE_st ASN1_ADB_TABLE;
typedef struct ASN1_ADB_st ASN1_ADB;

struct ASN1_ADB_st {
    unsigned long flags;        /* Various flags */
    unsigned long offset;       /* Offset of selector field */
    int (*adb_cb)(long *psel);  /* Application callback */
    const ASN1_ADB_TABLE *tbl;  /* Table of possible types */
    long tblcount;              /* Number of entries in tbl */
    const ASN1_TEMPLATE *default_tt; /* Type to use if no match */
    const ASN1_TEMPLATE *null_tt; /* Type to use if selector is NULL */
};

struct ASN1_ADB_TABLE_st {
    long value;                 /* NID for an object or value for an int */
    const ASN1_TEMPLATE tt;     /* item for this value */
};

/* template flags */

/* Field is optional */
# define ASN1_TFLG_OPTIONAL      (0x1)

/* Field is a SET OF */
# define ASN1_TFLG_SET_OF        (0x1 << 1)

/* Field is a SEQUENCE OF */
# define ASN1_TFLG_SEQUENCE_OF   (0x2 << 1)

/*
 * Special case: this refers to a SET OF that will be sorted into DER order
 * when encoded *and* the corresponding STACK will be modified to match the
 * new order.
 */
# define ASN1_TFLG_SET_ORDER     (0x3 << 1)

/* Mask for SET OF or SEQUENCE OF */
# define ASN1_TFLG_SK_MASK       (0x3 << 1)

/*
 * These flags mean the tag should be taken from the tag field. If EXPLICIT
 * then the underlying type is used for the inner tag.
 */

/* IMPLICIT tagging */
# define ASN1_TFLG_IMPTAG        (0x1 << 3)

/* EXPLICIT tagging, inner tag from underlying type */
# define ASN1_TFLG_EXPTAG        (0x2 << 3)

# define ASN1_TFLG_TAG_MASK      (0x3 << 3)

/* context specific IMPLICIT */
# define ASN1_TFLG_IMPLICIT      (ASN1_TFLG_IMPTAG|ASN1_TFLG_CONTEXT)

/* context specific EXPLICIT */
# define ASN1_TFLG_EXPLICIT      (ASN1_TFLG_EXPTAG|ASN1_TFLG_CONTEXT)

/*
 * If tagging is in force these determine the type of tag to use. Otherwise
 * the tag is determined by the underlying type. These values reflect the
 * actual octet format.
 */

/* Universal tag */
# define ASN1_TFLG_UNIVERSAL     (0x0<<6)
/* Application tag */
# define ASN1_TFLG_APPLICATION   (0x1<<6)
/* Context specific tag */
# define ASN1_TFLG_CONTEXT       (0x2<<6)
/* Private tag */
# define ASN1_TFLG_PRIVATE       (0x3<<6)

# define ASN1_TFLG_TAG_CLASS     (0x3<<6)

/*
 * These are for ANY DEFINED BY type. In this case the 'item' field points to
 * an ASN1_ADB structure which contains a table of values to decode the
 * relevant type
 */

# define ASN1_TFLG_ADB_MASK      (0x3<<8)

# define ASN1_TFLG_ADB_OID       (0x1<<8)

# define ASN1_TFLG_ADB_INT       (0x1<<9)

/*
 * This flag when present in a SEQUENCE OF, SET OF or EXPLICIT causes
 * indefinite length constructed encoding to be used if required.
 */

# define ASN1_TFLG_NDEF          (0x1<<11)

/* Field is embedded and not a pointer */
# define ASN1_TFLG_EMBED         (0x1 << 12)

/* This is the actual ASN1 item itself */

struct ASN1_ITEM_st {
    char itype;                 /* The item type, primitive, SEQUENCE, CHOICE
                                 * or extern */
    long utype;                 /* underlying type */
    const ASN1_TEMPLATE *templates; /* If SEQUENCE or CHOICE this contains
                                     * the contents */
    long tcount;                /* Number of templates if SEQUENCE or CHOICE */
    const void *funcs;          /* further data and type-specific functions */
    /* funcs can be ASN1_PRIMITIVE_FUNCS*, ASN1_EXTERN_FUNCS*, or ASN1_AUX* */
    long size;                  /* Structure size (usually) */
    const char *sname;          /* Structure name */
};

/*
 * Cache for ASN1 tag and length, so we don't keep re-reading it for things
 * like CHOICE
 */

struct ASN1_TLC_st {
    char valid;                 /* Values below are valid */
    int ret;                    /* return value */
    long plen;                  /* length */
    int ptag;                   /* class value */
    int pclass;                 /* class value */
    int hdrlen;                 /* header length */
};

/* Typedefs for ASN1 function pointers */
typedef int ASN1_ex_d2i(ASN1_VALUE **pval, const unsigned char **in, long len,
                        const ASN1_ITEM *it, int tag, int aclass, char opt,
                        ASN1_TLC *ctx);

typedef int ASN1_ex_d2i_ex(ASN1_VALUE **pval, const unsigned char **in, long len,
                           const ASN1_ITEM *it, int tag, int aclass, char opt,
                           ASN1_TLC *ctx, OSSL_LIB_CTX *libctx,
                           const char *propq);
typedef int ASN1_ex_i2d(const ASN1_VALUE **pval, unsigned char **out,
                        const ASN1_ITEM *it, int tag, int aclass);
typedef int ASN1_ex_new_func(ASN1_VALUE **pval, const ASN1_ITEM *it);
typedef int ASN1_ex_new_ex_func(ASN1_VALUE **pval, const ASN1_ITEM *it,
                                OSSL_LIB_CTX *libctx, const char *propq);
typedef void ASN1_ex_free_func(ASN1_VALUE **pval, const ASN1_ITEM *it);

typedef int ASN1_ex_print_func(BIO *out, const ASN1_VALUE **pval,
                               int indent, const char *fname,
                               const ASN1_PCTX *pctx);

typedef int ASN1_primitive_i2c(const ASN1_VALUE **pval, unsigned char *cont,
                               int *putype, const ASN1_ITEM *it);
typedef int ASN1_primitive_c2i(ASN1_VALUE **pval, const unsigned char *cont,
                               int len, int utype, char *free_cont,
                               const ASN1_ITEM *it);
typedef int ASN1_primitive_print(BIO *out, const ASN1_VALUE **pval,
                                 const ASN1_ITEM *it, int indent,
                                 const ASN1_PCTX *pctx);

typedef struct ASN1_EXTERN_FUNCS_st {
    void *app_data;
    ASN1_ex_new_func *asn1_ex_new;
    ASN1_ex_free_func *asn1_ex_free;
    ASN1_ex_free_func *asn1_ex_clear;
    ASN1_ex_d2i *asn1_ex_d2i;
    ASN1_ex_i2d *asn1_ex_i2d;
    ASN1_ex_print_func *asn1_ex_print;
    ASN1_ex_new_ex_func *asn1_ex_new_ex;
    ASN1_ex_d2i_ex *asn1_ex_d2i_ex;
} ASN1_EXTERN_FUNCS;

typedef struct ASN1_PRIMITIVE_FUNCS_st {
    void *app_data;
    unsigned long flags;
    ASN1_ex_new_func *prim_new;
    ASN1_ex_free_func *prim_free;
    ASN1_ex_free_func *prim_clear;
    ASN1_primitive_c2i *prim_c2i;
    ASN1_primitive_i2c *prim_i2c;
    ASN1_primitive_print *prim_print;
} ASN1_PRIMITIVE_FUNCS;

/*
 * This is the ASN1_AUX structure: it handles various miscellaneous
 * requirements. For example the use of reference counts and an informational
 * callback. The "informational callback" is called at various points during
 * the ASN1 encoding and decoding. It can be used to provide minor
 * customisation of the structures used. This is most useful where the
 * supplied routines *almost* do the right thing but need some extra help at
 * a few points. If the callback returns zero then it is assumed a fatal
 * error has occurred and the main operation should be abandoned. If major
 * changes in the default behaviour are required then an external type is
 * more appropriate.
 * For the operations ASN1_OP_I2D_PRE, ASN1_OP_I2D_POST, ASN1_OP_PRINT_PRE, and
 * ASN1_OP_PRINT_POST, meanwhile a variant of the callback with const parameter
 * 'in' is provided to make clear statically that its input is not modified. If
 * and only if this variant is in use the flag ASN1_AFLG_CONST_CB must be set.
 */

typedef int ASN1_aux_cb(int operation, ASN1_VALUE **in, const ASN1_ITEM *it,
                        void *exarg);
typedef int ASN1_aux_const_cb(int operation, const ASN1_VALUE **in,
                              const ASN1_ITEM *it, void *exarg);

typedef struct ASN1_AUX_st {
    void *app_data;
    int flags;
    int ref_offset;             /* Offset of reference value */
    int ref_lock;               /* Offset of lock value */
    ASN1_aux_cb *asn1_cb;
    int enc_offset;             /* Offset of ASN1_ENCODING structure */
    ASN1_aux_const_cb *asn1_const_cb; /* for ASN1_OP_I2D_ and ASN1_OP_PRINT_ */
} ASN1_AUX;

/* For print related callbacks exarg points to this structure */
typedef struct ASN1_PRINT_ARG_st {
    BIO *out;
    int indent;
    const ASN1_PCTX *pctx;
} ASN1_PRINT_ARG;

/* For streaming related callbacks exarg points to this structure */
typedef struct ASN1_STREAM_ARG_st {
    /* BIO to stream through */
    BIO *out;
    /* BIO with filters appended */
    BIO *ndef_bio;
    /* Streaming I/O boundary */
    unsigned char **boundary;
} ASN1_STREAM_ARG;

/* Flags in ASN1_AUX */

/* Use a reference count */
# define ASN1_AFLG_REFCOUNT      1
/* Save the encoding of structure (useful for signatures) */
# define ASN1_AFLG_ENCODING      2
/* The Sequence length is invalid */
# define ASN1_AFLG_BROKEN        4
/* Use the new asn1_const_cb */
# define ASN1_AFLG_CONST_CB      8

/* operation values for asn1_cb */

# define ASN1_OP_NEW_PRE         0
# define ASN1_OP_NEW_POST        1
# define ASN1_OP_FREE_PRE        2
# define ASN1_OP_FREE_POST       3
# define ASN1_OP_D2I_PRE         4
# define ASN1_OP_D2I_POST        5
# define ASN1_OP_I2D_PRE         6
# define ASN1_OP_I2D_POST        7
# define ASN1_OP_PRINT_PRE       8
# define ASN1_OP_PRINT_POST      9
# define ASN1_OP_STREAM_PRE      10
# define ASN1_OP_STREAM_POST     11
# define ASN1_OP_DETACHED_PRE    12
# define ASN1_OP_DETACHED_POST   13
# define ASN1_OP_DUP_PRE         14
# define ASN1_OP_DUP_POST        15
# define ASN1_OP_GET0_LIBCTX     16
# define ASN1_OP_GET0_PROPQ      17

/* Macro to implement a primitive type */
# define IMPLEMENT_ASN1_TYPE(stname) IMPLEMENT_ASN1_TYPE_ex(stname, stname, 0)
# define IMPLEMENT_ASN1_TYPE_ex(itname, vname, ex) \
                                ASN1_ITEM_start(itname) \
                                        ASN1_ITYPE_PRIMITIVE, V_##vname, NULL, 0, NULL, ex, #itname \
                                ASN1_ITEM_end(itname)

/* Macro to implement a multi string type */
# define IMPLEMENT_ASN1_MSTRING(itname, mask) \
                                ASN1_ITEM_start(itname) \
                                        ASN1_ITYPE_MSTRING, mask, NULL, 0, NULL, sizeof(ASN1_STRING), #itname \
                                ASN1_ITEM_end(itname)

# define IMPLEMENT_EXTERN_ASN1(sname, tag, fptrs) \
        ASN1_ITEM_start(sname) \
                ASN1_ITYPE_EXTERN, \
                tag, \
                NULL, \
                0, \
                &fptrs, \
                0, \
                #sname \
        ASN1_ITEM_end(sname)

/* Macro to implement standard functions in terms of ASN1_ITEM structures */

# define IMPLEMENT_ASN1_FUNCTIONS(stname) IMPLEMENT_ASN1_FUNCTIONS_fname(stname, stname, stname)

# define IMPLEMENT_ASN1_FUNCTIONS_name(stname, itname) IMPLEMENT_ASN1_FUNCTIONS_fname(stname, itname, itname)

# define IMPLEMENT_ASN1_FUNCTIONS_ENCODE_name(stname, itname) \
                        IMPLEMENT_ASN1_FUNCTIONS_ENCODE_fname(stname, itname, itname)

# define IMPLEMENT_STATIC_ASN1_ALLOC_FUNCTIONS(stname) \
                IMPLEMENT_ASN1_ALLOC_FUNCTIONS_pfname(static, stname, stname, stname)

# define IMPLEMENT_ASN1_ALLOC_FUNCTIONS(stname) \
                IMPLEMENT_ASN1_ALLOC_FUNCTIONS_fname(stname, stname, stname)

# define IMPLEMENT_ASN1_ALLOC_FUNCTIONS_pfname(pre, stname, itname, fname) \
        pre stname *fname##_new(void) \
        { \
                return (stname *)ASN1_item_new(ASN1_ITEM_rptr(itname)); \
        } \
        pre void fname##_free(stname *a) \
        { \
                ASN1_item_free((ASN1_VALUE *)a, ASN1_ITEM_rptr(itname)); \
        }

# define IMPLEMENT_ASN1_ALLOC_FUNCTIONS_fname(stname, itname, fname) \
        stname *fname##_new(void) \
        { \
                return (stname *)ASN1_item_new(ASN1_ITEM_rptr(itname)); \
        } \
        void fname##_free(stname *a) \
        { \
                ASN1_item_free((ASN1_VALUE *)a, ASN1_ITEM_rptr(itname)); \
        }

# define IMPLEMENT_ASN1_FUNCTIONS_fname(stname, itname, fname) \
        IMPLEMENT_ASN1_ENCODE_FUNCTIONS_fname(stname, itname, fname) \
        IMPLEMENT_ASN1_ALLOC_FUNCTIONS_fname(stname, itname, fname)

# define IMPLEMENT_ASN1_ENCODE_FUNCTIONS_fname(stname, itname, fname) \
        stname *d2i_##fname(stname **a, const unsigned char **in, long len) \
        { \
                return (stname *)ASN1_item_d2i((ASN1_VALUE **)a, in, len, ASN1_ITEM_rptr(itname));\
        } \
        int i2d_##fname(const stname *a, unsigned char **out) \
        { \
                return ASN1_item_i2d((const ASN1_VALUE *)a, out, ASN1_ITEM_rptr(itname));\
        }

# define IMPLEMENT_ASN1_NDEF_FUNCTION(stname) \
        int i2d_##stname##_NDEF(const stname *a, unsigned char **out) \
        { \
                return ASN1_item_ndef_i2d((const ASN1_VALUE *)a, out, ASN1_ITEM_rptr(stname));\
        }

# define IMPLEMENT_STATIC_ASN1_ENCODE_FUNCTIONS(stname) \
        static stname *d2i_##stname(stname **a, \
                                   const unsigned char **in, long len) \
        { \
                return (stname *)ASN1_item_d2i((ASN1_VALUE **)a, in, len, \
                                               ASN1_ITEM_rptr(stname)); \
        } \
        static int i2d_##stname(const stname *a, unsigned char **out) \
        { \
                return ASN1_item_i2d((const ASN1_VALUE *)a, out, \
                                     ASN1_ITEM_rptr(stname)); \
        }

# define IMPLEMENT_ASN1_DUP_FUNCTION(stname) \
        stname * stname##_dup(const stname *x) \
        { \
        return ASN1_item_dup(ASN1_ITEM_rptr(stname), x); \
        }

# define IMPLEMENT_ASN1_PRINT_FUNCTION(stname) \
        IMPLEMENT_ASN1_PRINT_FUNCTION_fname(stname, stname, stname)

# define IMPLEMENT_ASN1_PRINT_FUNCTION_fname(stname, itname, fname) \
        int fname##_print_ctx(BIO *out, const stname *x, int indent, \
                                                const ASN1_PCTX *pctx) \
        { \
                return ASN1_item_print(out, (const ASN1_VALUE *)x, indent, \
                        ASN1_ITEM_rptr(itname), pctx); \
        }

/* external definitions for primitive types */

DECLARE_ASN1_ITEM(ASN1_BOOLEAN)
DECLARE_ASN1_ITEM(ASN1_TBOOLEAN)
DECLARE_ASN1_ITEM(ASN1_FBOOLEAN)
DECLARE_ASN1_ITEM(ASN1_SEQUENCE)
DECLARE_ASN1_ITEM(CBIGNUM)
DECLARE_ASN1_ITEM(BIGNUM)
DECLARE_ASN1_ITEM(INT32)
DECLARE_ASN1_ITEM(ZINT32)
DECLARE_ASN1_ITEM(UINT32)
DECLARE_ASN1_ITEM(ZUINT32)
DECLARE_ASN1_ITEM(INT64)
DECLARE_ASN1_ITEM(ZINT64)
DECLARE_ASN1_ITEM(UINT64)
DECLARE_ASN1_ITEM(ZUINT64)

# ifndef OPENSSL_NO_DEPRECATED_3_0
/*
 * LONG and ZLONG are strongly discouraged for use as stored data, as the
 * underlying C type (long) differs in size depending on the architecture.
 * They are designed with 32-bit longs in mind.
 */
DECLARE_ASN1_ITEM(LONG)
DECLARE_ASN1_ITEM(ZLONG)
# endif

SKM_DEFINE_STACK_OF_INTERNAL(ASN1_VALUE, ASN1_VALUE, ASN1_VALUE)
#define sk_ASN1_VALUE_num(sk) OPENSSL_sk_num(ossl_check_const_ASN1_VALUE_sk_type(sk))
#define sk_ASN1_VALUE_value(sk, idx) ((ASN1_VALUE *)OPENSSL_sk_value(ossl_check_const_ASN1_VALUE_sk_type(sk), (idx)))
#define sk_ASN1_VALUE_new(cmp) ((STACK_OF(ASN1_VALUE) *)OPENSSL_sk_new(ossl_check_ASN1_VALUE_compfunc_type(cmp)))
#define sk_ASN1_VALUE_new_null() ((STACK_OF(ASN1_VALUE) *)OPENSSL_sk_new_null())
#define sk_ASN1_VALUE_new_reserve(cmp, n) ((STACK_OF(ASN1_VALUE) *)OPENSSL_sk_new_reserve(ossl_check_ASN1_VALUE_compfunc_type(cmp), (n)))
#define sk_ASN1_VALUE_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_ASN1_VALUE_sk_type(sk), (n))
#define sk_ASN1_VALUE_free(sk) OPENSSL_sk_free(ossl_check_ASN1_VALUE_sk_type(sk))
#define sk_ASN1_VALUE_zero(sk) OPENSSL_sk_zero(ossl_check_ASN1_VALUE_sk_type(sk))
#define sk_ASN1_VALUE_delete(sk, i) ((ASN1_VALUE *)OPENSSL_sk_delete(ossl_check_ASN1_VALUE_sk_type(sk), (i)))
#define sk_ASN1_VALUE_delete_ptr(sk, ptr) ((ASN1_VALUE *)OPENSSL_sk_delete_ptr(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr)))
#define sk_ASN1_VALUE_push(sk, ptr) OPENSSL_sk_push(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr))
#define sk_ASN1_VALUE_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr))
#define sk_ASN1_VALUE_pop(sk) ((ASN1_VALUE *)OPENSSL_sk_pop(ossl_check_ASN1_VALUE_sk_type(sk)))
#define sk_ASN1_VALUE_shift(sk) ((ASN1_VALUE *)OPENSSL_sk_shift(ossl_check_ASN1_VALUE_sk_type(sk)))
#define sk_ASN1_VALUE_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_ASN1_VALUE_sk_type(sk),ossl_check_ASN1_VALUE_freefunc_type(freefunc))
#define sk_ASN1_VALUE_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr), (idx))
#define sk_ASN1_VALUE_set(sk, idx, ptr) ((ASN1_VALUE *)OPENSSL_sk_set(ossl_check_ASN1_VALUE_sk_type(sk), (idx), ossl_check_ASN1_VALUE_type(ptr)))
#define sk_ASN1_VALUE_find(sk, ptr) OPENSSL_sk_find(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr))
#define sk_ASN1_VALUE_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr))
#define sk_ASN1_VALUE_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_type(ptr), pnum)
#define sk_ASN1_VALUE_sort(sk) OPENSSL_sk_sort(ossl_check_ASN1_VALUE_sk_type(sk))
#define sk_ASN1_VALUE_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_ASN1_VALUE_sk_type(sk))
#define sk_ASN1_VALUE_dup(sk) ((STACK_OF(ASN1_VALUE) *)OPENSSL_sk_dup(ossl_check_const_ASN1_VALUE_sk_type(sk)))
#define sk_ASN1_VALUE_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(ASN1_VALUE) *)OPENSSL_sk_deep_copy(ossl_check_const_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_copyfunc_type(copyfunc), ossl_check_ASN1_VALUE_freefunc_type(freefunc)))
#define sk_ASN1_VALUE_set_cmp_func(sk, cmp) ((sk_ASN1_VALUE_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_ASN1_VALUE_sk_type(sk), ossl_check_ASN1_VALUE_compfunc_type(cmp)))



/* Functions used internally by the ASN1 code */

int ASN1_item_ex_new(ASN1_VALUE **pval, const ASN1_ITEM *it);
void ASN1_item_ex_free(ASN1_VALUE **pval, const ASN1_ITEM *it);

int ASN1_item_ex_d2i(ASN1_VALUE **pval, const unsigned char **in, long len,
                     const ASN1_ITEM *it, int tag, int aclass, char opt,
                     ASN1_TLC *ctx);

int ASN1_item_ex_i2d(const ASN1_VALUE **pval, unsigned char **out,
                     const ASN1_ITEM *it, int tag, int aclass);

/* Legacy compatibility */
# define IMPLEMENT_ASN1_FUNCTIONS_const(name) IMPLEMENT_ASN1_FUNCTIONS(name)
# define IMPLEMENT_ASN1_ENCODE_FUNCTIONS_const_fname(stname, itname, fname) \
         IMPLEMENT_ASN1_ENCODE_FUNCTIONS_fname(stname, itname, fname)

#ifdef  __cplusplus
}
#endif
#endif
                                                                                                                                                                                                                                                                                                                                                                                                                               node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/bio.h                0000664 0000000 0000000 00000115644 14746647661 0027677 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /*
 * WARNING: do not edit!
 * Generated by Makefile from include/openssl/bio.h.in
 *
 * Copyright 1995-2022 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */


#ifndef OPENSSL_BIO_H
# define OPENSSL_BIO_H
# pragma once

# include <openssl/macros.h>
# ifndef OPENSSL_NO_DEPRECATED_3_0
#  define HEADER_BIO_H
# endif

# include <openssl/e_os2.h>

# ifndef OPENSSL_NO_STDIO
#  include <stdio.h>
# endif
# include <stdarg.h>

# include <openssl/crypto.h>
# include <openssl/bioerr.h>
# include <openssl/core.h>

#ifdef  __cplusplus
extern "C" {
#endif

/* There are the classes of BIOs */
# define BIO_TYPE_DESCRIPTOR     0x0100 /* socket, fd, connect or accept */
# define BIO_TYPE_FILTER         0x0200
# define BIO_TYPE_SOURCE_SINK    0x0400

/* These are the 'types' of BIOs */
# define BIO_TYPE_NONE             0
# define BIO_TYPE_MEM            ( 1|BIO_TYPE_SOURCE_SINK)
# define BIO_TYPE_FILE           ( 2|BIO_TYPE_SOURCE_SINK)

# define BIO_TYPE_FD             ( 4|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)
# define BIO_TYPE_SOCKET         ( 5|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)
# define BIO_TYPE_NULL           ( 6|BIO_TYPE_SOURCE_SINK)
# define BIO_TYPE_SSL            ( 7|BIO_TYPE_FILTER)
# define BIO_TYPE_MD             ( 8|BIO_TYPE_FILTER)
# define BIO_TYPE_BUFFER         ( 9|BIO_TYPE_FILTER)
# define BIO_TYPE_CIPHER         (10|BIO_TYPE_FILTER)
# define BIO_TYPE_BASE64         (11|BIO_TYPE_FILTER)
# define BIO_TYPE_CONNECT        (12|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)
# define BIO_TYPE_ACCEPT         (13|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)

# define BIO_TYPE_NBIO_TEST      (16|BIO_TYPE_FILTER)/* server proxy BIO */
# define BIO_TYPE_NULL_FILTER    (17|BIO_TYPE_FILTER)
# define BIO_TYPE_BIO            (19|BIO_TYPE_SOURCE_SINK)/* half a BIO pair */
# define BIO_TYPE_LINEBUFFER     (20|BIO_TYPE_FILTER)
# define BIO_TYPE_DGRAM          (21|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)
# define BIO_TYPE_ASN1           (22|BIO_TYPE_FILTER)
# define BIO_TYPE_COMP           (23|BIO_TYPE_FILTER)
# ifndef OPENSSL_NO_SCTP
#  define BIO_TYPE_DGRAM_SCTP    (24|BIO_TYPE_SOURCE_SINK|BIO_TYPE_DESCRIPTOR)
# endif
# define BIO_TYPE_CORE_TO_PROV   (25|BIO_TYPE_SOURCE_SINK)

#define BIO_TYPE_START           128

/*
 * BIO_FILENAME_READ|BIO_CLOSE to open or close on free.
 * BIO_set_fp(in,stdin,BIO_NOCLOSE);
 */
# define BIO_NOCLOSE             0x00
# define BIO_CLOSE               0x01

/*
 * These are used in the following macros and are passed to BIO_ctrl()
 */
# define BIO_CTRL_RESET          1/* opt - rewind/zero etc */
# define BIO_CTRL_EOF            2/* opt - are we at the eof */
# define BIO_CTRL_INFO           3/* opt - extra tit-bits */
# define BIO_CTRL_SET            4/* man - set the 'IO' type */
# define BIO_CTRL_GET            5/* man - get the 'IO' type */
# define BIO_CTRL_PUSH           6/* opt - internal, used to signify change */
# define BIO_CTRL_POP            7/* opt - internal, used to signify change */
# define BIO_CTRL_GET_CLOSE      8/* man - set the 'close' on free */
# define BIO_CTRL_SET_CLOSE      9/* man - set the 'close' on free */
# define BIO_CTRL_PENDING        10/* opt - is their more data buffered */
# define BIO_CTRL_FLUSH          11/* opt - 'flush' buffered output */
# define BIO_CTRL_DUP            12/* man - extra stuff for 'duped' BIO */
# define BIO_CTRL_WPENDING       13/* opt - number of bytes still to write */
# define BIO_CTRL_SET_CALLBACK   14/* opt - set callback function */
# define BIO_CTRL_GET_CALLBACK   15/* opt - set callback function */

# define BIO_CTRL_PEEK           29/* BIO_f_buffer special */
# define BIO_CTRL_SET_FILENAME   30/* BIO_s_file special */

/* dgram BIO stuff */
# define BIO_CTRL_DGRAM_CONNECT       31/* BIO dgram special */
# define BIO_CTRL_DGRAM_SET_CONNECTED 32/* allow for an externally connected
                                         * socket to be passed in */
# define BIO_CTRL_DGRAM_SET_RECV_TIMEOUT 33/* setsockopt, essentially */
# define BIO_CTRL_DGRAM_GET_RECV_TIMEOUT 34/* getsockopt, essentially */
# define BIO_CTRL_DGRAM_SET_SEND_TIMEOUT 35/* setsockopt, essentially */
# define BIO_CTRL_DGRAM_GET_SEND_TIMEOUT 36/* getsockopt, essentially */

# define BIO_CTRL_DGRAM_GET_RECV_TIMER_EXP 37/* flag whether the last */
# define BIO_CTRL_DGRAM_GET_SEND_TIMER_EXP 38/* I/O operation timed out */

/* #ifdef IP_MTU_DISCOVER */
# define BIO_CTRL_DGRAM_MTU_DISCOVER       39/* set DF bit on egress packets */
/* #endif */

# define BIO_CTRL_DGRAM_QUERY_MTU          40/* as kernel for current MTU */
# define BIO_CTRL_DGRAM_GET_FALLBACK_MTU   47
# define BIO_CTRL_DGRAM_GET_MTU            41/* get cached value for MTU */
# define BIO_CTRL_DGRAM_SET_MTU            42/* set cached value for MTU.
                                              * want to use this if asking
                                              * the kernel fails */

# define BIO_CTRL_DGRAM_MTU_EXCEEDED       43/* check whether the MTU was
                                              * exceed in the previous write
                                              * operation */

# define BIO_CTRL_DGRAM_GET_PEER           46
# define BIO_CTRL_DGRAM_SET_PEER           44/* Destination for the data */

# define BIO_CTRL_DGRAM_SET_NEXT_TIMEOUT   45/* Next DTLS handshake timeout
                                              * to adjust socket timeouts */
# define BIO_CTRL_DGRAM_SET_DONT_FRAG      48

# define BIO_CTRL_DGRAM_GET_MTU_OVERHEAD   49

/* Deliberately outside of OPENSSL_NO_SCTP - used in bss_dgram.c */
#  define BIO_CTRL_DGRAM_SCTP_SET_IN_HANDSHAKE    50
# ifndef OPENSSL_NO_SCTP
/* SCTP stuff */
#  define BIO_CTRL_DGRAM_SCTP_ADD_AUTH_KEY                51
#  define BIO_CTRL_DGRAM_SCTP_NEXT_AUTH_KEY               52
#  define BIO_CTRL_DGRAM_SCTP_AUTH_CCS_RCVD               53
#  define BIO_CTRL_DGRAM_SCTP_GET_SNDINFO         60
#  define BIO_CTRL_DGRAM_SCTP_SET_SNDINFO         61
#  define BIO_CTRL_DGRAM_SCTP_GET_RCVINFO         62
#  define BIO_CTRL_DGRAM_SCTP_SET_RCVINFO         63
#  define BIO_CTRL_DGRAM_SCTP_GET_PRINFO                  64
#  define BIO_CTRL_DGRAM_SCTP_SET_PRINFO                  65
#  define BIO_CTRL_DGRAM_SCTP_SAVE_SHUTDOWN               70
# endif

# define BIO_CTRL_DGRAM_SET_PEEK_MODE      71

/*
 * internal BIO:
 * # define BIO_CTRL_SET_KTLS_SEND                 72
 * # define BIO_CTRL_SET_KTLS_SEND_CTRL_MSG        74
 * # define BIO_CTRL_CLEAR_KTLS_CTRL_MSG           75
 */

# define BIO_CTRL_GET_KTLS_SEND                 73
# define BIO_CTRL_GET_KTLS_RECV                 76

# define BIO_CTRL_DGRAM_SCTP_WAIT_FOR_DRY       77
# define BIO_CTRL_DGRAM_SCTP_MSG_WAITING        78

/* BIO_f_prefix controls */
# define BIO_CTRL_SET_PREFIX                    79
# define BIO_CTRL_SET_INDENT                    80
# define BIO_CTRL_GET_INDENT                    81

# ifndef OPENSSL_NO_KTLS
#  define BIO_get_ktls_send(b)         \
     (BIO_ctrl(b, BIO_CTRL_GET_KTLS_SEND, 0, NULL) > 0)
#  define BIO_get_ktls_recv(b)         \
     (BIO_ctrl(b, BIO_CTRL_GET_KTLS_RECV, 0, NULL) > 0)
# else
#  define BIO_get_ktls_send(b)  (0)
#  define BIO_get_ktls_recv(b)  (0)
# endif

/* modifiers */
# define BIO_FP_READ             0x02
# define BIO_FP_WRITE            0x04
# define BIO_FP_APPEND           0x08
# define BIO_FP_TEXT             0x10

# define BIO_FLAGS_READ          0x01
# define BIO_FLAGS_WRITE         0x02
# define BIO_FLAGS_IO_SPECIAL    0x04
# define BIO_FLAGS_RWS (BIO_FLAGS_READ|BIO_FLAGS_WRITE|BIO_FLAGS_IO_SPECIAL)
# define BIO_FLAGS_SHOULD_RETRY  0x08
# ifndef OPENSSL_NO_DEPRECATED_3_0
/* This #define was replaced by an internal constant and should not be used. */
#  define BIO_FLAGS_UPLINK       0
# endif

# define BIO_FLAGS_BASE64_NO_NL  0x100

/*
 * This is used with memory BIOs:
 * BIO_FLAGS_MEM_RDONLY means we shouldn't free up or change the data in any way;
 * BIO_FLAGS_NONCLEAR_RST means we shouldn't clear data on reset.
 */
# define BIO_FLAGS_MEM_RDONLY    0x200
# define BIO_FLAGS_NONCLEAR_RST  0x400
# define BIO_FLAGS_IN_EOF        0x800

/* the BIO FLAGS values 0x1000 to 0x4000 are reserved for internal KTLS flags */

typedef union bio_addr_st BIO_ADDR;
typedef struct bio_addrinfo_st BIO_ADDRINFO;

int BIO_get_new_index(void);
void BIO_set_flags(BIO *b, int flags);
int BIO_test_flags(const BIO *b, int flags);
void BIO_clear_flags(BIO *b, int flags);

# define BIO_get_flags(b) BIO_test_flags(b, ~(0x0))
# define BIO_set_retry_special(b) \
                BIO_set_flags(b, (BIO_FLAGS_IO_SPECIAL|BIO_FLAGS_SHOULD_RETRY))
# define BIO_set_retry_read(b) \
                BIO_set_flags(b, (BIO_FLAGS_READ|BIO_FLAGS_SHOULD_RETRY))
# define BIO_set_retry_write(b) \
                BIO_set_flags(b, (BIO_FLAGS_WRITE|BIO_FLAGS_SHOULD_RETRY))

/* These are normally used internally in BIOs */
# define BIO_clear_retry_flags(b) \
                BIO_clear_flags(b, (BIO_FLAGS_RWS|BIO_FLAGS_SHOULD_RETRY))
# define BIO_get_retry_flags(b) \
                BIO_test_flags(b, (BIO_FLAGS_RWS|BIO_FLAGS_SHOULD_RETRY))

/* These should be used by the application to tell why we should retry */
# define BIO_should_read(a)              BIO_test_flags(a, BIO_FLAGS_READ)
# define BIO_should_write(a)             BIO_test_flags(a, BIO_FLAGS_WRITE)
# define BIO_should_io_special(a)        BIO_test_flags(a, BIO_FLAGS_IO_SPECIAL)
# define BIO_retry_type(a)               BIO_test_flags(a, BIO_FLAGS_RWS)
# define BIO_should_retry(a)             BIO_test_flags(a, BIO_FLAGS_SHOULD_RETRY)

/*
 * The next three are used in conjunction with the BIO_should_io_special()
 * condition.  After this returns true, BIO *BIO_get_retry_BIO(BIO *bio, int
 * *reason); will walk the BIO stack and return the 'reason' for the special
 * and the offending BIO. Given a BIO, BIO_get_retry_reason(bio) will return
 * the code.
 */
/*
 * Returned from the SSL bio when the certificate retrieval code had an error
 */
# define BIO_RR_SSL_X509_LOOKUP          0x01
/* Returned from the connect BIO when a connect would have blocked */
# define BIO_RR_CONNECT                  0x02
/* Returned from the accept BIO when an accept would have blocked */
# define BIO_RR_ACCEPT                   0x03

/* These are passed by the BIO callback */
# define BIO_CB_FREE     0x01
# define BIO_CB_READ     0x02
# define BIO_CB_WRITE    0x03
# define BIO_CB_PUTS     0x04
# define BIO_CB_GETS     0x05
# define BIO_CB_CTRL     0x06

/*
 * The callback is called before and after the underling operation, The
 * BIO_CB_RETURN flag indicates if it is after the call
 */
# define BIO_CB_RETURN   0x80
# define BIO_CB_return(a) ((a)|BIO_CB_RETURN)
# define BIO_cb_pre(a)   (!((a)&BIO_CB_RETURN))
# define BIO_cb_post(a)  ((a)&BIO_CB_RETURN)

# ifndef OPENSSL_NO_DEPRECATED_3_0
typedef long (*BIO_callback_fn)(BIO *b, int oper, const char *argp, int argi,
                                long argl, long ret);
OSSL_DEPRECATEDIN_3_0 BIO_callback_fn BIO_get_callback(const BIO *b);
OSSL_DEPRECATEDIN_3_0 void BIO_set_callback(BIO *b, BIO_callback_fn callback);
OSSL_DEPRECATEDIN_3_0 long BIO_debug_callback(BIO *bio, int cmd,
                                               const char *argp, int argi,
                                               long argl, long ret);
# endif

typedef long (*BIO_callback_fn_ex)(BIO *b, int oper, const char *argp,
                                   size_t len, int argi,
                                   long argl, int ret, size_t *processed);
BIO_callback_fn_ex BIO_get_callback_ex(const BIO *b);
void BIO_set_callback_ex(BIO *b, BIO_callback_fn_ex callback);
long BIO_debug_callback_ex(BIO *bio, int oper, const char *argp, size_t len,
                           int argi, long argl, int ret, size_t *processed);

char *BIO_get_callback_arg(const BIO *b);
void BIO_set_callback_arg(BIO *b, char *arg);

typedef struct bio_method_st BIO_METHOD;

const char *BIO_method_name(const BIO *b);
int BIO_method_type(const BIO *b);

typedef int BIO_info_cb(BIO *, int, int);
typedef BIO_info_cb bio_info_cb;  /* backward compatibility */

SKM_DEFINE_STACK_OF_INTERNAL(BIO, BIO, BIO)
#define sk_BIO_num(sk) OPENSSL_sk_num(ossl_check_const_BIO_sk_type(sk))
#define sk_BIO_value(sk, idx) ((BIO *)OPENSSL_sk_value(ossl_check_const_BIO_sk_type(sk), (idx)))
#define sk_BIO_new(cmp) ((STACK_OF(BIO) *)OPENSSL_sk_new(ossl_check_BIO_compfunc_type(cmp)))
#define sk_BIO_new_null() ((STACK_OF(BIO) *)OPENSSL_sk_new_null())
#define sk_BIO_new_reserve(cmp, n) ((STACK_OF(BIO) *)OPENSSL_sk_new_reserve(ossl_check_BIO_compfunc_type(cmp), (n)))
#define sk_BIO_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_BIO_sk_type(sk), (n))
#define sk_BIO_free(sk) OPENSSL_sk_free(ossl_check_BIO_sk_type(sk))
#define sk_BIO_zero(sk) OPENSSL_sk_zero(ossl_check_BIO_sk_type(sk))
#define sk_BIO_delete(sk, i) ((BIO *)OPENSSL_sk_delete(ossl_check_BIO_sk_type(sk), (i)))
#define sk_BIO_delete_ptr(sk, ptr) ((BIO *)OPENSSL_sk_delete_ptr(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr)))
#define sk_BIO_push(sk, ptr) OPENSSL_sk_push(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr))
#define sk_BIO_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr))
#define sk_BIO_pop(sk) ((BIO *)OPENSSL_sk_pop(ossl_check_BIO_sk_type(sk)))
#define sk_BIO_shift(sk) ((BIO *)OPENSSL_sk_shift(ossl_check_BIO_sk_type(sk)))
#define sk_BIO_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_BIO_sk_type(sk),ossl_check_BIO_freefunc_type(freefunc))
#define sk_BIO_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr), (idx))
#define sk_BIO_set(sk, idx, ptr) ((BIO *)OPENSSL_sk_set(ossl_check_BIO_sk_type(sk), (idx), ossl_check_BIO_type(ptr)))
#define sk_BIO_find(sk, ptr) OPENSSL_sk_find(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr))
#define sk_BIO_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr))
#define sk_BIO_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_BIO_sk_type(sk), ossl_check_BIO_type(ptr), pnum)
#define sk_BIO_sort(sk) OPENSSL_sk_sort(ossl_check_BIO_sk_type(sk))
#define sk_BIO_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_BIO_sk_type(sk))
#define sk_BIO_dup(sk) ((STACK_OF(BIO) *)OPENSSL_sk_dup(ossl_check_const_BIO_sk_type(sk)))
#define sk_BIO_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(BIO) *)OPENSSL_sk_deep_copy(ossl_check_const_BIO_sk_type(sk), ossl_check_BIO_copyfunc_type(copyfunc), ossl_check_BIO_freefunc_type(freefunc)))
#define sk_BIO_set_cmp_func(sk, cmp) ((sk_BIO_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_BIO_sk_type(sk), ossl_check_BIO_compfunc_type(cmp)))



/* Prefix and suffix callback in ASN1 BIO */
typedef int asn1_ps_func (BIO *b, unsigned char **pbuf, int *plen,
                          void *parg);

typedef void (*BIO_dgram_sctp_notification_handler_fn) (BIO *b,
                                                        void *context,
                                                        void *buf);
# ifndef OPENSSL_NO_SCTP
/* SCTP parameter structs */
struct bio_dgram_sctp_sndinfo {
    uint16_t snd_sid;
    uint16_t snd_flags;
    uint32_t snd_ppid;
    uint32_t snd_context;
};

struct bio_dgram_sctp_rcvinfo {
    uint16_t rcv_sid;
    uint16_t rcv_ssn;
    uint16_t rcv_flags;
    uint32_t rcv_ppid;
    uint32_t rcv_tsn;
    uint32_t rcv_cumtsn;
    uint32_t rcv_context;
};

struct bio_dgram_sctp_prinfo {
    uint16_t pr_policy;
    uint32_t pr_value;
};
# endif

/*
 * #define BIO_CONN_get_param_hostname BIO_ctrl
 */

# define BIO_C_SET_CONNECT                       100
# define BIO_C_DO_STATE_MACHINE                  101
# define BIO_C_SET_NBIO                          102
/* # define BIO_C_SET_PROXY_PARAM                   103 */
# define BIO_C_SET_FD                            104
# define BIO_C_GET_FD                            105
# define BIO_C_SET_FILE_PTR                      106
# define BIO_C_GET_FILE_PTR                      107
# define BIO_C_SET_FILENAME                      108
# define BIO_C_SET_SSL                           109
# define BIO_C_GET_SSL                           110
# define BIO_C_SET_MD                            111
# define BIO_C_GET_MD                            112
# define BIO_C_GET_CIPHER_STATUS                 113
# define BIO_C_SET_BUF_MEM                       114
# define BIO_C_GET_BUF_MEM_PTR                   115
# define BIO_C_GET_BUFF_NUM_LINES                116
# define BIO_C_SET_BUFF_SIZE                     117
# define BIO_C_SET_ACCEPT                        118
# define BIO_C_SSL_MODE                          119
# define BIO_C_GET_MD_CTX                        120
/* # define BIO_C_GET_PROXY_PARAM                   121 */
# define BIO_C_SET_BUFF_READ_DATA                122/* data to read first */
# define BIO_C_GET_CONNECT                       123
# define BIO_C_GET_ACCEPT                        124
# define BIO_C_SET_SSL_RENEGOTIATE_BYTES         125
# define BIO_C_GET_SSL_NUM_RENEGOTIATES          126
# define BIO_C_SET_SSL_RENEGOTIATE_TIMEOUT       127
# define BIO_C_FILE_SEEK                         128
# define BIO_C_GET_CIPHER_CTX                    129
# define BIO_C_SET_BUF_MEM_EOF_RETURN            130/* return end of input
                                                     * value */
# define BIO_C_SET_BIND_MODE                     131
# define BIO_C_GET_BIND_MODE                     132
# define BIO_C_FILE_TELL                         133
# define BIO_C_GET_SOCKS                         134
# define BIO_C_SET_SOCKS                         135

# define BIO_C_SET_WRITE_BUF_SIZE                136/* for BIO_s_bio */
# define BIO_C_GET_WRITE_BUF_SIZE                137
# define BIO_C_MAKE_BIO_PAIR                     138
# define BIO_C_DESTROY_BIO_PAIR                  139
# define BIO_C_GET_WRITE_GUARANTEE               140
# define BIO_C_GET_READ_REQUEST                  141
# define BIO_C_SHUTDOWN_WR                       142
# define BIO_C_NREAD0                            143
# define BIO_C_NREAD                             144
# define BIO_C_NWRITE0                           145
# define BIO_C_NWRITE                            146
# define BIO_C_RESET_READ_REQUEST                147
# define BIO_C_SET_MD_CTX                        148

# define BIO_C_SET_PREFIX                        149
# define BIO_C_GET_PREFIX                        150
# define BIO_C_SET_SUFFIX                        151
# define BIO_C_GET_SUFFIX                        152

# define BIO_C_SET_EX_ARG                        153
# define BIO_C_GET_EX_ARG                        154

# define BIO_C_SET_CONNECT_MODE                  155

# define BIO_set_app_data(s,arg)         BIO_set_ex_data(s,0,arg)
# define BIO_get_app_data(s)             BIO_get_ex_data(s,0)

# define BIO_set_nbio(b,n)             BIO_ctrl(b,BIO_C_SET_NBIO,(n),NULL)

# ifndef OPENSSL_NO_SOCK
/* IP families we support, for BIO_s_connect() and BIO_s_accept() */
/* Note: the underlying operating system may not support some of them */
#  define BIO_FAMILY_IPV4                         4
#  define BIO_FAMILY_IPV6                         6
#  define BIO_FAMILY_IPANY                        256

/* BIO_s_connect() */
#  define BIO_set_conn_hostname(b,name) BIO_ctrl(b,BIO_C_SET_CONNECT,0, \
                                                 (char *)(name))
#  define BIO_set_conn_port(b,port)     BIO_ctrl(b,BIO_C_SET_CONNECT,1, \
                                                 (char *)(port))
#  define BIO_set_conn_address(b,addr)  BIO_ctrl(b,BIO_C_SET_CONNECT,2, \
                                                 (char *)(addr))
#  define BIO_set_conn_ip_family(b,f)   BIO_int_ctrl(b,BIO_C_SET_CONNECT,3,f)
#  define BIO_get_conn_hostname(b)      ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_CONNECT,0))
#  define BIO_get_conn_port(b)          ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_CONNECT,1))
#  define BIO_get_conn_address(b)       ((const BIO_ADDR *)BIO_ptr_ctrl(b,BIO_C_GET_CONNECT,2))
#  define BIO_get_conn_ip_family(b)     BIO_ctrl(b,BIO_C_GET_CONNECT,3,NULL)
#  define BIO_set_conn_mode(b,n)        BIO_ctrl(b,BIO_C_SET_CONNECT_MODE,(n),NULL)

/* BIO_s_accept() */
#  define BIO_set_accept_name(b,name)   BIO_ctrl(b,BIO_C_SET_ACCEPT,0, \
                                                 (char *)(name))
#  define BIO_set_accept_port(b,port)   BIO_ctrl(b,BIO_C_SET_ACCEPT,1, \
                                                 (char *)(port))
#  define BIO_get_accept_name(b)        ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_ACCEPT,0))
#  define BIO_get_accept_port(b)        ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_ACCEPT,1))
#  define BIO_get_peer_name(b)          ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_ACCEPT,2))
#  define BIO_get_peer_port(b)          ((const char *)BIO_ptr_ctrl(b,BIO_C_GET_ACCEPT,3))
/* #define BIO_set_nbio(b,n)    BIO_ctrl(b,BIO_C_SET_NBIO,(n),NULL) */
#  define BIO_set_nbio_accept(b,n)      BIO_ctrl(b,BIO_C_SET_ACCEPT,2,(n)?(void *)"a":NULL)
#  define BIO_set_accept_bios(b,bio)    BIO_ctrl(b,BIO_C_SET_ACCEPT,3, \
                                                 (char *)(bio))
#  define BIO_set_accept_ip_family(b,f) BIO_int_ctrl(b,BIO_C_SET_ACCEPT,4,f)
#  define BIO_get_accept_ip_family(b)   BIO_ctrl(b,BIO_C_GET_ACCEPT,4,NULL)

/* Aliases kept for backward compatibility */
#  define BIO_BIND_NORMAL                 0
#  define BIO_BIND_REUSEADDR              BIO_SOCK_REUSEADDR
#  define BIO_BIND_REUSEADDR_IF_UNUSED    BIO_SOCK_REUSEADDR
#  define BIO_set_bind_mode(b,mode) BIO_ctrl(b,BIO_C_SET_BIND_MODE,mode,NULL)
#  define BIO_get_bind_mode(b)    BIO_ctrl(b,BIO_C_GET_BIND_MODE,0,NULL)
# endif /* OPENSSL_NO_SOCK */

# define BIO_do_connect(b)       BIO_do_handshake(b)
# define BIO_do_accept(b)        BIO_do_handshake(b)

# define BIO_do_handshake(b)     BIO_ctrl(b,BIO_C_DO_STATE_MACHINE,0,NULL)

/* BIO_s_datagram(), BIO_s_fd(), BIO_s_socket(), BIO_s_accept() and BIO_s_connect() */
# define BIO_set_fd(b,fd,c)      BIO_int_ctrl(b,BIO_C_SET_FD,c,fd)
# define BIO_get_fd(b,c)         BIO_ctrl(b,BIO_C_GET_FD,0,(char *)(c))

/* BIO_s_file() */
# define BIO_set_fp(b,fp,c)      BIO_ctrl(b,BIO_C_SET_FILE_PTR,c,(char *)(fp))
# define BIO_get_fp(b,fpp)       BIO_ctrl(b,BIO_C_GET_FILE_PTR,0,(char *)(fpp))

/* BIO_s_fd() and BIO_s_file() */
# define BIO_seek(b,ofs) (int)BIO_ctrl(b,BIO_C_FILE_SEEK,ofs,NULL)
# define BIO_tell(b)     (int)BIO_ctrl(b,BIO_C_FILE_TELL,0,NULL)

/*
 * name is cast to lose const, but might be better to route through a
 * function so we can do it safely
 */
# ifdef CONST_STRICT
/*
 * If you are wondering why this isn't defined, its because CONST_STRICT is
 * purely a compile-time kludge to allow const to be checked.
 */
int BIO_read_filename(BIO *b, const char *name);
# else
#  define BIO_read_filename(b,name) (int)BIO_ctrl(b,BIO_C_SET_FILENAME, \
                BIO_CLOSE|BIO_FP_READ,(char *)(name))
# endif
# define BIO_write_filename(b,name) (int)BIO_ctrl(b,BIO_C_SET_FILENAME, \
                BIO_CLOSE|BIO_FP_WRITE,name)
# define BIO_append_filename(b,name) (int)BIO_ctrl(b,BIO_C_SET_FILENAME, \
                BIO_CLOSE|BIO_FP_APPEND,name)
# define BIO_rw_filename(b,name) (int)BIO_ctrl(b,BIO_C_SET_FILENAME, \
                BIO_CLOSE|BIO_FP_READ|BIO_FP_WRITE,name)

/*
 * WARNING WARNING, this ups the reference count on the read bio of the SSL
 * structure.  This is because the ssl read BIO is now pointed to by the
 * next_bio field in the bio.  So when you free the BIO, make sure you are
 * doing a BIO_free_all() to catch the underlying BIO.
 */
# define BIO_set_ssl(b,ssl,c)    BIO_ctrl(b,BIO_C_SET_SSL,c,(char *)(ssl))
# define BIO_get_ssl(b,sslp)     BIO_ctrl(b,BIO_C_GET_SSL,0,(char *)(sslp))
# define BIO_set_ssl_mode(b,client)      BIO_ctrl(b,BIO_C_SSL_MODE,client,NULL)
# define BIO_set_ssl_renegotiate_bytes(b,num) \
        BIO_ctrl(b,BIO_C_SET_SSL_RENEGOTIATE_BYTES,num,NULL)
# define BIO_get_num_renegotiates(b) \
        BIO_ctrl(b,BIO_C_GET_SSL_NUM_RENEGOTIATES,0,NULL)
# define BIO_set_ssl_renegotiate_timeout(b,seconds) \
        BIO_ctrl(b,BIO_C_SET_SSL_RENEGOTIATE_TIMEOUT,seconds,NULL)

/* defined in evp.h */
/* #define BIO_set_md(b,md)     BIO_ctrl(b,BIO_C_SET_MD,1,(char *)(md)) */

# define BIO_get_mem_data(b,pp)  BIO_ctrl(b,BIO_CTRL_INFO,0,(char *)(pp))
# define BIO_set_mem_buf(b,bm,c) BIO_ctrl(b,BIO_C_SET_BUF_MEM,c,(char *)(bm))
# define BIO_get_mem_ptr(b,pp)   BIO_ctrl(b,BIO_C_GET_BUF_MEM_PTR,0, \
                                          (char *)(pp))
# define BIO_set_mem_eof_return(b,v) \
                                BIO_ctrl(b,BIO_C_SET_BUF_MEM_EOF_RETURN,v,NULL)

/* For the BIO_f_buffer() type */
# define BIO_get_buffer_num_lines(b)     BIO_ctrl(b,BIO_C_GET_BUFF_NUM_LINES,0,NULL)
# define BIO_set_buffer_size(b,size)     BIO_ctrl(b,BIO_C_SET_BUFF_SIZE,size,NULL)
# define BIO_set_read_buffer_size(b,size) BIO_int_ctrl(b,BIO_C_SET_BUFF_SIZE,size,0)
# define BIO_set_write_buffer_size(b,size) BIO_int_ctrl(b,BIO_C_SET_BUFF_SIZE,size,1)
# define BIO_set_buffer_read_data(b,buf,num) BIO_ctrl(b,BIO_C_SET_BUFF_READ_DATA,num,buf)

/* Don't use the next one unless you know what you are doing :-) */
# define BIO_dup_state(b,ret)    BIO_ctrl(b,BIO_CTRL_DUP,0,(char *)(ret))

# define BIO_reset(b)            (int)BIO_ctrl(b,BIO_CTRL_RESET,0,NULL)
# define BIO_eof(b)              (int)BIO_ctrl(b,BIO_CTRL_EOF,0,NULL)
# define BIO_set_close(b,c)      (int)BIO_ctrl(b,BIO_CTRL_SET_CLOSE,(c),NULL)
# define BIO_get_close(b)        (int)BIO_ctrl(b,BIO_CTRL_GET_CLOSE,0,NULL)
# define BIO_pending(b)          (int)BIO_ctrl(b,BIO_CTRL_PENDING,0,NULL)
# define BIO_wpending(b)         (int)BIO_ctrl(b,BIO_CTRL_WPENDING,0,NULL)
/* ...pending macros have inappropriate return type */
size_t BIO_ctrl_pending(BIO *b);
size_t BIO_ctrl_wpending(BIO *b);
# define BIO_flush(b)            (int)BIO_ctrl(b,BIO_CTRL_FLUSH,0,NULL)
# define BIO_get_info_callback(b,cbp) (int)BIO_ctrl(b,BIO_CTRL_GET_CALLBACK,0, \
                                                   cbp)
# define BIO_set_info_callback(b,cb) (int)BIO_callback_ctrl(b,BIO_CTRL_SET_CALLBACK,cb)

/* For the BIO_f_buffer() type */
# define BIO_buffer_get_num_lines(b) BIO_ctrl(b,BIO_CTRL_GET,0,NULL)
# define BIO_buffer_peek(b,s,l) BIO_ctrl(b,BIO_CTRL_PEEK,(l),(s))

/* For BIO_s_bio() */
# define BIO_set_write_buf_size(b,size) (int)BIO_ctrl(b,BIO_C_SET_WRITE_BUF_SIZE,size,NULL)
# define BIO_get_write_buf_size(b,size) (size_t)BIO_ctrl(b,BIO_C_GET_WRITE_BUF_SIZE,size,NULL)
# define BIO_make_bio_pair(b1,b2)   (int)BIO_ctrl(b1,BIO_C_MAKE_BIO_PAIR,0,b2)
# define BIO_destroy_bio_pair(b)    (int)BIO_ctrl(b,BIO_C_DESTROY_BIO_PAIR,0,NULL)
# define BIO_shutdown_wr(b) (int)BIO_ctrl(b, BIO_C_SHUTDOWN_WR, 0, NULL)
/* macros with inappropriate type -- but ...pending macros use int too: */
# define BIO_get_write_guarantee(b) (int)BIO_ctrl(b,BIO_C_GET_WRITE_GUARANTEE,0,NULL)
# define BIO_get_read_request(b)    (int)BIO_ctrl(b,BIO_C_GET_READ_REQUEST,0,NULL)
size_t BIO_ctrl_get_write_guarantee(BIO *b);
size_t BIO_ctrl_get_read_request(BIO *b);
int BIO_ctrl_reset_read_request(BIO *b);

/* ctrl macros for dgram */
# define BIO_ctrl_dgram_connect(b,peer)  \
                     (int)BIO_ctrl(b,BIO_CTRL_DGRAM_CONNECT,0, (char *)(peer))
# define BIO_ctrl_set_connected(b,peer) \
         (int)BIO_ctrl(b, BIO_CTRL_DGRAM_SET_CONNECTED, 0, (char *)(peer))
# define BIO_dgram_recv_timedout(b) \
         (int)BIO_ctrl(b, BIO_CTRL_DGRAM_GET_RECV_TIMER_EXP, 0, NULL)
# define BIO_dgram_send_timedout(b) \
         (int)BIO_ctrl(b, BIO_CTRL_DGRAM_GET_SEND_TIMER_EXP, 0, NULL)
# define BIO_dgram_get_peer(b,peer) \
         (int)BIO_ctrl(b, BIO_CTRL_DGRAM_GET_PEER, 0, (char *)(peer))
# define BIO_dgram_set_peer(b,peer) \
         (int)BIO_ctrl(b, BIO_CTRL_DGRAM_SET_PEER, 0, (char *)(peer))
# define BIO_dgram_get_mtu_overhead(b) \
         (unsigned int)BIO_ctrl((b), BIO_CTRL_DGRAM_GET_MTU_OVERHEAD, 0, NULL)

/* ctrl macros for BIO_f_prefix */
# define BIO_set_prefix(b,p) BIO_ctrl((b), BIO_CTRL_SET_PREFIX, 0, (void *)(p))
# define BIO_set_indent(b,i) BIO_ctrl((b), BIO_CTRL_SET_INDENT, (i), NULL)
# define BIO_get_indent(b) BIO_ctrl((b), BIO_CTRL_GET_INDENT, 0, NULL)

#define BIO_get_ex_new_index(l, p, newf, dupf, freef) \
    CRYPTO_get_ex_new_index(CRYPTO_EX_INDEX_BIO, l, p, newf, dupf, freef)
int BIO_set_ex_data(BIO *bio, int idx, void *data);
void *BIO_get_ex_data(const BIO *bio, int idx);
uint64_t BIO_number_read(BIO *bio);
uint64_t BIO_number_written(BIO *bio);

/* For BIO_f_asn1() */
int BIO_asn1_set_prefix(BIO *b, asn1_ps_func *prefix,
                        asn1_ps_func *prefix_free);
int BIO_asn1_get_prefix(BIO *b, asn1_ps_func **pprefix,
                        asn1_ps_func **pprefix_free);
int BIO_asn1_set_suffix(BIO *b, asn1_ps_func *suffix,
                        asn1_ps_func *suffix_free);
int BIO_asn1_get_suffix(BIO *b, asn1_ps_func **psuffix,
                        asn1_ps_func **psuffix_free);

const BIO_METHOD *BIO_s_file(void);
BIO *BIO_new_file(const char *filename, const char *mode);
BIO *BIO_new_from_core_bio(OSSL_LIB_CTX *libctx, OSSL_CORE_BIO *corebio);
# ifndef OPENSSL_NO_STDIO
BIO *BIO_new_fp(FILE *stream, int close_flag);
# endif
BIO *BIO_new_ex(OSSL_LIB_CTX *libctx, const BIO_METHOD *method);
BIO *BIO_new(const BIO_METHOD *type);
int BIO_free(BIO *a);
void BIO_set_data(BIO *a, void *ptr);
void *BIO_get_data(BIO *a);
void BIO_set_init(BIO *a, int init);
int BIO_get_init(BIO *a);
void BIO_set_shutdown(BIO *a, int shut);
int BIO_get_shutdown(BIO *a);
void BIO_vfree(BIO *a);
int BIO_up_ref(BIO *a);
int BIO_read(BIO *b, void *data, int dlen);
int BIO_read_ex(BIO *b, void *data, size_t dlen, size_t *readbytes);
int BIO_gets(BIO *bp, char *buf, int size);
int BIO_get_line(BIO *bio, char *buf, int size);
int BIO_write(BIO *b, const void *data, int dlen);
int BIO_write_ex(BIO *b, const void *data, size_t dlen, size_t *written);
int BIO_puts(BIO *bp, const char *buf);
int BIO_indent(BIO *b, int indent, int max);
long BIO_ctrl(BIO *bp, int cmd, long larg, void *parg);
long BIO_callback_ctrl(BIO *b, int cmd, BIO_info_cb *fp);
void *BIO_ptr_ctrl(BIO *bp, int cmd, long larg);
long BIO_int_ctrl(BIO *bp, int cmd, long larg, int iarg);
BIO *BIO_push(BIO *b, BIO *append);
BIO *BIO_pop(BIO *b);
void BIO_free_all(BIO *a);
BIO *BIO_find_type(BIO *b, int bio_type);
BIO *BIO_next(BIO *b);
void BIO_set_next(BIO *b, BIO *next);
BIO *BIO_get_retry_BIO(BIO *bio, int *reason);
int BIO_get_retry_reason(BIO *bio);
void BIO_set_retry_reason(BIO *bio, int reason);
BIO *BIO_dup_chain(BIO *in);

int BIO_nread0(BIO *bio, char **buf);
int BIO_nread(BIO *bio, char **buf, int num);
int BIO_nwrite0(BIO *bio, char **buf);
int BIO_nwrite(BIO *bio, char **buf, int num);

const BIO_METHOD *BIO_s_mem(void);
const BIO_METHOD *BIO_s_secmem(void);
BIO *BIO_new_mem_buf(const void *buf, int len);
# ifndef OPENSSL_NO_SOCK
const BIO_METHOD *BIO_s_socket(void);
const BIO_METHOD *BIO_s_connect(void);
const BIO_METHOD *BIO_s_accept(void);
# endif
const BIO_METHOD *BIO_s_fd(void);
const BIO_METHOD *BIO_s_log(void);
const BIO_METHOD *BIO_s_bio(void);
const BIO_METHOD *BIO_s_null(void);
const BIO_METHOD *BIO_f_null(void);
const BIO_METHOD *BIO_f_buffer(void);
const BIO_METHOD *BIO_f_readbuffer(void);
const BIO_METHOD *BIO_f_linebuffer(void);
const BIO_METHOD *BIO_f_nbio_test(void);
const BIO_METHOD *BIO_f_prefix(void);
const BIO_METHOD *BIO_s_core(void);
# ifndef OPENSSL_NO_DGRAM
const BIO_METHOD *BIO_s_datagram(void);
int BIO_dgram_non_fatal_error(int error);
BIO *BIO_new_dgram(int fd, int close_flag);
#  ifndef OPENSSL_NO_SCTP
const BIO_METHOD *BIO_s_datagram_sctp(void);
BIO *BIO_new_dgram_sctp(int fd, int close_flag);
int BIO_dgram_is_sctp(BIO *bio);
int BIO_dgram_sctp_notification_cb(BIO *b,
                BIO_dgram_sctp_notification_handler_fn handle_notifications,
                void *context);
int BIO_dgram_sctp_wait_for_dry(BIO *b);
int BIO_dgram_sctp_msg_waiting(BIO *b);
#  endif
# endif

# ifndef OPENSSL_NO_SOCK
int BIO_sock_should_retry(int i);
int BIO_sock_non_fatal_error(int error);
int BIO_socket_wait(int fd, int for_read, time_t max_time);
# endif
int BIO_wait(BIO *bio, time_t max_time, unsigned int nap_milliseconds);
int BIO_do_connect_retry(BIO *bio, int timeout, int nap_milliseconds);

int BIO_fd_should_retry(int i);
int BIO_fd_non_fatal_error(int error);
int BIO_dump_cb(int (*cb) (const void *data, size_t len, void *u),
                void *u, const void *s, int len);
int BIO_dump_indent_cb(int (*cb) (const void *data, size_t len, void *u),
                       void *u, const void *s, int len, int indent);
int BIO_dump(BIO *b, const void *bytes, int len);
int BIO_dump_indent(BIO *b, const void *bytes, int len, int indent);
# ifndef OPENSSL_NO_STDIO
int BIO_dump_fp(FILE *fp, const void *s, int len);
int BIO_dump_indent_fp(FILE *fp, const void *s, int len, int indent);
# endif
int BIO_hex_string(BIO *out, int indent, int width, const void *data,
                   int datalen);

# ifndef OPENSSL_NO_SOCK
BIO_ADDR *BIO_ADDR_new(void);
int BIO_ADDR_rawmake(BIO_ADDR *ap, int family,
                     const void *where, size_t wherelen, unsigned short port);
void BIO_ADDR_free(BIO_ADDR *);
void BIO_ADDR_clear(BIO_ADDR *ap);
int BIO_ADDR_family(const BIO_ADDR *ap);
int BIO_ADDR_rawaddress(const BIO_ADDR *ap, void *p, size_t *l);
unsigned short BIO_ADDR_rawport(const BIO_ADDR *ap);
char *BIO_ADDR_hostname_string(const BIO_ADDR *ap, int numeric);
char *BIO_ADDR_service_string(const BIO_ADDR *ap, int numeric);
char *BIO_ADDR_path_string(const BIO_ADDR *ap);

const BIO_ADDRINFO *BIO_ADDRINFO_next(const BIO_ADDRINFO *bai);
int BIO_ADDRINFO_family(const BIO_ADDRINFO *bai);
int BIO_ADDRINFO_socktype(const BIO_ADDRINFO *bai);
int BIO_ADDRINFO_protocol(const BIO_ADDRINFO *bai);
const BIO_ADDR *BIO_ADDRINFO_address(const BIO_ADDRINFO *bai);
void BIO_ADDRINFO_free(BIO_ADDRINFO *bai);

enum BIO_hostserv_priorities {
    BIO_PARSE_PRIO_HOST, BIO_PARSE_PRIO_SERV
};
int BIO_parse_hostserv(const char *hostserv, char **host, char **service,
                       enum BIO_hostserv_priorities hostserv_prio);
enum BIO_lookup_type {
    BIO_LOOKUP_CLIENT, BIO_LOOKUP_SERVER
};
int BIO_lookup(const char *host, const char *service,
               enum BIO_lookup_type lookup_type,
               int family, int socktype, BIO_ADDRINFO **res);
int BIO_lookup_ex(const char *host, const char *service,
                  int lookup_type, int family, int socktype, int protocol,
                  BIO_ADDRINFO **res);
int BIO_sock_error(int sock);
int BIO_socket_ioctl(int fd, long type, void *arg);
int BIO_socket_nbio(int fd, int mode);
int BIO_sock_init(void);
# ifndef OPENSSL_NO_DEPRECATED_1_1_0
#  define BIO_sock_cleanup() while(0) continue
# endif
int BIO_set_tcp_ndelay(int sock, int turn_on);
# ifndef OPENSSL_NO_DEPRECATED_1_1_0
OSSL_DEPRECATEDIN_1_1_0 struct hostent *BIO_gethostbyname(const char *name);
OSSL_DEPRECATEDIN_1_1_0 int BIO_get_port(const char *str, unsigned short *port_ptr);
OSSL_DEPRECATEDIN_1_1_0 int BIO_get_host_ip(const char *str, unsigned char *ip);
OSSL_DEPRECATEDIN_1_1_0 int BIO_get_accept_socket(char *host_port, int mode);
OSSL_DEPRECATEDIN_1_1_0 int BIO_accept(int sock, char **ip_port);
# endif

union BIO_sock_info_u {
    BIO_ADDR *addr;
};
enum BIO_sock_info_type {
    BIO_SOCK_INFO_ADDRESS
};
int BIO_sock_info(int sock,
                  enum BIO_sock_info_type type, union BIO_sock_info_u *info);

#  define BIO_SOCK_REUSEADDR    0x01
#  define BIO_SOCK_V6_ONLY      0x02
#  define BIO_SOCK_KEEPALIVE    0x04
#  define BIO_SOCK_NONBLOCK     0x08
#  define BIO_SOCK_NODELAY      0x10

int BIO_socket(int domain, int socktype, int protocol, int options);
int BIO_connect(int sock, const BIO_ADDR *addr, int options);
int BIO_bind(int sock, const BIO_ADDR *addr, int options);
int BIO_listen(int sock, const BIO_ADDR *addr, int options);
int BIO_accept_ex(int accept_sock, BIO_ADDR *addr, int options);
int BIO_closesocket(int sock);

BIO *BIO_new_socket(int sock, int close_flag);
BIO *BIO_new_connect(const char *host_port);
BIO *BIO_new_accept(const char *host_port);
# endif /* OPENSSL_NO_SOCK*/

BIO *BIO_new_fd(int fd, int close_flag);

int BIO_new_bio_pair(BIO **bio1, size_t writebuf1,
                     BIO **bio2, size_t writebuf2);
/*
 * If successful, returns 1 and in *bio1, *bio2 two BIO pair endpoints.
 * Otherwise returns 0 and sets *bio1 and *bio2 to NULL. Size 0 uses default
 * value.
 */

void BIO_copy_next_retry(BIO *b);

/*
 * long BIO_ghbn_ctrl(int cmd,int iarg,char *parg);
 */

# define ossl_bio__attr__(x)
# if defined(__GNUC__) && defined(__STDC_VERSION__) \
    && !defined(__MINGW32__) && !defined(__MINGW64__) \
    && !defined(__APPLE__)
    /*
     * Because we support the 'z' modifier, which made its appearance in C99,
     * we can't use __attribute__ with pre C99 dialects.
     */
#  if __STDC_VERSION__ >= 199901L
#   undef ossl_bio__attr__
#   define ossl_bio__attr__ __attribute__
#   if __GNUC__*10 + __GNUC_MINOR__ >= 44
#    define ossl_bio__printf__ __gnu_printf__
#   else
#    define ossl_bio__printf__ __printf__
#   endif
#  endif
# endif
int BIO_printf(BIO *bio, const char *format, ...)
ossl_bio__attr__((__format__(ossl_bio__printf__, 2, 3)));
int BIO_vprintf(BIO *bio, const char *format, va_list args)
ossl_bio__attr__((__format__(ossl_bio__printf__, 2, 0)));
int BIO_snprintf(char *buf, size_t n, const char *format, ...)
ossl_bio__attr__((__format__(ossl_bio__printf__, 3, 4)));
int BIO_vsnprintf(char *buf, size_t n, const char *format, va_list args)
ossl_bio__attr__((__format__(ossl_bio__printf__, 3, 0)));
# undef ossl_bio__attr__
# undef ossl_bio__printf__


BIO_METHOD *BIO_meth_new(int type, const char *name);
void BIO_meth_free(BIO_METHOD *biom);
int (*BIO_meth_get_write(const BIO_METHOD *biom)) (BIO *, const char *, int);
int (*BIO_meth_get_write_ex(const BIO_METHOD *biom)) (BIO *, const char *, size_t,
                                                size_t *);
int BIO_meth_set_write(BIO_METHOD *biom,
                       int (*write) (BIO *, const char *, int));
int BIO_meth_set_write_ex(BIO_METHOD *biom,
                       int (*bwrite) (BIO *, const char *, size_t, size_t *));
int (*BIO_meth_get_read(const BIO_METHOD *biom)) (BIO *, char *, int);
int (*BIO_meth_get_read_ex(const BIO_METHOD *biom)) (BIO *, char *, size_t, size_t *);
int BIO_meth_set_read(BIO_METHOD *biom,
                      int (*read) (BIO *, char *, int));
int BIO_meth_set_read_ex(BIO_METHOD *biom,
                         int (*bread) (BIO *, char *, size_t, size_t *));
int (*BIO_meth_get_puts(const BIO_METHOD *biom)) (BIO *, const char *);
int BIO_meth_set_puts(BIO_METHOD *biom,
                      int (*puts) (BIO *, const char *));
int (*BIO_meth_get_gets(const BIO_METHOD *biom)) (BIO *, char *, int);
int BIO_meth_set_gets(BIO_METHOD *biom,
                      int (*ossl_gets) (BIO *, char *, int));
long (*BIO_meth_get_ctrl(const BIO_METHOD *biom)) (BIO *, int, long, void *);
int BIO_meth_set_ctrl(BIO_METHOD *biom,
                      long (*ctrl) (BIO *, int, long, void *));
int (*BIO_meth_get_create(const BIO_METHOD *bion)) (BIO *);
int BIO_meth_set_create(BIO_METHOD *biom, int (*create) (BIO *));
int (*BIO_meth_get_destroy(const BIO_METHOD *biom)) (BIO *);
int BIO_meth_set_destroy(BIO_METHOD *biom, int (*destroy) (BIO *));
long (*BIO_meth_get_callback_ctrl(const BIO_METHOD *biom))
                                 (BIO *, int, BIO_info_cb *);
int BIO_meth_set_callback_ctrl(BIO_METHOD *biom,
                               long (*callback_ctrl) (BIO *, int,
                                                      BIO_info_cb *));

# ifdef  __cplusplus
}
# endif
#endif
                                                                                            node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/cmp.h                0000664 0000000 0000000 00000120243 14746647661 0027674 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /*
 * WARNING: do not edit!
 * Generated by Makefile from include/openssl/cmp.h.in
 *
 * Copyright 2007-2023 The OpenSSL Project Authors. All Rights Reserved.
 * Copyright Nokia 2007-2019
 * Copyright Siemens AG 2015-2019
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */



#ifndef OPENSSL_CMP_H
# define OPENSSL_CMP_H

# include <openssl/opensslconf.h>
# ifndef OPENSSL_NO_CMP

#  include <openssl/crmf.h>
#  include <openssl/cmperr.h>
#  include <openssl/cmp_util.h>
#  include <openssl/http.h>

/* explicit #includes not strictly needed since implied by the above: */
#  include <openssl/types.h>
#  include <openssl/safestack.h>
#  include <openssl/x509.h>
#  include <openssl/x509v3.h>

#  ifdef __cplusplus
extern "C" {
#  endif

#  define OSSL_CMP_PVNO 2

/*-
 *   PKIFailureInfo ::= BIT STRING {
 *   -- since we can fail in more than one way!
 *   -- More codes may be added in the future if/when required.
 *       badAlg              (0),
 *       -- unrecognized or unsupported Algorithm Identifier
 *       badMessageCheck     (1),
 *       -- integrity check failed (e.g., signature did not verify)
 *       badRequest          (2),
 *       -- transaction not permitted or supported
 *       badTime             (3),
 *       -- messageTime was not sufficiently close to the system time,
 *       -- as defined by local policy
 *       badCertId           (4),
 *       -- no certificate could be found matching the provided criteria
 *       badDataFormat       (5),
 *       -- the data submitted has the wrong format
 *       wrongAuthority      (6),
 *       -- the authority indicated in the request is different from the
 *       -- one creating the response token
 *       incorrectData       (7),
 *       -- the requester's data is incorrect (for notary services)
 *       missingTimeStamp    (8),
 *       -- when the timestamp is missing but should be there
 *       -- (by policy)
 *       badPOP              (9),
 *       -- the proof-of-possession failed
 *       certRevoked         (10),
 *          -- the certificate has already been revoked
 *       certConfirmed       (11),
 *          -- the certificate has already been confirmed
 *       wrongIntegrity      (12),
 *          -- invalid integrity, password based instead of signature or
 *          -- vice versa
 *       badRecipientNonce   (13),
 *          -- invalid recipient nonce, either missing or wrong value
 *       timeNotAvailable    (14),
 *          -- the TSA's time source is not available
 *       unacceptedPolicy    (15),
 *          -- the requested TSA policy is not supported by the TSA.
 *       unacceptedExtension (16),
 *          -- the requested extension is not supported by the TSA.
 *       addInfoNotAvailable (17),
 *          -- the additional information requested could not be
 *          -- understood or is not available
 *       badSenderNonce      (18),
 *          -- invalid sender nonce, either missing or wrong size
 *       badCertTemplate     (19),
 *          -- invalid cert. template or missing mandatory information
 *       signerNotTrusted    (20),
 *          -- signer of the message unknown or not trusted
 *       transactionIdInUse  (21),
 *          -- the transaction identifier is already in use
 *       unsupportedVersion  (22),
 *          -- the version of the message is not supported
 *       notAuthorized       (23),
 *          -- the sender was not authorized to make the preceding
 *          -- request or perform the preceding action
 *       systemUnavail       (24),
 *       -- the request cannot be handled due to system unavailability
 *       systemFailure       (25),
 *       -- the request cannot be handled due to system failure
 *       duplicateCertReq    (26)
 *       -- certificate cannot be issued because a duplicate
 *       -- certificate already exists
 *   }
 */
#  define OSSL_CMP_PKIFAILUREINFO_badAlg 0
#  define OSSL_CMP_PKIFAILUREINFO_badMessageCheck 1
#  define OSSL_CMP_PKIFAILUREINFO_badRequest 2
#  define OSSL_CMP_PKIFAILUREINFO_badTime 3
#  define OSSL_CMP_PKIFAILUREINFO_badCertId 4
#  define OSSL_CMP_PKIFAILUREINFO_badDataFormat 5
#  define OSSL_CMP_PKIFAILUREINFO_wrongAuthority 6
#  define OSSL_CMP_PKIFAILUREINFO_incorrectData 7
#  define OSSL_CMP_PKIFAILUREINFO_missingTimeStamp 8
#  define OSSL_CMP_PKIFAILUREINFO_badPOP 9
#  define OSSL_CMP_PKIFAILUREINFO_certRevoked 10
#  define OSSL_CMP_PKIFAILUREINFO_certConfirmed 11
#  define OSSL_CMP_PKIFAILUREINFO_wrongIntegrity 12
#  define OSSL_CMP_PKIFAILUREINFO_badRecipientNonce 13
#  define OSSL_CMP_PKIFAILUREINFO_timeNotAvailable 14
#  define OSSL_CMP_PKIFAILUREINFO_unacceptedPolicy 15
#  define OSSL_CMP_PKIFAILUREINFO_unacceptedExtension 16
#  define OSSL_CMP_PKIFAILUREINFO_addInfoNotAvailable 17
#  define OSSL_CMP_PKIFAILUREINFO_badSenderNonce 18
#  define OSSL_CMP_PKIFAILUREINFO_badCertTemplate 19
#  define OSSL_CMP_PKIFAILUREINFO_signerNotTrusted 20
#  define OSSL_CMP_PKIFAILUREINFO_transactionIdInUse 21
#  define OSSL_CMP_PKIFAILUREINFO_unsupportedVersion 22
#  define OSSL_CMP_PKIFAILUREINFO_notAuthorized 23
#  define OSSL_CMP_PKIFAILUREINFO_systemUnavail 24
#  define OSSL_CMP_PKIFAILUREINFO_systemFailure 25
#  define OSSL_CMP_PKIFAILUREINFO_duplicateCertReq 26
#  define OSSL_CMP_PKIFAILUREINFO_MAX 26
#  define OSSL_CMP_PKIFAILUREINFO_MAX_BIT_PATTERN \
    ((1 << (OSSL_CMP_PKIFAILUREINFO_MAX + 1)) - 1)
#  if OSSL_CMP_PKIFAILUREINFO_MAX_BIT_PATTERN > INT_MAX
#   error CMP_PKIFAILUREINFO_MAX bit pattern does not fit in type int
#  endif

typedef ASN1_BIT_STRING OSSL_CMP_PKIFAILUREINFO;

#  define OSSL_CMP_CTX_FAILINFO_badAlg (1 << 0)
#  define OSSL_CMP_CTX_FAILINFO_badMessageCheck (1 << 1)
#  define OSSL_CMP_CTX_FAILINFO_badRequest (1 << 2)
#  define OSSL_CMP_CTX_FAILINFO_badTime (1 << 3)
#  define OSSL_CMP_CTX_FAILINFO_badCertId (1 << 4)
#  define OSSL_CMP_CTX_FAILINFO_badDataFormat (1 << 5)
#  define OSSL_CMP_CTX_FAILINFO_wrongAuthority (1 << 6)
#  define OSSL_CMP_CTX_FAILINFO_incorrectData (1 << 7)
#  define OSSL_CMP_CTX_FAILINFO_missingTimeStamp (1 << 8)
#  define OSSL_CMP_CTX_FAILINFO_badPOP (1 << 9)
#  define OSSL_CMP_CTX_FAILINFO_certRevoked (1 << 10)
#  define OSSL_CMP_CTX_FAILINFO_certConfirmed (1 << 11)
#  define OSSL_CMP_CTX_FAILINFO_wrongIntegrity (1 << 12)
#  define OSSL_CMP_CTX_FAILINFO_badRecipientNonce (1 << 13)
#  define OSSL_CMP_CTX_FAILINFO_timeNotAvailable (1 << 14)
#  define OSSL_CMP_CTX_FAILINFO_unacceptedPolicy (1 << 15)
#  define OSSL_CMP_CTX_FAILINFO_unacceptedExtension (1 << 16)
#  define OSSL_CMP_CTX_FAILINFO_addInfoNotAvailable (1 << 17)
#  define OSSL_CMP_CTX_FAILINFO_badSenderNonce (1 << 18)
#  define OSSL_CMP_CTX_FAILINFO_badCertTemplate (1 << 19)
#  define OSSL_CMP_CTX_FAILINFO_signerNotTrusted (1 << 20)
#  define OSSL_CMP_CTX_FAILINFO_transactionIdInUse (1 << 21)
#  define OSSL_CMP_CTX_FAILINFO_unsupportedVersion (1 << 22)
#  define OSSL_CMP_CTX_FAILINFO_notAuthorized (1 << 23)
#  define OSSL_CMP_CTX_FAILINFO_systemUnavail (1 << 24)
#  define OSSL_CMP_CTX_FAILINFO_systemFailure (1 << 25)
#  define OSSL_CMP_CTX_FAILINFO_duplicateCertReq (1 << 26)

/*-
 *   PKIStatus ::= INTEGER {
 *       accepted                (0),
 *       -- you got exactly what you asked for
 *       grantedWithMods        (1),
 *       -- you got something like what you asked for; the
 *       -- requester is responsible for ascertaining the differences
 *       rejection              (2),
 *       -- you don't get it, more information elsewhere in the message
 *       waiting                (3),
 *       -- the request body part has not yet been processed; expect to
 *       -- hear more later (note: proper handling of this status
 *       -- response MAY use the polling req/rep PKIMessages specified
 *       -- in Section 5.3.22; alternatively, polling in the underlying
 *       -- transport layer MAY have some utility in this regard)
 *       revocationWarning      (4),
 *       -- this message contains a warning that a revocation is
 *       -- imminent
 *       revocationNotification (5),
 *       -- notification that a revocation has occurred
 *       keyUpdateWarning       (6)
 *       -- update already done for the oldCertId specified in
 *       -- CertReqMsg
 *   }
 */
#  define OSSL_CMP_PKISTATUS_request                -3
#  define OSSL_CMP_PKISTATUS_trans                  -2
#  define OSSL_CMP_PKISTATUS_unspecified            -1
#  define OSSL_CMP_PKISTATUS_accepted               0
#  define OSSL_CMP_PKISTATUS_grantedWithMods        1
#  define OSSL_CMP_PKISTATUS_rejection              2
#  define OSSL_CMP_PKISTATUS_waiting                3
#  define OSSL_CMP_PKISTATUS_revocationWarning      4
#  define OSSL_CMP_PKISTATUS_revocationNotification 5
#  define OSSL_CMP_PKISTATUS_keyUpdateWarning       6

typedef ASN1_INTEGER OSSL_CMP_PKISTATUS;
DECLARE_ASN1_ITEM(OSSL_CMP_PKISTATUS)

#  define OSSL_CMP_CERTORENCCERT_CERTIFICATE 0
#  define OSSL_CMP_CERTORENCCERT_ENCRYPTEDCERT 1

/* data type declarations */
typedef struct ossl_cmp_ctx_st OSSL_CMP_CTX;
typedef struct ossl_cmp_pkiheader_st OSSL_CMP_PKIHEADER;
DECLARE_ASN1_FUNCTIONS(OSSL_CMP_PKIHEADER)
typedef struct ossl_cmp_msg_st OSSL_CMP_MSG;
DECLARE_ASN1_DUP_FUNCTION(OSSL_CMP_MSG)
DECLARE_ASN1_ENCODE_FUNCTIONS(OSSL_CMP_MSG, OSSL_CMP_MSG, OSSL_CMP_MSG)
typedef struct ossl_cmp_certstatus_st OSSL_CMP_CERTSTATUS;
SKM_DEFINE_STACK_OF_INTERNAL(OSSL_CMP_CERTSTATUS, OSSL_CMP_CERTSTATUS, OSSL_CMP_CERTSTATUS)
#define sk_OSSL_CMP_CERTSTATUS_num(sk) OPENSSL_sk_num(ossl_check_const_OSSL_CMP_CERTSTATUS_sk_type(sk))
#define sk_OSSL_CMP_CERTSTATUS_value(sk, idx) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_value(ossl_check_const_OSSL_CMP_CERTSTATUS_sk_type(sk), (idx)))
#define sk_OSSL_CMP_CERTSTATUS_new(cmp) ((STACK_OF(OSSL_CMP_CERTSTATUS) *)OPENSSL_sk_new(ossl_check_OSSL_CMP_CERTSTATUS_compfunc_type(cmp)))
#define sk_OSSL_CMP_CERTSTATUS_new_null() ((STACK_OF(OSSL_CMP_CERTSTATUS) *)OPENSSL_sk_new_null())
#define sk_OSSL_CMP_CERTSTATUS_new_reserve(cmp, n) ((STACK_OF(OSSL_CMP_CERTSTATUS) *)OPENSSL_sk_new_reserve(ossl_check_OSSL_CMP_CERTSTATUS_compfunc_type(cmp), (n)))
#define sk_OSSL_CMP_CERTSTATUS_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), (n))
#define sk_OSSL_CMP_CERTSTATUS_free(sk) OPENSSL_sk_free(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk))
#define sk_OSSL_CMP_CERTSTATUS_zero(sk) OPENSSL_sk_zero(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk))
#define sk_OSSL_CMP_CERTSTATUS_delete(sk, i) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_delete(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), (i)))
#define sk_OSSL_CMP_CERTSTATUS_delete_ptr(sk, ptr) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_delete_ptr(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr)))
#define sk_OSSL_CMP_CERTSTATUS_push(sk, ptr) OPENSSL_sk_push(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr))
#define sk_OSSL_CMP_CERTSTATUS_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr))
#define sk_OSSL_CMP_CERTSTATUS_pop(sk) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_pop(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk)))
#define sk_OSSL_CMP_CERTSTATUS_shift(sk) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_shift(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk)))
#define sk_OSSL_CMP_CERTSTATUS_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk),ossl_check_OSSL_CMP_CERTSTATUS_freefunc_type(freefunc))
#define sk_OSSL_CMP_CERTSTATUS_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr), (idx))
#define sk_OSSL_CMP_CERTSTATUS_set(sk, idx, ptr) ((OSSL_CMP_CERTSTATUS *)OPENSSL_sk_set(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), (idx), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr)))
#define sk_OSSL_CMP_CERTSTATUS_find(sk, ptr) OPENSSL_sk_find(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr))
#define sk_OSSL_CMP_CERTSTATUS_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr))
#define sk_OSSL_CMP_CERTSTATUS_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_type(ptr), pnum)
#define sk_OSSL_CMP_CERTSTATUS_sort(sk) OPENSSL_sk_sort(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk))
#define sk_OSSL_CMP_CERTSTATUS_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_OSSL_CMP_CERTSTATUS_sk_type(sk))
#define sk_OSSL_CMP_CERTSTATUS_dup(sk) ((STACK_OF(OSSL_CMP_CERTSTATUS) *)OPENSSL_sk_dup(ossl_check_const_OSSL_CMP_CERTSTATUS_sk_type(sk)))
#define sk_OSSL_CMP_CERTSTATUS_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(OSSL_CMP_CERTSTATUS) *)OPENSSL_sk_deep_copy(ossl_check_const_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_copyfunc_type(copyfunc), ossl_check_OSSL_CMP_CERTSTATUS_freefunc_type(freefunc)))
#define sk_OSSL_CMP_CERTSTATUS_set_cmp_func(sk, cmp) ((sk_OSSL_CMP_CERTSTATUS_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_OSSL_CMP_CERTSTATUS_sk_type(sk), ossl_check_OSSL_CMP_CERTSTATUS_compfunc_type(cmp)))

typedef struct ossl_cmp_itav_st OSSL_CMP_ITAV;
DECLARE_ASN1_DUP_FUNCTION(OSSL_CMP_ITAV)
SKM_DEFINE_STACK_OF_INTERNAL(OSSL_CMP_ITAV, OSSL_CMP_ITAV, OSSL_CMP_ITAV)
#define sk_OSSL_CMP_ITAV_num(sk) OPENSSL_sk_num(ossl_check_const_OSSL_CMP_ITAV_sk_type(sk))
#define sk_OSSL_CMP_ITAV_value(sk, idx) ((OSSL_CMP_ITAV *)OPENSSL_sk_value(ossl_check_const_OSSL_CMP_ITAV_sk_type(sk), (idx)))
#define sk_OSSL_CMP_ITAV_new(cmp) ((STACK_OF(OSSL_CMP_ITAV) *)OPENSSL_sk_new(ossl_check_OSSL_CMP_ITAV_compfunc_type(cmp)))
#define sk_OSSL_CMP_ITAV_new_null() ((STACK_OF(OSSL_CMP_ITAV) *)OPENSSL_sk_new_null())
#define sk_OSSL_CMP_ITAV_new_reserve(cmp, n) ((STACK_OF(OSSL_CMP_ITAV) *)OPENSSL_sk_new_reserve(ossl_check_OSSL_CMP_ITAV_compfunc_type(cmp), (n)))
#define sk_OSSL_CMP_ITAV_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_OSSL_CMP_ITAV_sk_type(sk), (n))
#define sk_OSSL_CMP_ITAV_free(sk) OPENSSL_sk_free(ossl_check_OSSL_CMP_ITAV_sk_type(sk))
#define sk_OSSL_CMP_ITAV_zero(sk) OPENSSL_sk_zero(ossl_check_OSSL_CMP_ITAV_sk_type(sk))
#define sk_OSSL_CMP_ITAV_delete(sk, i) ((OSSL_CMP_ITAV *)OPENSSL_sk_delete(ossl_check_OSSL_CMP_ITAV_sk_type(sk), (i)))
#define sk_OSSL_CMP_ITAV_delete_ptr(sk, ptr) ((OSSL_CMP_ITAV *)OPENSSL_sk_delete_ptr(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr)))
#define sk_OSSL_CMP_ITAV_push(sk, ptr) OPENSSL_sk_push(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr))
#define sk_OSSL_CMP_ITAV_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr))
#define sk_OSSL_CMP_ITAV_pop(sk) ((OSSL_CMP_ITAV *)OPENSSL_sk_pop(ossl_check_OSSL_CMP_ITAV_sk_type(sk)))
#define sk_OSSL_CMP_ITAV_shift(sk) ((OSSL_CMP_ITAV *)OPENSSL_sk_shift(ossl_check_OSSL_CMP_ITAV_sk_type(sk)))
#define sk_OSSL_CMP_ITAV_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_OSSL_CMP_ITAV_sk_type(sk),ossl_check_OSSL_CMP_ITAV_freefunc_type(freefunc))
#define sk_OSSL_CMP_ITAV_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr), (idx))
#define sk_OSSL_CMP_ITAV_set(sk, idx, ptr) ((OSSL_CMP_ITAV *)OPENSSL_sk_set(ossl_check_OSSL_CMP_ITAV_sk_type(sk), (idx), ossl_check_OSSL_CMP_ITAV_type(ptr)))
#define sk_OSSL_CMP_ITAV_find(sk, ptr) OPENSSL_sk_find(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr))
#define sk_OSSL_CMP_ITAV_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr))
#define sk_OSSL_CMP_ITAV_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_type(ptr), pnum)
#define sk_OSSL_CMP_ITAV_sort(sk) OPENSSL_sk_sort(ossl_check_OSSL_CMP_ITAV_sk_type(sk))
#define sk_OSSL_CMP_ITAV_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_OSSL_CMP_ITAV_sk_type(sk))
#define sk_OSSL_CMP_ITAV_dup(sk) ((STACK_OF(OSSL_CMP_ITAV) *)OPENSSL_sk_dup(ossl_check_const_OSSL_CMP_ITAV_sk_type(sk)))
#define sk_OSSL_CMP_ITAV_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(OSSL_CMP_ITAV) *)OPENSSL_sk_deep_copy(ossl_check_const_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_copyfunc_type(copyfunc), ossl_check_OSSL_CMP_ITAV_freefunc_type(freefunc)))
#define sk_OSSL_CMP_ITAV_set_cmp_func(sk, cmp) ((sk_OSSL_CMP_ITAV_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_OSSL_CMP_ITAV_sk_type(sk), ossl_check_OSSL_CMP_ITAV_compfunc_type(cmp)))

typedef struct ossl_cmp_revrepcontent_st OSSL_CMP_REVREPCONTENT;
typedef struct ossl_cmp_pkisi_st OSSL_CMP_PKISI;
DECLARE_ASN1_FUNCTIONS(OSSL_CMP_PKISI)
DECLARE_ASN1_DUP_FUNCTION(OSSL_CMP_PKISI)
SKM_DEFINE_STACK_OF_INTERNAL(OSSL_CMP_PKISI, OSSL_CMP_PKISI, OSSL_CMP_PKISI)
#define sk_OSSL_CMP_PKISI_num(sk) OPENSSL_sk_num(ossl_check_const_OSSL_CMP_PKISI_sk_type(sk))
#define sk_OSSL_CMP_PKISI_value(sk, idx) ((OSSL_CMP_PKISI *)OPENSSL_sk_value(ossl_check_const_OSSL_CMP_PKISI_sk_type(sk), (idx)))
#define sk_OSSL_CMP_PKISI_new(cmp) ((STACK_OF(OSSL_CMP_PKISI) *)OPENSSL_sk_new(ossl_check_OSSL_CMP_PKISI_compfunc_type(cmp)))
#define sk_OSSL_CMP_PKISI_new_null() ((STACK_OF(OSSL_CMP_PKISI) *)OPENSSL_sk_new_null())
#define sk_OSSL_CMP_PKISI_new_reserve(cmp, n) ((STACK_OF(OSSL_CMP_PKISI) *)OPENSSL_sk_new_reserve(ossl_check_OSSL_CMP_PKISI_compfunc_type(cmp), (n)))
#define sk_OSSL_CMP_PKISI_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_OSSL_CMP_PKISI_sk_type(sk), (n))
#define sk_OSSL_CMP_PKISI_free(sk) OPENSSL_sk_free(ossl_check_OSSL_CMP_PKISI_sk_type(sk))
#define sk_OSSL_CMP_PKISI_zero(sk) OPENSSL_sk_zero(ossl_check_OSSL_CMP_PKISI_sk_type(sk))
#define sk_OSSL_CMP_PKISI_delete(sk, i) ((OSSL_CMP_PKISI *)OPENSSL_sk_delete(ossl_check_OSSL_CMP_PKISI_sk_type(sk), (i)))
#define sk_OSSL_CMP_PKISI_delete_ptr(sk, ptr) ((OSSL_CMP_PKISI *)OPENSSL_sk_delete_ptr(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr)))
#define sk_OSSL_CMP_PKISI_push(sk, ptr) OPENSSL_sk_push(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr))
#define sk_OSSL_CMP_PKISI_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr))
#define sk_OSSL_CMP_PKISI_pop(sk) ((OSSL_CMP_PKISI *)OPENSSL_sk_pop(ossl_check_OSSL_CMP_PKISI_sk_type(sk)))
#define sk_OSSL_CMP_PKISI_shift(sk) ((OSSL_CMP_PKISI *)OPENSSL_sk_shift(ossl_check_OSSL_CMP_PKISI_sk_type(sk)))
#define sk_OSSL_CMP_PKISI_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_OSSL_CMP_PKISI_sk_type(sk),ossl_check_OSSL_CMP_PKISI_freefunc_type(freefunc))
#define sk_OSSL_CMP_PKISI_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr), (idx))
#define sk_OSSL_CMP_PKISI_set(sk, idx, ptr) ((OSSL_CMP_PKISI *)OPENSSL_sk_set(ossl_check_OSSL_CMP_PKISI_sk_type(sk), (idx), ossl_check_OSSL_CMP_PKISI_type(ptr)))
#define sk_OSSL_CMP_PKISI_find(sk, ptr) OPENSSL_sk_find(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr))
#define sk_OSSL_CMP_PKISI_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr))
#define sk_OSSL_CMP_PKISI_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_type(ptr), pnum)
#define sk_OSSL_CMP_PKISI_sort(sk) OPENSSL_sk_sort(ossl_check_OSSL_CMP_PKISI_sk_type(sk))
#define sk_OSSL_CMP_PKISI_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_OSSL_CMP_PKISI_sk_type(sk))
#define sk_OSSL_CMP_PKISI_dup(sk) ((STACK_OF(OSSL_CMP_PKISI) *)OPENSSL_sk_dup(ossl_check_const_OSSL_CMP_PKISI_sk_type(sk)))
#define sk_OSSL_CMP_PKISI_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(OSSL_CMP_PKISI) *)OPENSSL_sk_deep_copy(ossl_check_const_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_copyfunc_type(copyfunc), ossl_check_OSSL_CMP_PKISI_freefunc_type(freefunc)))
#define sk_OSSL_CMP_PKISI_set_cmp_func(sk, cmp) ((sk_OSSL_CMP_PKISI_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_OSSL_CMP_PKISI_sk_type(sk), ossl_check_OSSL_CMP_PKISI_compfunc_type(cmp)))

typedef struct ossl_cmp_certrepmessage_st OSSL_CMP_CERTREPMESSAGE;
SKM_DEFINE_STACK_OF_INTERNAL(OSSL_CMP_CERTREPMESSAGE, OSSL_CMP_CERTREPMESSAGE, OSSL_CMP_CERTREPMESSAGE)
#define sk_OSSL_CMP_CERTREPMESSAGE_num(sk) OPENSSL_sk_num(ossl_check_const_OSSL_CMP_CERTREPMESSAGE_sk_type(sk))
#define sk_OSSL_CMP_CERTREPMESSAGE_value(sk, idx) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_value(ossl_check_const_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), (idx)))
#define sk_OSSL_CMP_CERTREPMESSAGE_new(cmp) ((STACK_OF(OSSL_CMP_CERTREPMESSAGE) *)OPENSSL_sk_new(ossl_check_OSSL_CMP_CERTREPMESSAGE_compfunc_type(cmp)))
#define sk_OSSL_CMP_CERTREPMESSAGE_new_null() ((STACK_OF(OSSL_CMP_CERTREPMESSAGE) *)OPENSSL_sk_new_null())
#define sk_OSSL_CMP_CERTREPMESSAGE_new_reserve(cmp, n) ((STACK_OF(OSSL_CMP_CERTREPMESSAGE) *)OPENSSL_sk_new_reserve(ossl_check_OSSL_CMP_CERTREPMESSAGE_compfunc_type(cmp), (n)))
#define sk_OSSL_CMP_CERTREPMESSAGE_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), (n))
#define sk_OSSL_CMP_CERTREPMESSAGE_free(sk) OPENSSL_sk_free(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk))
#define sk_OSSL_CMP_CERTREPMESSAGE_zero(sk) OPENSSL_sk_zero(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk))
#define sk_OSSL_CMP_CERTREPMESSAGE_delete(sk, i) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_delete(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), (i)))
#define sk_OSSL_CMP_CERTREPMESSAGE_delete_ptr(sk, ptr) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_delete_ptr(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr)))
#define sk_OSSL_CMP_CERTREPMESSAGE_push(sk, ptr) OPENSSL_sk_push(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr))
#define sk_OSSL_CMP_CERTREPMESSAGE_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr))
#define sk_OSSL_CMP_CERTREPMESSAGE_pop(sk) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_pop(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk)))
#define sk_OSSL_CMP_CERTREPMESSAGE_shift(sk) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_shift(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk)))
#define sk_OSSL_CMP_CERTREPMESSAGE_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk),ossl_check_OSSL_CMP_CERTREPMESSAGE_freefunc_type(freefunc))
#define sk_OSSL_CMP_CERTREPMESSAGE_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr), (idx))
#define sk_OSSL_CMP_CERTREPMESSAGE_set(sk, idx, ptr) ((OSSL_CMP_CERTREPMESSAGE *)OPENSSL_sk_set(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), (idx), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr)))
#define sk_OSSL_CMP_CERTREPMESSAGE_find(sk, ptr) OPENSSL_sk_find(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr))
#define sk_OSSL_CMP_CERTREPMESSAGE_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr))
#define sk_OSSL_CMP_CERTREPMESSAGE_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_type(ptr), pnum)
#define sk_OSSL_CMP_CERTREPMESSAGE_sort(sk) OPENSSL_sk_sort(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk))
#define sk_OSSL_CMP_CERTREPMESSAGE_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_OSSL_CMP_CERTREPMESSAGE_sk_type(sk))
#define sk_OSSL_CMP_CERTREPMESSAGE_dup(sk) ((STACK_OF(OSSL_CMP_CERTREPMESSAGE) *)OPENSSL_sk_dup(ossl_check_const_OSSL_CMP_CERTREPMESSAGE_sk_type(sk)))
#define sk_OSSL_CMP_CERTREPMESSAGE_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(OSSL_CMP_CERTREPMESSAGE) *)OPENSSL_sk_deep_copy(ossl_check_const_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_copyfunc_type(copyfunc), ossl_check_OSSL_CMP_CERTREPMESSAGE_freefunc_type(freefunc)))
#define sk_OSSL_CMP_CERTREPMESSAGE_set_cmp_func(sk, cmp) ((sk_OSSL_CMP_CERTREPMESSAGE_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_OSSL_CMP_CERTREPMESSAGE_sk_type(sk), ossl_check_OSSL_CMP_CERTREPMESSAGE_compfunc_type(cmp)))

typedef struct ossl_cmp_pollrep_st OSSL_CMP_POLLREP;
typedef STACK_OF(OSSL_CMP_POLLREP) OSSL_CMP_POLLREPCONTENT;
typedef struct ossl_cmp_certresponse_st OSSL_CMP_CERTRESPONSE;
SKM_DEFINE_STACK_OF_INTERNAL(OSSL_CMP_CERTRESPONSE, OSSL_CMP_CERTRESPONSE, OSSL_CMP_CERTRESPONSE)
#define sk_OSSL_CMP_CERTRESPONSE_num(sk) OPENSSL_sk_num(ossl_check_const_OSSL_CMP_CERTRESPONSE_sk_type(sk))
#define sk_OSSL_CMP_CERTRESPONSE_value(sk, idx) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_value(ossl_check_const_OSSL_CMP_CERTRESPONSE_sk_type(sk), (idx)))
#define sk_OSSL_CMP_CERTRESPONSE_new(cmp) ((STACK_OF(OSSL_CMP_CERTRESPONSE) *)OPENSSL_sk_new(ossl_check_OSSL_CMP_CERTRESPONSE_compfunc_type(cmp)))
#define sk_OSSL_CMP_CERTRESPONSE_new_null() ((STACK_OF(OSSL_CMP_CERTRESPONSE) *)OPENSSL_sk_new_null())
#define sk_OSSL_CMP_CERTRESPONSE_new_reserve(cmp, n) ((STACK_OF(OSSL_CMP_CERTRESPONSE) *)OPENSSL_sk_new_reserve(ossl_check_OSSL_CMP_CERTRESPONSE_compfunc_type(cmp), (n)))
#define sk_OSSL_CMP_CERTRESPONSE_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), (n))
#define sk_OSSL_CMP_CERTRESPONSE_free(sk) OPENSSL_sk_free(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk))
#define sk_OSSL_CMP_CERTRESPONSE_zero(sk) OPENSSL_sk_zero(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk))
#define sk_OSSL_CMP_CERTRESPONSE_delete(sk, i) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_delete(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), (i)))
#define sk_OSSL_CMP_CERTRESPONSE_delete_ptr(sk, ptr) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_delete_ptr(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr)))
#define sk_OSSL_CMP_CERTRESPONSE_push(sk, ptr) OPENSSL_sk_push(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr))
#define sk_OSSL_CMP_CERTRESPONSE_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr))
#define sk_OSSL_CMP_CERTRESPONSE_pop(sk) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_pop(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk)))
#define sk_OSSL_CMP_CERTRESPONSE_shift(sk) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_shift(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk)))
#define sk_OSSL_CMP_CERTRESPONSE_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk),ossl_check_OSSL_CMP_CERTRESPONSE_freefunc_type(freefunc))
#define sk_OSSL_CMP_CERTRESPONSE_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr), (idx))
#define sk_OSSL_CMP_CERTRESPONSE_set(sk, idx, ptr) ((OSSL_CMP_CERTRESPONSE *)OPENSSL_sk_set(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), (idx), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr)))
#define sk_OSSL_CMP_CERTRESPONSE_find(sk, ptr) OPENSSL_sk_find(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr))
#define sk_OSSL_CMP_CERTRESPONSE_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr))
#define sk_OSSL_CMP_CERTRESPONSE_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_type(ptr), pnum)
#define sk_OSSL_CMP_CERTRESPONSE_sort(sk) OPENSSL_sk_sort(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk))
#define sk_OSSL_CMP_CERTRESPONSE_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_OSSL_CMP_CERTRESPONSE_sk_type(sk))
#define sk_OSSL_CMP_CERTRESPONSE_dup(sk) ((STACK_OF(OSSL_CMP_CERTRESPONSE) *)OPENSSL_sk_dup(ossl_check_const_OSSL_CMP_CERTRESPONSE_sk_type(sk)))
#define sk_OSSL_CMP_CERTRESPONSE_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(OSSL_CMP_CERTRESPONSE) *)OPENSSL_sk_deep_copy(ossl_check_const_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_copyfunc_type(copyfunc), ossl_check_OSSL_CMP_CERTRESPONSE_freefunc_type(freefunc)))
#define sk_OSSL_CMP_CERTRESPONSE_set_cmp_func(sk, cmp) ((sk_OSSL_CMP_CERTRESPONSE_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_OSSL_CMP_CERTRESPONSE_sk_type(sk), ossl_check_OSSL_CMP_CERTRESPONSE_compfunc_type(cmp)))

typedef STACK_OF(ASN1_UTF8STRING) OSSL_CMP_PKIFREETEXT;

/*
 * function DECLARATIONS
 */

/* from cmp_asn.c */
OSSL_CMP_ITAV *OSSL_CMP_ITAV_create(ASN1_OBJECT *type, ASN1_TYPE *value);
void OSSL_CMP_ITAV_set0(OSSL_CMP_ITAV *itav, ASN1_OBJECT *type,
                        ASN1_TYPE *value);
ASN1_OBJECT *OSSL_CMP_ITAV_get0_type(const OSSL_CMP_ITAV *itav);
ASN1_TYPE *OSSL_CMP_ITAV_get0_value(const OSSL_CMP_ITAV *itav);
int OSSL_CMP_ITAV_push0_stack_item(STACK_OF(OSSL_CMP_ITAV) **itav_sk_p,
                                   OSSL_CMP_ITAV *itav);
void OSSL_CMP_ITAV_free(OSSL_CMP_ITAV *itav);
void OSSL_CMP_MSG_free(OSSL_CMP_MSG *msg);

/* from cmp_ctx.c */
OSSL_CMP_CTX *OSSL_CMP_CTX_new(OSSL_LIB_CTX *libctx, const char *propq);
void OSSL_CMP_CTX_free(OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_reinit(OSSL_CMP_CTX *ctx);
/* CMP general options: */
#  define OSSL_CMP_OPT_LOG_VERBOSITY 0
/* CMP transfer options: */
#  define OSSL_CMP_OPT_KEEP_ALIVE 10
#  define OSSL_CMP_OPT_MSG_TIMEOUT 11
#  define OSSL_CMP_OPT_TOTAL_TIMEOUT 12
/* CMP request options: */
#  define OSSL_CMP_OPT_VALIDITY_DAYS 20
#  define OSSL_CMP_OPT_SUBJECTALTNAME_NODEFAULT 21
#  define OSSL_CMP_OPT_SUBJECTALTNAME_CRITICAL 22
#  define OSSL_CMP_OPT_POLICIES_CRITICAL 23
#  define OSSL_CMP_OPT_POPO_METHOD 24
#  define OSSL_CMP_OPT_IMPLICIT_CONFIRM 25
#  define OSSL_CMP_OPT_DISABLE_CONFIRM 26
#  define OSSL_CMP_OPT_REVOCATION_REASON 27
/* CMP protection options: */
#  define OSSL_CMP_OPT_UNPROTECTED_SEND 30
#  define OSSL_CMP_OPT_UNPROTECTED_ERRORS 31
#  define OSSL_CMP_OPT_OWF_ALGNID 32
#  define OSSL_CMP_OPT_MAC_ALGNID 33
#  define OSSL_CMP_OPT_DIGEST_ALGNID 34
#  define OSSL_CMP_OPT_IGNORE_KEYUSAGE 35
#  define OSSL_CMP_OPT_PERMIT_TA_IN_EXTRACERTS_FOR_IR 36
int OSSL_CMP_CTX_set_option(OSSL_CMP_CTX *ctx, int opt, int val);
int OSSL_CMP_CTX_get_option(const OSSL_CMP_CTX *ctx, int opt);
/* CMP-specific callback for logging and outputting the error queue: */
int OSSL_CMP_CTX_set_log_cb(OSSL_CMP_CTX *ctx, OSSL_CMP_log_cb_t cb);
#  define OSSL_CMP_CTX_set_log_verbosity(ctx, level) \
    OSSL_CMP_CTX_set_option(ctx, OSSL_CMP_OPT_LOG_VERBOSITY, level)
void OSSL_CMP_CTX_print_errors(const OSSL_CMP_CTX *ctx);
/* message transfer: */
int OSSL_CMP_CTX_set1_serverPath(OSSL_CMP_CTX *ctx, const char *path);
int OSSL_CMP_CTX_set1_server(OSSL_CMP_CTX *ctx, const char *address);
int OSSL_CMP_CTX_set_serverPort(OSSL_CMP_CTX *ctx, int port);
int OSSL_CMP_CTX_set1_proxy(OSSL_CMP_CTX *ctx, const char *name);
int OSSL_CMP_CTX_set1_no_proxy(OSSL_CMP_CTX *ctx, const char *names);
int OSSL_CMP_CTX_set_http_cb(OSSL_CMP_CTX *ctx, OSSL_HTTP_bio_cb_t cb);
int OSSL_CMP_CTX_set_http_cb_arg(OSSL_CMP_CTX *ctx, void *arg);
void *OSSL_CMP_CTX_get_http_cb_arg(const OSSL_CMP_CTX *ctx);
typedef OSSL_CMP_MSG *(*OSSL_CMP_transfer_cb_t) (OSSL_CMP_CTX *ctx,
                                                 const OSSL_CMP_MSG *req);
int OSSL_CMP_CTX_set_transfer_cb(OSSL_CMP_CTX *ctx, OSSL_CMP_transfer_cb_t cb);
int OSSL_CMP_CTX_set_transfer_cb_arg(OSSL_CMP_CTX *ctx, void *arg);
void *OSSL_CMP_CTX_get_transfer_cb_arg(const OSSL_CMP_CTX *ctx);
/* server authentication: */
int OSSL_CMP_CTX_set1_srvCert(OSSL_CMP_CTX *ctx, X509 *cert);
int OSSL_CMP_CTX_set1_expected_sender(OSSL_CMP_CTX *ctx, const X509_NAME *name);
int OSSL_CMP_CTX_set0_trustedStore(OSSL_CMP_CTX *ctx, X509_STORE *store);
X509_STORE *OSSL_CMP_CTX_get0_trustedStore(const OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_set1_untrusted(OSSL_CMP_CTX *ctx, STACK_OF(X509) *certs);
STACK_OF(X509) *OSSL_CMP_CTX_get0_untrusted(const OSSL_CMP_CTX *ctx);
/* client authentication: */
int OSSL_CMP_CTX_set1_cert(OSSL_CMP_CTX *ctx, X509 *cert);
int OSSL_CMP_CTX_build_cert_chain(OSSL_CMP_CTX *ctx, X509_STORE *own_trusted,
                                  STACK_OF(X509) *candidates);
int OSSL_CMP_CTX_set1_pkey(OSSL_CMP_CTX *ctx, EVP_PKEY *pkey);
int OSSL_CMP_CTX_set1_referenceValue(OSSL_CMP_CTX *ctx,
                                     const unsigned char *ref, int len);
int OSSL_CMP_CTX_set1_secretValue(OSSL_CMP_CTX *ctx,
                                  const unsigned char *sec, int len);
/* CMP message header and extra certificates: */
int OSSL_CMP_CTX_set1_recipient(OSSL_CMP_CTX *ctx, const X509_NAME *name);
int OSSL_CMP_CTX_push0_geninfo_ITAV(OSSL_CMP_CTX *ctx, OSSL_CMP_ITAV *itav);
int OSSL_CMP_CTX_reset_geninfo_ITAVs(OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_set1_extraCertsOut(OSSL_CMP_CTX *ctx,
                                    STACK_OF(X509) *extraCertsOut);
/* certificate template: */
int OSSL_CMP_CTX_set0_newPkey(OSSL_CMP_CTX *ctx, int priv, EVP_PKEY *pkey);
EVP_PKEY *OSSL_CMP_CTX_get0_newPkey(const OSSL_CMP_CTX *ctx, int priv);
int OSSL_CMP_CTX_set1_issuer(OSSL_CMP_CTX *ctx, const X509_NAME *name);
int OSSL_CMP_CTX_set1_subjectName(OSSL_CMP_CTX *ctx, const X509_NAME *name);
int OSSL_CMP_CTX_push1_subjectAltName(OSSL_CMP_CTX *ctx,
                                      const GENERAL_NAME *name);
int OSSL_CMP_CTX_set0_reqExtensions(OSSL_CMP_CTX *ctx, X509_EXTENSIONS *exts);
int OSSL_CMP_CTX_reqExtensions_have_SAN(OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_push0_policy(OSSL_CMP_CTX *ctx, POLICYINFO *pinfo);
int OSSL_CMP_CTX_set1_oldCert(OSSL_CMP_CTX *ctx, X509 *cert);
int OSSL_CMP_CTX_set1_p10CSR(OSSL_CMP_CTX *ctx, const X509_REQ *csr);
/* misc body contents: */
int OSSL_CMP_CTX_push0_genm_ITAV(OSSL_CMP_CTX *ctx, OSSL_CMP_ITAV *itav);
/* certificate confirmation: */
typedef int (*OSSL_CMP_certConf_cb_t) (OSSL_CMP_CTX *ctx, X509 *cert,
                                       int fail_info, const char **txt);
int OSSL_CMP_certConf_cb(OSSL_CMP_CTX *ctx, X509 *cert, int fail_info,
                         const char **text);
int OSSL_CMP_CTX_set_certConf_cb(OSSL_CMP_CTX *ctx, OSSL_CMP_certConf_cb_t cb);
int OSSL_CMP_CTX_set_certConf_cb_arg(OSSL_CMP_CTX *ctx, void *arg);
void *OSSL_CMP_CTX_get_certConf_cb_arg(const OSSL_CMP_CTX *ctx);
/* result fetching: */
int OSSL_CMP_CTX_get_status(const OSSL_CMP_CTX *ctx);
OSSL_CMP_PKIFREETEXT *OSSL_CMP_CTX_get0_statusString(const OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_get_failInfoCode(const OSSL_CMP_CTX *ctx);
#  define OSSL_CMP_PKISI_BUFLEN 1024
X509 *OSSL_CMP_CTX_get0_newCert(const OSSL_CMP_CTX *ctx);
STACK_OF(X509) *OSSL_CMP_CTX_get1_newChain(const OSSL_CMP_CTX *ctx);
STACK_OF(X509) *OSSL_CMP_CTX_get1_caPubs(const OSSL_CMP_CTX *ctx);
STACK_OF(X509) *OSSL_CMP_CTX_get1_extraCertsIn(const OSSL_CMP_CTX *ctx);
int OSSL_CMP_CTX_set1_transactionID(OSSL_CMP_CTX *ctx,
                                    const ASN1_OCTET_STRING *id);
int OSSL_CMP_CTX_set1_senderNonce(OSSL_CMP_CTX *ctx,
                                  const ASN1_OCTET_STRING *nonce);

/* from cmp_status.c */
char *OSSL_CMP_CTX_snprint_PKIStatus(const OSSL_CMP_CTX *ctx, char *buf,
                                     size_t bufsize);
char *OSSL_CMP_snprint_PKIStatusInfo(const OSSL_CMP_PKISI *statusInfo,
                                     char *buf, size_t bufsize);
OSSL_CMP_PKISI *
OSSL_CMP_STATUSINFO_new(int status, int fail_info, const char *text);

/* from cmp_hdr.c */
ASN1_OCTET_STRING *OSSL_CMP_HDR_get0_transactionID(const
                                                   OSSL_CMP_PKIHEADER *hdr);
ASN1_OCTET_STRING *OSSL_CMP_HDR_get0_recipNonce(const OSSL_CMP_PKIHEADER *hdr);

/* from cmp_msg.c */
OSSL_CMP_PKIHEADER *OSSL_CMP_MSG_get0_header(const OSSL_CMP_MSG *msg);
int OSSL_CMP_MSG_get_bodytype(const OSSL_CMP_MSG *msg);
int OSSL_CMP_MSG_update_transactionID(OSSL_CMP_CTX *ctx, OSSL_CMP_MSG *msg);
int OSSL_CMP_MSG_update_recipNonce(OSSL_CMP_CTX *ctx, OSSL_CMP_MSG *msg);
OSSL_CRMF_MSG *OSSL_CMP_CTX_setup_CRM(OSSL_CMP_CTX *ctx, int for_KUR, int rid);
OSSL_CMP_MSG *OSSL_CMP_MSG_read(const char *file, OSSL_LIB_CTX *libctx,
                                const char *propq);
int OSSL_CMP_MSG_write(const char *file, const OSSL_CMP_MSG *msg);
OSSL_CMP_MSG *d2i_OSSL_CMP_MSG_bio(BIO *bio, OSSL_CMP_MSG **msg);
int i2d_OSSL_CMP_MSG_bio(BIO *bio, const OSSL_CMP_MSG *msg);

/* from cmp_vfy.c */
int OSSL_CMP_validate_msg(OSSL_CMP_CTX *ctx, const OSSL_CMP_MSG *msg);
int OSSL_CMP_validate_cert_path(const OSSL_CMP_CTX *ctx,
                                X509_STORE *trusted_store, X509 *cert);

/* from cmp_http.c */
OSSL_CMP_MSG *OSSL_CMP_MSG_http_perform(OSSL_CMP_CTX *ctx,
                                        const OSSL_CMP_MSG *req);

/* from cmp_server.c */
typedef struct ossl_cmp_srv_ctx_st OSSL_CMP_SRV_CTX;
OSSL_CMP_MSG *OSSL_CMP_SRV_process_request(OSSL_CMP_SRV_CTX *srv_ctx,
                                           const OSSL_CMP_MSG *req);
OSSL_CMP_MSG * OSSL_CMP_CTX_server_perform(OSSL_CMP_CTX *client_ctx,
                                           const OSSL_CMP_MSG *req);
OSSL_CMP_SRV_CTX *OSSL_CMP_SRV_CTX_new(OSSL_LIB_CTX *libctx, const char *propq);
void OSSL_CMP_SRV_CTX_free(OSSL_CMP_SRV_CTX *srv_ctx);
typedef OSSL_CMP_PKISI *(*OSSL_CMP_SRV_cert_request_cb_t)
    (OSSL_CMP_SRV_CTX *srv_ctx, const OSSL_CMP_MSG *req, int certReqId,
     const OSSL_CRMF_MSG *crm, const X509_REQ *p10cr,
     X509 **certOut, STACK_OF(X509) **chainOut, STACK_OF(X509) **caPubs);
typedef OSSL_CMP_PKISI *(*OSSL_CMP_SRV_rr_cb_t)(OSSL_CMP_SRV_CTX *srv_ctx,
                                                const OSSL_CMP_MSG *req,
                                                const X509_NAME *issuer,
                                                const ASN1_INTEGER *serial);
typedef int (*OSSL_CMP_SRV_genm_cb_t)(OSSL_CMP_SRV_CTX *srv_ctx,
                                      const OSSL_CMP_MSG *req,
                                      const STACK_OF(OSSL_CMP_ITAV) *in,
                                      STACK_OF(OSSL_CMP_ITAV) **out);
typedef void (*OSSL_CMP_SRV_error_cb_t)(OSSL_CMP_SRV_CTX *srv_ctx,
                                        const OSSL_CMP_MSG *req,
                                        const OSSL_CMP_PKISI *statusInfo,
                                        const ASN1_INTEGER *errorCode,
                                        const OSSL_CMP_PKIFREETEXT *errDetails);
typedef int (*OSSL_CMP_SRV_certConf_cb_t)(OSSL_CMP_SRV_CTX *srv_ctx,
                                          const OSSL_CMP_MSG *req,
                                          int certReqId,
                                          const ASN1_OCTET_STRING *certHash,
                                          const OSSL_CMP_PKISI *si);
typedef int (*OSSL_CMP_SRV_pollReq_cb_t)(OSSL_CMP_SRV_CTX *srv_ctx,
                                         const OSSL_CMP_MSG *req, int certReqId,
                                         OSSL_CMP_MSG **certReq,
                                         int64_t *check_after);
int OSSL_CMP_SRV_CTX_init(OSSL_CMP_SRV_CTX *srv_ctx, void *custom_ctx,
                          OSSL_CMP_SRV_cert_request_cb_t process_cert_request,
                          OSSL_CMP_SRV_rr_cb_t process_rr,
                          OSSL_CMP_SRV_genm_cb_t process_genm,
                          OSSL_CMP_SRV_error_cb_t process_error,
                          OSSL_CMP_SRV_certConf_cb_t process_certConf,
                          OSSL_CMP_SRV_pollReq_cb_t process_pollReq);
OSSL_CMP_CTX *OSSL_CMP_SRV_CTX_get0_cmp_ctx(const OSSL_CMP_SRV_CTX *srv_ctx);
void *OSSL_CMP_SRV_CTX_get0_custom_ctx(const OSSL_CMP_SRV_CTX *srv_ctx);
int OSSL_CMP_SRV_CTX_set_send_unprotected_errors(OSSL_CMP_SRV_CTX *srv_ctx,
                                                 int val);
int OSSL_CMP_SRV_CTX_set_accept_unprotected(OSSL_CMP_SRV_CTX *srv_ctx, int val);
int OSSL_CMP_SRV_CTX_set_accept_raverified(OSSL_CMP_SRV_CTX *srv_ctx, int val);
int OSSL_CMP_SRV_CTX_set_grant_implicit_confirm(OSSL_CMP_SRV_CTX *srv_ctx,
                                                int val);

/* from cmp_client.c */
X509 *OSSL_CMP_exec_certreq(OSSL_CMP_CTX *ctx, int req_type,
                            const OSSL_CRMF_MSG *crm);
#  define OSSL_CMP_IR    0
#  define OSSL_CMP_CR    2
#  define OSSL_CMP_P10CR 4
#  define OSSL_CMP_KUR   7
#  define OSSL_CMP_exec_IR_ses(ctx) \
    OSSL_CMP_exec_certreq(ctx, OSSL_CMP_IR, NULL)
#  define OSSL_CMP_exec_CR_ses(ctx) \
    OSSL_CMP_exec_certreq(ctx, OSSL_CMP_CR, NULL)
#  define OSSL_CMP_exec_P10CR_ses(ctx) \
    OSSL_CMP_exec_certreq(ctx, OSSL_CMP_P10CR, NULL)
#  define OSSL_CMP_exec_KUR_ses(ctx) \
    OSSL_CMP_exec_certreq(ctx, OSSL_CMP_KUR, NULL)
int OSSL_CMP_try_certreq(OSSL_CMP_CTX *ctx, int req_type,
                         const OSSL_CRMF_MSG *crm, int *checkAfter);
int OSSL_CMP_exec_RR_ses(OSSL_CMP_CTX *ctx);
STACK_OF(OSSL_CMP_ITAV) *OSSL_CMP_exec_GENM_ses(OSSL_CMP_CTX *ctx);

#  ifdef  __cplusplus
}
#  endif
# endif /* !defined(OPENSSL_NO_CMP) */
#endif /* !defined(OPENSSL_CMP_H) */
                                                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/openssl/config/archs/solaris64-x86_64-gcc/asm/include/openssl/cms.h                0000664 0000000 0000000 00000102441 14746647661 0027677 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        /*
 * WARNING: do not edit!
 * Generated by Makefile from include/openssl/cms.h.in
 *
 * Copyright 2008-2021 The OpenSSL Project Authors. All Rights Reserved.
 *
 * Licensed under the Apache License 2.0 (the "License").  You may not use
 * this file except in compliance with the License.  You can obtain a copy
 * in the file LICENSE in the source distribution or at
 * https://www.openssl.org/source/license.html
 */



#ifndef OPENSSL_CMS_H
# define OPENSSL_CMS_H
# pragma once

# include <openssl/macros.h>
# ifndef OPENSSL_NO_DEPRECATED_3_0
#  define HEADER_CMS_H
# endif

# include <openssl/opensslconf.h>

# ifndef OPENSSL_NO_CMS
# include <openssl/x509.h>
# include <openssl/x509v3.h>
# include <openssl/cmserr.h>
# ifdef __cplusplus
extern "C" {
# endif

typedef struct CMS_ContentInfo_st CMS_ContentInfo;
typedef struct CMS_SignerInfo_st CMS_SignerInfo;
typedef struct CMS_CertificateChoices CMS_CertificateChoices;
typedef struct CMS_RevocationInfoChoice_st CMS_RevocationInfoChoice;
typedef struct CMS_RecipientInfo_st CMS_RecipientInfo;
typedef struct CMS_ReceiptRequest_st CMS_ReceiptRequest;
typedef struct CMS_Receipt_st CMS_Receipt;
typedef struct CMS_RecipientEncryptedKey_st CMS_RecipientEncryptedKey;
typedef struct CMS_OtherKeyAttribute_st CMS_OtherKeyAttribute;

SKM_DEFINE_STACK_OF_INTERNAL(CMS_SignerInfo, CMS_SignerInfo, CMS_SignerInfo)
#define sk_CMS_SignerInfo_num(sk) OPENSSL_sk_num(ossl_check_const_CMS_SignerInfo_sk_type(sk))
#define sk_CMS_SignerInfo_value(sk, idx) ((CMS_SignerInfo *)OPENSSL_sk_value(ossl_check_const_CMS_SignerInfo_sk_type(sk), (idx)))
#define sk_CMS_SignerInfo_new(cmp) ((STACK_OF(CMS_SignerInfo) *)OPENSSL_sk_new(ossl_check_CMS_SignerInfo_compfunc_type(cmp)))
#define sk_CMS_SignerInfo_new_null() ((STACK_OF(CMS_SignerInfo) *)OPENSSL_sk_new_null())
#define sk_CMS_SignerInfo_new_reserve(cmp, n) ((STACK_OF(CMS_SignerInfo) *)OPENSSL_sk_new_reserve(ossl_check_CMS_SignerInfo_compfunc_type(cmp), (n)))
#define sk_CMS_SignerInfo_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_CMS_SignerInfo_sk_type(sk), (n))
#define sk_CMS_SignerInfo_free(sk) OPENSSL_sk_free(ossl_check_CMS_SignerInfo_sk_type(sk))
#define sk_CMS_SignerInfo_zero(sk) OPENSSL_sk_zero(ossl_check_CMS_SignerInfo_sk_type(sk))
#define sk_CMS_SignerInfo_delete(sk, i) ((CMS_SignerInfo *)OPENSSL_sk_delete(ossl_check_CMS_SignerInfo_sk_type(sk), (i)))
#define sk_CMS_SignerInfo_delete_ptr(sk, ptr) ((CMS_SignerInfo *)OPENSSL_sk_delete_ptr(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr)))
#define sk_CMS_SignerInfo_push(sk, ptr) OPENSSL_sk_push(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr))
#define sk_CMS_SignerInfo_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr))
#define sk_CMS_SignerInfo_pop(sk) ((CMS_SignerInfo *)OPENSSL_sk_pop(ossl_check_CMS_SignerInfo_sk_type(sk)))
#define sk_CMS_SignerInfo_shift(sk) ((CMS_SignerInfo *)OPENSSL_sk_shift(ossl_check_CMS_SignerInfo_sk_type(sk)))
#define sk_CMS_SignerInfo_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_CMS_SignerInfo_sk_type(sk),ossl_check_CMS_SignerInfo_freefunc_type(freefunc))
#define sk_CMS_SignerInfo_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr), (idx))
#define sk_CMS_SignerInfo_set(sk, idx, ptr) ((CMS_SignerInfo *)OPENSSL_sk_set(ossl_check_CMS_SignerInfo_sk_type(sk), (idx), ossl_check_CMS_SignerInfo_type(ptr)))
#define sk_CMS_SignerInfo_find(sk, ptr) OPENSSL_sk_find(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr))
#define sk_CMS_SignerInfo_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr))
#define sk_CMS_SignerInfo_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_type(ptr), pnum)
#define sk_CMS_SignerInfo_sort(sk) OPENSSL_sk_sort(ossl_check_CMS_SignerInfo_sk_type(sk))
#define sk_CMS_SignerInfo_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_CMS_SignerInfo_sk_type(sk))
#define sk_CMS_SignerInfo_dup(sk) ((STACK_OF(CMS_SignerInfo) *)OPENSSL_sk_dup(ossl_check_const_CMS_SignerInfo_sk_type(sk)))
#define sk_CMS_SignerInfo_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(CMS_SignerInfo) *)OPENSSL_sk_deep_copy(ossl_check_const_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_copyfunc_type(copyfunc), ossl_check_CMS_SignerInfo_freefunc_type(freefunc)))
#define sk_CMS_SignerInfo_set_cmp_func(sk, cmp) ((sk_CMS_SignerInfo_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_CMS_SignerInfo_sk_type(sk), ossl_check_CMS_SignerInfo_compfunc_type(cmp)))
SKM_DEFINE_STACK_OF_INTERNAL(CMS_RecipientEncryptedKey, CMS_RecipientEncryptedKey, CMS_RecipientEncryptedKey)
#define sk_CMS_RecipientEncryptedKey_num(sk) OPENSSL_sk_num(ossl_check_const_CMS_RecipientEncryptedKey_sk_type(sk))
#define sk_CMS_RecipientEncryptedKey_value(sk, idx) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_value(ossl_check_const_CMS_RecipientEncryptedKey_sk_type(sk), (idx)))
#define sk_CMS_RecipientEncryptedKey_new(cmp) ((STACK_OF(CMS_RecipientEncryptedKey) *)OPENSSL_sk_new(ossl_check_CMS_RecipientEncryptedKey_compfunc_type(cmp)))
#define sk_CMS_RecipientEncryptedKey_new_null() ((STACK_OF(CMS_RecipientEncryptedKey) *)OPENSSL_sk_new_null())
#define sk_CMS_RecipientEncryptedKey_new_reserve(cmp, n) ((STACK_OF(CMS_RecipientEncryptedKey) *)OPENSSL_sk_new_reserve(ossl_check_CMS_RecipientEncryptedKey_compfunc_type(cmp), (n)))
#define sk_CMS_RecipientEncryptedKey_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), (n))
#define sk_CMS_RecipientEncryptedKey_free(sk) OPENSSL_sk_free(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk))
#define sk_CMS_RecipientEncryptedKey_zero(sk) OPENSSL_sk_zero(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk))
#define sk_CMS_RecipientEncryptedKey_delete(sk, i) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_delete(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), (i)))
#define sk_CMS_RecipientEncryptedKey_delete_ptr(sk, ptr) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_delete_ptr(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr)))
#define sk_CMS_RecipientEncryptedKey_push(sk, ptr) OPENSSL_sk_push(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr))
#define sk_CMS_RecipientEncryptedKey_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr))
#define sk_CMS_RecipientEncryptedKey_pop(sk) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_pop(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk)))
#define sk_CMS_RecipientEncryptedKey_shift(sk) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_shift(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk)))
#define sk_CMS_RecipientEncryptedKey_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk),ossl_check_CMS_RecipientEncryptedKey_freefunc_type(freefunc))
#define sk_CMS_RecipientEncryptedKey_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr), (idx))
#define sk_CMS_RecipientEncryptedKey_set(sk, idx, ptr) ((CMS_RecipientEncryptedKey *)OPENSSL_sk_set(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), (idx), ossl_check_CMS_RecipientEncryptedKey_type(ptr)))
#define sk_CMS_RecipientEncryptedKey_find(sk, ptr) OPENSSL_sk_find(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr))
#define sk_CMS_RecipientEncryptedKey_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr))
#define sk_CMS_RecipientEncryptedKey_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_type(ptr), pnum)
#define sk_CMS_RecipientEncryptedKey_sort(sk) OPENSSL_sk_sort(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk))
#define sk_CMS_RecipientEncryptedKey_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_CMS_RecipientEncryptedKey_sk_type(sk))
#define sk_CMS_RecipientEncryptedKey_dup(sk) ((STACK_OF(CMS_RecipientEncryptedKey) *)OPENSSL_sk_dup(ossl_check_const_CMS_RecipientEncryptedKey_sk_type(sk)))
#define sk_CMS_RecipientEncryptedKey_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(CMS_RecipientEncryptedKey) *)OPENSSL_sk_deep_copy(ossl_check_const_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_copyfunc_type(copyfunc), ossl_check_CMS_RecipientEncryptedKey_freefunc_type(freefunc)))
#define sk_CMS_RecipientEncryptedKey_set_cmp_func(sk, cmp) ((sk_CMS_RecipientEncryptedKey_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_CMS_RecipientEncryptedKey_sk_type(sk), ossl_check_CMS_RecipientEncryptedKey_compfunc_type(cmp)))
SKM_DEFINE_STACK_OF_INTERNAL(CMS_RecipientInfo, CMS_RecipientInfo, CMS_RecipientInfo)
#define sk_CMS_RecipientInfo_num(sk) OPENSSL_sk_num(ossl_check_const_CMS_RecipientInfo_sk_type(sk))
#define sk_CMS_RecipientInfo_value(sk, idx) ((CMS_RecipientInfo *)OPENSSL_sk_value(ossl_check_const_CMS_RecipientInfo_sk_type(sk), (idx)))
#define sk_CMS_RecipientInfo_new(cmp) ((STACK_OF(CMS_RecipientInfo) *)OPENSSL_sk_new(ossl_check_CMS_RecipientInfo_compfunc_type(cmp)))
#define sk_CMS_RecipientInfo_new_null() ((STACK_OF(CMS_RecipientInfo) *)OPENSSL_sk_new_null())
#define sk_CMS_RecipientInfo_new_reserve(cmp, n) ((STACK_OF(CMS_RecipientInfo) *)OPENSSL_sk_new_reserve(ossl_check_CMS_RecipientInfo_compfunc_type(cmp), (n)))
#define sk_CMS_RecipientInfo_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_CMS_RecipientInfo_sk_type(sk), (n))
#define sk_CMS_RecipientInfo_free(sk) OPENSSL_sk_free(ossl_check_CMS_RecipientInfo_sk_type(sk))
#define sk_CMS_RecipientInfo_zero(sk) OPENSSL_sk_zero(ossl_check_CMS_RecipientInfo_sk_type(sk))
#define sk_CMS_RecipientInfo_delete(sk, i) ((CMS_RecipientInfo *)OPENSSL_sk_delete(ossl_check_CMS_RecipientInfo_sk_type(sk), (i)))
#define sk_CMS_RecipientInfo_delete_ptr(sk, ptr) ((CMS_RecipientInfo *)OPENSSL_sk_delete_ptr(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr)))
#define sk_CMS_RecipientInfo_push(sk, ptr) OPENSSL_sk_push(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr))
#define sk_CMS_RecipientInfo_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr))
#define sk_CMS_RecipientInfo_pop(sk) ((CMS_RecipientInfo *)OPENSSL_sk_pop(ossl_check_CMS_RecipientInfo_sk_type(sk)))
#define sk_CMS_RecipientInfo_shift(sk) ((CMS_RecipientInfo *)OPENSSL_sk_shift(ossl_check_CMS_RecipientInfo_sk_type(sk)))
#define sk_CMS_RecipientInfo_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_CMS_RecipientInfo_sk_type(sk),ossl_check_CMS_RecipientInfo_freefunc_type(freefunc))
#define sk_CMS_RecipientInfo_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr), (idx))
#define sk_CMS_RecipientInfo_set(sk, idx, ptr) ((CMS_RecipientInfo *)OPENSSL_sk_set(ossl_check_CMS_RecipientInfo_sk_type(sk), (idx), ossl_check_CMS_RecipientInfo_type(ptr)))
#define sk_CMS_RecipientInfo_find(sk, ptr) OPENSSL_sk_find(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr))
#define sk_CMS_RecipientInfo_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr))
#define sk_CMS_RecipientInfo_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_type(ptr), pnum)
#define sk_CMS_RecipientInfo_sort(sk) OPENSSL_sk_sort(ossl_check_CMS_RecipientInfo_sk_type(sk))
#define sk_CMS_RecipientInfo_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_CMS_RecipientInfo_sk_type(sk))
#define sk_CMS_RecipientInfo_dup(sk) ((STACK_OF(CMS_RecipientInfo) *)OPENSSL_sk_dup(ossl_check_const_CMS_RecipientInfo_sk_type(sk)))
#define sk_CMS_RecipientInfo_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(CMS_RecipientInfo) *)OPENSSL_sk_deep_copy(ossl_check_const_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_copyfunc_type(copyfunc), ossl_check_CMS_RecipientInfo_freefunc_type(freefunc)))
#define sk_CMS_RecipientInfo_set_cmp_func(sk, cmp) ((sk_CMS_RecipientInfo_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_CMS_RecipientInfo_sk_type(sk), ossl_check_CMS_RecipientInfo_compfunc_type(cmp)))
SKM_DEFINE_STACK_OF_INTERNAL(CMS_RevocationInfoChoice, CMS_RevocationInfoChoice, CMS_RevocationInfoChoice)
#define sk_CMS_RevocationInfoChoice_num(sk) OPENSSL_sk_num(ossl_check_const_CMS_RevocationInfoChoice_sk_type(sk))
#define sk_CMS_RevocationInfoChoice_value(sk, idx) ((CMS_RevocationInfoChoice *)OPENSSL_sk_value(ossl_check_const_CMS_RevocationInfoChoice_sk_type(sk), (idx)))
#define sk_CMS_RevocationInfoChoice_new(cmp) ((STACK_OF(CMS_RevocationInfoChoice) *)OPENSSL_sk_new(ossl_check_CMS_RevocationInfoChoice_compfunc_type(cmp)))
#define sk_CMS_RevocationInfoChoice_new_null() ((STACK_OF(CMS_RevocationInfoChoice) *)OPENSSL_sk_new_null())
#define sk_CMS_RevocationInfoChoice_new_reserve(cmp, n) ((STACK_OF(CMS_RevocationInfoChoice) *)OPENSSL_sk_new_reserve(ossl_check_CMS_RevocationInfoChoice_compfunc_type(cmp), (n)))
#define sk_CMS_RevocationInfoChoice_reserve(sk, n) OPENSSL_sk_reserve(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), (n))
#define sk_CMS_RevocationInfoChoice_free(sk) OPENSSL_sk_free(ossl_check_CMS_RevocationInfoChoice_sk_type(sk))
#define sk_CMS_RevocationInfoChoice_zero(sk) OPENSSL_sk_zero(ossl_check_CMS_RevocationInfoChoice_sk_type(sk))
#define sk_CMS_RevocationInfoChoice_delete(sk, i) ((CMS_RevocationInfoChoice *)OPENSSL_sk_delete(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), (i)))
#define sk_CMS_RevocationInfoChoice_delete_ptr(sk, ptr) ((CMS_RevocationInfoChoice *)OPENSSL_sk_delete_ptr(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr)))
#define sk_CMS_RevocationInfoChoice_push(sk, ptr) OPENSSL_sk_push(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr))
#define sk_CMS_RevocationInfoChoice_unshift(sk, ptr) OPENSSL_sk_unshift(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr))
#define sk_CMS_RevocationInfoChoice_pop(sk) ((CMS_RevocationInfoChoice *)OPENSSL_sk_pop(ossl_check_CMS_RevocationInfoChoice_sk_type(sk)))
#define sk_CMS_RevocationInfoChoice_shift(sk) ((CMS_RevocationInfoChoice *)OPENSSL_sk_shift(ossl_check_CMS_RevocationInfoChoice_sk_type(sk)))
#define sk_CMS_RevocationInfoChoice_pop_free(sk, freefunc) OPENSSL_sk_pop_free(ossl_check_CMS_RevocationInfoChoice_sk_type(sk),ossl_check_CMS_RevocationInfoChoice_freefunc_type(freefunc))
#define sk_CMS_RevocationInfoChoice_insert(sk, ptr, idx) OPENSSL_sk_insert(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr), (idx))
#define sk_CMS_RevocationInfoChoice_set(sk, idx, ptr) ((CMS_RevocationInfoChoice *)OPENSSL_sk_set(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), (idx), ossl_check_CMS_RevocationInfoChoice_type(ptr)))
#define sk_CMS_RevocationInfoChoice_find(sk, ptr) OPENSSL_sk_find(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr))
#define sk_CMS_RevocationInfoChoice_find_ex(sk, ptr) OPENSSL_sk_find_ex(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr))
#define sk_CMS_RevocationInfoChoice_find_all(sk, ptr, pnum) OPENSSL_sk_find_all(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_type(ptr), pnum)
#define sk_CMS_RevocationInfoChoice_sort(sk) OPENSSL_sk_sort(ossl_check_CMS_RevocationInfoChoice_sk_type(sk))
#define sk_CMS_RevocationInfoChoice_is_sorted(sk) OPENSSL_sk_is_sorted(ossl_check_const_CMS_RevocationInfoChoice_sk_type(sk))
#define sk_CMS_RevocationInfoChoice_dup(sk) ((STACK_OF(CMS_RevocationInfoChoice) *)OPENSSL_sk_dup(ossl_check_const_CMS_RevocationInfoChoice_sk_type(sk)))
#define sk_CMS_RevocationInfoChoice_deep_copy(sk, copyfunc, freefunc) ((STACK_OF(CMS_RevocationInfoChoice) *)OPENSSL_sk_deep_copy(ossl_check_const_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_copyfunc_type(copyfunc), ossl_check_CMS_RevocationInfoChoice_freefunc_type(freefunc)))
#define sk_CMS_RevocationInfoChoice_set_cmp_func(sk, cmp) ((sk_CMS_RevocationInfoChoice_compfunc)OPENSSL_sk_set_cmp_func(ossl_check_CMS_RevocationInfoChoice_sk_type(sk), ossl_check_CMS_RevocationInfoChoice_compfunc_type(cmp)))


DECLARE_ASN1_FUNCTIONS(CMS_ContentInfo)
DECLARE_ASN1_FUNCTIONS(CMS_ReceiptRequest)
DECLARE_ASN1_PRINT_FUNCTION(CMS_ContentInfo)

CMS_ContentInfo *CMS_ContentInfo_new_ex(OSSL_LIB_CTX *libctx, const char *propq);

# define CMS_SIGNERINFO_ISSUER_SERIAL    0
# define CMS_SIGNERINFO_KEYIDENTIFIER    1

# define CMS_RECIPINFO_NONE              -1
# define CMS_RECIPINFO_TRANS             0
# define CMS_RECIPINFO_AGREE             1
# define CMS_RECIPINFO_KEK               2
# define CMS_RECIPINFO_PASS              3
# define CMS_RECIPINFO_OTHER             4

/* S/MIME related flags */

# define CMS_TEXT                        0x1
# define CMS_NOCERTS                     0x2
# define CMS_NO_CONTENT_VERIFY           0x4
# define CMS_NO_ATTR_VERIFY              0x8
# define CMS_NOSIGS                      \
                        (CMS_NO_CONTENT_VERIFY|CMS_NO_ATTR_VERIFY)
# define CMS_NOINTERN                    0x10
# define CMS_NO_SIGNER_CERT_VERIFY       0x20
# define CMS_NOVERIFY                    0x20
# define CMS_DETACHED                    0x40
# define CMS_BINARY                      0x80
# define CMS_NOATTR                      0x100
# define CMS_NOSMIMECAP                  0x200
# define CMS_NOOLDMIMETYPE               0x400
# define CMS_CRLFEOL                     0x800
# define CMS_STREAM                      0x1000
# define CMS_NOCRL                       0x2000
# define CMS_PARTIAL                     0x4000
# define CMS_REUSE_DIGEST                0x8000
# define CMS_USE_KEYID                   0x10000
# define CMS_DEBUG_DECRYPT               0x20000
# define CMS_KEY_PARAM                   0x40000
# define CMS_ASCIICRLF                   0x80000
# define CMS_CADES                       0x100000
# define CMS_USE_ORIGINATOR_KEYID        0x200000

const ASN1_OBJECT *CMS_get0_type(const CMS_ContentInfo *cms);

BIO *CMS_dataInit(CMS_ContentInfo *cms, BIO *icont);
int CMS_dataFinal(CMS_ContentInfo *cms, BIO *bio);

ASN1_OCTET_STRING **CMS_get0_content(CMS_ContentInfo *cms);
int CMS_is_detached(CMS_ContentInfo *cms);
int CMS_set_detached(CMS_ContentInfo *cms, int detached);

# ifdef OPENSSL_PEM_H
DECLARE_PEM_rw(CMS, CMS_ContentInfo)
# endif
int CMS_stream(unsigned char ***boundary, CMS_ContentInfo *cms);
CMS_ContentInfo *d2i_CMS_bio(BIO *bp, CMS_ContentInfo **cms);
int i2d_CMS_bio(BIO *bp, CMS_ContentInfo *cms);

BIO *BIO_new_CMS(BIO *out, CMS_ContentInfo *cms);
int i2d_CMS_bio_stream(BIO *out, CMS_ContentInfo *cms, BIO *in, int flags);
int PEM_write_bio_CMS_stream(BIO *out, CMS_ContentInfo *cms, BIO *in,
                             int flags);
CMS_ContentInfo *SMIME_read_CMS(BIO *bio, BIO **bcont);
CMS_ContentInfo *SMIME_read_CMS_ex(BIO *bio, int flags, BIO **bcont, CMS_ContentInfo **ci);
int SMIME_write_CMS(BIO *bio, CMS_ContentInfo *cms, BIO *data, int flags);

int CMS_final(CMS_ContentInfo *cms, BIO *data, BIO *dcont,
              unsigned int flags);

CMS_ContentInfo *CMS_sign(X509 *signcert, EVP_PKEY *pkey,
                          STACK_OF(X509) *certs, BIO *data,
                          unsigned int flags);
CMS_ContentInfo *CMS_sign_ex(X509 *signcert, EVP_PKEY *pkey,
                             STACK_OF(X509) *certs, BIO *data,
                             unsigned int flags, OSSL_LIB_CTX *ctx,
                             const char *propq);

CMS_ContentInfo *CMS_sign_receipt(CMS_SignerInfo *si,
                                  X509 *signcert, EVP_PKEY *pkey,
                                  STACK_OF(X509) *certs, unsigned int flags);

int CMS_data(CMS_ContentInfo *cms, BIO *out, unsigned int flags);
CMS_ContentInfo *CMS_data_create(BIO *in, unsigned int flags);
CMS_ContentInfo *CMS_data_create_ex(BIO *in, unsigned int flags,
                                    OSSL_LIB_CTX *ctx, const char *propq);

int CMS_digest_verify(CMS_ContentInfo *cms, BIO *dcont, BIO *out,
                      unsigned int flags);
CMS_ContentInfo *CMS_digest_create(BIO *in, const EVP_MD *md,
                                   unsigned int flags);
CMS_ContentInfo *CMS_digest_create_ex(BIO *in, const EVP_MD *md,
                                      unsigned int flags, OSSL_LIB_CTX *ctx,
                                      const char *propq);

int CMS_EncryptedData_decrypt(CMS_ContentInfo *cms,
                              const unsigned char *key, size_t keylen,
                              BIO *dcont, BIO *out, unsigned int flags);

CMS_ContentInfo *CMS_EncryptedData_encrypt(BIO *in, const EVP_CIPHER *cipher,
                                           const unsigned char *key,
                                           size_t keylen, unsigned int flags);
CMS_ContentInfo *CMS_EncryptedData_encrypt_ex(BIO *in, const EVP_CIPHER *cipher,
                                              const unsigned char *key,
                                              size_t keylen, unsigned int fl