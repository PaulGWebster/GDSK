wer of 2 (>= 4).
  void Align(int m);
  // Insert the smallest number of zero bytes possible to align the pc offset
  // to a mulitple of m. m must be a power of 2 (>= 2).
  void DataAlign(int m);
  // Aligns code to something that's optimal for a jump target for the platform.
  void CodeTargetAlign();
  void LoopHeaderAlign() { CodeTargetAlign(); }

  // Branch instructions
  void bclr(BOfield bo, int condition_bit, LKBit lk);
  void blr();
  void bc(int branch_offset, BOfield bo, int condition_bit, LKBit lk = LeaveLK);
  void b(int branch_offset, LKBit lk);

  void bcctr(BOfield bo, int condition_bit, LKBit lk);
  void bctr();
  void bctrl();

  // Convenience branch instructions using labels
  void b(Label* L, LKBit lk = LeaveLK) { b(branch_offset(L), lk); }

  inline CRegister cmpi_optimization(CRegister cr) {
    // Check whether the branch is preceded by an optimizable cmpi against 0.
    // The cmpi can be deleted if it is also preceded by an instruction that
    // sets the register used by the compare and supports a dot form.
    unsigned int sradi_mask = kOpcodeMask | kExt2OpcodeVariant2Mask;
    unsigned int srawi_mask = kOpcodeMask | kExt2OpcodeMask;
    int pos = pc_offset();
    int cmpi_pos = pc_offset() - kInstrSize;

    if (cmpi_pos > 0 && optimizable_cmpi_pos_ == cmpi_pos &&
        cmpi_cr_.code() == cr.code() && last_bound_pos_ != pos) {
      int xpos = cmpi_pos - kInstrSize;
      int xinstr = instr_at(xpos);
      int cmpi_ra = (instr_at(cmpi_pos) & 0x1f0000) >> 16;
      // ra is at the same bit position for the three cases below.
      int ra = (xinstr & 0x1f0000) >> 16;
      if (cmpi_ra == ra) {
        if ((xinstr & sradi_mask) == (EXT2 | SRADIX)) {
          cr = cr0;
          instr_at_put(xpos, xinstr | SetRC);
          pc_ -= kInstrSize;
        } else if ((xinstr & srawi_mask) == (EXT2 | SRAWIX)) {
          cr = cr0;
          instr_at_put(xpos, xinstr | SetRC);
          pc_ -= kInstrSize;
        } else if ((xinstr & kOpcodeMask) == ANDIx) {
          cr = cr0;
          pc_ -= kInstrSize;
          // nothing to do here since andi. records.
        }
        // didn't match one of the above, must keep cmpwi.
      }
    }
    return cr;
  }

  void bc_short(Condition cond, Label* L, CRegister cr = cr7,
                LKBit lk = LeaveLK) {
    DCHECK(cond != al);
    DCHECK(cr.code() >= 0 && cr.code() <= 7);

    cr = cmpi_optimization(cr);

    int b_offset = branch_offset(L);

    switch (cond) {
      case eq:
        bc(b_offset, BT, encode_crbit(cr, CR_EQ), lk);
        break;
      case ne:
        bc(b_offset, BF, encode_crbit(cr, CR_EQ), lk);
        break;
      case gt:
        bc(b_offset, BT, encode_crbit(cr, CR_GT), lk);
        break;
      case le:
        bc(b_offset, BF, encode_crbit(cr, CR_GT), lk);
        break;
      case lt:
        bc(b_offset, BT, encode_crbit(cr, CR_LT), lk);
        break;
      case ge:
        bc(b_offset, BF, encode_crbit(cr, CR_LT), lk);
        break;
      case unordered:
        bc(b_offset, BT, encode_crbit(cr, CR_FU), lk);
        break;
      case ordered:
        bc(b_offset, BF, encode_crbit(cr, CR_FU), lk);
        break;
      case overflow:
        bc(b_offset, BT, encode_crbit(cr, CR_SO), lk);
        break;
      case nooverflow:
        bc(b_offset, BF, encode_crbit(cr, CR_SO), lk);
        break;
      default:
        UNIMPLEMENTED();
    }
  }

  void bclr(Condition cond, CRegister cr = cr7, LKBit lk = LeaveLK) {
    DCHECK(cond != al);
    DCHECK(cr.code() >= 0 && cr.code() <= 7);

    cr = cmpi_optimization(cr);

    switch (cond) {
      case eq:
        bclr(BT, encode_crbit(cr, CR_EQ), lk);
        break;
      case ne:
        bclr(BF, encode_crbit(cr, CR_EQ), lk);
        break;
      case gt:
        bclr(BT, encode_crbit(cr, CR_GT), lk);
        break;
      case le:
        bclr(BF, encode_crbit(cr, CR_GT), lk);
        break;
      case lt:
        bclr(BT, encode_crbit(cr, CR_LT), lk);
        break;
      case ge:
        bclr(BF, encode_crbit(cr, CR_LT), lk);
        break;
      case unordered:
        bclr(BT, encode_crbit(cr, CR_FU), lk);
        break;
      case ordered:
        bclr(BF, encode_crbit(cr, CR_FU), lk);
        break;
      case overflow:
        bclr(BT, encode_crbit(cr, CR_SO), lk);
        break;
      case nooverflow:
        bclr(BF, encode_crbit(cr, CR_SO), lk);
        break;
      default:
        UNIMPLEMENTED();
    }
  }

  void isel(Register rt, Register ra, Register rb, int cb);
  void isel(Condition cond, Register rt, Register ra, Register rb,
            CRegister cr = cr7) {
    DCHECK(cond != al);
    DCHECK(cr.code() >= 0 && cr.code() <= 7);

    cr = cmpi_optimization(cr);

    switch (cond) {
      case eq:
        isel(rt, ra, rb, encode_crbit(cr, CR_EQ));
        break;
      case ne:
        isel(rt, rb, ra, encode_crbit(cr, CR_EQ));
        break;
      case gt:
        isel(rt, ra, rb, encode_crbit(cr, CR_GT));
        break;
      case le:
        isel(rt, rb, ra, encode_crbit(cr, CR_GT));
        break;
      case lt:
        isel(rt, ra, rb, encode_crbit(cr, CR_LT));
        break;
      case ge:
        isel(rt, rb, ra, encode_crbit(cr, CR_LT));
        break;
      case unordered:
        isel(rt, ra, rb, encode_crbit(cr, CR_FU));
        break;
      case ordered:
        isel(rt, rb, ra, encode_crbit(cr, CR_FU));
        break;
      case overflow:
        isel(rt, ra, rb, encode_crbit(cr, CR_SO));
        break;
      case nooverflow:
        isel(rt, rb, ra, encode_crbit(cr, CR_SO));
        break;
      default:
        UNIMPLEMENTED();
    }
  }

  void b(Condition cond, Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    if (cond == al) {
      b(L, lk);
      return;
    }

    if ((L->is_bound() && is_near(L, cond))) {
      bc_short(cond, L, cr, lk);
      return;
    }

    Label skip;
    Condition neg_cond = NegateCondition(cond);
    bc_short(neg_cond, &skip, cr);
    b(L, lk);
    bind(&skip);
  }

  void bne(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(ne, L, cr, lk);
  }
  void beq(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(eq, L, cr, lk);
  }
  void blt(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(lt, L, cr, lk);
  }
  void bge(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(ge, L, cr, lk);
  }
  void ble(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(le, L, cr, lk);
  }
  void bgt(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(gt, L, cr, lk);
  }
  void bunordered(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(unordered, L, cr, lk);
  }
  void bordered(Label* L, CRegister cr = cr7, LKBit lk = LeaveLK) {
    b(ordered, L, cr, lk);
  }
  void boverflow(Label* L, CRegister cr = cr0, LKBit lk = LeaveLK) {
    b(overflow, L, cr, lk);
  }
  void bnooverflow(Label* L, CRegister cr = cr0, LKBit lk = LeaveLK) {
    b(nooverflow, L, cr, lk);
  }

  // Decrement CTR; branch if CTR != 0
  void bdnz(Label* L, LKBit lk = LeaveLK) {
    bc(branch_offset(L), DCBNZ, 0, lk);
  }

  // Data-processing instructions

  void sub(Register dst, Register src1, Register src2, OEBit s = LeaveOE,
           RCBit r = LeaveRC);

  void subc(Register dst, Register src1, Register src2, OEBit s = LeaveOE,
            RCBit r = LeaveRC);
  void sube(Register dst, Register src1, Register src2, OEBit s = LeaveOE,
            RCBit r = LeaveRC);

  void subfic(Register dst, Register src, const Operand& imm);

  void add(Register dst, Register src1, Register src2, OEBit s = LeaveOE,
           RCBit r = LeaveRC);

  void addc(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
            RCBit r = LeaveRC);
  void adde(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
            RCBit r = LeaveRC);
  void addze(Register dst, Register src1, OEBit o = LeaveOE, RCBit r = LeaveRC);

  void mullw(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
             RCBit r = LeaveRC);

  void mulhw(Register dst, Register src1, Register src2, RCBit r = LeaveRC);
  void mulhwu(Register dst, Register src1, Register src2, RCBit r = LeaveRC);
  void mulhd(Register dst, Register src1, Register src2, RCBit r = LeaveRC);
  void mulhdu(Register dst, Register src1, Register src2, RCBit r = LeaveRC);
  void mulli(Register dst, Register src, const Operand& imm);

  void divw(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
            RCBit r = LeaveRC);
  void divwu(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
             RCBit r = LeaveRC);

  void addi(Register dst, Register src, const Operand& imm);
  void addis(Register dst, Register src, const Operand& imm);
  void addic(Register dst, Register src, const Operand& imm);

  void andi(Register ra, Register rs, const Operand& imm);
  void andis(Register ra, Register rs, const Operand& imm);
  void ori(Register dst, Register src, const Operand& imm);
  void oris(Register dst, Register src, const Operand& imm);
  void xori(Register dst, Register src, const Operand& imm);
  void xoris(Register ra, Register rs, const Operand& imm);
  void cmpi(Register src1, const Operand& src2, CRegister cr = cr7);
  void cmpli(Register src1, const Operand& src2, CRegister cr = cr7);
  void cmpwi(Register src1, const Operand& src2, CRegister cr = cr7);
  void cmplwi(Register src1, const Operand& src2, CRegister cr = cr7);
  void li(Register dst, const Operand& src);
  void lis(Register dst, const Operand& imm);
  void mr(Register dst, Register src);

  void lbz(Register dst, const MemOperand& src);
  void lhz(Register dst, const MemOperand& src);
  void lha(Register dst, const MemOperand& src);
  void lwz(Register dst, const MemOperand& src);
  void lwzu(Register dst, const MemOperand& src);
  void lwa(Register dst, const MemOperand& src);
  void stb(Register dst, const MemOperand& src);
  void sth(Register dst, const MemOperand& src);
  void stw(Register dst, const MemOperand& src);
  void stwu(Register dst, const MemOperand& src);
  void neg(Register rt, Register ra, OEBit o = LeaveOE, RCBit c = LeaveRC);

#if V8_TARGET_ARCH_PPC64
  void ld(Register rd, const MemOperand& src);
  void ldu(Register rd, const MemOperand& src);
  void std(Register rs, const MemOperand& src);
  void stdu(Register rs, const MemOperand& src);
  void rldic(Register dst, Register src, int sh, int mb, RCBit r = LeaveRC);
  void rldicl(Register dst, Register src, int sh, int mb, RCBit r = LeaveRC);
  void rldcl(Register ra, Register rs, Register rb, int mb, RCBit r = LeaveRC);
  void rldicr(Register dst, Register src, int sh, int me, RCBit r = LeaveRC);
  void rldimi(Register dst, Register src, int sh, int mb, RCBit r = LeaveRC);
  void sldi(Register dst, Register src, const Operand& val, RCBit rc = LeaveRC);
  void srdi(Register dst, Register src, const Operand& val, RCBit rc = LeaveRC);
  void clrrdi(Register dst, Register src, const Operand& val,
              RCBit rc = LeaveRC);
  void clrldi(Register dst, Register src, const Operand& val,
              RCBit rc = LeaveRC);
  void sradi(Register ra, Register rs, int sh, RCBit r = LeaveRC);
  void rotld(Register ra, Register rs, Register rb, RCBit r = LeaveRC);
  void rotldi(Register ra, Register rs, int sh, RCBit r = LeaveRC);
  void rotrdi(Register ra, Register rs, int sh, RCBit r = LeaveRC);
  void mulld(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
             RCBit r = LeaveRC);
  void divd(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
            RCBit r = LeaveRC);
  void divdu(Register dst, Register src1, Register src2, OEBit o = LeaveOE,
             RCBit r = LeaveRC);
#endif

  void rlwinm(Register ra, Register rs, int sh, int mb, int me,
              RCBit rc = LeaveRC);
  void rlwimi(Register ra, Register rs, int sh, int mb, int me,
              RCBit rc = LeaveRC);
  void rlwnm(Register ra, Register rs, Register rb, int mb, int me,
             RCBit rc = LeaveRC);
  void slwi(Register dst, Register src, const Operand& val, RCBit rc = LeaveRC);
  void srwi(Register dst, Register src, const Operand& val, RCBit rc = LeaveRC);
  void clrrwi(Register dst, Register src, const Operand& val,
              RCBit rc = LeaveRC);
  void clrlwi(Register dst, Register src, const Operand& val,
              RCBit rc = LeaveRC);
  void rotlw(Register ra, Register rs, Register rb, RCBit r = LeaveRC);
  void rotlwi(Register ra, Register rs, int sh, RCBit r = LeaveRC);
  void rotrwi(Register ra, Register rs, int sh, RCBit r = LeaveRC);

  void subi(Register dst, Register src1, const Operand& src2);

  void mov(Register dst, const Operand& src);
  void bitwise_mov(Register dst, intptr_t value);
  void bitwise_mov32(Register dst, int32_t value);
  void bitwise_add32(Register dst, Register src, int32_t value);

  // Patch the offset to the return address after Call.
  void patch_pc_address(Register dst, int pc_offset, int return_address_offset);

  // Load the position of the label relative to the generated code object
  // pointer in a register.
  void mov_label_offset(Register dst, Label* label);

  // dst = base + label position + delta
  void add_label_offset(Register dst, Register base, Label* label,
                        int delta = 0);

  // Load the address of the label in a register and associate with an
  // internal reference relocation.
  void mov_label_addr(Register dst, Label* label);

  // Emit the address of the label (i.e. a jump table entry) and associate with
  // an internal reference relocation.
  void emit_label_addr(Label* label);

  // Multiply instructions
  void mul(Register dst, Register src1, Register src2, OEBit s = LeaveOE,
           RCBit r = LeaveRC);

  // Miscellaneous arithmetic instructions

  // Special register access
  void crxor(int bt, int ba, int bb);
  void crclr(int bt) { crxor(bt, bt, bt); }
  void creqv(int bt, int ba, int bb);
  void crset(int bt) { creqv(bt, bt, bt); }
  void mflr(Register dst);
  void mtlr(Register src);
  void mtctr(Register src);
  void mtxer(Register src);
  void mcrfs(CRegister cr, FPSCRBit bit);
  void mfcr(Register dst);
  void mtcrf(Register src, uint8_t FXM);
#if V8_TARGET_ARCH_PPC64
  void mffprd(Register dst, DoubleRegister src);
  void mffprwz(Register dst, DoubleRegister src);
  void mtfprd(DoubleRegister dst, Register src);
  void mtfprwz(DoubleRegister dst, Register src);
  void mtfprwa(DoubleRegister dst, Register src);
#endif

  // Exception-generating instructions and debugging support
  void stop(Condition cond = al, int32_t code = kDefaultStopCode,
            CRegister cr = cr7);

  void bkpt(uint32_t imm16);  // v5 and above

  void dcbf(Register ra, Register rb);
  void sync();
  void lwsync();
  void icbi(Register ra, Register rb);
  void isync();

  // Support for floating point
  void lfd(const DoubleRegister frt, const MemOperand& src);
  void lfdu(const DoubleRegister frt, const MemOperand& src);
  void lfs(const DoubleRegister frt, const MemOperand& src);
  void lfsu(const DoubleRegister frt, const MemOperand& src);
  void stfd(const DoubleRegister frs, const MemOperand& src);
  void stfdu(const DoubleRegister frs, const MemOperand& src);
  void stfs(const DoubleRegister frs, const MemOperand& src);
  void stfsu(const DoubleRegister frs, const MemOperand& src);

  void fadd(const DoubleRegister frt, const DoubleRegister fra,
            const DoubleRegister frb, RCBit rc = LeaveRC);
  void fsub(const DoubleRegister frt, const DoubleRegister fra,
            const DoubleRegister frb, RCBit rc = LeaveRC);
  void fdiv(const DoubleRegister frt, const DoubleRegister fra,
            const DoubleRegister frb, RCBit rc = LeaveRC);
  void fmul(const DoubleRegister frt, const DoubleRegister fra,
            const DoubleRegister frc, RCBit rc = LeaveRC);
  void fcmpu(const DoubleRegister fra, const DoubleRegister frb,
             CRegister cr = cr7);
  void fmr(const DoubleRegister frt, const DoubleRegister frb,
           RCBit rc = LeaveRC);
  void fctiwz(const DoubleRegister frt, const DoubleRegister frb);
  void fctiw(const DoubleRegister frt, const DoubleRegister frb);
  void fctiwuz(const DoubleRegister frt, const DoubleRegister frb);
  void frin(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void friz(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void frip(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void frim(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void frsp(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void fcfid(const DoubleRegister frt, const DoubleRegister frb,
             RCBit rc = LeaveRC);
  void fcfidu(const DoubleRegister frt, const DoubleRegister frb,
              RCBit rc = LeaveRC);
  void fcfidus(const DoubleRegister frt, const DoubleRegister frb,
               RCBit rc = LeaveRC);
  void fcfids(const DoubleRegister frt, const DoubleRegister frb,
              RCBit rc = LeaveRC);
  void fctid(const DoubleRegister frt, const DoubleRegister frb,
             RCBit rc = LeaveRC);
  void fctidz(const DoubleRegister frt, const DoubleRegister frb,
              RCBit rc = LeaveRC);
  void fctidu(const DoubleRegister frt, const DoubleRegister frb,
              RCBit rc = LeaveRC);
  void fctiduz(const DoubleRegister frt, const DoubleRegister frb,
               RCBit rc = LeaveRC);
  void fsel(const DoubleRegister frt, const DoubleRegister fra,
            const DoubleRegister frc, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void fneg(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void mtfsb0(FPSCRBit bit, RCBit rc = LeaveRC);
  void mtfsb1(FPSCRBit bit, RCBit rc = LeaveRC);
  void mtfsfi(int bf, int immediate, RCBit rc = LeaveRC);
  void mffs(const DoubleRegister frt, RCBit rc = LeaveRC);
  void mtfsf(const DoubleRegister frb, bool L = 1, int FLM = 0, bool W = 0,
             RCBit rc = LeaveRC);
  void fsqrt(const DoubleRegister frt, const DoubleRegister frb,
             RCBit rc = LeaveRC);
  void fabs(const DoubleRegister frt, const DoubleRegister frb,
            RCBit rc = LeaveRC);
  void fmadd(const DoubleRegister frt, const DoubleRegister fra,
             const DoubleRegister frc, const DoubleRegister frb,
             RCBit rc = LeaveRC);
  void fmsub(const DoubleRegister frt, const DoubleRegister fra,
             const DoubleRegister frc, const DoubleRegister frb,
             RCBit rc = LeaveRC);
  void fcpsgn(const DoubleRegister frt, const DoubleRegister fra,
              const DoubleRegister frc, RCBit rc = LeaveRC);

  // Vector instructions
  void mfvsrd(const Register ra, const Simd128Register r);
  void mfvsrwz(const Register ra, const Simd128Register r);
  void mtvsrd(const Simd128Register rt, const Register ra);
  void mtvsrdd(const Simd128Register rt, const Register ra, const Register rb);
  void lxvd(const Simd128Register rt, const MemOperand& src);
  void lxvx(const Simd128Register rt, const MemOperand& src);
  void lxsdx(const Simd128Register rt, const MemOperand& src);
  void lxsibzx(const Simd128Register rt, const MemOperand& src);
  void lxsihzx(const Simd128Register rt, const MemOperand& src);
  void lxsiwzx(const Simd128Register rt, const MemOperand& src);
  void stxsdx(const Simd128Register rs, const MemOperand& dst);
  void stxsibx(const Simd128Register rs, const MemOperand& dst);
  void stxsihx(const Simd128Register rs, const MemOperand& dst);
  void stxsiwx(const Simd128Register rs, const MemOperand& dst);
  void stxvd(const Simd128Register rt, const MemOperand& dst);
  void stxvx(const Simd128Register rt, const MemOperand& dst);
  void xxspltib(const Simd128Register rt, const Operand& imm);

  // Prefixed instructioons.
  void paddi(Register dst, Register src, const Operand& imm);
  void pli(Register dst, const Operand& imm);
  void psubi(Register dst, Register src, const Operand& imm);
  void plbz(Register dst, const MemOperand& src);
  void plhz(Register dst, const MemOperand& src);
  void plha(Register dst, const MemOperand& src);
  void plwz(Register dst, const MemOperand& src);
  void plwa(Register dst, const MemOperand& src);
  void pld(Register dst, const MemOperand& src);
  void plfs(DoubleRegister dst, const MemOperand& src);
  void plfd(DoubleRegister dst, const MemOperand& src);
  void pstb(Register src, const MemOperand& dst);
  void psth(Register src, const MemOperand& dst);
  void pstw(Register src, const MemOperand& dst);
  void pstd(Register src, const MemOperand& dst);
  void pstfs(const DoubleRegister src, const MemOperand& dst);
  void pstfd(const DoubleRegister src, const MemOperand& dst);

  // Pseudo instructions

  // Different nop operations are used by the code generator to detect certain
  // states of the generated code.
  enum NopMarkerTypes {
    NON_MARKING_NOP = 0,
    GROUP_ENDING_NOP,
    DEBUG_BREAK_NOP,
    // IC markers.
    PROPERTY_ACCESS_INLINED,
    PROPERTY_ACCESS_INLINED_CONTEXT,
    PROPERTY_ACCESS_INLINED_CONTEXT_DONT_DELETE,
    // Helper values.
    LAST_CODE_MARKER,
    FIRST_IC_MARKER = PROPERTY_ACCESS_INLINED
  };

  void nop(int type = 0);  // 0 is the default non-marking type.

  void push(Register src) {
#if V8_TARGET_ARCH_PPC64
    stdu(src, MemOperand(sp, -kSystemPointerSize));
#else
    stwu(src, MemOperand(sp, -kSystemPointerSize));
#endif
  }

  void pop(Register dst) {
#if V8_TARGET_ARCH_PPC64
    ld(dst, MemOperand(sp));
#else
    lwz(dst, MemOperand(sp));
#endif
    addi(sp, sp, Operand(kSystemPointerSize));
  }

  void pop() { addi(sp, sp, Operand(kSystemPointerSize)); }

  // Jump unconditionally to given label.
  void jmp(Label* L) { b(L); }

  // Check the code size generated from label to here.
  int SizeOfCodeGeneratedSince(Label* label) {
    return pc_offset() - label->pos();
  }

  // Check the number of instructions generated from label to here.
  int InstructionsGeneratedSince(Label* label) {
    return SizeOfCodeGeneratedSince(label) / kInstrSize;
  }

  // Class for scoping postponing the trampoline pool generation.
  class V8_NODISCARD BlockTrampolinePoolScope {
   public:
    explicit BlockTrampolinePoolScope(Assembler* assem) : assem_(assem) {
      assem_->StartBlockTrampolinePool();
    }
    ~BlockTrampolinePoolScope() { assem_->EndBlockTrampolinePool(); }

   private:
    Assembler* assem_;

    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockTrampolinePoolScope);
  };

  // Class for scoping disabling constant pool entry merging
  class V8_NODISCARD BlockConstantPoolEntrySharingScope {
   public:
    explicit BlockConstantPoolEntrySharingScope(Assembler* assem)
        : assem_(assem) {
      assem_->StartBlockConstantPoolEntrySharing();
    }
    ~BlockConstantPoolEntrySharingScope() {
      assem_->EndBlockConstantPoolEntrySharing();
    }

   private:
    Assembler* assem_;

    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockConstantPoolEntrySharingScope);
  };

  // Record a deoptimization reason that can be used by a log or cpu profiler.
  // Use --trace-deopt to enable.
  void RecordDeoptReason(DeoptimizeReason reason, uint32_t node_id,
                         SourcePosition position, int id);

  // Writes a single byte or word of data in the code stream.  Used
  // for inline tables, e.g., jump-tables.
  void db(uint8_t data);
  void dd(uint32_t data);
  void dq(uint64_t data);
  void dp(uintptr_t data);

  // Read/patch instructions
  Instr instr_at(int pos) {
    return *reinterpret_cast<Instr*>(buffer_start_ + pos);
  }
  void instr_at_put(int pos, Instr instr) {
    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
  }
  static Instr instr_at(Address pc) { return *reinterpret_cast<Instr*>(pc); }
  static void instr_at_put(Address pc, Instr instr) {
    *reinterpret_cast<Instr*>(pc) = instr;
  }
  static Condition GetCondition(Instr instr);

  static bool IsLis(Instr instr);
  static bool IsLi(Instr instr);
  static bool IsAddic(Instr instr);
  static bool IsOri(Instr instr);

  static bool IsBranch(Instr instr);
  static Register GetRA(Instr instr);
  static Register GetRB(Instr instr);
#if V8_TARGET_ARCH_PPC64
  static bool Is64BitLoadIntoR12(Instr instr1, Instr instr2, Instr instr3,
                                 Instr instr4, Instr instr5);
#else
  static bool Is32BitLoadIntoR12(Instr instr1, Instr instr2);
#endif

  static bool IsCmpRegister(Instr instr);
  static bool IsCmpImmediate(Instr instr);
  static bool IsRlwinm(Instr instr);
  static bool IsAndi(Instr instr);
#if V8_TARGET_ARCH_PPC64
  static bool IsRldicl(Instr instr);
#endif
  static bool IsCrSet(Instr instr);
  static Register GetCmpImmediateRegister(Instr instr);
  static int GetCmpImmediateRawImmediate(Instr instr);
  static bool IsNop(Instr instr, int type = NON_MARKING_NOP);

  // Postpone the generation of the trampoline pool for the specified number of
  // instructions.
  void BlockTrampolinePoolFor(int instructions);
  void CheckTrampolinePool();

  // For mov.  Return the number of actual instructions required to
  // load the operand into a register.  This can be anywhere from
  // one (constant pool small section) to five instructions (full
  // 64-bit sequence).
  //
  // The value returned is only valid as long as no entries are added to the
  // constant pool between this call and the actual instruction being emitted.
  int instructions_required_for_mov(Register dst, const Operand& src) const;

  // Decide between using the constant pool vs. a mov immediate sequence.
  bool use_constant_pool_for_mov(Register dst, const Operand& src,
                                 bool canOptimize) const;

  // The code currently calls CheckBuffer() too often. This has the side
  // effect of randomly growing the buffer in the middle of multi-instruction
  // sequences.
  //
  // This function allows outside callers to check and grow the buffer
  void EnsureSpaceFor(int space_needed);

  int EmitConstantPool() { return constant_pool_builder_.Emit(this); }

  bool ConstantPoolAccessIsInOverflow() const {
    return constant_pool_builder_.NextAccess(ConstantPoolEntry::INTPTR) ==
           ConstantPoolEntry::OVERFLOWED;
  }

  Label* ConstantPoolPosition() {
    return constant_pool_builder_.EmittedPosition();
  }

  void EmitRelocations();

 protected:
  int buffer_space() const { return reloc_info_writer.pos() - pc_; }

  // Decode instruction(s) at pos and return backchain to previous
  // label reference or kEndOfChain.
  int target_at(int pos);

  // Patch instruction(s) at pos to target target_pos (e.g. branch)
  void target_at_put(int pos, int target_pos, bool* is_branch = nullptr);

  // Record reloc info for current pc_
  void RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data = 0);
  ConstantPoolEntry::Access ConstantPoolAddEntry(RelocInfo::Mode rmode,
                                                 intptr_t value) {
    bool sharing_ok =
        RelocInfo::IsNoInfo(rmode) ||
        (!options().record_reloc_info_for_serialization &&
         RelocInfo::IsShareableRelocMode(rmode) &&
         !is_constant_pool_entry_sharing_blocked() &&
         // TODO(johnyan): make the following rmode shareable
         !RelocInfo::IsWasmCall(rmode) && !RelocInfo::IsWasmStubCall(rmode));
    return constant_pool_builder_.AddEntry(pc_offset(), value, sharing_ok);
  }
  ConstantPoolEntry::Access ConstantPoolAddEntry(base::Double value) {
    return constant_pool_builder_.AddEntry(pc_offset(), value);
  }

  // Block the emission of the trampoline pool before pc_offset.
  void BlockTrampolinePoolBefore(int pc_offset) {
    if (no_trampoline_pool_before_ < pc_offset)
      no_trampoline_pool_before_ = pc_offset;
  }

  void StartBlockTrampolinePool() { trampoline_pool_blocked_nesting_++; }
  void EndBlockTrampolinePool() {
    int count = --trampoline_pool_blocked_nesting_;
    if (count == 0) CheckTrampolinePoolQuick();
  }
  bool is_trampoline_pool_blocked() const {
    return trampoline_pool_blocked_nesting_ > 0;
  }

  void StartBlockConstantPoolEntrySharing() {
    constant_pool_entry_sharing_blocked_nesting_++;
  }
  void EndBlockConstantPoolEntrySharing() {
    constant_pool_entry_sharing_blocked_nesting_--;
  }
  bool is_constant_pool_entry_sharing_blocked() const {
    return constant_pool_entry_sharing_blocked_nesting_ > 0;
  }

  bool has_exception() const { return internal_trampoline_exception_; }

  bool is_trampoline_emitted() const { return trampoline_emitted_; }

  // InstructionStream generation
  // The relocation writer's position is at least kGap bytes below the end of
  // the generated instructions. This is so that multi-instruction sequences do
  // not have to check for overflow. The same is true for writes of large
  // relocation info entries.
  static constexpr int kGap = 32;
  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);

  RelocInfoWriter reloc_info_writer;

 private:
  // Avoid overflows for displacements etc.
  static const int kMaximalBufferSize = 512 * MB;

  // Repeated checking whether the trampoline pool should be emitted is rather
  // expensive. By default we only check again once a number of instructions
  // has been generated.
  int next_trampoline_check_;  // pc offset of next buffer check.

  // Emission of the trampoline pool may be blocked in some code sequences.
  int trampoline_pool_blocked_nesting_;  // Block emission if this is not zero.
  int no_trampoline_pool_before_;  // Block emission before this pc offset.

  // Do not share constant pool entries.
  int constant_pool_entry_sharing_blocked_nesting_;

  // Relocation info generation
  // Each relocation is encoded as a variable size value
  static constexpr int kMaxRelocSize = RelocInfoWriter::kMaxSize;
  std::vector<DeferredRelocInfo> relocations_;

  // Scratch registers available for use by the Assembler.
  RegList scratch_register_list_;

  // The bound position, before this we cannot do instruction elimination.
  int last_bound_pos_;
  // Optimizable cmpi information.
  int optimizable_cmpi_pos_;
  CRegister cmpi_cr_ = CRegister::no_reg();

  ConstantPoolBuilder constant_pool_builder_;

  void CheckBuffer() {
    if (buffer_space() <= kGap) {
      GrowBuffer();
    }
  }

  void GrowBuffer(int needed = 0);
  // Code emission
  void emit(Instr x) {
    CheckBuffer();
    *reinterpret_cast<Instr*>(pc_) = x;
    pc_ += kInstrSize;
    CheckTrampolinePoolQuick();
  }

  void emit_prefix(Instr x) {
    // Prefixed instructions cannot cross 64-byte boundaries. Add a nop if the
    // boundary will be crossed mid way.
    // Code is set to be 64-byte aligned on PPC64 after relocation (look for
    // kCodeAlignment). We use pc_offset() instead of pc_ as current pc_
    // alignment could be different after relocation.
    if (((pc_offset() + sizeof(Instr)) & 63) == 0) {
      nop();
    }
    // Do not emit trampoline pool in between prefix and suffix.
    CHECK(is_trampoline_pool_blocked());
    emit(x);
  }

  void TrackBranch() {
    DCHECK(!trampoline_emitted_);
    int count = tracked_branch_count_++;
    if (count == 0) {
      // We leave space (kMaxBlockTrampolineSectionSize)
      // for BlockTrampolinePoolScope buffer.
      next_trampoline_check_ =
          pc_offset() + kMaxCondBranchReach - kMaxBlockTrampolineSectionSize;
    } else {
      next_trampoline_check_ -= kTrampolineSlotsSize;
    }
  }

  inline void UntrackBranch();
  // Instruction generation
  void a_form(Instr instr, DoubleRegister frt, DoubleRegister fra,
              DoubleRegister frb, RCBit r);
  void d_form(Instr instr, Register rt, Register ra, const intptr_t val,
              bool signed_disp);
  void xo_form(Instr instr, Register rt, Register ra, Register rb, OEBit o,
               RCBit r);
  void md_form(Instr instr, Register ra, Register rs, int shift, int maskbit,
               RCBit r);
  void mds_form(Instr instr, Register ra, Register rs, Register rb, int maskbit,
                RCBit r);

  // Labels
  void print(Label* L);
  int max_reach_from(int pos);
  void bind_to(Label* L, int pos);
  void next(Label* L);

  class Trampoline {
   public:
    Trampoline() {
      next_slot_ = 0;
      free_slot_count_ = 0;
    }
    Trampoline(int start, int slot_count) {
      next_slot_ = start;
      free_slot_count_ = slot_count;
    }
    int take_slot() {
      int trampoline_slot = kInvalidSlotPos;
      if (free_slot_count_ <= 0) {
        // We have run out of space on trampolines.
        // Make sure we fail in debug mode, so we become aware of each case
        // when this happens.
        DCHECK(0);
        // Internal exception will be caught.
      } else {
        trampoline_slot = next_slot_;
        free_slot_count_--;
        next_slot_ += kTrampolineSlotsSize;
      }
      return trampoline_slot;
    }

   private:
    int next_slot_;
    int free_slot_count_;
  };

  int32_t get_trampoline_entry();
  int tracked_branch_count_;
  // If trampoline is emitted, generated code is becoming large. As
  // this is already a slow case which can possibly break our code
  // generation for the extreme case, we use this information to
  // trigger different mode of branch instruction generation, where we
  // no longer use a single branch instruction.
  bool trampoline_emitted_;
  static constexpr int kTrampolineSlotsSize = kInstrSize;
  static constexpr int kMaxCondBranchReach = (1 << (16 - 1)) - 1;
  static constexpr int kMaxBlockTrampolineSectionSize = 64 * kInstrSize;
  static constexpr int kInvalidSlotPos = -1;

  Trampoline trampoline_;
  bool internal_trampoline_exception_;

  void AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate);

  int WriteCodeComments();

  friend class RegExpMacroAssemblerPPC;
  friend class RelocInfo;
  friend class BlockTrampolinePoolScope;
  friend class EnsureSpace;
  friend class UseScratchRegisterScope;
};

class EnsureSpace {
 public:
  explicit EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }
};

class PatchingAssembler : public Assembler {
 public:
  PatchingAssembler(const AssemblerOptions& options, uint8_t* address,
                    int instructions);
  ~PatchingAssembler();
};

class V8_EXPORT_PRIVATE V8_NODISCARD UseScratchRegisterScope {
 public:
  explicit UseScratchRegisterScope(Assembler* assembler)
      : assembler_(assembler),
        old_available_(*assembler->GetScratchRegisterList()) {}

  ~UseScratchRegisterScope() {
    *assembler_->GetScratchRegisterList() = old_available_;
  }

  Register Acquire() {
    return assembler_->GetScratchRegisterList()->PopFirst();
  }

  // Check if we have registers available to acquire.
  bool CanAcquire() const {
    return !assembler_->GetScratchRegisterList()->is_empty();
  }

 private:
  friend class Assembler;
  friend class MacroAssembler;

  Assembler* assembler_;
  RegList old_available_;
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_PPC_ASSEMBLER_PPC_H_
                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/codegen/ppc/constants-ppc.cc                                                0000664 0000000 0000000 00000003140 14746647661 0022123 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64

#include "src/codegen/ppc/constants-ppc.h"

namespace v8 {
namespace internal {

// These register names are defined in a way to match the native disassembler
// formatting. See for example the command "objdump -d <binary file>".
const char* Registers::names_[kNumRegisters] = {
    "r0",  "sp",  "r2",  "r3",  "r4",  "r5",  "r6",  "r7",  "r8",  "r9",  "r10",
    "r11", "ip",  "r13", "r14", "r15", "r16", "r17", "r18", "r19", "r20", "r21",
    "r22", "r23", "r24", "r25", "r26", "r27", "r28", "r29", "r30", "fp"};

const char* DoubleRegisters::names_[kNumDoubleRegisters] = {
    "d0",  "d1",  "d2",  "d3",  "d4",  "d5",  "d6",  "d7",  "d8",  "d9",  "d10",
    "d11", "d12", "d13", "d14", "d15", "d16", "d17", "d18", "d19", "d20", "d21",
    "d22", "d23", "d24", "d25", "d26", "d27", "d28", "d29", "d30", "d31"};

int DoubleRegisters::Number(const char* name) {
  for (int i = 0; i < kNumDoubleRegisters; i++) {
    if (strcmp(names_[i], name) == 0) {
      return i;
    }
  }

  // No register with the requested name found.
  return kNoRegister;
}

int Registers::Number(const char* name) {
  // Look through the canonical names.
  for (int i = 0; i < kNumRegisters; i++) {
    if (strcmp(names_[i], name) == 0) {
      return i;
    }
  }

  // No register with the requested name found.
  return kNoRegister;
}
}  // namespace internal
}  // namespace v8

#endif  // V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
                                                                                                                                                                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/codegen/ppc/constants-ppc.h                                                 0000664 0000000 0000000 00000600015 14746647661 0021771 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_PPC_CONSTANTS_PPC_H_
#define V8_CODEGEN_PPC_CONSTANTS_PPC_H_

#include <stdint.h>

#include "src/base/logging.h"
#include "src/base/macros.h"
#include "src/common/globals.h"

// UNIMPLEMENTED_ macro for PPC.
#ifdef DEBUG
#define UNIMPLEMENTED_PPC()                                                \
  v8::internal::PrintF("%s, \tline %d: \tfunction %s not implemented. \n", \
                       __FILE__, __LINE__, __func__)
#else
#define UNIMPLEMENTED_PPC()
#endif

#if (V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64) &&                    \
    (V8_OS_AIX || (V8_TARGET_ARCH_PPC64 && V8_TARGET_BIG_ENDIAN && \
                   (!defined(_CALL_ELF) || _CALL_ELF == 1)))
#define ABI_USES_FUNCTION_DESCRIPTORS 1
#else
#define ABI_USES_FUNCTION_DESCRIPTORS 0
#endif

#if !(V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64) || V8_OS_AIX || \
    V8_TARGET_ARCH_PPC64
#define ABI_PASSES_HANDLES_IN_REGS 1
#else
#define ABI_PASSES_HANDLES_IN_REGS 0
#endif

#if !(V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64) || !V8_TARGET_ARCH_PPC64 || \
    V8_TARGET_LITTLE_ENDIAN || (defined(_CALL_ELF) && _CALL_ELF == 2)
#define ABI_RETURNS_OBJECT_PAIRS_IN_REGS 1
#else
#define ABI_RETURNS_OBJECT_PAIRS_IN_REGS 0
#endif

#if !(V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64) || \
    (V8_TARGET_ARCH_PPC64 &&                     \
     (V8_TARGET_LITTLE_ENDIAN || (defined(_CALL_ELF) && _CALL_ELF == 2)))
#define ABI_CALL_VIA_IP 1
#else
#define ABI_CALL_VIA_IP 0
#endif

#if !(V8_HOST_ARCH_PPC || V8_HOST_ARCH_PPC64) || V8_OS_AIX || \
    V8_TARGET_ARCH_PPC64
#define ABI_TOC_REGISTER 2
#else
#define ABI_TOC_REGISTER 13
#endif
namespace v8 {
namespace internal {

// TODO(sigurds): Change this value once we use relative jumps.
constexpr size_t kMaxPCRelativeCodeRangeInMB = 0;

// Used to encode a boolean value when emitting 32 bit
// opcodes which will indicate the presence of function descriptors
constexpr int kHasFunctionDescriptorBitShift = 4;
constexpr int kHasFunctionDescriptorBitMask = 1
                                              << kHasFunctionDescriptorBitShift;

// Number of registers
const int kNumRegisters = 32;

// FP support.
const int kNumDoubleRegisters = 32;

const int kNoRegister = -1;

// Used in embedded constant pool builder - max reach in bits for
// various load instructions (one less due to unsigned)
const int kLoadPtrMaxReachBits = 15;
const int kLoadDoubleMaxReachBits = 15;

// The actual value of the kRootRegister is offset from the IsolateData's start
// to take advantage of negative displacement values.
constexpr int kRootRegisterBias = 128;

// sign-extend the least significant 5-bits of value <imm>
#define SIGN_EXT_IMM5(imm) ((static_cast<int>(imm) << 27) >> 27)

// sign-extend the least significant 16-bits of value <imm>
#define SIGN_EXT_IMM16(imm) ((static_cast<int>(imm) << 16) >> 16)

// sign-extend the least significant 14-bits of value <imm>
#define SIGN_EXT_IMM18(imm) ((static_cast<int>(imm) << 14) >> 14)

// sign-extend the least significant 22-bits of value <imm>
#define SIGN_EXT_IMM22(imm) ((static_cast<int>(imm) << 10) >> 10)

// sign-extend the least significant 26-bits of value <imm>
#define SIGN_EXT_IMM26(imm) ((static_cast<int>(imm) << 6) >> 6)

// sign-extend the least significant 34-bits of prefix+suffix value <imm>
#define SIGN_EXT_IMM34(imm) ((static_cast<int64_t>(imm) << 30) >> 30)

// -----------------------------------------------------------------------------
// Conditions.

// Defines constants and accessor classes to assemble, disassemble and
// simulate PPC instructions.
//
// Section references in the code refer to the "PowerPC Microprocessor
// Family: The Programmer.s Reference Guide" from 10/95
// https://www-01.ibm.com/chips/techlib/techlib.nsf/techdocs/852569B20050FF778525699600741775/$file/prg.pdf
//

// Constants for specific fields are defined in their respective named enums.
// General constants are in an anonymous enum in class Instr.
enum Condition : int {
  kNoCondition = -1,
  eq = 0,         // Equal.
  ne = 1,         // Not equal.
  ge = 2,         // Greater or equal.
  lt = 3,         // Less than.
  gt = 4,         // Greater than.
  le = 5,         // Less then or equal
  unordered = 6,  // Floating-point unordered
  ordered = 7,
  overflow = 8,  // Summary overflow
  nooverflow = 9,
  al = 10,  // Always.

  // Unified cross-platform condition names/aliases.
  // Do not set unsigned constants equal to their signed variants.
  // We need to be able to differentiate between signed and unsigned enum
  // constants in order to emit the right instructions (i.e CmpS64 vs CmpU64).
  kEqual = eq,
  kNotEqual = ne,
  kLessThan = lt,
  kGreaterThan = gt,
  kLessThanEqual = le,
  kGreaterThanEqual = ge,
  kUnsignedLessThan = 11,
  kUnsignedGreaterThan = 12,
  kUnsignedLessThanEqual = 13,
  kUnsignedGreaterThanEqual = 14,
  kOverflow = overflow,
  kNoOverflow = nooverflow,
  kZero = 15,
  kNotZero = 16,
};

inline Condition to_condition(Condition cond) {
  switch (cond) {
    case kUnsignedLessThan:
      return lt;
    case kUnsignedGreaterThan:
      return gt;
    case kUnsignedLessThanEqual:
      return le;
    case kUnsignedGreaterThanEqual:
      return ge;
    case kZero:
      return eq;
    case kNotZero:
      return ne;
    default:
      break;
  }
  return cond;
}

inline bool is_signed(Condition cond) {
  switch (cond) {
    case kEqual:
    case kNotEqual:
    case kLessThan:
    case kGreaterThan:
    case kLessThanEqual:
    case kGreaterThanEqual:
    case kOverflow:
    case kNoOverflow:
    case kZero:
    case kNotZero:
      return true;

    case kUnsignedLessThan:
    case kUnsignedGreaterThan:
    case kUnsignedLessThanEqual:
    case kUnsignedGreaterThanEqual:
      return false;

    default:
      UNREACHABLE();
  }
}

inline Condition NegateCondition(Condition cond) {
  DCHECK(cond != al);
  return static_cast<Condition>(cond ^ ne);
}

// -----------------------------------------------------------------------------
// Instructions encoding.

// Instr is merely used by the Assembler to distinguish 32bit integers
// representing instructions from usual 32 bit values.
// Instruction objects are pointers to 32bit values, and provide methods to
// access the various ISA fields.
using Instr = uint32_t;

#define PPC_XX3_OPCODE_SCALAR_LIST(V)                                 \
  /* VSX Scalar Add Double-Precision */                               \
  V(xsadddp, XSADDDP, 0xF0000100)                                     \
  /* VSX Scalar Add Single-Precision */                               \
  V(xsaddsp, XSADDSP, 0xF0000000)                                     \
  /* VSX Scalar Compare Ordered Double-Precision */                   \
  V(xscmpodp, XSCMPODP, 0xF0000158)                                   \
  /* VSX Scalar Compare Unordered Double-Precision */                 \
  V(xscmpudp, XSCMPUDP, 0xF0000118)                                   \
  /* VSX Scalar Copy Sign Double-Precision */                         \
  V(xscpsgndp, XSCPSGNDP, 0xF0000580)                                 \
  /* VSX Scalar Divide Double-Precision */                            \
  V(xsdivdp, XSDIVDP, 0xF00001C0)                                     \
  /* VSX Scalar Divide Single-Precision */                            \
  V(xsdivsp, XSDIVSP, 0xF00000C0)                                     \
  /* VSX Scalar Multiply-Add Type-A Double-Precision */               \
  V(xsmaddadp, XSMADDADP, 0xF0000108)                                 \
  /* VSX Scalar Multiply-Add Type-A Single-Precision */               \
  V(xsmaddasp, XSMADDASP, 0xF0000008)                                 \
  /* VSX Scalar Multiply-Add Type-M Double-Precision */               \
  V(xsmaddmdp, XSMADDMDP, 0xF0000148)                                 \
  /* VSX Scalar Multiply-Add Type-M Single-Precision */               \
  V(xsmaddmsp, XSMADDMSP, 0xF0000048)                                 \
  /* VSX Scalar Maximum Double-Precision */                           \
  V(xsmaxdp, XSMAXDP, 0xF0000500)                                     \
  /* VSX Scalar Minimum Double-Precision */                           \
  V(xsmindp, XSMINDP, 0xF0000540)                                     \
  /* VSX Scalar Multiply-Subtract Type-A Double-Precision */          \
  V(xsmsubadp, XSMSUBADP, 0xF0000188)                                 \
  /* VSX Scalar Multiply-Subtract Type-A Single-Precision */          \
  V(xsmsubasp, XSMSUBASP, 0xF0000088)                                 \
  /* VSX Scalar Multiply-Subtract Type-M Double-Precision */          \
  V(xsmsubmdp, XSMSUBMDP, 0xF00001C8)                                 \
  /* VSX Scalar Multiply-Subtract Type-M Single-Precision */          \
  V(xsmsubmsp, XSMSUBMSP, 0xF00000C8)                                 \
  /* VSX Scalar Multiply Double-Precision */                          \
  V(xsmuldp, XSMULDP, 0xF0000180)                                     \
  /* VSX Scalar Multiply Single-Precision */                          \
  V(xsmulsp, XSMULSP, 0xF0000080)                                     \
  /* VSX Scalar Negative Multiply-Add Type-A Double-Precision */      \
  V(xsnmaddadp, XSNMADDADP, 0xF0000508)                               \
  /* VSX Scalar Negative Multiply-Add Type-A Single-Precision */      \
  V(xsnmaddasp, XSNMADDASP, 0xF0000408)                               \
  /* VSX Scalar Negative Multiply-Add Type-M Double-Precision */      \
  V(xsnmaddmdp, XSNMADDMDP, 0xF0000548)                               \
  /* VSX Scalar Negative Multiply-Add Type-M Single-Precision */      \
  V(xsnmaddmsp, XSNMADDMSP, 0xF0000448)                               \
  /* VSX Scalar Negative Multiply-Subtract Type-A Double-Precision */ \
  V(xsnmsubadp, XSNMSUBADP, 0xF0000588)                               \
  /* VSX Scalar Negative Multiply-Subtract Type-A Single-Precision */ \
  V(xsnmsubasp, XSNMSUBASP, 0xF0000488)                               \
  /* VSX Scalar Negative Multiply-Subtract Type-M Double-Precision */ \
  V(xsnmsubmdp, XSNMSUBMDP, 0xF00005C8)                               \
  /* VSX Scalar Negative Multiply-Subtract Type-M Single-Precision */ \
  V(xsnmsubmsp, XSNMSUBMSP, 0xF00004C8)                               \
  /* VSX Scalar Reciprocal Estimate Double-Precision */               \
  V(xsredp, XSREDP, 0xF0000168)                                       \
  /* VSX Scalar Subtract Double-Precision */                          \
  V(xssubdp, XSSUBDP, 0xF0000140)                                     \
  /* VSX Scalar Subtract Single-Precision */                          \
  V(xssubsp, XSSUBSP, 0xF0000040)                                     \
  /* VSX Scalar Test for software Divide Double-Precision */          \
  V(xstdivdp, XSTDIVDP, 0xF00001E8)

#define PPC_XX3_OPCODE_VECTOR_A_FORM_LIST(V)         \
  /* VSX Vector Compare Equal To Single-Precision */ \
  V(xvcmpeqsp, XVCMPEQSP, 0xF0000218)                \
  /* VSX Vector Compare Equal To Double-Precision */ \
  V(xvcmpeqdp, XVCMPEQDP, 0xF0000318)

#define PPC_XX3_OPCODE_VECTOR_B_FORM_LIST(V)                                  \
  /* VSX Vector Add Double-Precision */                                       \
  V(xvadddp, XVADDDP, 0xF0000300)                                             \
  /* VSX Vector Add Single-Precision */                                       \
  V(xvaddsp, XVADDSP, 0xF0000200)                                             \
  /* VSX Vector Compare Equal To Double-Precision & record CR6 */             \
  V(xvcmpeqdpx, XVCMPEQDPx, 0xF0000718)                                       \
  /* VSX Vector Compare Equal To Single-Precision & record CR6 */             \
  V(xvcmpeqspx, XVCMPEQSPx, 0xF0000618)                                       \
  /* VSX Vector Compare Greater Than or Equal To Double-Precision */          \
  V(xvcmpgedp, XVCMPGEDP, 0xF0000398)                                         \
  /* VSX Vector Compare Greater Than or Equal To Double-Precision & record */ \
  /* CR6 */                                                                   \
  V(xvcmpgedpx, XVCMPGEDPx, 0xF0000798)                                       \
  /* VSX Vector Compare Greater Than or Equal To Single-Precision */          \
  V(xvcmpgesp, XVCMPGESP, 0xF0000298)                                         \
  /* VSX Vector Compare Greater Than or Equal To Single-Precision & record */ \
  /* CR6 */                                                                   \
  V(xvcmpgespx, XVCMPGESPx, 0xF0000698)                                       \
  /* VSX Vector Compare Greater Than Double-Precision */                      \
  V(xvcmpgtdp, XVCMPGTDP, 0xF0000358)                                         \
  /* VSX Vector Compare Greater Than Double-Precision & record CR6 */         \
  V(xvcmpgtdpx, XVCMPGTDPx, 0xF0000758)                                       \
  /* VSX Vector Compare Greater Than Single-Precision */                      \
  V(xvcmpgtsp, XVCMPGTSP, 0xF0000258)                                         \
  /* VSX Vector Compare Greater Than Single-Precision & record CR6 */         \
  V(xvcmpgtspx, XVCMPGTSPx, 0xF0000658)                                       \
  /* VSX Vector Copy Sign Double-Precision */                                 \
  V(xvcpsgndp, XVCPSGNDP, 0xF0000780)                                         \
  /* VSX Vector Copy Sign Single-Precision */                                 \
  V(xvcpsgnsp, XVCPSGNSP, 0xF0000680)                                         \
  /* VSX Vector Divide Double-Precision */                                    \
  V(xvdivdp, XVDIVDP, 0xF00003C0)                                             \
  /* VSX Vector Divide Single-Precision */                                    \
  V(xvdivsp, XVDIVSP, 0xF00002C0)                                             \
  /* VSX Vector Multiply-Add Type-A Double-Precision */                       \
  V(xvmaddadp, XVMADDADP, 0xF0000308)                                         \
  /* VSX Vector Multiply-Add Type-A Single-Precision */                       \
  V(xvmaddasp, XVMADDASP, 0xF0000208)                                         \
  /* VSX Vector Multiply-Add Type-M Double-Precision */                       \
  V(xvmaddmdp, XVMADDMDP, 0xF0000348)                                         \
  /* VSX Vector Multiply-Add Type-M Single-Precision */                       \
  V(xvmaddmsp, XVMADDMSP, 0xF0000248)                                         \
  /* VSX Vector Maximum Double-Precision */                                   \
  V(xvmaxdp, XVMAXDP, 0xF0000700)                                             \
  /* VSX Vector Maximum Single-Precision */                                   \
  V(xvmaxsp, XVMAXSP, 0xF0000600)                                             \
  /* VSX Vector Minimum Double-Precision */                                   \
  V(xvmindp, XVMINDP, 0xF0000740)                                             \
  /* VSX Vector Minimum Single-Precision */                                   \
  V(xvminsp, XVMINSP, 0xF0000640)                                             \
  /* VSX Vector Multiply-Subtract Type-A Double-Precision */                  \
  V(xvmsubadp, XVMSUBADP, 0xF0000388)                                         \
  /* VSX Vector Multiply-Subtract Type-A Single-Precision */                  \
  V(xvmsubasp, XVMSUBASP, 0xF0000288)                                         \
  /* VSX Vector Multiply-Subtract Type-M Double-Precision */                  \
  V(xvmsubmdp, XVMSUBMDP, 0xF00003C8)                                         \
  /* VSX Vector Multiply-Subtract Type-M Single-Precision */                  \
  V(xvmsubmsp, XVMSUBMSP, 0xF00002C8)                                         \
  /* VSX Vector Multiply Double-Precision */                                  \
  V(xvmuldp, XVMULDP, 0xF0000380)                                             \
  /* VSX Vector Multiply Single-Precision */                                  \
  V(xvmulsp, XVMULSP, 0xF0000280)                                             \
  /* VSX Vector Negative Multiply-Add Type-A Double-Precision */              \
  V(xvnmaddadp, XVNMADDADP, 0xF0000708)                                       \
  /* VSX Vector Negative Multiply-Add Type-A Single-Precision */              \
  V(xvnmaddasp, XVNMADDASP, 0xF0000608)                                       \
  /* VSX Vector Negative Multiply-Add Type-M Double-Precision */              \
  V(xvnmaddmdp, XVNMADDMDP, 0xF0000748)                                       \
  /* VSX Vector Negative Multiply-Add Type-M Single-Precision */              \
  V(xvnmaddmsp, XVNMADDMSP, 0xF0000648)                                       \
  /* VSX Vector Negative Multiply-Subtract Type-A Double-Precision */         \
  V(xvnmsubadp, XVNMSUBADP, 0xF0000788)                                       \
  /* VSX Vector Negative Multiply-Subtract Type-A Single-Precision */         \
  V(xvnmsubasp, XVNMSUBASP, 0xF0000688)                                       \
  /* VSX Vector Negative Multiply-Subtract Type-M Double-Precision */         \
  V(xvnmsubmdp, XVNMSUBMDP, 0xF00007C8)                                       \
  /* VSX Vector Negative Multiply-Subtract Type-M Single-Precision */         \
  V(xvnmsubmsp, XVNMSUBMSP, 0xF00006C8)                                       \
  /* VSX Vector Reciprocal Estimate Double-Precision */                       \
  V(xvredp, XVREDP, 0xF0000368)                                               \
  /* VSX Vector Subtract Double-Precision */                                  \
  V(xvsubdp, XVSUBDP, 0xF0000340)                                             \
  /* VSX Vector Subtract Single-Precision */                                  \
  V(xvsubsp, XVSUBSP, 0xF0000240)                                             \
  /* VSX Vector Test for software Divide Double-Precision */                  \
  V(xvtdivdp, XVTDIVDP, 0xF00003E8)                                           \
  /* VSX Vector Test for software Divide Single-Precision */                  \
  V(xvtdivsp, XVTDIVSP, 0xF00002E8)                                           \
  /* VSX Logical AND */                                                       \
  V(xxland, XXLAND, 0xF0000410)                                               \
  /* VSX Logical AND with Complement */                                       \
  V(xxlandc, XXLANDC, 0xF0000450)                                             \
  /* VSX Logical Equivalence */                                               \
  V(xxleqv, XXLEQV, 0xF00005D0)                                               \
  /* VSX Logical NAND */                                                      \
  V(xxlnand, XXLNAND, 0xF0000590)                                             \
  /* VSX Logical NOR */                                                       \
  V(xxlnor, XXLNOR, 0xF0000510)                                               \
  /* VSX Logical OR */                                                        \
  V(xxlor, XXLOR, 0xF0000490)                                                 \
  /* VSX Logical OR with Complement */                                        \
  V(xxlorc, XXLORC, 0xF0000550)                                               \
  /* VSX Logical XOR */                                                       \
  V(xxlxor, XXLXOR, 0xF00004D0)                                               \
  /* VSX Merge High Word */                                                   \
  V(xxmrghw, XXMRGHW, 0xF0000090)                                             \
  /* VSX Merge Low Word */                                                    \
  V(xxmrglw, XXMRGLW, 0xF0000190)                                             \
  /* VSX Permute Doubleword Immediate */                                      \
  V(xxpermdi, XXPERMDI, 0xF0000050)                                           \
  /* VSX Shift Left Double by Word Immediate */                               \
  V(xxsldwi, XXSLDWI, 0xF0000010)                                             \
  /* VSX Splat Word */                                                        \
  V(xxspltw, XXSPLTW, 0xF0000290)

#define PPC_XX3_OPCODE_VECTOR_LIST(V)  \
  PPC_XX3_OPCODE_VECTOR_A_FORM_LIST(V) \
  PPC_XX3_OPCODE_VECTOR_B_FORM_LIST(V)

#define PPC_Z23_OPCODE_LIST(V)                                    \
  /* Decimal Quantize */                                          \
  V(dqua, DQUA, 0xEC000006)                                       \
  /* Decimal Quantize Immediate */                                \
  V(dquai, DQUAI, 0xEC000086)                                     \
  /* Decimal Quantize Immediate Quad */                           \
  V(dquaiq, DQUAIQ, 0xFC000086)                                   \
  /* Decimal Quantize Quad */                                     \
  V(dquaq, DQUAQ, 0xFC000006)                                     \
  /* Decimal Floating Round To FP Integer Without Inexact */      \
  V(drintn, DRINTN, 0xEC0001C6)                                   \
  /* Decimal Floating Round To FP Integer Without Inexact Quad */ \
  V(drintnq, DRINTNQ, 0xFC0001C6)                                 \
  /* Decimal Floating Round To FP Integer With Inexact */         \
  V(drintx, DRINTX, 0xEC0000C6)                                   \
  /* Decimal Floating Round To FP Integer With Inexact Quad */    \
  V(drintxq, DRINTXQ, 0xFC0000C6)                                 \
  /* Decimal Floating Reround */                                  \
  V(drrnd, DRRND, 0xEC000046)                                     \
  /* Decimal Floating Reround Quad */                             \
  V(drrndq, DRRNDQ, 0xFC000046)

#define PPC_Z22_OPCODE_LIST(V)                                  \
  /* Decimal Floating Shift Coefficient Left Immediate */       \
  V(dscli, DSCLI, 0xEC000084)                                   \
  /* Decimal Floating Shift Coefficient Left Immediate Quad */  \
  V(dscliq, DSCLIQ, 0xFC000084)                                 \
  /* Decimal Floating Shift Coefficient Right Immediate */      \
  V(dscri, DSCRI, 0xEC0000C4)                                   \
  /* Decimal Floating Shift Coefficient Right Immediate Quad */ \
  V(dscriq, DSCRIQ, 0xFC0000C4)                                 \
  /* Decimal Floating Test Data Class */                        \
  V(dtstdc, DTSTDC, 0xEC000184)                                 \
  /* Decimal Floating Test Data Class Quad */                   \
  V(dtstdcq, DTSTDCQ, 0xFC000184)                               \
  /* Decimal Floating Test Data Group */                        \
  V(dtstdg, DTSTDG, 0xEC0001C4)                                 \
  /* Decimal Floating Test Data Group Quad */                   \
  V(dtstdgq, DTSTDGQ, 0xFC0001C4)

#define PPC_XX2_OPCODE_VECTOR_A_FORM_LIST(V)                                 \
  /* VSX Vector Absolute Value Double-Precision */                           \
  V(xvabsdp, XVABSDP, 0xF0000764)                                            \
  /* VSX Vector Negate Double-Precision */                                   \
  V(xvnegdp, XVNEGDP, 0xF00007E4)                                            \
  /* VSX Vector Square Root Double-Precision */                              \
  V(xvsqrtdp, XVSQRTDP, 0xF000032C)                                          \
  /* VSX Vector Absolute Value Single-Precision */                           \
  V(xvabssp, XVABSSP, 0xF0000664)                                            \
  /* VSX Vector Negate Single-Precision */                                   \
  V(xvnegsp, XVNEGSP, 0xF00006E4)                                            \
  /* VSX Vector Reciprocal Estimate Single-Precision */                      \
  V(xvresp, XVRESP, 0xF0000268)                                              \
  /* VSX Vector Reciprocal Square Root Estimate Single-Precision */          \
  V(xvrsqrtesp, XVRSQRTESP, 0xF0000228)                                      \
  /* VSX Vector Square Root Single-Precision */                              \
  V(xvsqrtsp, XVSQRTSP, 0xF000022C)                                          \
  /* VSX Vector Convert Single-Precision to Signed Fixed-Point Word */       \
  /* Saturate */                                                             \
  V(xvcvspsxws, XVCVSPSXWS, 0xF0000260)                                      \
  /* VSX Vector Convert Single-Precision to Unsigned Fixed-Point Word */     \
  /* Saturate */                                                             \
  V(xvcvspuxws, XVCVSPUXWS, 0xF0000220)                                      \
  /* VSX Vector Convert Signed Fixed-Point Word to Single-Precision */       \
  V(xvcvsxwsp, XVCVSXWSP, 0xF00002E0)                                        \
  /* VSX Vector Convert Unsigned Fixed-Point Word to Single-Precision */     \
  V(xvcvuxwsp, XVCVUXWSP, 0xF00002A0)                                        \
  /* VSX Vector Round to Double-Precision Integer toward +Infinity */        \
  V(xvrdpip, XVRDPIP, 0xF00003A4)                                            \
  /* VSX Vector Round to Double-Precision Integer toward -Infinity */        \
  V(xvrdpim, XVRDPIM, 0xF00003E4)                                            \
  /* VSX Vector Round to Double-Precision Integer toward Zero */             \
  V(xvrdpiz, XVRDPIZ, 0xF0000364)                                            \
  /* VSX Vector Round to Double-Precision Integer */                         \
  V(xvrdpi, XVRDPI, 0xF0000324)                                              \
  /* VSX Vector Round to Single-Precision Integer toward +Infinity */        \
  V(xvrspip, XVRSPIP, 0xF00002A4)                                            \
  /* VSX Vector Round to Single-Precision Integer toward -Infinity */        \
  V(xvrspim, XVRSPIM, 0xF00002E4)                                            \
  /* VSX Vector Round to Single-Precision Integer toward Zero */             \
  V(xvrspiz, XVRSPIZ, 0xF0000264)                                            \
  /* VSX Vector Round to Single-Precision Integer */                         \
  V(xvrspi, XVRSPI, 0xF0000224)                                              \
  /* VSX Vector Convert Signed Fixed-Point Doubleword to Double-Precision */ \
  V(xvcvsxddp, XVCVSXDDP, 0xF00007E0)                                        \
  /* VSX Vector Convert Unsigned Fixed-Point Doubleword to Double- */        \
  /* Precision */                                                            \
  V(xvcvuxddp, XVCVUXDDP, 0xF00007A0)                                        \
  /* VSX Vector Convert Single-Precision to Double-Precision */              \
  V(xvcvspdp, XVCVSPDP, 0xF0000724)                                          \
  /* VSX Vector Convert Double-Precision to Single-Precision */              \
  V(xvcvdpsp, XVCVDPSP, 0xF0000624)                                          \
  /* VSX Vector Convert Double-Precision to Signed Fixed-Point Word */       \
  /* Saturate */                                                             \
  V(xvcvdpsxws, XVCVDPSXWS, 0xF0000360)                                      \
  /* VSX Vector Convert Double-Precision to Unsigned Fixed-Point Word */     \
  /* Saturate */                                                             \
  V(xvcvdpuxws, XVCVDPUXWS, 0xF0000320)

#define PPC_XX2_OPCODE_SCALAR_A_FORM_LIST(V)                                \
  /* VSX Scalar Convert Double-Precision to Single-Precision format Non- */ \
  /* signalling */                                                          \
  V(xscvdpspn, XSCVDPSPN, 0xF000042C)                                       \
  /* VSX Scalar Convert Single-Precision to Double-Precision format Non- */ \
  /* signalling */                                                          \
  V(xscvspdpn, XSCVSPDPN, 0xF000052C)

#define PPC_XX2_OPCODE_B_FORM_LIST(V)  \
  /* Vector Byte-Reverse Quadword */   \
  V(xxbrq, XXBRQ, 0xF01F076C)          \
  /* Vector Byte-Reverse Doubleword */ \
  V(xxbrd, XXBRD, 0xF017076C)          \
  /* Vector Byte-Reverse Word */       \
  V(xxbrw, XXBRW, 0xF00F076C)          \
  /* Vector Byte-Reverse Halfword */   \
  V(xxbrh, XXBRH, 0xF007076C)

#define PPC_XX2_OPCODE_UNUSED_LIST(V)                                        \
  /* VSX Scalar Square Root Double-Precision */                              \
  V(xssqrtdp, XSSQRTDP, 0xF000012C)                                          \
  /* VSX Scalar Reciprocal Estimate Single-Precision */                      \
  V(xsresp, XSRESP, 0xF0000068)                                              \
  /* VSX Scalar Reciprocal Square Root Estimate Single-Precision */          \
  V(xsrsqrtesp, XSRSQRTESP, 0xF0000028)                                      \
  /* VSX Scalar Square Root Single-Precision */                              \
  V(xssqrtsp, XSSQRTSP, 0xF000002C)                                          \
  /* VSX Scalar Absolute Value Double-Precision */                           \
  V(xsabsdp, XSABSDP, 0xF0000564)                                            \
  /* VSX Scalar Convert Double-Precision to Single-Precision */              \
  V(xscvdpsp, XSCVDPSP, 0xF0000424)                                          \
  /* VSX Scalar Convert Double-Precision to Signed Fixed-Point Doubleword */ \
  /* Saturate */                                                             \
  V(xscvdpsxds, XSCVDPSXDS, 0xF0000560)                                      \
  /* VSX Scalar Convert Double-Precision to Signed Fixed-Point Word */       \
  /* Saturate */                                                             \
  V(xscvdpsxws, XSCVDPSXWS, 0xF0000160)                                      \
  /* VSX Scalar Convert Double-Precision to Unsigned Fixed-Point */          \
  /* Doubleword Saturate */                                                  \
  V(xscvdpuxds, XSCVDPUXDS, 0xF0000520)                                      \
  /* VSX Scalar Convert Double-Precision to Unsigned Fixed-Point Word */     \
  /* Saturate */                                                             \
  V(xscvdpuxws, XSCVDPUXWS, 0xF0000120)                                      \
  /* VSX Scalar Convert Single-Precision to Double-Precision (p=1) */        \
  V(xscvspdp, XSCVSPDP, 0xF0000524)                                          \
  /* VSX Scalar Convert Signed Fixed-Point Doubleword to Double-Precision */ \
  V(xscvsxddp, XSCVSXDDP, 0xF00005E0)                                        \
  /* VSX Scalar Convert Signed Fixed-Point Doubleword to Single-Precision */ \
  V(xscvsxdsp, XSCVSXDSP, 0xF00004E0)                                        \
  /* VSX Scalar Convert Unsigned Fixed-Point Doubleword to Double- */        \
  /* Precision */                                                            \
  V(xscvuxddp, XSCVUXDDP, 0xF00005A0)                                        \
  /* VSX Scalar Convert Unsigned Fixed-Point Doubleword to Single- */        \
  /* Precision */                                                            \
  V(xscvuxdsp, XSCVUXDSP, 0xF00004A0)                                        \
  /* VSX Scalar Negative Absolute Value Double-Precision */                  \
  V(xsnabsdp, XSNABSDP, 0xF00005A4)                                          \
  /* VSX Scalar Negate Double-Precision */                                   \
  V(xsnegdp, XSNEGDP, 0xF00005E4)                                            \
  /* VSX Scalar Round to Double-Precision Integer */                         \
  V(xsrdpi, XSRDPI, 0xF0000124)                                              \
  /* VSX Scalar Round to Double-Precision Integer using Current rounding */  \
  /* mode */                                                                 \
  V(xsrdpic, XSRDPIC, 0xF00001AC)                                            \
  /* VSX Scalar Round to Double-Precision Integer toward -Infinity */        \
  V(xsrdpim, XSRDPIM, 0xF00001E4)                                            \
  /* VSX Scalar Round to Double-Precision Integer toward +Infinity */        \
  V(xsrdpip, XSRDPIP, 0xF00001A4)                                            \
  /* VSX Scalar Round to Double-Precision Integer toward Zero */             \
  V(xsrdpiz, XSRDPIZ, 0xF0000164)                                            \
  /* VSX Scalar Round to Single-Precision */                                 \
  V(xsrsp, XSRSP, 0xF0000464)                                                \
  /* VSX Scalar Reciprocal Square Root Estimate Double-Precision */          \
  V(xsrsqrtedp, XSRSQRTEDP, 0xF0000128)                                      \
  /* VSX Scalar Test for software Square Root Double-Precision */            \
  V(xstsqrtdp, XSTSQRTDP, 0xF00001A8)                                        \
  /* VSX Vector Convert Double-Precision to Signed Fixed-Point Doubleword */ \
  /* Saturate */                                                             \
  V(xvcvdpsxds, XVCVDPSXDS, 0xF0000760)                                      \
  /* VSX Vector Convert Double-Precision to Unsigned Fixed-Point */          \
  /* Doubleword Saturate */                                                  \
  V(xvcvdpuxds, XVCVDPUXDS, 0xF0000720)                                      \
  /* VSX Vector Convert Single-Precision to Signed Fixed-Point Doubleword */ \
  /* Saturate */                                                             \
  V(xvcvspsxds, XVCVSPSXDS, 0xF0000660)                                      \
  /* VSX Vector Convert Single-Precision to Unsigned Fixed-Point */          \
  /* Doubleword Saturate */                                                  \
  V(xvcvspuxds, XVCVSPUXDS, 0xF0000620)                                      \
  /* VSX Vector Convert Signed Fixed-Point Doubleword to Single-Precision */ \
  V(xvcvsxdsp, XVCVSXDSP, 0xF00006E0)                                        \
  /* VSX Vector Convert Signed Fixed-Point Word to Double-Precision */       \
  V(xvcvsxwdp, XVCVSXWDP, 0xF00003E0)                                        \
  /* VSX Vector Convert Unsigned Fixed-Point Doubleword to Single- */        \
  /* Precision */                                                            \
  V(xvcvuxdsp, XVCVUXDSP, 0xF00006A0)                                        \
  /* VSX Vector Convert Unsigned Fixed-Point Word to Double-Precision */     \
  V(xvcvuxwdp, XVCVUXWDP, 0xF00003A0)                                        \
  /* VSX Vector Negative Absolute Value Double-Precision */                  \
  V(xvnabsdp, XVNABSDP, 0xF00007A4)                                          \
  /* VSX Vector Negative Absolute Value Single-Precision */                  \
  V(xvnabssp, XVNABSSP, 0xF00006A4)                                          \
  /* VSX Vector Round to Double-Precision Integer using Current rounding */  \
  /* mode */                                                                 \
  V(xvrdpic, XVRDPIC, 0xF00003AC)                                            \
  /* VSX Vector Round to Single-Precision Integer using Current rounding */  \
  /* mode */                                                                 \
  V(xvrspic, XVRSPIC, 0xF00002AC)                                            \
  /* VSX Vector Reciprocal Square Root Estimate Double-Precision */          \
  V(xvrsqrtedp, XVRSQRTEDP, 0xF0000328)                                      \
  /* VSX Vector Test for software Square Root Double-Precision */            \
  V(xvtsqrtdp, XVTSQRTDP, 0xF00003A8)                                        \
  /* VSX Vector Test for software Square Root Single-Precision */            \
  V(xvtsqrtsp, XVTSQRTSP, 0xF00002A8)                                        \
  /* Vector Splat Immediate Byte */                                          \
  V(xxspltib, XXSPLTIB, 0xF00002D0)

#define PPC_XX2_OPCODE_LIST(V)         \
  PPC_XX2_OPCODE_VECTOR_A_FORM_LIST(V) \
  PPC_XX2_OPCODE_SCALAR_A_FORM_LIST(V) \
  PPC_XX2_OPCODE_B_FORM_LIST(V)        \
  PPC_XX2_OPCODE_UNUSED_LIST(V)

#define PPC_EVX_OPCODE_LIST(V)                                                \
  /* Vector Load Double Word into Double Word by External PID Indexed */      \
  V(evlddepx, EVLDDEPX, 0x7C00063E)                                           \
  /* Vector Store Double of Double by External PID Indexed */                 \
  V(evstddepx, EVSTDDEPX, 0x7C00073E)                                         \
  /* Bit Reversed Increment */                                                \
  V(brinc, BRINC, 0x1000020F)                                                 \
  /* Vector Absolute Value */                                                 \
  V(evabs, EVABS, 0x10000208)                                                 \
  /* Vector Add Immediate Word */                                             \
  V(evaddiw, EVADDIW, 0x10000202)                                             \
  /* Vector Add Signed, Modulo, Integer to Accumulator Word */                \
  V(evaddsmiaaw, EVADDSMIAAW, 0x100004C9)                                     \
  /* Vector Add Signed, Saturate, Integer to Accumulator Word */              \
  V(evaddssiaaw, EVADDSSIAAW, 0x100004C1)                                     \
  /* Vector Add Unsigned, Modulo, Integer to Accumulator Word */              \
  V(evaddumiaaw, EVADDUMIAAW, 0x100004C8)                                     \
  /* Vector Add Unsigned, Saturate, Integer to Accumulator Word */            \
  V(evaddusiaaw, EVADDUSIAAW, 0x100004C0)                                     \
  /* Vector Add Word */                                                       \
  V(evaddw, EVADDW, 0x10000200)                                               \
  /* Vector AND */                                                            \
  V(evand, EVAND, 0x10000211)                                                 \
  /* Vector AND with Complement */                                            \
  V(evandc, EVANDC, 0x10000212)                                               \
  /* Vector Compare Equal */                                                  \
  V(evcmpeq, EVCMPEQ, 0x10000234)                                             \
  /* Vector Compare Greater Than Signed */                                    \
  V(evcmpgts, EVCMPGTS, 0x10000231)                                           \
  /* Vector Compare Greater Than Unsigned */                                  \
  V(evcmpgtu, EVCMPGTU, 0x10000230)                                           \
  /* Vector Compare Less Than Signed */                                       \
  V(evcmplts, EVCMPLTS, 0x10000233)                                           \
  /* Vector Compare Less Than Unsigned */                                     \
  V(evcmpltu, EVCMPLTU, 0x10000232)                                           \
  /* Vector Count Leading Signed Bits Word */                                 \
  V(evcntlsw, EVCNTLSW, 0x1000020E)                                           \
  /* Vector Count Leading Zeros Word */                                       \
  V(evcntlzw, EVCNTLZW, 0x1000020D)                                           \
  /* Vector Divide Word Signed */                                             \
  V(evdivws, EVDIVWS, 0x100004C6)                                             \
  /* Vector Divide Word Unsigned */                                           \
  V(evdivwu, EVDIVWU, 0x100004C7)                                             \
  /* Vector Equivalent */                                                     \
  V(eveqv, EVEQV, 0x10000219)                                                 \
  /* Vector Extend Sign Byte */                                               \
  V(evextsb, EVEXTSB, 0x1000020A)                                             \
  /* Vector Extend Sign Half Word */                                          \
  V(evextsh, EVEXTSH, 0x1000020B)                                             \
  /* Vector Load Double Word into Double Word */                              \
  V(evldd, EVLDD, 0x10000301)                                                 \
  /* Vector Load Double Word into Double Word Indexed */                      \
  V(evlddx, EVLDDX, 0x10000300)                                               \
  /* Vector Load Double into Four Half Words */                               \
  V(evldh, EVLDH, 0x10000305)                                                 \
  /* Vector Load Double into Four Half Words Indexed */                       \
  V(evldhx, EVLDHX, 0x10000304)                                               \
  /* Vector Load Double into Two Words */                                     \
  V(evldw, EVLDW, 0x10000303)                                                 \
  /* Vector Load Double into Two Words Indexed */                             \
  V(evldwx, EVLDWX, 0x10000302)                                               \
  /* Vector Load Half Word into Half Words Even and Splat */                  \
  V(evlhhesplat, EVLHHESPLAT, 0x10000309)                                     \
  /* Vector Load Half Word into Half Words Even and Splat Indexed */          \
  V(evlhhesplatx, EVLHHESPLATX, 0x10000308)                                   \
  /* Vector Load Half Word into Half Word Odd Signed and Splat */             \
  V(evlhhossplat, EVLHHOSSPLAT, 0x1000030F)                                   \
  /* Vector Load Half Word into Half Word Odd Signed and Splat Indexed */     \
  V(evlhhossplatx, EVLHHOSSPLATX, 0x1000030E)                                 \
  /* Vector Load Half Word into Half Word Odd Unsigned and Splat */           \
  V(evlhhousplat, EVLHHOUSPLAT, 0x1000030D)                                   \
  /* Vector Load Half Word into Half Word Odd Unsigned and Splat Indexed */   \
  V(evlhhousplatx, EVLHHOUSPLATX, 0x1000030C)                                 \
  /* Vector Load Word into Two Half Words Even */                             \
  V(evlwhe, EVLWHE, 0x10000311)                                               \
  /* Vector Load Word into Two Half Words Odd Signed (with sign extension) */ \
  V(evlwhos, EVLWHOS, 0x10000317)                                             \
  /* Vector Load Word into Two Half Words Odd Signed Indexed (with sign */    \
  /* extension) */                                                            \
  V(evlwhosx, EVLWHOSX, 0x10000316)                                           \
  /* Vector Load Word into Two Half Words Odd Unsigned (zero-extended) */     \
  V(evlwhou, EVLWHOU, 0x10000315)                                             \
  /* Vector Load Word into Two Half Words Odd Unsigned Indexed (zero- */      \
  /* extended) */                                                             \
  V(evlwhoux, EVLWHOUX, 0x10000314)                                           \
  /* Vector Load Word into Two Half Words and Splat */                        \
  V(evlwhsplat, EVLWHSPLAT, 0x1000031D)                                       \
  /* Vector Load Word into Two Half Words and Splat Indexed */                \
  V(evlwhsplatx, EVLWHSPLATX, 0x1000031C)                                     \
  /* Vector Load Word into Word and Splat */                                  \
  V(evlwwsplat, EVLWWSPLAT, 0x10000319)                                       \
  /* Vector Load Word into Word and Splat Indexed */                          \
  V(evlwwsplatx, EVLWWSPLATX, 0x10000318)                                     \
  /* Vector Merge High */                                                     \
  V(evmergehi, EVMERGEHI, 0x1000022C)                                         \
  /* Vector Merge High/Low */                                                 \
  V(evmergehilo, EVMERGEHILO, 0x1000022E)                                     \
  /* Vector Merge Low */                                                      \
  V(evmergelo, EVMERGELO, 0x1000022D)                                         \
  /* Vector Merge Low/High */                                                 \
  V(evmergelohi, EVMERGELOHI, 0x1000022F)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Signed, Modulo, Fractional */ \
  /* and Accumulate */                                                        \
  V(evmhegsmfaa, EVMHEGSMFAA, 0x1000052B)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Signed, Modulo, Fractional */ \
  /* and Accumulate Negative */                                               \
  V(evmhegsmfan, EVMHEGSMFAN, 0x100005AB)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Signed, Modulo, Integer */    \
  /* and Accumulate */                                                        \
  V(evmhegsmiaa, EVMHEGSMIAA, 0x10000529)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Signed, Modulo, Integer */    \
  /* and Accumulate Negative */                                               \
  V(evmhegsmian, EVMHEGSMIAN, 0x100005A9)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Unsigned, Modulo, Integer */  \
  /* and Accumulate */                                                        \
  V(evmhegumiaa, EVMHEGUMIAA, 0x10000528)                                     \
  /* Vector Multiply Half Words, Even, Guarded, Unsigned, Modulo, Integer */  \
  /* and Accumulate Negative */                                               \
  V(evmhegumian, EVMHEGUMIAN, 0x100005A8)                                     \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Fractional */          \
  V(evmhesmf, EVMHESMF, 0x1000040B)                                           \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Fractional to */       \
  /* Accumulator */                                                           \
  V(evmhesmfa, EVMHESMFA, 0x1000042B)                                         \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Fractional and */      \
  /* Accumulate into Words */                                                 \
  V(evmhesmfaaw, EVMHESMFAAW, 0x1000050B)                                     \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Fractional and */      \
  /* Accumulate Negative into Words */                                        \
  V(evmhesmfanw, EVMHESMFANW, 0x1000058B)                                     \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Integer */             \
  V(evmhesmi, EVMHESMI, 0x10000409)                                           \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Integer to */          \
  /* Accumulator */                                                           \
  V(evmhesmia, EVMHESMIA, 0x10000429)                                         \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Integer and */         \
  /* Accumulate into Words */                                                 \
  V(evmhesmiaaw, EVMHESMIAAW, 0x10000509)                                     \
  /* Vector Multiply Half Words, Even, Signed, Modulo, Integer and */         \
  /* Accumulate Negative into Words */                                        \
  V(evmhesmianw, EVMHESMIANW, 0x10000589)                                     \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Fractional */        \
  V(evmhessf, EVMHESSF, 0x10000403)                                           \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Fractional to */     \
  /* Accumulator */                                                           \
  V(evmhessfa, EVMHESSFA, 0x10000423)                                         \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Fractional and */    \
  /* Accumulate into Words */                                                 \
  V(evmhessfaaw, EVMHESSFAAW, 0x10000503)                                     \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Fractional and */    \
  /* Accumulate Negative into Words */                                        \
  V(evmhessfanw, EVMHESSFANW, 0x10000583)                                     \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Integer and */       \
  /* Accumulate into Words */                                                 \
  V(evmhessiaaw, EVMHESSIAAW, 0x10000501)                                     \
  /* Vector Multiply Half Words, Even, Signed, Saturate, Integer and */       \
  /* Accumulate Negative into Words */                                        \
  V(evmhessianw, EVMHESSIANW, 0x10000581)                                     \
  /* Vector Multiply Half Words, Even, Unsigned, Modulo, Integer */           \
  V(evmheumi, EVMHEUMI, 0x10000408)                                           \
  /* Vector Multiply Half Words, Even, Unsigned, Modulo, Integer to */        \
  /* Accumulator */                                                           \
  V(evmheumia, EVMHEUMIA, 0x10000428)                                         \
  /* Vector Multiply Half Words, Even, Unsigned, Modulo, Integer and */       \
  /* Accumulate into Words */                                                 \
  V(evmheumiaaw, EVMHEUMIAAW, 0x10000508)                                     \
  /* Vector Multiply Half Words, Even, Unsigned, Modulo, Integer and */       \
  /* Accumulate Negative into Words */                                        \
  V(evmheumianw, EVMHEUMIANW, 0x10000588)                                     \
  /* Vector Multiply Half Words, Even, Unsigned, Saturate, Integer and */     \
  /* Accumulate into Words */                                                 \
  V(evmheusiaaw, EVMHEUSIAAW, 0x10000500)                                     \
  /* Vector Multiply Half Words, Even, Unsigned, Saturate, Integer and */     \
  /* Accumulate Negative into Words */                                        \
  V(evmheusianw, EVMHEUSIANW, 0x10000580)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Signed, Modulo, Fractional */  \
  /* and Accumulate */                                                        \
  V(evmhogsmfaa, EVMHOGSMFAA, 0x1000052F)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Signed, Modulo, Fractional */  \
  /* and Accumulate Negative */                                               \
  V(evmhogsmfan, EVMHOGSMFAN, 0x100005AF)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Signed, Modulo, Integer, */    \
  /* and Accumulate */                                                        \
  V(evmhogsmiaa, EVMHOGSMIAA, 0x1000052D)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Signed, Modulo, Integer and */ \
  /* Accumulate Negative */                                                   \
  V(evmhogsmian, EVMHOGSMIAN, 0x100005AD)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Unsigned, Modulo, Integer */   \
  /* and Accumulate */                                                        \
  V(evmhogumiaa, EVMHOGUMIAA, 0x1000052C)                                     \
  /* Vector Multiply Half Words, Odd, Guarded, Unsigned, Modulo, Integer */   \
  /* and Accumulate Negative */                                               \
  V(evmhogumian, EVMHOGUMIAN, 0x100005AC)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Fractional */           \
  V(evmhosmf, EVMHOSMF, 0x1000040F)                                           \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Fractional to */        \
  /* Accumulator */                                                           \
  V(evmhosmfa, EVMHOSMFA, 0x1000042F)                                         \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Fractional and */       \
  /* Accumulate into Words */                                                 \
  V(evmhosmfaaw, EVMHOSMFAAW, 0x1000050F)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Fractional and */       \
  /* Accumulate Negative into Words */                                        \
  V(evmhosmfanw, EVMHOSMFANW, 0x1000058F)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Integer */              \
  V(evmhosmi, EVMHOSMI, 0x1000040D)                                           \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Integer to */           \
  /* Accumulator */                                                           \
  V(evmhosmia, EVMHOSMIA, 0x1000042D)                                         \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Integer and */          \
  /* Accumulate into Words */                                                 \
  V(evmhosmiaaw, EVMHOSMIAAW, 0x1000050D)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Modulo, Integer and */          \
  /* Accumulate Negative into Words */                                        \
  V(evmhosmianw, EVMHOSMIANW, 0x1000058D)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Fractional */         \
  V(evmhossf, EVMHOSSF, 0x10000407)                                           \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Fractional to */      \
  /* Accumulator */                                                           \
  V(evmhossfa, EVMHOSSFA, 0x10000427)                                         \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Fractional and */     \
  /* Accumulate into Words */                                                 \
  V(evmhossfaaw, EVMHOSSFAAW, 0x10000507)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Fractional and */     \
  /* Accumulate Negative into Words */                                        \
  V(evmhossfanw, EVMHOSSFANW, 0x10000587)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Integer and */        \
  /* Accumulate into Words */                                                 \
  V(evmhossiaaw, EVMHOSSIAAW, 0x10000505)                                     \
  /* Vector Multiply Half Words, Odd, Signed, Saturate, Integer and */        \
  /* Accumulate Negative into Words */                                        \
  V(evmhossianw, EVMHOSSIANW, 0x10000585)                                     \
  /* Vector Multiply Half Words, Odd, Unsigned, Modulo, Integer */            \
  V(evmhoumi, EVMHOUMI, 0x1000040C)                                           \
  /* Vector Multiply Half Words, Odd, Unsigned, Modulo, Integer to */         \
  /* Accumulator */                                                           \
  V(evmhoumia, EVMHOUMIA, 0x1000042C)                                         \
  /* Vector Multiply Half Words, Odd, Unsigned, Modulo, Integer and */        \
  /* Accumulate into Words */                                                 \
  V(evmhoumiaaw, EVMHOUMIAAW, 0x1000050C)                                     \
  /* Vector Multiply Half Words, Odd, Unsigned, Modulo, Integer and */        \
  /* Accumulate Negative into Words */                                        \
  V(evmhoumianw, EVMHOUMIANW, 0x1000058C)                                     \
  /* Vector Multiply Half Words, Odd, Unsigned, Saturate, Integer and */      \
  /* Accumulate into Words */                                                 \
  V(evmhousiaaw, EVMHOUSIAAW, 0x10000504)                                     \
  /* Vector Multiply Half Words, Odd, Unsigned, Saturate, Integer and */      \
  /* Accumulate Negative into Words */                                        \
  V(evmhousianw, EVMHOUSIANW, 0x10000584)                                     \
  /* Initialize Accumulator */                                                \
  V(evmra, EVMRA, 0x100004C4)                                                 \
  /* Vector Multiply Word High Signed, Modulo, Fractional */                  \
  V(evmwhsmf, EVMWHSMF, 0x1000044F)                                           \
  /* Vector Multiply Word High Signed, Modulo, Fractional to Accumulator */   \
  V(evmwhsmfa, EVMWHSMFA, 0x1000046F)                                         \
  /* Vector Multiply Word High Signed, Modulo, Integer */                     \
  V(evmwhsmi, EVMWHSMI, 0x1000044D)                                           \
  /* Vector Multiply Word High Signed, Modulo, Integer to Accumulator */      \
  V(evmwhsmia, EVMWHSMIA, 0x1000046D)                                         \
  /* Vector Multiply Word High Signed, Saturate, Fractional */                \
  V(evmwhssf, EVMWHSSF, 0x10000447)                                           \
  /* Vector Multiply Word High Signed, Saturate, Fractional to Accumulator */ \
  V(evmwhssfa, EVMWHSSFA, 0x10000467)                                         \
  /* Vector Multiply Word High Unsigned, Modulo, Integer */                   \
  V(evmwhumi, EVMWHUMI, 0x1000044C)                                           \
  /* Vector Multiply Word High Unsigned, Modulo, Integer to Accumulator */    \
  V(evmwhumia, EVMWHUMIA, 0x1000046C)                                         \
  /* Vector Multiply Word Low Signed, Modulo, Integer and Accumulate in */    \
  /* Words */                                                                 \
  V(evmwlsmiaaw, EVMWLSMIAAW, 0x10000549)                                     \
  /* Vector Multiply Word Low Signed, Modulo, Integer and Accumulate */       \
  /* Negative in Words */                                                     \
  V(evmwlsmianw, EVMWLSMIANW, 0x100005C9)                                     \
  /* Vector Multiply Word Low Signed, Saturate, Integer and Accumulate in */  \
  /* Words */                                                                 \
  V(evmwlssiaaw, EVMWLSSIAAW, 0x10000541)                                     \
  /* Vector Multiply Word Low Signed, Saturate, Integer and Accumulate */     \
  /* Negative in Words */                                                     \
  V(evmwlssianw, EVMWLSSIANW, 0x100005C1)                                     \
  /* Vector Multiply Word Low Unsigned, Modulo, Integer */                    \
  V(evmwlumi, EVMWLUMI, 0x10000448)                                           \
  /* Vector Multiply Word Low Unsigned, Modulo, Integer to Accumulator */     \
  V(evmwlumia, EVMWLUMIA, 0x10000468)                                         \
  /* Vector Multiply Word Low Unsigned, Modulo, Integer and Accumulate in */  \
  /* Words */                                                                 \
  V(evmwlumiaaw, EVMWLUMIAAW, 0x10000548)                                     \
  /* Vector Multiply Word Low Unsigned, Modulo, Integer and Accumulate */     \
  /* Negative in Words */                                                     \
  V(evmwlumianw, EVMWLUMIANW, 0x100005C8)                                     \
  /* Vector Multiply Word Low Unsigned, Saturate, Integer and Accumulate */   \
  /* in Words */                                                              \
  V(evmwlusiaaw, EVMWLUSIAAW, 0x10000540)                                     \
  /* Vector Multiply Word Low Unsigned, Saturate, Integer and Accumulate */   \
  /* Negative in Words */                                                     \
  V(evmwlusianw, EVMWLUSIANW, 0x100005C0)                                     \
  /* Vector Multiply Word Signed, Modulo, Fractional */                       \
  V(evmwsmf, EVMWSMF, 0x1000045B)                                             \
  /* Vector Multiply Word Signed, Modulo, Fractional to Accumulator */        \
  V(evmwsmfa, EVMWSMFA, 0x1000047B)                                           \
  /* Vector Multiply Word Signed, Modulo, Fractional and Accumulate */        \
  V(evmwsmfaa, EVMWSMFAA, 0x1000055B)                                         \
  /* Vector Multiply Word Signed, Modulo, Fractional and Accumulate */        \
  /* Negative */                                                              \
  V(evmwsmfan, EVMWSMFAN, 0x100005DB)                                         \
  /* Vector Multiply Word Signed, Modulo, Integer */                          \
  V(evmwsmi, EVMWSMI, 0x10000459)                                             \
  /* Vector Multiply Word Signed, Modulo, Integer to Accumulator */           \
  V(evmwsmia, EVMWSMIA, 0x10000479)                                           \
  /* Vector Multiply Word Signed, Modulo, Integer and Accumulate */           \
  V(evmwsmiaa, EVMWSMIAA, 0x10000559)                                         \
  /* Vector Multiply Word Signed, Modulo, Integer and Accumulate Negative */  \
  V(evmwsmian, EVMWSMIAN, 0x100005D9)                                         \
  /* Vector Multiply Word Signed, Saturate, Fractional */                     \
  V(evmwssf, EVMWSSF, 0x10000453)                                             \
  /* Vector Multiply Word Signed, Saturate, Fractional to Accumulator */      \
  V(evmwssfa, EVMWSSFA, 0x10000473)                                           \
  /* Vector Multiply Word Signed, Saturate, Fractional and Accumulate */      \
  V(evmwssfaa, EVMWSSFAA, 0x10000553)                                         \
  /* Vector Multiply Word Signed, Saturate, Fractional and Accumulate */      \
  /* Negative */                                                              \
  V(evmwssfan, EVMWSSFAN, 0x100005D3)                                         \
  /* Vector Multiply Word Unsigned, Modulo, Integer */                        \
  V(evmwumi, EVMWUMI, 0x10000458)                                             \
  /* Vector Multiply Word Unsigned, Modulo, Integer to Accumulator */         \
  V(evmwumia, EVMWUMIA, 0x10000478)                                           \
  /* Vector Multiply Word Unsigned, Modulo, Integer and Accumulate */         \
  V(evmwumiaa, EVMWUMIAA, 0x10000558)                                         \
  /* Vector Multiply Word Unsigned, Modulo, Integer and Accumulate */         \
  /* Negative */                                                              \
  V(evmwumian, EVMWUMIAN, 0x100005D8)                                         \
  /* Vector NAND */                                                           \
  V(evnand, EVNAND, 0x1000021E)                                               \
  /* Vector Negate */                                                         \
  V(evneg, EVNEG, 0x10000209)                                                 \
  /* Vector NOR */                                                            \
  V(evnor, EVNOR, 0x10000218)                                                 \
  /* Vector OR */                                                             \
  V(evor, EVOR, 0x10000217)                                                   \
  /* Vector OR with Complement */                                             \
  V(evorc, EVORC, 0x1000021B)                                                 \
  /* Vector Rotate Left Word */                                               \
  V(evrlw, EVRLW, 0x10000228)                                                 \
  /* Vector Rotate Left Word Immediate */                                     \
  V(evrlwi, EVRLWI, 0x1000022A)                                               \
  /* Vector Round Word */                                                     \
  V(evrndw, EVRNDW, 0x1000020C)                                               \
  /* Vector Shift Left Word */                                                \
  V(evslw, EVSLW, 0x10000224)                                                 \
  /* Vector Shift Left Word Immediate */                                      \
  V(evslwi, EVSLWI, 0x10000226)                                               \
  /* Vector Splat Fractional Immediate */                                     \
  V(evsplatfi, EVSPLATFI, 0x1000022B)                                         \
  /* Vector Splat Immediate */                                                \
  V(evsplati, EVSPLATI, 0x10000229)                                           \
  /* Vector Shift Right Word Immediate Signed */                              \
  V(evsrwis, EVSRWIS, 0x10000223)                                             \
  /* Vector Shift Right Word Immediate Unsigned */                            \
  V(evsrwiu, EVSRWIU, 0x10000222)                                             \
  /* Vector Shift Right Word Signed */                                        \
  V(evsrws, EVSRWS, 0x10000221)                                               \
  /* Vector Shift Right Word Unsigned */                                      \
  V(evsrwu, EVSRWU, 0x10000220)                                               \
  /* Vector Store Double of Double */                                         \
  V(evstdd, EVSTDD, 0x10000321)                                               \
  /* Vector Store Double of Double Indexed */                                 \
  V(evstddx, EVSTDDX, 0x10000320)                                             \
  /* Vector Store Double of Four Half Words */                                \
  V(evstdh, EVSTDH, 0x10000325)                                               \
  /* Vector Store Double of Four Half Words Indexed */                        \
  V(evstdhx, EVSTDHX, 0x10000324)                                             \
  /* Vector Store Double of Two Words */                                      \
  V(evstdw, EVSTDW, 0x10000323)                                               \
  /* Vector Store Double of Two Words Indexed */                              \
  V(evstdwx, EVSTDWX, 0x10000322)                                             \
  /* Vector Store Word of Two Half Words from Even */                         \
  V(evstwhe, EVSTWHE, 0x10000331)                                             \
  /* Vector Store Word of Two Half Words from Even Indexed */                 \
  V(evstwhex, EVSTWHEX, 0x10000330)                                           \
  /* Vector Store Word of Two Half Words from Odd */                          \
  V(evstwho, EVSTWHO, 0x10000335)                                             \
  /* Vector Store Word of Two Half Words from Odd Indexed */                  \
  V(evstwhox, EVSTWHOX, 0x10000334)                                           \
  /* Vector Store Word of Word from Even */                                   \
  V(evstwwe, EVSTWWE, 0x10000339)                                             \
  /* Vector Store Word of Word from Even Indexed */                           \
  V(evstwwex, EVSTWWEX, 0x10000338)                                           \
  /* Vector Store Word of Word from Odd */                                    \
  V(evstwwo, EVSTWWO, 0x1000033D)                                             \
  /* Vector Store Word of Word from Odd Indexed */                            \
  V(evstwwox, EVSTWWOX, 0x1000033C)                                           \
  /* Vector Subtract Signed, Modulo, Integer to Accumulator Word */           \
  V(evsubfsmiaaw, EVSUBFSMIAAW, 0x100004CB)                                   \
  /* Vector Subtract Signed, Saturate, Integer to Accumulator Word */         \
  V(evsubfssiaaw, EVSUBFSSIAAW, 0x100004C3)                                   \
  /* Vector Subtract Unsigned, Modulo, Integer to Accumulator Word */         \
  V(evsubfumiaaw, EVSUBFUMIAAW, 0x100004CA)                                   \
  /* Vector Subtract Unsigned, Saturate, Integer to Accumulator Word */       \
  V(evsubfusiaaw, EVSUBFUSIAAW, 0x100004C2)                                   \
  /* Vector Subtract from Word */                                             \
  V(evsubfw, EVSUBFW, 0x10000204)                                             \
  /* Vector Subtract Immediate from Word */                                   \
  V(evsubifw, EVSUBIFW, 0x10000206)                                           \
  /* Vector XOR */                                                            \
  V(evxor, EVXOR, 0x10000216)                                                 \
  /* Floating-Point Double-Precision Absolute Value */                        \
  V(efdabs, EFDABS, 0x100002E4)                                               \
  /* Floating-Point Double-Precision Add */                                   \
  V(efdadd, EFDADD, 0x100002E0)                                               \
  /* Floating-Point Double-Precision Convert from Single-Precision */         \
  V(efdcfs, EFDCFS, 0x100002EF)                                               \
  /* Convert Floating-Point Double-Precision from Signed Fraction */          \
  V(efdcfsf, EFDCFSF, 0x100002F3)                                             \
  /* Convert Floating-Point Double-Precision from Signed Integer */           \
  V(efdcfsi, EFDCFSI, 0x100002F1)                                             \
  /* Convert Floating-Point Double-Precision from Signed Integer */           \
  /* Doubleword */                                                            \
  V(efdcfsid, EFDCFSID, 0x100002E3)                                           \
  /* Convert Floating-Point Double-Precision from Unsigned Fraction */        \
  V(efdcfuf, EFDCFUF, 0x100002F2)                                             \
  /* Convert Floating-Point Double-Precision from Unsigned Integer */         \
  V(efdcfui, EFDCFUI, 0x100002F0)                                             \
  /* Convert Floating-Point Double-Precision fromUnsigned Integer */          \
  /* Doubleword */                                                            \
  V(efdcfuid, EFDCFUID, 0x100002E2)                                           \
  /* Floating-Point Double-Precision Compare Equal */                         \
  V(efdcmpeq, EFDCMPEQ, 0x100002EE)                                           \
  /* Floating-Point Double-Precision Compare Greater Than */                  \
  V(efdcmpgt, EFDCMPGT, 0x100002EC)                                           \
  /* Floating-Point Double-Precision Compare Less Than */                     \
  V(efdcmplt, EFDCMPLT, 0x100002ED)                                           \
  /* Convert Floating-Point Double-Precision to Signed Fraction */            \
  V(efdctsf, EFDCTSF, 0x100002F7)                                             \
  /* Convert Floating-Point Double-Precision to Signed Integer */             \
  V(efdctsi, EFDCTSI, 0x100002F5)                                             \
  /* Convert Floating-Point Double-Precision to Signed Integer Doubleword */  \
  /* with Round toward Zero */                                                \
  V(efdctsidz, EFDCTSIDZ, 0x100002EB)                                         \
  /* Convert Floating-Point Double-Precision to Signed Integer with Round */  \
  /* toward Zero */                                                           \
  V(efdctsiz, EFDCTSIZ, 0x100002FA)                                           \
  /* Convert Floating-Point Double-Precision to Unsigned Fraction */          \
  V(efdctuf, EFDCTUF, 0x100002F6)                                             \
  /* Convert Floating-Point Double-Precision to Unsigned Integer */           \
  V(efdctui, EFDCTUI, 0x100002F4)                                             \
  /* Convert Floating-Point Double-Precision to Unsigned Integer */           \
  /* Doubleword with Round toward Zero */                                     \
  V(efdctuidz, EFDCTUIDZ, 0x100002EA)                                         \
  /* Convert Floating-Point Double-Precision to Unsigned Integer with */      \
  /* Round toward Zero */                                                     \
  V(efdctuiz, EFDCTUIZ, 0x100002F8)                                           \
  /* Floating-Point Double-Precision Divide */                                \
  V(efddiv, EFDDIV, 0x100002E9)                                               \
  /* Floating-Point Double-Precision Multiply */                              \
  V(efdmul, EFDMUL, 0x100002E8)                                               \
  /* Floating-Point Double-Precision Negative Absolute Value */               \
  V(efdnabs, EFDNABS, 0x100002E5)                                             \
  /* Floating-Point Double-Precision Negate */                                \
  V(efdneg, EFDNEG, 0x100002E6)                                               \
  /* Floating-Point Double-Precision Subtract */                              \
  V(efdsub, EFDSUB, 0x100002E1)                                               \
  /* Floating-Point Double-Precision Test Equal */                            \
  V(efdtsteq, EFDTSTEQ, 0x100002FE)                                           \
  /* Floating-Point Double-Precision Test Greater Than */                     \
  V(efdtstgt, EFDTSTGT, 0x100002FC)                                           \
  /* Floating-Point Double-Precision Test Less Than */                        \
  V(efdtstlt, EFDTSTLT, 0x100002FD)                                           \
  /* Floating-Point Single-Precision Convert from Double-Precision */         \
  V(efscfd, EFSCFD, 0x100002CF)                                               \
  /* Floating-Point Absolute Value */                                         \
  V(efsabs, EFSABS, 0x100002C4)                                               \
  /* Floating-Point Add */                                                    \
  V(efsadd, EFSADD, 0x100002C0)                                               \
  /* Convert Floating-Point from Signed Fraction */                           \
  V(efscfsf, EFSCFSF, 0x100002D3)                                             \
  /* Convert Floating-Point from Signed Integer */                            \
  V(efscfsi, EFSCFSI, 0x100002D1)                                             \
  /* Convert Floating-Point from Unsigned Fraction */                         \
  V(efscfuf, EFSCFUF, 0x100002D2)                                             \
  /* Convert Floating-Point from Unsigned Integer */                          \
  V(efscfui, EFSCFUI, 0x100002D0)                                             \
  /* Floating-Point Compare Equal */                                          \
  V(efscmpeq, EFSCMPEQ, 0x100002CE)                                           \
  /* Floating-Point Compare Greater Than */                                   \
  V(efscmpgt, EFSCMPGT, 0x100002CC)                                           \
  /* Floating-Point Compare Less Than */                                      \
  V(efscmplt, EFSCMPLT, 0x100002CD)                                           \
  /* Convert Floating-Point to Signed Fraction */                             \
  V(efsctsf, EFSCTSF, 0x100002D7)                                             \
  /* Convert Floating-Point to Signed Integer */                              \
  V(efsctsi, EFSCTSI, 0x100002D5)                                             \
  /* Convert Floating-Point to Signed Integer with Round toward Zero */       \
  V(efsctsiz, EFSCTSIZ, 0x100002DA)                                           \
  /* Convert Floating-Point to Unsigned Fraction */                           \
  V(efsctuf, EFSCTUF, 0x100002D6)                                             \
  /* Convert Floating-Point to Unsigned Integer */                            \
  V(efsctui, EFSCTUI, 0x100002D4)                                             \
  /* Convert Floating-Point to Unsigned Integer with Round toward Zero */     \
  V(efsctuiz, EFSCTUIZ, 0x100002D8)                                           \
  /* Floating-Point Divide */                                                 \
  V(efsdiv, EFSDIV, 0x100002C9)                                               \
  /* Floating-Point Multiply */                                               \
  V(efsmul, EFSMUL, 0x100002C8)                                               \
  /* Floating-Point Negative Absolute Value */                                \
  V(efsnabs, EFSNABS, 0x100002C5)                                             \
  /* Floating-Point Negate */                                                 \
  V(efsneg, EFSNEG, 0x100002C6)                                               \
  /* Floating-Point Subtract */                                               \
  V(efssub, EFSSUB, 0x100002C1)                                               \
  /* Floating-Point Test Equal */                                             \
  V(efststeq, EFSTSTEQ, 0x100002DE)                                           \
  /* Floating-Point Test Greater Than */                                      \
  V(efststgt, EFSTSTGT, 0x100002DC)                                           \
  /* Floating-Point Test Less Than */                                         \
  V(efststlt, EFSTSTLT, 0x100002DD)                                           \
  /* Vector Floating-Point Absolute Value */                                  \
  V(evfsabs, EVFSABS, 0x10000284)                                             \
  /* Vector Floating-Point Add */                                             \
  V(evfsadd, EVFSADD, 0x10000280)                                             \
  /* Vector Convert Floating-Point from Signed Fraction */                    \
  V(evfscfsf, EVFSCFSF, 0x10000293)                                           \
  /* Vector Convert Floating-Point from Signed Integer */                     \
  V(evfscfsi, EVFSCFSI, 0x10000291)                                           \
  /* Vector Convert Floating-Point from Unsigned Fraction */                  \
  V(evfscfuf, EVFSCFUF, 0x10000292)                                           \
  /* Vector Convert Floating-Point from Unsigned Integer */                   \
  V(evfscfui, EVFSCFUI, 0x10000290)                                           \
  /* Vector Floating-Point Compare Equal */                                   \
  V(evfscmpeq, EVFSCMPEQ, 0x1000028E)                                         \
  /* Vector Floating-Point Compare Greater Than */                            \
  V(evfscmpgt, EVFSCMPGT, 0x1000028C)                                         \
  /* Vector Floating-Point Compare Less Than */                               \
  V(evfscmplt, EVFSCMPLT, 0x1000028D)                                         \
  /* Vector Convert Floating-Point to Signed Fraction */                      \
  V(evfsctsf, EVFSCTSF, 0x10000297)                                           \
  /* Vector Convert Floating-Point to Signed Integer */                       \
  V(evfsctsi, EVFSCTSI, 0x10000295)                                           \
  /* Vector Convert Floating-Point to Signed Integer with Round toward */     \
  /* Zero */                                                                  \
  V(evfsctsiz, EVFSCTSIZ, 0x1000029A)                                         \
  /* Vector Convert Floating-Point to Unsigned Fraction */                    \
  V(evfsctuf, EVFSCTUF, 0x10000296)                                           \
  /* Vector Convert Floating-Point to Unsigned Integer */                     \
  V(evfsctui, EVFSCTUI, 0x10000294)                                           \
  /* Vector Convert Floating-Point to Unsigned Integer with Round toward */   \
  /* Zero */                                                                  \
  V(evfsctuiz, EVFSCTUIZ, 0x10000298)                                         \
  /* Vector Floating-Point Divide */                                          \
  V(evfsdiv, EVFSDIV, 0x10000289)                                             \
  /* Vector Floating-Point Multiply */                                        \
  V(evfsmul, EVFSMUL, 0x10000288)                                             \
  /* Vector Floating-Point Negative Absolute Value */                         \
  V(evfsnabs, EVFSNABS, 0x10000285)                                           \
  /* Vector Floating-Point Negate */                                          \
  V(evfsneg, EVFSNEG, 0x10000286)                                             \
  /* Vector Floating-Point Subtract */                                        \
  V(evfssub, EVFSSUB, 0x10000281)                                             \
  /* Vector Floating-Point Test Equal */                                      \
  V(evfststeq, EVFSTSTEQ, 0x1000029E)                                         \
  /* Vector Floating-Point Test Greater Than */                               \
  V(evfststgt, EVFSTSTGT, 0x1000029C)                                         \
  /* Vector Floating-Point Test Less Than */                                  \
  V(evfststlt, EVFSTSTLT, 0x1000029D)

#define PPC_VC_OPCODE_LIST(V)                                    \
  /* Vector Compare Bounds Single-Precision */                   \
  V(vcmpbfp, VCMPBFP, 0x100003C6)                                \
  /* Vector Compare Equal To Single-Precision */                 \
  V(vcmpeqfp, VCMPEQFP, 0x100000C6)                              \
  /* Vector Compare Equal To Unsigned Byte */                    \
  V(vcmpequb, VCMPEQUB, 0x10000006)                              \
  /* Vector Compare Equal To Unsigned Doubleword */              \
  V(vcmpequd, VCMPEQUD, 0x100000C7)                              \
  /* Vector Compare Equal To Unsigned Halfword */                \
  V(vcmpequh, VCMPEQUH, 0x10000046)                              \
  /* Vector Compare Equal To Unsigned Word */                    \
  V(vcmpequw, VCMPEQUW, 0x10000086)                              \
  /* Vector Compare Greater Than or Equal To Single-Precision */ \
  V(vcmpgefp, VCMPGEFP, 0x100001C6)                              \
  /* Vector Compare Greater Than Single-Precision */             \
  V(vcmpgtfp, VCMPGTFP, 0x100002C6)                              \
  /* Vector Compare Greater Than Signed Byte */                  \
  V(vcmpgtsb, VCMPGTSB, 0x10000306)                              \
  /* Vector Compare Greater Than Signed Doubleword */            \
  V(vcmpgtsd, VCMPGTSD, 0x100003C7)                              \
  /* Vector Compare Greater Than Signed Halfword */              \
  V(vcmpgtsh, VCMPGTSH, 0x10000346)                              \
  /* Vector Compare Greater Than Signed Word */                  \
  V(vcmpgtsw, VCMPGTSW, 0x10000386)                              \
  /* Vector Compare Greater Than Unsigned Byte */                \
  V(vcmpgtub, VCMPGTUB, 0x10000206)                              \
  /* Vector Compare Greater Than Unsigned Doubleword */          \
  V(vcmpgtud, VCMPGTUD, 0x100002C7)                              \
  /* Vector Compare Greater Than Unsigned Halfword */            \
  V(vcmpgtuh, VCMPGTUH, 0x10000246)                              \
  /* Vector Compare Greater Than Unsigned Word */                \
  V(vcmpgtuw, VCMPGTUW, 0x10000286)

#define PPC_X_OPCODE_A_FORM_LIST(V) \
  /* Modulo Signed Dword */         \
  V(modsd, MODSD, 0x7C000612)       \
  /*  Modulo Unsigned Dword */      \
  V(modud, MODUD, 0x7C000212)       \
  /* Modulo Signed Word */          \
  V(modsw, MODSW, 0x7C000616)       \
  /* Modulo Unsigned Word */        \
  V(moduw, MODUW, 0x7C000216)

#define PPC_X_OPCODE_B_FORM_LIST(V)      \
  /* XOR */                              \
  V(xor_, XORX, 0x7C000278)              \
  /* AND */                              \
  V(and_, ANDX, 0x7C000038)              \
  /* AND with Complement */              \
  V(andc, ANDCX, 0x7C000078)             \
  /* OR */                               \
  V(orx, ORX, 0x7C000378)                \
  /* OR with Complement */               \
  V(orc, ORC, 0x7C000338)                \
  /* NOR */                              \
  V(nor, NORX, 0x7C0000F8)               \
  /* Shift Right Word */                 \
  V(srw, SRWX, 0x7C000430)               \
  /* Shift Left Word */                  \
  V(slw, SLWX, 0x7C000030)               \
  /* Shift Right Algebraic Word */       \
  V(sraw, SRAW, 0x7C000630)              \
  /* Shift Left Doubleword */            \
  V(sld, SLDX, 0x7C000036)               \
  /* Shift Right Algebraic Doubleword */ \
  V(srad, SRAD, 0x7C000634)              \
  /* Shift Right Doubleword */           \
  V(srd, SRDX, 0x7C000436)

#define PPC_X_OPCODE_C_FORM_LIST(V)    \
  /* Count Leading Zeros Word */       \
  V(cntlzw, CNTLZWX, 0x7C000034)       \
  /* Count Leading Zeros Doubleword */ \
  V(cntlzd, CNTLZDX, 0x7C000074)       \
  /* Count Tailing Zeros Word */       \
  V(cnttzw, CNTTZWX, 0x7C000434)       \
  /* Count Tailing Zeros Doubleword */ \
  V(cnttzd, CNTTZDX, 0x7C000474)       \
  /* Population Count Byte-wise */     \
  V(popcntb, POPCNTB, 0x7C0000F4)      \
  /* Population Count Words */         \
  V(popcntw, POPCNTW, 0x7C0002F4)      \
  /* Population Count Doubleword */    \
  V(popcntd, POPCNTD, 0x7C0003F4)      \
  /* Extend Sign Byte */               \
  V(extsb, EXTSB, 0x7C000774)          \
  /* Extend Sign Halfword */           \
  V(extsh, EXTSH, 0x7C000734)

#define PPC_X_OPCODE_D_FORM_LIST(V)                     \
  /* Load Halfword Byte-Reverse Indexed */              \
  V(lhbrx, LHBRX, 0x7C00062C)                           \
  /* Load Word Byte-Reverse Indexed */                  \
  V(lwbrx, LWBRX, 0x7C00042C)                           \
  /* Load Doubleword Byte-Reverse Indexed */            \
  V(ldbrx, LDBRX, 0x7C000428)                           \
  /* Load Byte and Zero Indexed */                      \
  V(lbzx, LBZX, 0x7C0000AE)                             \
  /* Load Byte and Zero with Update Indexed */          \
  V(lbzux, LBZUX, 0x7C0000EE)                           \
  /* Load Halfword and Zero Indexed */                  \
  V(lhzx, LHZX, 0x7C00022E)                             \
  /* Load Halfword and Zero with Update Indexed */      \
  V(lhzux, LHZUX, 0x7C00026E)                           \
  /* Load Halfword Algebraic Indexed */                 \
  V(lhax, LHAX, 0x7C0002AE)                             \
  /* Load Word and Zero Indexed */                      \
  V(lwzx, LWZX, 0x7C00002E)                             \
  /* Load Word and Zero with Update Indexed */          \
  V(lwzux, LWZUX, 0x7C00006E)                           \
  /* Load Doubleword Indexed */                         \
  V(ldx, LDX, 0x7C00002A)                               \
  /* Load Doubleword with Update Indexed */             \
  V(ldux, LDUX, 0x7C00006A)                             \
  /* Load Floating-Point Double Indexed */              \
  V(lfdx, LFDX, 0x7C0004AE)                             \
  /* Load Floating-Point Single Indexed */              \
  V(lfsx, LFSX, 0x7C00042E)                             \
  /* Load Floating-Point Double with Update Indexed */  \
  V(lfdux, LFDUX, 0x7C0004EE)                           \
  /* Load Floating-Point Single with Update Indexed */  \
  V(lfsux, LFSUX, 0x7C00046E)                           \
  /* Store Byte with Update Indexed */                  \
  V(stbux, STBUX, 0x7C0001EE)                           \
  /* Store Byte Indexed */                              \
  V(stbx, STBX, 0x7C0001AE)                             \
  /* Store Halfword with Update Indexed */              \
  V(sthux, STHUX, 0x7C00036E)                           \
  /* Store Halfword Indexed */                          \
  V(sthx, STHX, 0x7C00032E)                             \
  /* Store Word with Update Indexed */                  \
  V(stwux, STWUX, 0x7C00016E)                           \
  /* Store Word Indexed */                              \
  V(stwx, STWX, 0x7C00012E)                             \
  /* Store Doubleword with Update Indexed */            \
  V(stdux, STDUX, 0x7C00016A)                           \
  /* Store Doubleword Indexed */                        \
  V(stdx, STDX, 0x7C00012A)                             \
  /* Store Floating-Point Double with Update Indexed */ \
  V(stfdux, STFDUX, 0x7C0005EE)                         \
  /* Store Floating-Point Double Indexed */             \
  V(stfdx, STFDX, 0x7C0005AE)                           \
  /* Store Floating-Point Single with Update Indexed */ \
  V(stfsux, STFSUX, 0x7C00056E)                         \
  /* Store Floating-Point Single Indexed */             \
  V(stfsx, STFSX, 0x7C00052E)                           \
  /* Store Doubleword Byte-Reverse Indexed */           \
  V(stdbrx, STDBRX, 0x7C000528)                         \
  /* Store Word Byte-Reverse Indexed */                 \
  V(stwbrx, STWBRX, 0x7C00052C)                         \
  /* Store Halfword Byte-Reverse Indexed */             \
  V(sthbrx, STHBRX, 0x7C00072C)                         \
  /* Load Vector Indexed */                             \
  V(lvx, LVX, 0x7C0000CE)                               \
  /* Store Vector Indexed */                            \
  V(stvx, STVX, 0x7C0001CE)

#define PPC_X_OPCODE_E_FORM_LIST(V)          \
  /* Shift Right Algebraic Word Immediate */ \
  V(srawi, SRAWIX, 0x7C000670)

#define PPC_X_OPCODE_F_FORM_LIST(V) \
  /* Compare */                     \
  V(cmp, CMP, 0x7C000000)           \
  /* Compare Logical */             \
  V(cmpl, CMPL, 0x7C000040)

#define PPC_X_OPCODE_G_FORM_LIST(V) \
  /* Byte-Reverse Halfword */       \
  V(brh, BRH, 0x7C0001B6)           \
  /* Byte-Reverse Word */           \
  V(brw, BRW, 0x7C000136)           \
  /* Byte-Reverse Doubleword */     \
  V(brd, BRD, 0x7C000176)

#define PPC_X_OPCODE_EH_S_FORM_LIST(V)                    \
  /* Store Byte Conditional Indexed */                    \
  V(stbcx, STBCX, 0x7C00056D)                             \
  /* Store Halfword Conditional Indexed Xform */          \
  V(sthcx, STHCX, 0x7C0005AD)                             \
  /* Store Word Conditional Indexed & record CR0 */       \
  V(stwcx, STWCX, 0x7C00012D)                             \
  /* Store Doubleword Conditional Indexed & record CR0 */ \
  V(stdcx, STDCX, 0x7C0001AD)

#define PPC_X_OPCODE_EH_L_FORM_LIST(V)          \
  /* Load Byte And Reserve Indexed */           \
  V(lbarx, LBARX, 0x7C000068)                   \
  /* Load Halfword And Reserve Indexed Xform */ \
  V(lharx, LHARX, 0x7C0000E8)                   \
  /* Load Word and Reserve Indexed */           \
  V(lwarx, LWARX, 0x7C000028)                   \
  /* Load Doubleword And Reserve Indexed */     \
  V(ldarx, LDARX, 0x7C0000A8)

#define PPC_X_OPCODE_UNUSED_LIST(V)                                           \
  /* Bit Permute Doubleword */                                                \
  V(bpermd, BPERMD, 0x7C0001F8)                                               \
  /* Extend Sign Word */                                                      \
  V(extsw, EXTSW, 0x7C0007B4)                                                 \
  /* Load Word Algebraic with Update Indexed */                               \
  V(lwaux, LWAUX, 0x7C0002EA)                                                 \
  /* Load Word Algebraic Indexed */                                           \
  V(lwax, LWAX, 0x7C0002AA)                                                   \
  /* Parity Doubleword */                                                     \
  V(prtyd, PRTYD, 0x7C000174)                                                 \
  /* Trap Doubleword */                                                       \
  V(td, TD, 0x7C000088)                                                       \
  /* Branch Conditional to Branch Target Address Register */                  \
  V(bctar, BCTAR, 0x4C000460)                                                 \
  /* Compare Byte */                                                          \
  V(cmpb, CMPB, 0x7C0003F8)                                                   \
  /* Data Cache Block Flush */                                                \
  V(dcbf, DCBF, 0x7C0000AC)                                                   \
  /* Data Cache Block Store */                                                \
  V(dcbst, DCBST, 0x7C00006C)                                                 \
  /* Data Cache Block Touch */                                                \
  V(dcbt, DCBT, 0x7C00022C)                                                   \
  /* Data Cache Block Touch for Store */                                      \
  V(dcbtst, DCBTST, 0x7C0001EC)                                               \
  /* Data Cache Block Zero */                                                 \
  V(dcbz, DCBZ, 0x7C0007EC)                                                   \
  /* Equivalent */                                                            \
  V(eqv, EQV, 0x7C000238)                                                     \
  /* Instruction Cache Block Invalidate */                                    \
  V(icbi, ICBI, 0x7C0007AC)                                                   \
  /* NAND */                                                                  \
  V(nand, NAND, 0x7C0003B8)                                                   \
  /* Parity Word */                                                           \
  V(prtyw, PRTYW, 0x7C000134)                                                 \
  /* Synchronize */                                                           \
  V(sync, SYNC, 0x7C0004AC)                                                   \
  /* Trap Word */                                                             \
  V(tw, TW, 0x7C000008)                                                       \
  /* ExecuExecuted No Operation */                                            \
  V(xnop, XNOP, 0x68000000)                                                   \
  /* Convert Binary Coded Decimal To Declets */                               \
  V(cbcdtd, CBCDTD, 0x7C000274)                                               \
  /* Convert Declets To Binary Coded Decimal */                               \
  V(cdtbcd, CDTBCD, 0x7C000234)                                               \
  /* Decimal Floating Add */                                                  \
  V(dadd, DADD, 0xEC000004)                                                   \
  /* Decimal Floating Add Quad */                                             \
  V(daddq, DADDQ, 0xFC000004)                                                 \
  /* Decimal Floating Convert From Fixed */                                   \
  V(dcffix, DCFFIX, 0xEC000644)                                               \
  /* Decimal Floating Convert From Fixed Quad */                              \
  V(dcffixq, DCFFIXQ, 0xFC000644)                                             \
  /* Decimal Floating Compare Ordered */                                      \
  V(dcmpo, DCMPO, 0xEC000104)                                                 \
  /* Decimal Floating Compare Ordered Quad */                                 \
  V(dcmpoq, DCMPOQ, 0xFC000104)                                               \
  /* Decimal Floating Compare Unordered */                                    \
  V(dcmpu, DCMPU, 0xEC000504)                                                 \
  /* Decimal Floating Compare Unordered Quad */                               \
  V(dcmpuq, DCMPUQ, 0xFC000504)                                               \
  /* Decimal Floating Convert To DFP Long */                                  \
  V(dctdp, DCTDP, 0xEC000204)                                                 \
  /* Decimal Floating Convert To Fixed */                                     \
  V(dctfix, DCTFIX, 0xEC000244)                                               \
  /* Decimal Floating Convert To Fixed Quad */                                \
  V(dctfixq, DCTFIXQ, 0xFC000244)                                             \
  /* Decimal Floating Convert To DFP Extended */                              \
  V(dctqpq, DCTQPQ, 0xFC000204)                                               \
  /* Decimal Floating Decode DPD To BCD */                                    \
  V(ddedpd, DDEDPD, 0xEC000284)                                               \
  /* Decimal Floating Decode DPD To BCD Quad */                               \
  V(ddedpdq, DDEDPDQ, 0xFC000284)                                             \
  /* Decimal Floating Divide */                                               \
  V(ddiv, DDIV, 0xEC000444)                                                   \
  /* Decimal Floating Divide Quad */                                          \
  V(ddivq, DDIVQ, 0xFC000444)                                                 \
  /* Decimal Floating Encode BCD To DPD */                                    \
  V(denbcd, DENBCD, 0xEC000684)                                               \
  /* Decimal Floating Encode BCD To DPD Quad */                               \
  V(denbcdq, DENBCDQ, 0xFC000684)                                             \
  /* Decimal Floating Insert Exponent */                                      \
  V(diex, DIEX, 0xEC0006C4)                                                   \
  /* Decimal Floating Insert Exponent Quad */                                 \
  V(diexq, DIEXQ, 0xFC0006C4)                                                 \
  /* Decimal Floating Multiply */                                             \
  V(dmul, DMUL, 0xEC000044)                                                   \
  /* Decimal Floating Multiply Quad */                                        \
  V(dmulq, DMULQ, 0xFC000044)                                                 \
  /* Decimal Floating Round To DFP Long */                                    \
  V(drdpq, DRDPQ, 0xFC000604)                                                 \
  /* Decimal Floating Round To DFP Short */                                   \
  V(drsp, DRSP, 0xEC000604)                                                   \
  /* Decimal Floating Subtract */                                             \
  V(dsub, DSUB, 0xEC000404)                                                   \
  /* Decimal Floating Subtract Quad */                                        \
  V(dsubq, DSUBQ, 0xFC000404)                                                 \
  /* Decimal Floating Test Exponent */                                        \
  V(dtstex, DTSTEX, 0xEC000144)                                               \
  /* Decimal Floating Test Exponent Quad */                                   \
  V(dtstexq, DTSTEXQ, 0xFC000144)                                             \
  /* Decimal Floating Test Significance */                                    \
  V(dtstsf, DTSTSF, 0xEC000544)                                               \
  /* Decimal Floating Test Significance Quad */                               \
  V(dtstsfq, DTSTSFQ, 0xFC000544)                                             \
  /* Decimal Floating Extract Exponent */                                     \
  V(dxex, DXEX, 0xEC0002C4)                                                   \
  /* Decimal Floating Extract Exponent Quad */                                \
  V(dxexq, DXEXQ, 0xFC0002C4)                                                 \
  /* Decorated Storage Notify */                                              \
  V(dsn, DSN, 0x7C0003C6)                                                     \
  /* Load Byte with Decoration Indexed */                                     \
  V(lbdx, LBDX, 0x7C000406)                                                   \
  /* Load Doubleword with Decoration Indexed */                               \
  V(lddx, LDDX, 0x7C0004C6)                                                   \
  /* Load Floating Doubleword with Decoration Indexed */                      \
  V(lfddx, LFDDX, 0x7C000646)                                                 \
  /* Load Halfword with Decoration Indexed */                                 \
  V(lhdx, LHDX, 0x7C000446)                                                   \
  /* Load Word with Decoration Indexed */                                     \
  V(lwdx, LWDX, 0x7C000486)                                                   \
  /* Store Byte with Decoration Indexed */                                    \
  V(stbdx, STBDX, 0x7C000506)                                                 \
  /* Store Doubleword with Decoration Indexed */                              \
  V(stddx, STDDX, 0x7C0005C6)                                                 \
  /* Store Floating Doubleword with Decoration Indexed */                     \
  V(stfddx, STFDDX, 0x7C000746)                                               \
  /* Store Halfword with Decoration Indexed */                                \
  V(sthdx, STHDX, 0x7C000546)                                                 \
  /* Store Word with Decoration Indexed */                                    \
  V(stwdx, STWDX, 0x7C000586)                                                 \
  /* Data Cache Block Allocate */                                             \
  V(dcba, DCBA, 0x7C0005EC)                                                   \
  /* Data Cache Block Invalidate */                                           \
  V(dcbi, DCBI, 0x7C0003AC)                                                   \
  /* Instruction Cache Block Touch */                                         \
  V(icbt, ICBT, 0x7C00002C)                                                   \
  /* Move to Condition Register from XER */                                   \
  V(mcrxr, MCRXR, 0x7C000400)                                                 \
  /* TLB Invalidate Local Indexed */                                          \
  V(tlbilx, TLBILX, 0x7C000024)                                               \
  /* TLB Invalidate Virtual Address Indexed */                                \
  V(tlbivax, TLBIVAX, 0x7C000624)                                             \
  /* TLB Read Entry */                                                        \
  V(tlbre, TLBRE, 0x7C000764)                                                 \
  /* TLB Search Indexed */                                                    \
  V(tlbsx, TLBSX, 0x7C000724)                                                 \
  /* TLB Write Entry */                                                       \
  V(tlbwe, TLBWE, 0x7C0007A4)                                                 \
  /* Write External Enable */                                                 \
  V(wrtee, WRTEE, 0x7C000106)                                                 \
  /* Write External Enable Immediate */                                       \
  V(wrteei, WRTEEI, 0x7C000146)                                               \
  /* Data Cache Read */                                                       \
  V(dcread, DCREAD, 0x7C00028C)                                               \
  /* Instruction Cache Read */                                                \
  V(icread, ICREAD, 0x7C0007CC)                                               \
  /* Data Cache Invalidate */                                                 \
  V(dci, DCI, 0x7C00038C)                                                     \
  /* Instruction Cache Invalidate */                                          \
  V(ici, ICI, 0x7C00078C)                                                     \
  /* Move From Device Control Register User Mode Indexed */                   \
  V(mfdcrux, MFDCRUX, 0x7C000246)                                             \
  /* Move From Device Control Register Indexed */                             \
  V(mfdcrx, MFDCRX, 0x7C000206)                                               \
  /* Move To Device Control Register User Mode Indexed */                     \
  V(mtdcrux, MTDCRUX, 0x7C000346)                                             \
  /* Move To Device Control Register Indexed */                               \
  V(mtdcrx, MTDCRX, 0x7C000306)                                               \
  /* Return From Debug Interrupt */                                           \
  V(rfdi, RFDI, 0x4C00004E)                                                   \
  /* Data Cache Block Flush by External PID */                                \
  V(dcbfep, DCBFEP, 0x7C0000FE)                                               \
  /* Data Cache Block Store by External PID */                                \
  V(dcbstep, DCBSTEP, 0x7C00007E)                                             \
  /* Data Cache Block Touch by External PID */                                \
  V(dcbtep, DCBTEP, 0x7C00027E)                                               \
  /* Data Cache Block Touch for Store by External PID */                      \
  V(dcbtstep, DCBTSTEP, 0x7C0001FE)                                           \
  /* Data Cache Block Zero by External PID */                                 \
  V(dcbzep, DCBZEP, 0x7C0007FE)                                               \
  /* Instruction Cache Block Invalidate by External PID */                    \
  V(icbiep, ICBIEP, 0x7C0007BE)                                               \
  /* Load Byte and Zero by External PID Indexed */                            \
  V(lbepx, LBEPX, 0x7C0000BE)                                                 \
  /* Load Floating-Point Double by External PID Indexed */                    \
  V(lfdepx, LFDEPX, 0x7C0004BE)                                               \
  /* Load Halfword and Zero by External PID Indexed */                        \
  V(lhepx, LHEPX, 0x7C00023E)                                                 \
  /* Load Vector by External PID Indexed */                                   \
  V(lvepx, LVEPX, 0x7C00024E)                                                 \
  /* Load Vector by External PID Indexed Last */                              \
  V(lvepxl, LVEPXL, 0x7C00020E)                                               \
  /* Load Word and Zero by External PID Indexed */                            \
  V(lwepx, LWEPX, 0x7C00003E)                                                 \
  /* Store Byte by External PID Indexed */                                    \
  V(stbepx, STBEPX, 0x7C0001BE)                                               \
  /* Store Floating-Point Double by External PID Indexed */                   \
  V(stfdepx, STFDEPX, 0x7C0005BE)                                             \
  /* Store Halfword by External PID Indexed */                                \
  V(sthepx, STHEPX, 0x7C00033E)                                               \
  /* Store Vector by External PID Indexed */                                  \
  V(stvepx, STVEPX, 0x7C00064E)                                               \
  /* Store Vector by External PID Indexed Last */                             \
  V(stvepxl, STVEPXL, 0x7C00060E)                                             \
  /* Store Word by External PID Indexed */                                    \
  V(stwepx, STWEPX, 0x7C00013E)                                               \
  /* Load Doubleword by External PID Indexed */                               \
  V(ldepx, LDEPX, 0x7C00003A)                                                 \
  /* Store Doubleword by External PID Indexed */                              \
  V(stdepx, STDEPX, 0x7C00013A)                                               \
  /* TLB Search and Reserve Indexed */                                        \
  V(tlbsrx, TLBSRX, 0x7C0006A5)                                               \
  /* External Control In Word Indexed */                                      \
  V(eciwx, ECIWX, 0x7C00026C)                                                 \
  /* External Control Out Word Indexed */                                     \
  V(ecowx, ECOWX, 0x7C00036C)                                                 \
  /* Data Cache Block Lock Clear */                                           \
  V(dcblc, DCBLC, 0x7C00030C)                                                 \
  /* Data Cache Block Lock Query */                                           \
  V(dcblq, DCBLQ, 0x7C00034D)                                                 \
  /* Data Cache Block Touch and Lock Set */                                   \
  V(dcbtls, DCBTLS, 0x7C00014C)                                               \
  /* Data Cache Block Touch for Store and Lock Set */                         \
  V(dcbtstls, DCBTSTLS, 0x7C00010C)                                           \
  /* Instruction Cache Block Lock Clear */                                    \
  V(icblc, ICBLC, 0x7C0001CC)                                                 \
  /* Instruction Cache Block Lock Query */                                    \
  V(icblq, ICBLQ, 0x7C00018D)                                                 \
  /* Instruction Cache Block Touch and Lock Set */                            \
  V(icbtls, ICBTLS, 0x7C0003CC)                                               \
  /* Floating Compare Ordered */                                              \
  V(fcmpo, FCMPO, 0xFC000040)                                                 \
  /* Floating Compare Unordered */                                            \
  V(fcmpu, FCMPU, 0xFC000000)                                                 \
  /* Floating Test for software Divide */                                     \
  V(ftdiv, FTDIV, 0xFC000100)                                                 \
  /* Floating Test for software Square Root */                                \
  V(ftsqrt, FTSQRT, 0xFC000140)                                               \
  /* Load Floating-Point as Integer Word Algebraic Indexed */                 \
  V(lfiwax, LFIWAX, 0x7C0006AE)                                               \
  /* Load Floating-Point as Integer Word and Zero Indexed */                  \
  V(lfiwzx, LFIWZX, 0x7C0006EE)                                               \
  /* Move To Condition Register from FPSCR */                                 \
  V(mcrfs, MCRFS, 0xFC000080)                                                 \
  /* Store Floating-Point as Integer Word Indexed */                          \
  V(stfiwx, STFIWX, 0x7C0007AE)                                               \
  /* Load Floating-Point Double Pair Indexed */                               \
  V(lfdpx, LFDPX, 0x7C00062E)                                                 \
  /* Store Floating-Point Double Pair Indexed */                              \
  V(stfdpx, STFDPX, 0x7C00072E)                                               \
  /* Floating Absolute Value */                                               \
  V(fabs, FABS, 0xFC000210)                                                   \
  /* Floating Convert From Integer Doubleword */                              \
  V(fcfid, FCFID, 0xFC00069C)                                                 \
  /* Floating Convert From Integer Doubleword Single */                       \
  V(fcfids, FCFIDS, 0xEC00069C)                                               \
  /* Floating Convert From Integer Doubleword Unsigned */                     \
  V(fcfidu, FCFIDU, 0xFC00079C)                                               \
  /* Floating Convert From Integer Doubleword Unsigned Single */              \
  V(fcfidus, FCFIDUS, 0xEC00079C)                                             \
  /* Floating Copy Sign */                                                    \
  V(fcpsgn, FCPSGN, 0xFC000010)                                               \
  /* Floating Convert To Integer Doubleword */                                \
  V(fctid, FCTID, 0xFC00065C)                                                 \
  /* Floating Convert To Integer Doubleword Unsigned */                       \
  V(fctidu, FCTIDU, 0xFC00075C)                                               \
  /* Floating Convert To Integer Doubleword Unsigned with round toward */     \
  /* Zero */                                                                  \
  V(fctiduz, FCTIDUZ, 0xFC00075E)                                             \
  /* Floating Convert To Integer Doubleword with round toward Zero */         \
  V(fctidz, FCTIDZ, 0xFC00065E)                                               \
  /* Floating Convert To Integer Word */                                      \
  V(fctiw, FCTIW, 0xFC00001C)                                                 \
  /* Floating Convert To Integer Word Unsigned */                             \
  V(fctiwu, FCTIWU, 0xFC00011C)                                               \
  /* Floating Convert To Integer Word Unsigned with round toward Zero */      \
  V(fctiwuz, FCTIWUZ, 0xFC00011E)                                             \
  /* Floating Convert To Integer Word with round to Zero */                   \
  V(fctiwz, FCTIWZ, 0xFC00001E)                                               \
  /* Floating Move Register */                                                \
  V(fmr, FMR, 0xFC000090)                                                     \
  /* Floating Negative Absolute Value */                                      \
  V(fnabs, FNABS, 0xFC000110)                                                 \
  /* Floating Negate */                                                       \
  V(fneg, FNEG, 0xFC000050)                                                   \
  /* Floating Round to Single-Precision */                                    \
  V(frsp, FRSP, 0xFC000018)                                                   \
  /* Move From FPSCR */                                                       \
  V(mffs, MFFS, 0xFC00048E)                                                   \
  /* Move To FPSCR Bit 0 */                                                   \
  V(mtfsb0, MTFSB0, 0xFC00008C)                                               \
  /* Move To FPSCR Bit 1 */                                                   \
  V(mtfsb1, MTFSB1, 0xFC00004C)                                               \
  /* Move To FPSCR Field Immediate */                                         \
  V(mtfsfi, MTFSFI, 0xFC00010C)                                               \
  /* Floating Round To Integer Minus */                                       \
  V(frim, FRIM, 0xFC0003D0)                                                   \
  /* Floating Round To Integer Nearest */                                     \
  V(frin, FRIN, 0xFC000310)                                                   \
  /* Floating Round To Integer Plus */                                        \
  V(frip, FRIP, 0xFC000390)                                                   \
  /* Floating Round To Integer toward Zero */                                 \
  V(friz, FRIZ, 0xFC000350)                                                   \
  /* Multiply Cross Halfword to Word Signed */                                \
  V(mulchw, MULCHW, 0x10000150)                                               \
  /* Multiply Cross Halfword to Word Unsigned */                              \
  V(mulchwu, MULCHWU, 0x10000110)                                             \
  /* Multiply High Halfword to Word Signed */                                 \
  V(mulhhw, MULHHW, 0x10000050)                                               \
  /* Multiply High Halfword to Word Unsigned */                               \
  V(mulhhwu, MULHHWU, 0x10000010)                                             \
  /* Multiply Low Halfword to Word Signed */                                  \
  V(mullhw, MULLHW, 0x10000350)                                               \
  /* Multiply Low Halfword to Word Unsigned */                                \
  V(mullhwu, MULLHWU, 0x10000310)                                             \
  /* Determine Leftmost Zero Byte DQ 56 E0000000 P 58 LSQ lq Load Quadword */ \
  V(dlmzb, DLMZB, 0x7C00009C)                                                 \
  /* Load Quadword And Reserve Indexed */                                     \
  V(lqarx, LQARX, 0x7C000228)                                                 \
  /* Store Quadword Conditional Indexed and record CR0 */                     \
  V(stqcx, STQCX, 0x7C00016D)                                                 \
  /* Load String Word Immediate */                                            \
  V(lswi, LSWI, 0x7C0004AA)                                                   \
  /* Load String Word Indexed */                                              \
  V(lswx, LSWX, 0x7C00042A)                                                   \
  /* Store String Word Immediate */                                           \
  V(stswi, STSWI, 0x7C0005AA)                                                 \
  /* Store String Word Indexed */                                             \
  V(stswx, STSWX, 0x7C00052A)                                                 \
  /* Clear BHRB */                                                            \
  V(clrbhrb, CLRBHRB, 0x7C00035C)                                             \
  /* Enforce In-order Execution of I/O */                                     \
  V(eieio, EIEIO, 0x7C0006AC)                                                 \
  /* Load Byte and Zero Caching Inhibited Indexed */                          \
  V(lbzcix, LBZCIX, 0x7C0006AA)                                               \
  /* Load Doubleword Caching Inhibited Indexed */                             \
  V(ldcix, LDCIX, 0x7C0006EA)                                                 \
  /* Load Halfword and Zero Caching Inhibited Indexed */                      \
  V(lhzcix, LHZCIX, 0x7C00066A)                                               \
  /* Load Word and Zero Caching Inhibited Indexed */                          \
  V(lwzcix, LWZCIX, 0x7C00062A)                                               \
  /* Move From Segment Register */                                            \
  V(mfsr, MFSR, 0x7C0004A6)                                                   \
  /* Move From Segment Register Indirect */                                   \
  V(mfsrin, MFSRIN, 0x7C000526)                                               \
  /* Move To Machine State Register Doubleword */                             \
  V(mtmsrd, MTMSRD, 0x7C000164)                                               \
  /* Move To Split Little Endian */                                           \
  V(mtsle, MTSLE, 0x7C000126)                                                 \
  /* Move To Segment Register */                                              \
  V(mtsr, MTSR, 0x7C0001A4)                                                   \
  /* Move To Segment Register Indirect */                                     \
  V(mtsrin, MTSRIN, 0x7C0001E4)                                               \
  /* SLB Find Entry ESID */                                                   \
  V(slbfee, SLBFEE, 0x7C0007A7)                                               \
  /* SLB Invalidate All */                                                    \
  V(slbia, SLBIA, 0x7C0003E4)                                                 \
  /* SLB Invalidate Entry */                                                  \
  V(slbie, SLBIE, 0x7C000364)                                                 \
  /* SLB Move From Entry ESID */                                              \
  V(slbmfee, SLBMFEE, 0x7C000726)                                             \
  /* SLB Move From Entry VSID */                                              \
  V(slbmfev, SLBMFEV, 0x7C0006A6)                                             \
  /* SLB Move To Entry */                                                     \
  V(slbmte, SLBMTE, 0x7C000324)                                               \
  /* Store Byte Caching Inhibited Indexed */                                  \
  V(stbcix, STBCIX, 0x7C0007AA)                                               \
  /* Store Doubleword Caching Inhibited Indexed */                            \
  V(stdcix, STDCIX, 0x7C0007EA)                                               \
  /* Store Halfword and Zero Caching Inhibited Indexed */                     \
  V(sthcix, STHCIX, 0x7C00076A)                                               \
  /* Store Word and Zero Caching Inhibited Indexed */                         \
  V(stwcix, STWCIX, 0x7C00072A)                                               \
  /* TLB Invalidate All */                                                    \
  V(tlbia, TLBIA, 0x7C0002E4)                                                 \
  /* TLB Invalidate Entry */                                                  \
  V(tlbie, TLBIE, 0x7C000264)                                                 \
  /* TLB Invalidate Entry Local */                                            \
  V(tlbiel, TLBIEL, 0x7C000224)                                               \
  /* Message Clear Privileged */                                              \
  V(msgclrp, MSGCLRP, 0x7C00015C)                                             \
  /* Message Send Privileged */                                               \
  V(msgsndp, MSGSNDP, 0x7C00011C)                                             \
  /* Message Clear */                                                         \
  V(msgclr, MSGCLR, 0x7C0001DC)                                               \
  /* Message Send */                                                          \
  V(msgsnd, MSGSND, 0x7C00019C)                                               \
  /* Move From Machine State Register */                                      \
  V(mfmsr, MFMSR, 0x7C0000A6)                                                 \
  /* Move To Machine State Register */                                        \
  V(mtmsr, MTMSR, 0x7C000124)                                                 \
  /* TLB Synchronize */                                                       \
  V(tlbsync, TLBSYNC, 0x7C00046C)                                             \
  /* Transaction Abort */                                                     \
  V(tabort, TABORT, 0x7C00071D)                                               \
  /* Transaction Abort Doubleword Conditional */                              \
  V(tabortdc, TABORTDC, 0x7C00065D)                                           \
  /* Transaction Abort Doubleword Conditional Immediate */                    \
  V(tabortdci, TABORTDCI, 0x7C0006DD)                                         \
  /* Transaction Abort Word Conditional */                                    \
  V(tabortwc, TABORTWC, 0x7C00061D)                                           \
  /* Transaction Abort Word Conditional Immediate */                          \
  V(tabortwci, TABORTWCI, 0x7C00069D)                                         \
  /* Transaction Begin */                                                     \
  V(tbegin, TBEGIN, 0x7C00051D)                                               \
  /* Transaction Check */                                                     \
  V(tcheck, TCHECK, 0x7C00059C)                                               \
  /* Transaction End */                                                       \
  V(tend, TEND, 0x7C00055C)                                                   \
  /* Transaction Recheckpoint */                                              \
  V(trechkpt, TRECHKPT, 0x7C0007DD)                                           \
  /* Transaction Reclaim */                                                   \
  V(treclaim, TRECLAIM, 0x7C00075D)                                           \
  /* Transaction Suspend or Resume */                                         \
  V(tsr, TSR, 0x7C0005DC)                                                     \
  /* Load Vector Element Byte Indexed */                                      \
  V(lvebx, LVEBX, 0x7C00000E)                                                 \
  /* Load Vector Element Halfword Indexed */                                  \
  V(lvehx, LVEHX, 0x7C00004E)                                                 \
  /* Load Vector Element Word Indexed */                                      \
  V(lvewx, LVEWX, 0x7C00008E)                                                 \
  /* Load Vector for Shift Left */                                            \
  V(lvsl, LVSL, 0x7C00000C)                                                   \
  /* Load Vector for Shift Right */                                           \
  V(lvsr, LVSR, 0x7C00004C)                                                   \
  /* Load Vector Indexed Last */                                              \
  V(lvxl, LVXL, 0x7C0002CE)                                                   \
  /* Store Vector Element Byte Indexed */                                     \
  V(stvebx, STVEBX, 0x7C00010E)                                               \
  /* Store Vector Element Halfword Indexed */                                 \
  V(stvehx, STVEHX, 0x7C00014E)                                               \
  /* Store Vector Element Word Indexed */                                     \
  V(stvewx, STVEWX, 0x7C00018E)                                               \
  /* Store Vector Indexed Last */                                             \
  V(stvxl, STVXL, 0x7C0003CE)                                                 \
  /* Floating Merge Even Word */                                              \
  V(fmrgew, FMRGEW, 0xFC00078C)                                               \
  /* Floating Merge Odd Word */                                               \
  V(fmrgow, FMRGOW, 0xFC00068C)                                               \
  /* Wait for Interrupt */                                                    \
  V(wait, WAIT, 0x7C00007C)

#define PPC_X_OPCODE_LIST(V)     \
  PPC_X_OPCODE_A_FORM_LIST(V)    \
  PPC_X_OPCODE_B_FORM_LIST(V)    \
  PPC_X_OPCODE_C_FORM_LIST(V)    \
  PPC_X_OPCODE_D_FORM_LIST(V)    \
  PPC_X_OPCODE_E_FORM_LIST(V)    \
  PPC_X_OPCODE_F_FORM_LIST(V)    \
  PPC_X_OPCODE_G_FORM_LIST(V)    \
  PPC_X_OPCODE_EH_L_FORM_LIST(V) \
  PPC_X_OPCODE_UNUSED_LIST(V)

#define PPC_EVS_OPCODE_LIST(V) \
  /* Vector Select */          \
  V(evsel, EVSEL, 0x10000278)

#define PPC_DS_OPCODE_LIST(V)            \
  /* Load Doubleword */                  \
  V(ld, LD, 0xE8000000)                  \
  /* Load Doubleword with Update */      \
  V(ldu, LDU, 0xE8000001)                \
  /* Load Word Algebraic */              \
  V(lwa, LWA, 0xE8000002)                \
  /* Store Doubleword */                 \
  V(std, STD, 0xF8000000)                \
  /* Store Doubleword with Update */     \
  V(stdu, STDU, 0xF8000001)              \
  /* Load Floating-Point Double Pair */  \
  V(lfdp, LFDP, 0xE4000000)              \
  /* Store Floating-Point Double Pair */ \
  V(stfdp, STFDP, 0xF4000000)            \
  /* Store Quadword */                   \
  V(stq, STQ, 0xF8000002)

#define PPC_DQ_OPCODE_LIST(V) V(lsq, LSQ, 0xE0000000)

#define PPC_D_OPCODE_LIST(V)                    \
  /* Trap Doubleword Immediate */               \
  V(tdi, TDI, 0x08000000)                       \
  /* Add Immediate */                           \
  V(addi, ADDI, 0x38000000)                     \
  /* Add Immediate Carrying */                  \
  V(addic, ADDIC, 0x30000000)                   \
  /* Add Immediate Carrying & record CR0 */     \
  V(addicx, ADDICx, 0x34000000)                 \
  /* Add Immediate Shifted */                   \
  V(addis, ADDIS, 0x3C000000)                   \
  /* AND Immediate & record CR0 */              \
  V(andix, ANDIx, 0x70000000)                   \
  /* AND Immediate Shifted & record CR0 */      \
  V(andisx, ANDISx, 0x74000000)                 \
  /* Compare Immediate */                       \
  V(cmpi, CMPI, 0x2C000000)                     \
  /* Compare Logical Immediate */               \
  V(cmpli, CMPLI, 0x28000000)                   \
  /* Load Byte and Zero */                      \
  V(lbz, LBZ, 0x88000000)                       \
  /* Load Byte and Zero with Update */          \
  V(lbzu, LBZU, 0x8C000000)                     \
  /* Load Halfword Algebraic */                 \
  V(lha, LHA, 0xA8000000)                       \
  /* Load Halfword Algebraic with Update */     \
  V(lhau, LHAU, 0xAC000000)                     \
  /* Load Halfword and Zero */                  \
  V(lhz, LHZ, 0xA0000000)                       \
  /* Load Halfword and Zero with Update */      \
  V(lhzu, LHZU, 0xA4000000)                     \
  /* Load Multiple Word */                      \
  V(lmw, LMW, 0xB8000000)                       \
  /* Load Word and Zero */                      \
  V(lwz, LWZ, 0x80000000)                       \
  /* Load Word and Zero with Update */          \
  V(lwzu, LWZU, 0x84000000)                     \
  /* Multiply Low Immediate */                  \
  V(mulli, MULLI, 0x1C000000)                   \
  /* OR Immediate */                            \
  V(ori, ORI, 0x60000000)                       \
  /* OR Immediate Shifted */                    \
  V(oris, ORIS, 0x64000000)                     \
  /* Store Byte */                              \
  V(stb, STB, 0x98000000)                       \
  /* Store Byte with Update */                  \
  V(stbu, STBU, 0x9C000000)                     \
  /* Store Halfword */                          \
  V(sth, STH, 0xB0000000)                       \
  /* Store Halfword with Update */              \
  V(sthu, STHU, 0xB4000000)                     \
  /* Store Multiple Word */                     \
  V(stmw, STMW, 0xBC000000)                     \
  /* Store Word */                              \
  V(stw, STW, 0x90000000)                       \
  /* Store Word with Update */                  \
  V(stwu, STWU, 0x94000000)                     \
  /* Subtract From Immediate Carrying */        \
  V(subfic, SUBFIC, 0x20000000)                 \
  /* Trap Word Immediate */                     \
  V(twi, TWI, 0x0C000000)                       \
  /* XOR Immediate */                           \
  V(xori, XORI, 0x68000000)                     \
  /* XOR Immediate Shifted */                   \
  V(xoris, XORIS, 0x6C000000)                   \
  /* Load Floating-Point Double */              \
  V(lfd, LFD, 0xC8000000)                       \
  /* Load Floating-Point Double with Update */  \
  V(lfdu, LFDU, 0xCC000000)                     \
  /* Load Floating-Point Single */              \
  V(lfs, LFS, 0xC0000000)                       \
  /* Load Floating-Point Single with Update */  \
  V(lfsu, LFSU, 0xC4000000)                     \
  /* Store Floating-Point Double */             \
  V(stfd, STFD, 0xD8000000)                     \
  /* Store Floating-Point Double with Update */ \
  V(stfdu, STFDU, 0xDC000000)                   \
  /* Store Floating-Point Single */             \
  V(stfs, STFS, 0xD0000000)                     \
  /* Store Floating-Point Single with Update */ \
  V(stfsu, STFSU, 0xD4000000)

#define PPC_XFL_OPCODE_LIST(V) \
  /* Move To FPSCR Fields */   \
  V(mtfsf, MTFSF, 0xFC00058E)

#define PPC_XFX_OPCODE_LIST(V)                  \
  /* Move From Condition Register */            \
  V(mfcr, MFCR, 0x7C000026)                     \
  /* Move From One Condition Register Field */  \
  V(mfocrf, MFOCRF, 0x7C100026)                 \
  /* Move From Special Purpose Register */      \
  V(mfspr, MFSPR, 0x7C0002A6)                   \
  /* Move To Condition Register Fields */       \
  V(mtcrf, MTCRF, 0x7C000120)                   \
  /* Move To One Condition Register Field */    \
  V(mtocrf, MTOCRF, 0x7C100120)                 \
  /* Move To Special Purpose Register */        \
  V(mtspr, MTSPR, 0x7C0003A6)                   \
  /* Debugger Notify Halt */                    \
  V(dnh, DNH, 0x4C00018C)                       \
  /* Move From Device Control Register */       \
  V(mfdcr, MFDCR, 0x7C000286)                   \
  /* Move To Device Control Register */         \
  V(mtdcr, MTDCR, 0x7C000386)                   \
  /* Move from Performance Monitor Register */  \
  V(mfpmr, MFPMR, 0x7C00029C)                   \
  /* Move To Performance Monitor Register */    \
  V(mtpmr, MTPMR, 0x7C00039C)                   \
  /* Move From Branch History Rolling Buffer */ \
  V(mfbhrbe, MFBHRBE, 0x7C00025C)               \
  /* Move From Time Base */                     \
  V(mftb, MFTB, 0x7C0002E6)

#define PPC_MDS_OPCODE_LIST(V)                  \
  /* Rotate Left Doubleword then Clear Left */  \
  V(rldcl, RLDCL, 0x78000010)                   \
  /* Rotate Left Doubleword then Clear Right */ \
  V(rldcr, RLDCR, 0x78000012)

#define PPC_A_OPCODE_LIST(V)                            \
  /* Integer Select */                                  \
  V(isel, ISEL, 0x7C00001E)                             \
  /* Floating Add */                                    \
  V(fadd, FADD, 0xFC00002A)                             \
  /* Floating Add Single */                             \
  V(fadds, FADDS, 0xEC00002A)                           \
  /* Floating Divide */                                 \
  V(fdiv, FDIV, 0xFC000024)                             \
  /* Floating Divide Single */                          \
  V(fdivs, FDIVS, 0xEC000024)                           \
  /* Floating Multiply-Add */                           \
  V(fmadd, FMADD, 0xFC00003A)                           \
  /* Floating Multiply-Add Single */                    \
  V(fmadds, FMADDS, 0xEC00003A)                         \
  /* Floating Multiply-Subtract */                      \
  V(fmsub, FMSUB, 0xFC000038)                           \
  /* Floating Multiply-Subtract Single */               \
  V(fmsubs, FMSUBS, 0xEC000038)                         \
  /* Floating Multiply */                               \
  V(fmul, FMUL, 0xFC000032)                             \
  /* Floating Multiply Single */                        \
  V(fmuls, FMULS, 0xEC000032)                           \
  /* Floating Negative Multiply-Add */                  \
  V(fnmadd, FNMADD, 0xFC00003E)                         \
  /* Floating Negative Multiply-Add Single */           \
  V(fnmadds, FNMADDS, 0xEC00003E)                       \
  /* Floating Negative Multiply-Subtract */             \
  V(fnmsub, FNMSUB, 0xFC00003C)                         \
  /* Floating Negative Multiply-Subtract Single */      \
  V(fnmsubs, FNMSUBS, 0xEC00003C)                       \
  /* Floating Reciprocal Estimate Single */             \
  V(fres, FRES, 0xEC000030)                             \
  /* Floating Reciprocal Square Root Estimate */        \
  V(frsqrte, FRSQRTE, 0xFC000034)                       \
  /* Floating Select */                                 \
  V(fsel, FSEL, 0xFC00002E)                             \
  /* Floating Square Root */                            \
  V(fsqrt, FSQRT, 0xFC00002C)                           \
  /* Floating Square Root Single */                     \
  V(fsqrts, FSQRTS, 0xEC00002C)                         \
  /* Floating Subtract */                               \
  V(fsub, FSUB, 0xFC000028)                             \
  /* Floating Subtract Single */                        \
  V(fsubs, FSUBS, 0xEC000028)                           \
  /* Floating Reciprocal Estimate */                    \
  V(fre, FRE, 0xFC000030)                               \
  /* Floating Reciprocal Square Root Estimate Single */ \
  V(frsqrtes, FRSQRTES, 0xEC000034)

#define PPC_VA_OPCODE_A_FORM_LIST(V)                            \
  /* Vector Permute */                                          \
  V(vperm, VPERM, 0x1000002B)                                   \
  /* Vector Multiply-Low-Add Unsigned Halfword Modulo */        \
  V(vmladduhm, VMLADDUHM, 0x10000022)                           \
  /* Vector Select */                                           \
  V(vsel, VSEL, 0x1000002A)                                     \
  /* Vector Multiply-Sum Mixed Byte Modulo */                   \
  V(vmsummbm, VMSUMMBM, 0x10000025)                             \
  /* Vector Multiply-Sum Signed Halfword Modulo */              \
  V(vmsumshm, VMSUMSHM, 0x10000028)                             \
  /* Vector Multiply-High-Round-Add Signed Halfword Saturate */ \
  V(vmhraddshs, VMHRADDSHS, 0x10000021)

#define PPC_VA_OPCODE_UNUSED_LIST(V)                             \
  /* Vector Add Extended & write Carry Unsigned Quadword */      \
  V(vaddecuq, VADDECUQ, 0x1000003D)                              \
  /* Vector Add Extended Unsigned Quadword Modulo */             \
  V(vaddeuqm, VADDEUQM, 0x1000003C)                              \
  /* Vector Multiply-Add Single-Precision */                     \
  V(vmaddfp, VMADDFP, 0x1000002E)                                \
  /* Vector Multiply-High-Add Signed Halfword Saturate */        \
  V(vmhaddshs, VMHADDSHS, 0x10000020)                            \
  /* Vector Multiply-Sum Signed Halfword Saturate */             \
  V(vmsumshs, VMSUMSHS, 0x10000029)                              \
  /* Vector Multiply-Sum Unsigned Byte Modulo */                 \
  V(vmsumubm, VMSUMUBM, 0x10000024)                              \
  /* Vector Multiply-Sum Unsigned Halfword Modulo */             \
  V(vmsumuhm, VMSUMUHM, 0x10000026)                              \
  /* Vector Multiply-Sum Unsigned Halfword Saturate */           \
  V(vmsumuhs, VMSUMUHS, 0x10000027)                              \
  /* Vector Negative Multiply-Subtract Single-Precision */       \
  V(vnmsubfp, VNMSUBFP, 0x1000002F)                              \
  /* Vector Shift Left Double by Octet Immediate */              \
  V(vsldoi, VSLDOI, 0x1000002C)                                  \
  /* Vector Subtract Extended & write Carry Unsigned Quadword */ \
  V(vsubecuq, VSUBECUQ, 0x1000003F)                              \
  /* Vector Subtract Extended Unsigned Quadword Modulo */        \
  V(vsubeuqm, VSUBEUQM, 0x1000003E)                              \
  /* Vector Permute and Exclusive-OR */                          \
  V(vpermxor, VPERMXOR, 0x1000002D)

#define PPC_VA_OPCODE_LIST(V)  \
  PPC_VA_OPCODE_A_FORM_LIST(V) \
  PPC_VA_OPCODE_UNUSED_LIST(V)

#define PPC_XX1_OPCODE_LIST(V)                             \
  /* Load VSR Scalar Doubleword Indexed */                 \
  V(lxsdx, LXSDX, 0x7C000498)                              \
  /* Load VSX Scalar as Integer Word Algebraic Indexed */  \
  V(lxsiwax, LXSIWAX, 0x7C000098)                          \
  /* Load VSX Scalar as Integer Byte & Zero Indexed */     \
  V(lxsibzx, LXSIBZX, 0x7C00061A)                          \
  /* Load VSX Scalar as Integer Halfword & Zero Indexed */ \
  V(lxsihzx, LXSIHZX, 0x7C00065A)                          \
  /* Load VSX Scalar as Integer Word and Zero Indexed */   \
  V(lxsiwzx, LXSIWZX, 0x7C000018)                          \
  /* Load VSX Scalar Single-Precision Indexed */           \
  V(lxsspx, LXSSPX, 0x7C000418)                            \
  /* Load VSR Vector Doubleword*2 Indexed */               \
  V(lxvd, LXVD, 0x7C000698)                                \
  /* Load VSX Vector Indexed */                            \
  V(lxvx, LXVX, 0x7C000218)                                \
  /* Load VSR Vector Doubleword & Splat Indexed */         \
  V(lxvdsx, LXVDSX, 0x7C000298)                            \
  /* Load VSR Vector Word*4 Indexed */                     \
  V(lxvw, LXVW, 0x7C000618)                                \
  /* Move To VSR Doubleword */                             \
  V(mtvsrd, MTVSRD, 0x7C000166)                            \
  /* Move To VSR Double Doubleword */                      \
  V(mtvsrdd, MTVSRDD, 0x7C000366)                          \
  /* Move To VSR Word Algebraic */                         \
  V(mtvsrwa, MTVSRWA, 0x7C0001A6)                          \
  /* Move To VSR Word and Zero */                          \
  V(mtvsrwz, MTVSRWZ, 0x7C0001E6)                          \
  /* Move From VSR Doubleword */                           \
  V(mfvsrd, MFVSRD, 0x7C000066)                            \
  /* Move From VSR Word and Zero */                        \
  V(mfvsrwz, MFVSRWZ, 0x7C0000E6)                          \
  /* Store VSR Scalar Doubleword Indexed */                \
  V(stxsdx, STXSDX, 0x7C000598)                            \
  /* Store VSX Scalar as Integer Word Indexed */           \
  V(stxsiwx, STXSIWX, 0x7C000118)                          \
  /* Store VSX Scalar as Integer Halfword Indexed */       \
  V(stxsihx, STXSIHX, 0x7C00075A)                          \
  /* Store VSX Scalar as Integer Byte Indexed */           \
  V(stxsibx, STXSIBX, 0x7C00071A)                          \
  /* Store VSR Scalar Word Indexed */                      \
  V(stxsspx, STXSSPX, 0x7C000518)                          \
  /* Store VSR Vector Doubleword*2 Indexed */              \
  V(stxvd, STXVD, 0x7C000798)                              \
  /* Store VSX Vector Indexed */                           \
  V(stxvx, STXVX, 0x7C000318)                              \
  /* Store VSR Vector Word*4 Indexed */                    \
  V(stxvw, STXVW, 0x7C000718)

#define PPC_B_OPCODE_LIST(V) \
  /* Branch Conditional */   \
  V(bc, BCX, 0x40000000)

#define PPC_XO_OPCODE_LIST(V)                                               \
  /* Divide Doubleword */                                                   \
  V(divd, DIVD, 0x7C0003D2)                                                 \
  /* Divide Doubleword Extended */                                          \
  V(divde, DIVDE, 0x7C000352)                                               \
  /* Divide Doubleword Extended & record OV */                              \
  V(divdeo, DIVDEO, 0x7C000752)                                             \
  /* Divide Doubleword Extended Unsigned */                                 \
  V(divdeu, DIVDEU, 0x7C000312)                                             \
  /* Divide Doubleword Extended Unsigned & record OV */                     \
  V(divdeuo, DIVDEUO, 0x7C000712)                                           \
  /* Divide Doubleword & record OV */                                       \
  V(divdo, DIVDO, 0x7C0007D2)                                               \
  /* Divide Doubleword Unsigned */                                          \
  V(divdu, DIVDU, 0x7C000392)                                               \
  /* Divide Doubleword Unsigned & record OV */                              \
  V(divduo, DIVDUO, 0x7C000792)                                             \
  /* Multiply High Doubleword */                                            \
  V(mulhd, MULHD, 0x7C000092)                                               \
  /* Multiply High Doubleword Unsigned */                                   \
  V(mulhdu, MULHDU, 0x7C000012)                                             \
  /* Multiply Low Doubleword */                                             \
  V(mulld, MULLD, 0x7C0001D2)                                               \
  /* Multiply Low Doubleword & record OV */                                 \
  V(mulldo, MULLDO, 0x7C0005D2)                                             \
  /* Add */                                                                 \
  V(add, ADDX, 0x7C000214)                                                  \
  /* Add Carrying */                                                        \
  V(addc, ADDCX, 0x7C000014)                                                \
  /* Add Carrying & record OV */                                            \
  V(addco, ADDCO, 0x7C000414)                                               \
  /* Add Extended */                                                        \
  V(adde, ADDEX, 0x7C000114)                                                \
  /* Add Extended & record OV & record OV */                                \
  V(addeo, ADDEO, 0x7C000514)                                               \
  /* Add to Minus One Extended */                                           \
  V(addme, ADDME, 0x7C0001D4)                                               \
  /* Add to Minus One Extended & record OV */                               \
  V(addmeo, ADDMEO, 0x7C0005D4)                                             \
  /* Add & record OV */                                                     \
  V(addo, ADDO, 0x7C000614)                                                 \
  /* Add to Zero Extended */                                                \
  V(addze, ADDZEX, 0x7C000194)                                              \
  /* Add to Zero Extended & record OV */                                    \
  V(addzeo, ADDZEO, 0x7C000594)                                             \
  /* Divide Word Format */                                                  \
  V(divw, DIVW, 0x7C0003D6)                                                 \
  /* Divide Word Extended */                                                \
  V(divwe, DIVWE, 0x7C000356)                                               \
  /* Divide Word Extended & record OV */                                    \
  V(divweo, DIVWEO, 0x7C000756)                                             \
  /* Divide Word Extended Unsigned */                                       \
  V(divweu, DIVWEU, 0x7C000316)                                             \
  /* Divide Word Extended Unsigned & record OV */                           \
  V(divweuo, DIVWEUO, 0x7C000716)                                           \
  /* Divide Word & record OV */                                             \
  V(divwo, DIVWO, 0x7C0007D6)                                               \
  /* Divide Word Unsigned */                                                \
  V(divwu, DIVWU, 0x7C000396)                                               \
  /* Divide Word Unsigned & record OV */                                    \
  V(divwuo, DIVWUO, 0x7C000796)                                             \
  /* Multiply High Word */                                                  \
  V(mulhw, MULHWX, 0x7C000096)                                              \
  /* Multiply High Word Unsigned */                                         \
  V(mulhwu, MULHWUX, 0x7C000016)                                            \
  /* Multiply Low Word */                                                   \
  V(mullw, MULLW, 0x7C0001D6)                                               \
  /* Multiply Low Word & record OV */                                       \
  V(mullwo, MULLWO, 0x7C0005D6)                                             \
  /* Negate */                                                              \
  V(neg, NEGX, 0x7C0000D0)                                                  \
  /* Negate & record OV */                                                  \
  V(nego, NEGO, 0x7C0004D0)                                                 \
  /* Subtract From */                                                       \
  V(subf, SUBFX, 0x7C000050)                                                \
  /* Subtract From Carrying */                                              \
  V(subfc, SUBFCX, 0x7C000010)                                              \
  /* Subtract From Carrying & record OV */                                  \
  V(subfco, SUBFCO, 0x7C000410)                                             \
  /* Subtract From Extended */                                              \
  V(subfe, SUBFEX, 0x7C000110)                                              \
  /* Subtract From Extended & record OV */                                  \
  V(subfeo, SUBFEO, 0x7C000510)                                             \
  /* Subtract From Minus One Extended */                                    \
  V(subfme, SUBFME, 0x7C0001D0)                                             \
  /* Subtract From Minus One Extended & record OV */                        \
  V(subfmeo, SUBFMEO, 0x7C0005D0)                                           \
  /* Subtract From & record OV */                                           \
  V(subfo, SUBFO, 0x7C000450)                                               \
  /* Subtract From Zero Extended */                                         \
  V(subfze, SUBFZE, 0x7C000190)                                             \
  /* Subtract From Zero Extended & record OV */                             \
  V(subfzeo, SUBFZEO, 0x7C000590)                                           \
  /* Add and Generate Sixes */                                              \
  V(addg, ADDG, 0x7C000094)                                                 \
  /* Multiply Accumulate Cross Halfword to Word Modulo Signed */            \
  V(macchw, MACCHW, 0x10000158)                                             \
  /* Multiply Accumulate Cross Halfword to Word Saturate Signed */          \
  V(macchws, MACCHWS, 0x100001D8)                                           \
  /* Multiply Accumulate Cross Halfword to Word Saturate Unsigned */        \
  V(macchwsu, MACCHWSU, 0x10000198)                                         \
  /* Multiply Accumulate Cross Halfword to Word Modulo Unsigned */          \
  V(macchwu, MACCHWU, 0x10000118)                                           \
  /* Multiply Accumulate High Halfword to Word Modulo Signed */             \
  V(machhw, MACHHW, 0x10000058)                                             \
  /* Multiply Accumulate High Halfword to Word Saturate Signed */           \
  V(machhws, MACHHWS, 0x100000D8)                                           \
  /* Multiply Accumulate High Halfword to Word Saturate Unsigned */         \
  V(machhwsu, MACHHWSU, 0x10000098)                                         \
  /* Multiply Accumulate High Halfword to Word Modulo Unsigned */           \
  V(machhwu, MACHHWU, 0x10000018)                                           \
  /* Multiply Accumulate Low Halfword to Word Modulo Signed */              \
  V(maclhw, MACLHW, 0x10000358)                                             \
  /* Multiply Accumulate Low Halfword to Word Saturate Signed */            \
  V(maclhws, MACLHWS, 0x100003D8)                                           \
  /* Multiply Accumulate Low Halfword to Word Saturate Unsigned */          \
  V(maclhwsu, MACLHWSU, 0x10000398)                                         \
  /* Multiply Accumulate Low Halfword to Word Modulo Unsigned */            \
  V(maclhwu, MACLHWU, 0x10000318)                                           \
  /* Negative Multiply Accumulate Cross Halfword to Word Modulo Signed */   \
  V(nmacchw, NMACCHW, 0x1000015C)                                           \
  /* Negative Multiply Accumulate Cross Halfword to Word Saturate Signed */ \
  V(nmacchws, NMACCHWS, 0x100001DC)                                         \
  /* Negative Multiply Accumulate High Halfword to Word Modulo Signed */    \
  V(nmachhw, NMACHHW, 0x1000005C)                                           \
  /* Negative Multiply Accumulate High Halfword to Word Saturate Signed */  \
  V(nmachhws, NMACHHWS, 0x100000DC)                                         \
  /* Negative Multiply Accumulate Low Halfword to Word Modulo Signed */     \
  V(nmaclhw, NMACLHW, 0x1000035C)                                           \
  /* Negative Multiply Accumulate Low Halfword to Word Saturate Signed */   \
  V(nmaclhws, NMACLHWS, 0x100003DC)

#define PPC_XL_OPCODE_LIST(V)                       \
  /* Branch Conditional to Count Register */        \
  V(bcctr, BCCTRX, 0x4C000420)                      \
  /* Branch Conditional to Link Register */         \
  V(bclr, BCLRX, 0x4C000020)                        \
  /* Condition Register AND */                      \
  V(crand, CRAND, 0x4C000202)                       \
  /* Condition Register AND with Complement */      \
  V(crandc, CRANDC, 0x4C000102)                     \
  /* Condition Register Equivalent */               \
  V(creqv, CREQV, 0x4C000242)                       \
  /* Condition Register NAND */                     \
  V(crnand, CRNAND, 0x4C0001C2)                     \
  /* Condition Register NOR */                      \
  V(crnor, CRNOR, 0x4C000042)                       \
  /* Condition Register OR */                       \
  V(cror, CROR, 0x4C000382)                         \
  /* Condition Register OR with Complement */       \
  V(crorc, CRORC, 0x4C000342)                       \
  /* Condition Register XOR */                      \
  V(crxor, CRXOR, 0x4C000182)                       \
  /* Instruction Synchronize */                     \
  V(isync, ISYNC, 0x4C00012C)                       \
  /* Move Condition Register Field */               \
  V(mcrf, MCRF, 0x4C000000)                         \
  /* Return From Critical Interrupt */              \
  V(rfci, RFCI, 0x4C000066)                         \
  /* Return From Interrupt */                       \
  V(rfi, RFI, 0x4C000064)                           \
  /* Return From Machine Check Interrupt */         \
  V(rfmci, RFMCI, 0x4C00004C)                       \
  /* Embedded Hypervisor Privilege */               \
  V(ehpriv, EHPRIV, 0x7C00021C)                     \
  /* Return From Guest Interrupt */                 \
  V(rfgi, RFGI, 0x4C0000CC)                         \
  /* Doze */                                        \
  V(doze, DOZE, 0x4C000324)                         \
  /* Return From Interrupt Doubleword Hypervisor */ \
  V(hrfid, HRFID, 0x4C000224)                       \
  /* Nap */                                         \
  V(nap, NAP, 0x4C000364)                           \
  /* Return from Event Based Branch */              \
  V(rfebb, RFEBB, 0x4C000124)                       \
  /* Return from Interrupt Doubleword */            \
  V(rfid, RFID, 0x4C000024)                         \
  /* Rip Van Winkle */                              \
  V(rvwinkle, RVWINKLE, 0x4C0003E4)                 \
  /* Sleep */                                       \
  V(sleep, SLEEP, 0x4C0003A4)

#define PPC_XX4_OPCODE_LIST(V) \
  /* VSX Select */             \
  V(xxsel, XXSEL, 0xF0000030)

#define PPC_I_OPCODE_LIST(V) \
  /* Branch */               \
  V(b, BX, 0x48000000)

#define PPC_M_OPCODE_LIST(V)                          \
  /* Rotate Left Word Immediate then Mask Insert */   \
  V(rlwimi, RLWIMIX, 0x50000000)                      \
  /* Rotate Left Word Immediate then AND with Mask */ \
  V(rlwinm, RLWINMX, 0x54000000)                      \
  /* Rotate Left Word then AND with Mask */           \
  V(rlwnm, RLWNMX, 0x5C000000)

#define PPC_VX_OPCODE_A_FORM_LIST(V)     \
  /* Vector Splat Byte */                \
  V(vspltb, VSPLTB, 0x1000020C)          \
  /* Vector Splat Word */                \
  V(vspltw, VSPLTW, 0x1000028C)          \
  /* Vector Splat Halfword */            \
  V(vsplth, VSPLTH, 0x1000024C)          \
  /* Vector Extract Unsigned Byte */     \
  V(vextractub, VEXTRACTUB, 0x1000020D)  \
  /* Vector Extract Unsigned Halfword */ \
  V(vextractuh, VEXTRACTUH, 0x1000024D)  \
  /* Vector Extract Unsigned Word */     \
  V(vextractuw, VEXTRACTUW, 0x1000028D)  \
  /* Vector Extract Doubleword */        \
  V(vextractd, VEXTRACTD, 0x100002CD)    \
  /* Vector Insert Byte */               \
  V(vinsertb, VINSERTB, 0x1000030D)      \
  /* Vector Insert Halfword */           \
  V(vinserth, VINSERTH, 0x1000034D)      \
  /* Vector Insert Word */               \
  V(vinsertw, VINSERTW, 0x1000038D)      \
  /* Vector Insert Doubleword */         \
  V(vinsertd, VINSERTD, 0x100003CD)

#define PPC_VX_OPCODE_B_FORM_LIST(V)                       \
  /* Vector Logical OR */                                  \
  V(vor, VOR, 0x10000484)                                  \
  /* Vector Logical XOR */                                 \
  V(vxor, VXOR, 0x100004C4)                                \
  /* Vector Logical NOR */                                 \
  V(vnor, VNOR, 0x10000504)                                \
  /* Vector Shift Right by Octet */                        \
  V(vsro, VSRO, 0x1000044C)                                \
  /* Vector Shift Left by Octet */                         \
  V(vslo, VSLO, 0x1000040C)                                \
  /* Vector Add Unsigned Doubleword Modulo */              \
  V(vaddudm, VADDUDM, 0x100000C0)                          \
  /* Vector Add Unsigned Word Modulo */                    \
  V(vadduwm, VADDUWM, 0x10000080)                          \
  /* Vector Add Unsigned Halfword Modulo */                \
  V(vadduhm, VADDUHM, 0x10000040)                          \
  /* Vector Add Unsigned Byte Modulo */                    \
  V(vaddubm, VADDUBM, 0x10000000)                          \
  /* Vector Add Single-Precision */                        \
  V(vaddfp, VADDFP, 0x1000000A)                            \
  /* Vector Subtract Single-Precision */                   \
  V(vsubfp, VSUBFP, 0x1000004A)                            \
  /* Vector Subtract Unsigned Doubleword Modulo */         \
  V(vsubudm, VSUBUDM, 0x100004C0)                          \
  /* Vector Subtract Unsigned Word Modulo */               \
  V(vsubuwm, VSUBUWM, 0x10000480)                          \
  /* Vector Subtract Unsigned Halfword Modulo */           \
  V(vsubuhm, VSUBUHM, 0x10000440)                          \
  /* Vector Subtract Unsigned Byte Modulo */               \
  V(vsububm, VSUBUBM, 0x10000400)                          \
  /* Vector Multiply Unsigned Word Modulo */               \
  V(vmuluwm, VMULUWM, 0x10000089)                          \
  /* Vector Pack Unsigned Halfword Unsigned Modulo */      \
  V(vpkuhum, VPKUHUM, 0x1000000E)                          \
  /* Vector Multiply Even Signed Byte */                   \
  V(vmulesb, VMULESB, 0x10000308)                          \
  /* Vector Multiply Even Unsigned Byte */                 \
  V(vmuleub, VMULEUB, 0x10000208)                          \
  /* Vector Multiply Odd Signed Byte */                    \
  V(vmulosb, VMULOSB, 0x10000108)                          \
  /* Vector Multiply Odd Unsigned Byte */                  \
  V(vmuloub, VMULOUB, 0x10000008)                          \
  /* Vector Multiply Even Unsigned Halfword */             \
  V(vmuleuh, VMULEUH, 0x10000248)                          \
  /* Vector Multiply Even Signed Halfword */               \
  V(vmulesh, VMULESH, 0x10000348)                          \
  /* Vector Multiply Odd Unsigned Halfword */              \
  V(vmulouh, VMULOUH, 0x10000048)                          \
  /* Vector Multiply Odd Signed Halfword */                \
  V(vmulosh, VMULOSH, 0x10000148)                          \
  /* Vector Multiply Even Signed Word */                   \
  V(vmulesw, VMULESW, 0x10000388)                          \
  /* Vector Multiply Even Unsigned Word */                 \
  V(vmuleuw, VMULEUW, 0x10000288)                          \
  /* Vector Multiply Odd Signed Word */                    \
  V(vmulosw, VMULOSW, 0x10000188)                          \
  /* Vector Multiply Odd Unsigned Word */                  \
  V(vmulouw, VMULOUW, 0x10000088)                          \
  /* Vector Multiply Low Doubleword */                     \
  V(vmulld, VMULLD, 0x100001C9)                            \
  /* Vector Sum across Quarter Signed Halfword Saturate */ \
  V(vsum4shs, VSUM4SHS, 0x10000648)                        \
  /* Vector Pack Unsigned Word Unsigned Saturate */        \
  V(vpkuwus, VPKUWUS, 0x100000CE)                          \
  /* Vector Sum across Half Signed Word Saturate */        \
  V(vsum2sws, VSUM2SWS, 0x10000688)                        \
  /* Vector Pack Unsigned Doubleword Unsigned Modulo */    \
  V(vpkudum, VPKUDUM, 0x1000044E)                          \
  /* Vector Maximum Signed Byte */                         \
  V(vmaxsb, VMAXSB, 0x10000102)                            \
  /* Vector Maximum Unsigned Byte */                       \
  V(vmaxub, VMAXUB, 0x10000002)                            \
  /* Vector Maximum Signed Doubleword */                   \
  V(vmaxsd, VMAXSD, 0x100001C2)                            \
  /* Vector Maximum Unsigned Doubleword */                 \
  V(vmaxud, VMAXUD, 0x100000C2)                            \
  /* Vector Maximum Signed Halfword */                     \
  V(vmaxsh, VMAXSH, 0x10000142)                            \
  /* Vector Maximum Unsigned Halfword */                   \
  V(vmaxuh, VMAXUH, 0x10000042)                            \
  /* Vector Maximum Signed Word */                         \
  V(vmaxsw, VMAXSW, 0x10000182)                            \
  /* Vector Maximum Unsigned Word */                       \
  V(vmaxuw, VMAXUW, 0x10000082)                            \
  /* Vector Minimum Signed Byte */                         \
  V(vminsb, VMINSB, 0x10000302)                            \
  /* Vector Minimum Unsigned Byte */                       \
  V(vminub, VMINUB, 0x10000202)                            \
  /* Vector Minimum Signed Doubleword */                   \
  V(vminsd, VMINSD, 0x100003C2)                            \
  /* Vector Minimum Unsigned Doubleword */                 \
  V(vminud, VMINUD, 0x100002C2)                            \
  /* Vector Minimum Signed Halfword */                     \
  V(vminsh, VMINSH, 0x10000342)                            \
  /* Vector Minimum Unsigned Halfword */                   \
  V(vminuh, VMINUH, 0x10000242)                            \
  /* Vector Minimum Signed Word */                         \
  V(vminsw, VMINSW, 0x10000382)                            \
  /* Vector Minimum Unsigned Word */                       \
  V(vminuw, VMINUW, 0x10000282)                            \
  /* Vector Shift Left Byte */                             \
  V(vslb, VSLB, 0x10000104)                                \
  /* Vector Shift Left Word */                             \
  V(vslw, VSLW, 0x10000184)                                \
  /* Vector Shift Left Halfword */                         \
  V(vslh, VSLH, 0x10000144)                                \
  /* Vector Shift Left Doubleword */                       \
  V(vsld, VSLD, 0x100005C4)                                \
  /* Vector Shift Right Byte */                            \
  V(vsrb, VSRB, 0x10000204)                                \
  /* Vector Shift Right Word */                            \
  V(vsrw, VSRW, 0x10000284)                                \
  /* Vector Shift Right Halfword */                        \
  V(vsrh, VSRH, 0x10000244)                                \
  /* Vector Shift Right Doubleword */                      \
  V(vsrd, VSRD, 0x100006C4)                                \
  /* Vector Shift Right Algebraic Byte */                  \
  V(vsrab, VSRAB, 0x10000304)                              \
  /* Vector Shift Right Algebraic Word */                  \
  V(vsraw, VSRAW, 0x10000384)                              \
  /* Vector Shift Right Algebraic Halfword */              \
  V(vsrah, VSRAH, 0x10000344)                              \
  /* Vector Shift Right Algebraic Doubleword */            \
  V(vsrad, VSRAD, 0x100003C4)                              \
  /* Vector Logical AND */                                 \
  V(vand, VAND, 0x10000404)                                \
  /* Vector Pack Signed Word Signed Saturate */            \
  V(vpkswss, VPKSWSS, 0x100001CE)                          \
  /* Vector Pack Signed Word Unsigned Saturate */          \
  V(vpkswus, VPKSWUS, 0x1000014E)                          \
  /* Vector Pack Signed Halfword Signed Saturate */        \
  V(vpkshss, VPKSHSS, 0x1000018E)                          \
  /* Vector Pack Signed Halfword Unsigned Saturate */      \
  V(vpkshus, VPKSHUS, 0x1000010E)                          \
  /* Vector Add Signed Halfword Saturate */                \
  V(vaddshs, VADDSHS, 0x10000340)                          \
  /* Vector Subtract Signed Halfword Saturate */           \
  V(vsubshs, VSUBSHS, 0x10000740)                          \
  /* Vector Add Unsigned Halfword Saturate */              \
  V(vadduhs, VADDUHS, 0x10000240)                          \
  /* Vector Subtract Unsigned Halfword Saturate */         \
  V(vsubuhs, VSUBUHS, 0x10000640)                          \
  /* Vector Add Signed Byte Saturate */                    \
  V(vaddsbs, VADDSBS, 0x10000300)                          \
  /* Vector Subtract Signed Byte Saturate */               \
  V(vsubsbs, VSUBSBS, 0x10000700)                          \
  /* Vector Add Unsigned Byte Saturate */                  \
  V(vaddubs, VADDUBS, 0x10000200)                          \
  /* Vector Subtract Unsigned Byte Saturate */             \
  V(vsububs, VSUBUBS, 0x10000600)                          \
  /* Vector Average Unsigned Byte */                       \
  V(vavgub, VAVGUB, 0x10000402)                            \
  /* Vector Average Unsigned Halfword */                   \
  V(vavguh, VAVGUH, 0x10000442)                            \
  /* Vector Logical AND with Complement */                 \
  V(vandc, VANDC, 0x10000444)                              \
  /* Vector Minimum Single-Precision */                    \
  V(vminfp, VMINFP, 0x1000044A)                            \
  /* Vector Maximum Single-Precision */                    \
  V(vmaxfp, VMAXFP, 0x1000040A)                            \
  /* Vector Bit Permute Quadword */                        \
  V(vbpermq, VBPERMQ, 0x1000054C)                          \
  /* Vector Merge High Byte */                             \
  V(vmrghb, VMRGHB, 0x1000000C)                            \
  /* Vector Merge High Halfword */                         \
  V(vmrghh, VMRGHH, 0x1000004C)                            \
  /* Vector Merge High Word */                             \
  V(vmrghw, VMRGHW, 0x1000008C)                            \
  /* Vector Merge Low Byte */                              \
  V(vmrglb, VMRGLB, 0x1000010C)                            \
  /* Vector Merge Low Halfword */                          \
  V(vmrglh, VMRGLH, 0x1000014C)                            \
  /* Vector Merge Low Word */                              \
  V(vmrglw, VMRGLW, 0x1000018C)

#define PPC_VX_OPCODE_C_FORM_LIST(V)       \
  /* Vector Unpack Low Signed Word */      \
  V(vupklsw, VUPKLSW, 0x100006CE)          \
  /* Vector Unpack High Signed Word */     \
  V(vupkhsw, VUPKHSW, 0x1000064E)          \
  /* Vector Unpack Low Signed Halfword */  \
  V(vupklsh, VUPKLSH, 0x100002CE)          \
  /* Vector Unpack High Signed Halfword */ \
  V(vupkhsh, VUPKHSH, 0x1000024E)          \
  /* Vector Unpack Low Signed Byte */      \
  V(vupklsb, VUPKLSB, 0x1000028E)          \
  /* Vector Unpack High Signed Byte */     \
  V(vupkhsb, VUPKHSB, 0x1000020E)          \
  /* Vector Population Count Byte */       \
  V(vpopcntb, VPOPCNTB, 0x10000703)

#define PPC_VX_OPCODE_D_FORM_LIST(V) \
  /* Vector Negate Word */           \
  V(vnegw, VNEGW, 0x10060602)        \
  /* Vector Negate Doubleword */     \
  V(vnegd, VNEGD, 0x10070602)

#define PPC_VX_OPCODE_E_FORM_LIST(V)           \
  /* Vector Splat Immediate Signed Byte */     \
  V(vspltisb, VSPLTISB, 0x1000030C)            \
  /* Vector Splat Immediate Signed Halfword */ \
  V(vspltish, VSPLTISH, 0x1000034C)            \
  /* Vector Splat Immediate Signed Word */     \
  V(vspltisw, VSPLTISW, 0x1000038C)

#define PPC_VX_OPCODE_F_FORM_LIST(V)    \
  /* Vector Extract Byte Mask */        \
  V(vextractbm, VEXTRACTBM, 0x10080642) \
  /* Vector Extract Halfword Mask */    \
  V(vextracthm, VEXTRACTHM, 0x10090642) \
  /* Vector Extract Word Mask */        \
  V(vextractwm, VEXTRACTWM, 0x100A0642) \
  /* Vector Extract Doubleword Mask */  \
  V(vextractdm, VEXTRACTDM, 0x100B0642)

#define PPC_VX_OPCODE_G_FORM_LIST(V)         \
  /* Vector Insert Word from GPR using       \
immediate-specified index */                 \
  V(vinsw, VINSW, 0x100000CF)                \
  /* Vector Insert Doubleword from GPR using \
immediate-specified index */                 \
  V(vinsd, VINSD, 0x100001CF)

#define PPC_VX_OPCODE_UNUSED_LIST(V)                                      \
  /* Decimal Add Modulo */                                                \
  V(bcdadd, BCDADD, 0xF0000400)                                           \
  /* Decimal Subtract Modulo */                                           \
  V(bcdsub, BCDSUB, 0xF0000440)                                           \
  /* Move From Vector Status and Control Register */                      \
  V(mfvscr, MFVSCR, 0x10000604)                                           \
  /* Move To Vector Status and Control Register */                        \
  V(mtvscr, MTVSCR, 0x10000644)                                           \
  /* Vector Add & write Carry Unsigned Quadword */                        \
  V(vaddcuq, VADDCUQ, 0x10000140)                                         \
  /* Vector Add and Write Carry-Out Unsigned Word */                      \
  V(vaddcuw, VADDCUW, 0x10000180)                                         \
  /* Vector Add Signed Word Saturate */                                   \
  V(vaddsws, VADDSWS, 0x10000380)                                         \
  /* Vector Add Unsigned Quadword Modulo */                               \
  V(vadduqm, VADDUQM, 0x10000100)                                         \
  /* Vector Add Unsigned Word Saturate */                                 \
  V(vadduws, VADDUWS, 0x10000280)                                         \
  /* Vector Average Signed Byte */                                        \
  V(vavgsb, VAVGSB, 0x10000502)                                           \
  /* Vector Average Signed Halfword */                                    \
  V(vavgsh, VAVGSH, 0x10000542)                                           \
  /* Vector Average Signed Word */                                        \
  V(vavgsw, VAVGSW, 0x10000582)                                           \
  /* Vector Average Unsigned Word */                                      \
  V(vavguw, VAVGUW, 0x10000482)                                           \
  /* Vector Convert From Signed Fixed-Point Word To Single-Precision */   \
  V(vcfsx, VCFSX, 0x1000034A)                                             \
  /* Vector Convert From Unsigned Fixed-Point Word To Single-Precision */ \
  V(vcfux, VCFUX, 0x1000030A)                                             \
  /* Vector Count Leading Zeros Byte */                                   \
  V(vclzb, VCLZB, 0x10000702)                                             \
  /* Vector Count Leading Zeros Doubleword */                             \
  V(vclzd, VCLZD, 0x100007C2)                                             \
  /* Vector Count Leading Zeros Halfword */                               \
  V(vclzh, VCLZH, 0x10000742)                                             \
  /* Vector Count Leading Zeros Word */                                   \
  V(vclzw, VCLZW, 0x10000782)                                             \
  /* Vector Convert From Single-Precision To Signed Fixed-Point Word */   \
  /* Saturate */                                                          \
  V(vctsxs, VCTSXS, 0x100003CA)                                           \
  /* Vector Convert From Single-Precision To Unsigned Fixed-Point Word */ \
  /* Saturate */                                                          \
  V(vctuxs, VCTUXS, 0x1000038A)                                           \
  /* Vector Equivalence */                                                \
  V(veqv, VEQV, 0x10000684)                                               \
  /* Vector 2 Raised to the Exponent Estimate Single-Precision */         \
  V(vexptefp, VEXPTEFP, 0x1000018A)                                       \
  /* Vector Gather Bits by Byte by Doubleword */                          \
  V(vgbbd, VGBBD, 0x1000050C)                                             \
  /* Vector Log Base 2 Estimate Single-Precision */                       \
  V(vlogefp, VLOGEFP, 0x100001CA)                                         \
  /* Vector NAND */                                                       \
  V(vnand, VNAND, 0x10000584)                                             \
  /* Vector OR with Complement */                                         \
  V(vorc, VORC, 0x10000544)                                               \
  /* Vector Pack Pixel */                                                 \
  V(vpkpx, VPKPX, 0x1000030E)                                             \
  /* Vector Pack Signed Doubleword Signed Saturate */                     \
  V(vpksdss, VPKSDSS, 0x100005CE)                                         \
  /* Vector Pack Signed Doubleword Unsigned Saturate */                   \
  V(vpksdus, VPKSDUS, 0x1000054E)                                         \
  /* Vector Pack Unsigned Doubleword Unsigned Saturate */                 \
  V(vpkudus, VPKUDUS, 0x100004CE)                                         \
  /* Vector Pack Unsigned Halfword Unsigned Saturate */                   \
  V(vpkuhus, VPKUHUS, 0x1000008E)                                         \
  /* Vector Pack Unsigned Word Unsigned Modulo */                         \
  V(vpkuwum, VPKUWUM, 0x1000004E)                                         \
  /* Vector Polynomial Multiply-Sum Byte */                               \
  V(vpmsumb, VPMSUMB, 0x10000408)                                         \
  /* Vector Polynomial Multiply-Sum Doubleword */                         \
  V(vpmsumd, VPMSUMD, 0x100004C8)                                         \
  /* Vector Polynomial Multiply-Sum Halfword */                           \
  V(vpmsumh, VPMSUMH, 0x10000448)                                         \
  /* Vector Polynomial Multiply-Sum Word */                               \
  V(vpmsumw, VPMSUMW, 0x10000488)                                         \
  /* Vector Population Count Doubleword */                                \
  V(vpopcntd, VPOPCNTD, 0x100007C3)                                       \
  /* Vector Population Count Halfword */                                  \
  V(vpopcnth, VPOPCNTH, 0x10000743)                                       \
  /* Vector Population Count Word */                                      \
  V(vpopcntw, VPOPCNTW, 0x10000783)                                       \
  /* Vector Reciprocal Estimate Single-Precision */                       \
  V(vrefp, VREFP, 0x1000010A)                                             \
  /* Vector Round to Single-Precision Integer toward -Infinity */         \
  V(vrfim, VRFIM, 0x100002CA)                                             \
  /* Vector Round to Single-Precision Integer Nearest */                  \
  V(vrfin, VRFIN, 0x1000020A)                                             \
  /* Vector Round to Single-Precision Integer toward +Infinity */         \
  V(vrfip, VRFIP, 0x1000028A)                                             \
  /* Vector Round to Single-Precision Integer toward Zero */              \
  V(vrfiz, VRFIZ, 0x1000024A)                                             \
  /* Vector Rotate Left Byte */                                           \
  V(vrlb, VRLB, 0x10000004)                                               \
  /* Vector Rotate Left Doubleword */                                     \
  V(vrld, VRLD, 0x100000C4)                                               \
  /* Vector Rotate Left Halfword */                                       \
  V(vrlh, VRLH, 0x10000044)                                               \
  /* Vector Rotate Left Word */                                           \
  V(vrlw, VRLW, 0x10000084)                                               \
  /* Vector Reciprocal Square Root Estimate Single-Precision */           \
  V(vrsqrtefp, VRSQRTEFP, 0x1000014A)                                     \
  /* Vector Shift Left */                                                 \
  V(vsl, VSL, 0x100001C4)                                                 \
  /* Vector Shift Right */                                                \
  V(vsr, VSR, 0x100002C4)                                                 \
  /* Vector Subtract & write Carry Unsigned Quadword */                   \
  V(vsubcuq, VSUBCUQ, 0x10000540)                                         \
  /* Vector Subtract and Write Carry-Out Unsigned Word */                 \
  V(vsubcuw, VSUBCUW, 0x10000580)                                         \
  /* Vector Subtract Signed Word Saturate */                              \
  V(vsubsws, VSUBSWS, 0x10000780)                                         \
  /* Vector Subtract Unsigned Quadword Modulo */                          \
  V(vsubuqm, VSUBUQM, 0x10000500)                                         \
  /* Vector Subtract Unsigned Word Saturate */                            \
  V(vsubuws, VSUBUWS, 0x10000680)                                         \
  /* Vector Sum across Quarter Signed Byte Saturate */                    \
  V(vsum4sbs, VSUM4SBS, 0x10000708)                                       \
  /* Vector Sum across Quarter Unsigned Byte Saturate */                  \
  V(vsum4bus, VSUM4BUS, 0x10000608)                                       \
  /* Vector Sum across Signed Word Saturate */                            \
  V(vsumsws, VSUMSWS, 0x10000788)                                         \
  /* Vector Unpack High Pixel */                                          \
  V(vupkhpx, VUPKHPX, 0x1000034E)                                         \
  /* Vector Unpack Low Pixel */                                           \
  V(vupklpx, VUPKLPX, 0x100003CE)                                         \
  /* Vector AES Cipher */                                                 \
  V(vcipher, VCIPHER, 0x10000508)                                         \
  /* Vector AES Cipher Last */                                            \
  V(vcipherlast, VCIPHERLAST, 0x10000509)                                 \
  /* Vector AES Inverse Cipher */                                         \
  V(vncipher, VNCIPHER, 0x10000548)                                       \
  /* Vector AES Inverse Cipher Last */                                    \
  V(vncipherlast, VNCIPHERLAST, 0x10000549)                               \
  /* Vector AES S-Box */                                                  \
  V(vsbox, VSBOX, 0x100005C8)                                             \
  /* Vector SHA-512 Sigma Doubleword */                                   \
  V(vshasigmad, VSHASIGMAD, 0x100006C2)                                   \
  /* Vector SHA-256 Sigma Word */                                         \
  V(vshasigmaw, VSHASIGMAW, 0x10000682)                                   \
  /* Vector Merge Even Word */                                            \
  V(vmrgew, VMRGEW, 0x1000078C)                                           \
  /* Vector Merge Odd Word */                                             \
  V(vmrgow, VMRGOW, 0x1000068C)

#define PPC_VX_OPCODE_LIST(V)  \
  PPC_VX_OPCODE_A_FORM_LIST(V) \
  PPC_VX_OPCODE_B_FORM_LIST(V) \
  PPC_VX_OPCODE_C_FORM_LIST(V) \
  PPC_VX_OPCODE_D_FORM_LIST(V) \
  PPC_VX_OPCODE_E_FORM_LIST(V) \
  PPC_VX_OPCODE_F_FORM_LIST(V) \
  PPC_VX_OPCODE_G_FORM_LIST(V) \
  PPC_VX_OPCODE_UNUSED_LIST(V)

#define PPC_XS_OPCODE_LIST(V)                      \
  /* Shift Right Algebraic Doubleword Immediate */ \
  V(sradi, SRADIX, 0x7C000674)

#define PPC_MD_OPCODE_LIST(V)                             \
  /* Rotate Left Doubleword Immediate then Clear */       \
  V(rldic, RLDIC, 0x78000008)                             \
  /* Rotate Left Doubleword Immediate then Clear Left */  \
  V(rldicl, RLDICL, 0x78000000)                           \
  /* Rotate Left Doubleword Immediate then Clear Right */ \
  V(rldicr, RLDICR, 0x78000004)                           \
  /* Rotate Left Doubleword Immediate then Mask Insert */ \
  V(rldimi, RLDIMI, 0x7800000C)

#define PPC_SC_OPCODE_LIST(V) \
  /* System Call */           \
  V(sc, SC, 0x44000002)

#define PPC_PREFIX_OPCODE_TYPE_00_LIST(V)        \
  V(pload_store_8ls, PLOAD_STORE_8LS, 0x4000000) \
  V(pplwa, PPLWA, 0xA4000000)                    \
  V(ppld, PPLD, 0xE4000000)                      \
  V(ppstd, PPSTD, 0xF4000000)

#define PPC_PREFIX_OPCODE_TYPE_10_LIST(V) \
  V(pload_store_mls, PLOAD_STORE_MLS, 0x6000000)

#define PPC_OPCODE_LIST(V)          \
  PPC_X_OPCODE_LIST(V)              \
  PPC_X_OPCODE_EH_S_FORM_LIST(V)    \
  PPC_XO_OPCODE_LIST(V)             \
  PPC_DS_OPCODE_LIST(V)             \
  PPC_DQ_OPCODE_LIST(V)             \
  PPC_MDS_OPCODE_LIST(V)            \
  PPC_MD_OPCODE_LIST(V)             \
  PPC_XS_OPCODE_LIST(V)             \
  PPC_D_OPCODE_LIST(V)              \
  PPC_I_OPCODE_LIST(V)              \
  PPC_B_OPCODE_LIST(V)              \
  PPC_XL_OPCODE_LIST(V)             \
  PPC_A_OPCODE_LIST(V)              \
  PPC_XFX_OPCODE_LIST(V)            \
  PPC_M_OPCODE_LIST(V)              \
  PPC_SC_OPCODE_LIST(V)             \
  PPC_Z23_OPCODE_LIST(V)            \
  PPC_Z22_OPCODE_LIST(V)            \
  PPC_EVX_OPCODE_LIST(V)            \
  PPC_XFL_OPCODE_LIST(V)            \
  PPC_EVS_OPCODE_LIST(V)            \
  PPC_VX_OPCODE_LIST(V)             \
  PPC_VA_OPCODE_LIST(V)             \
  PPC_VC_OPCODE_LIST(V)             \
  PPC_XX1_OPCODE_LIST(V)            \
  PPC_XX2_OPCODE_LIST(V)            \
  PPC_XX3_OPCODE_VECTOR_LIST(V)     \
  PPC_XX3_OPCODE_SCALAR_LIST(V)     \
  PPC_XX4_OPCODE_LIST(V)            \
  PPC_PREFIX_OPCODE_TYPE_00_LIST(V) \
  PPC_PREFIX_OPCODE_TYPE_10_LIST(V)

enum Opcode : uint32_t {
#define DECLARE_INSTRUCTION(name, opcode_name, opcode_value) \
  opcode_name = opcode_value,
  PPC_OPCODE_LIST(DECLARE_INSTRUCTION)
#undef DECLARE_INSTRUCTION
      EXTP = 0x4000000,  // Extended code set prefixed
  EXT0 = 0x10000000,     // Extended code set 0
  EXT1 = 0x4C000000,     // Extended code set 1
  EXT2 = 0x7C000000,     // Extended code set 2
  EXT3 = 0xEC000000,     // Extended code set 3
  EXT4 = 0xFC000000,     // Extended code set 4
  EXT5 = 0x78000000,     // Extended code set 5 - 64bit only
  EXT6 = 0xF0000000,     // Extended code set 6
};

// Instruction encoding bits and masks.
enum {
  // Instruction encoding bit
  B1 = 1 << 1,
  B2 = 1 << 2,
  B3 = 1 << 3,
  B4 = 1 << 4,
  B5 = 1 << 5,
  B7 = 1 << 7,
  B8 = 1 << 8,
  B9 = 1 << 9,
  B12 = 1 << 12,
  B18 = 1 << 18,
  B19 = 1 << 19,
  B20 = 1 << 20,
  B22 = 1 << 22,
  B23 = 1 << 23,
  B24 = 1 << 24,
  B25 = 1 << 25,
  B26 = 1 << 26,
  B27 = 1 << 27,
  B28 = 1 << 28,
  B6 = 1 << 6,
  B10 = 1 << 10,
  B11 = 1 << 11,
  B16 = 1 << 16,
  B17 = 1 << 17,
  B21 = 1 << 21,

  // Instruction bit masks
  kCondMask = 0x1F << 21,
  kOff12Mask = (1 << 12) - 1,
  kImm24Mask = (1 << 24) - 1,
  kOff16Mask = (1 << 16) - 1,
  kImm16Mask = (1 << 16) - 1,
  kImm18Mask = (1 << 18) - 1,
  kImm22Mask = (1 << 22) - 1,
  kImm26Mask = (1 << 26) - 1,
  kBOfieldMask = 0x1f << 21,
  kOpcodeMask = 0x3f << 26,
  kExt1OpcodeMask = 0x3ff << 1,
  kExt2OpcodeMask = 0x3ff << 1,
  kExt2OpcodeVariant2Mask = 0x1ff << 2,
  kExt5OpcodeMask = 0x3 << 2,
  kBOMask = 0x1f << 21,
  kBIMask = 0x1F << 16,
  kBDMask = 0x14 << 2,
  kAAMask = 0x01 << 1,
  kLKMask = 0x01,
  kRCMask = 0x01,
  kTOMask = 0x1f << 21
};

// -----------------------------------------------------------------------------
// Addressing modes and instruction variants.

// Overflow Exception
enum OEBit {
  SetOE = 1 << 10,   // Set overflow exception
  LeaveOE = 0 << 10  // No overflow exception
};

// Record bit
enum RCBit {   // Bit 0
  SetRC = 1,   // LT,GT,EQ,SO
  LeaveRC = 0  // None
};
// Exclusive Access hint bit
enum EHBit {   // Bit 0
  SetEH = 1,   // Exclusive Access
  LeaveEH = 0  // Atomic Update
};

// Link bit
enum LKBit {   // Bit 0
  SetLK = 1,   // Load effective address of next instruction
  LeaveLK = 0  // No action
};

// Prefixed R bit.
enum PRBit { SetPR = 1, LeavePR = 0 };

enum BOfield {        // Bits 25-21
  DCBNZF = 0 << 21,   // Decrement CTR; branch if CTR != 0 and condition false
  DCBEZF = 2 << 21,   // Decrement CTR; branch if CTR == 0 and condition false
  BF = 4 << 21,       // Branch if condition false
  DCBNZT = 8 << 21,   // Decrement CTR; branch if CTR != 0 and condition true
  DCBEZT = 10 << 21,  // Decrement CTR; branch if CTR == 0 and condition true
  BT = 12 << 21,      // Branch if condition true
  DCBNZ = 16 << 21,   // Decrement CTR; branch if CTR != 0
  DCBEZ = 18 << 21,   // Decrement CTR; branch if CTR == 0
  BA = 20 << 21       // Branch always
};

#if V8_OS_AIX
#undef CR_LT
#undef CR_GT
#undef CR_EQ
#undef CR_SO
#endif

enum CRBit { CR_LT = 0, CR_GT = 1, CR_EQ = 2, CR_SO = 3, CR_FU = 3 };

#define CRWIDTH 4

// These are the documented bit positions biased down by 32
enum FPSCRBit {
  VXSOFT = 21,  // 53: Software-Defined Condition
  VXSQRT = 22,  // 54: Invalid Square Root
  VXCVI = 23    // 55: Invalid Integer Convert
};

// -----------------------------------------------------------------------------
// Supervisor Call (svc) specific support.

// Special Software Interrupt codes when used in the presence of the PPC
// simulator.
// svc (formerly swi) provides a 24bit immediate value. Use bits 22:0 for
// standard SoftwareInterrupCode. Bit 23 is reserved for the stop feature.
enum SoftwareInterruptCodes {
  // transition to C code
  kCallRtRedirected = 0x10,
  // break point
  kBreakpoint = 0x821008,  // bits23-0 of 0x7d821008 = twge r2, r2
  // stop
  kStopCode = 1 << 23
};
const uint32_t kStopCodeMask = kStopCode - 1;
const uint32_t kMaxStopCode = kStopCode - 1;
const int32_t kDefaultStopCode = -1;

// FP rounding modes.
enum FPRoundingMode {
  RN = 0,  // Round to Nearest.
  RZ = 1,  // Round towards zero.
  RP = 2,  // Round towards Plus Infinity.
  RM = 3,  // Round towards Minus Infinity.

  // Aliases.
  kRoundToNearest = RN,
  kRoundToZero = RZ,
  kRoundToPlusInf = RP,
  kRoundToMinusInf = RM
};

const uint32_t kFPRoundingModeMask = 3;

enum CheckForInexactConversion {
  kCheckForInexactConversion,
  kDontCheckForInexactConversion
};

// -----------------------------------------------------------------------------
// Specific instructions, constants, and masks.
// These constants are declared in assembler-arm.cc, as they use named registers
// and other constants.

// add(sp, sp, 4) instruction (aka Pop())
extern const Instr kPopInstruction;

// str(r, MemOperand(sp, 4, NegPreIndex), al) instruction (aka push(r))
// register r is not encoded.
extern const Instr kPushRegPattern;

// ldr(r, MemOperand(sp, 4, PostIndex), al) instruction (aka pop(r))
// register r is not encoded.
extern const Instr kPopRegPattern;

// use TWI to indicate redirection call for simulation mode
const Instr rtCallRedirInstr = TWI;

// -----------------------------------------------------------------------------
// Instruction abstraction.

// The class Instruction enables access to individual fields defined in the PPC
// architecture instruction set encoding.
// Note that the Assembler uses typedef int32_t Instr.
//
// Example: Test whether the instruction at ptr does set the condition code
// bits.
//
// bool InstructionSetsConditionCodes(uint8_t* ptr) {
//   Instruction* instr = Instruction::At(ptr);
//   int type = instr->TypeValue();
//   return ((type == 0) || (type == 1)) && instr->HasS();
// }
//

constexpr uint8_t kInstrSize = 4;
constexpr uint8_t kInstrSizeLog2 = 2;
constexpr uint8_t kPcLoadDelta = 8;

class Instruction {
 public:
// Helper macro to define static accessors.
// We use the cast to char* trick to bypass the strict anti-aliasing rules.
#define DECLARE_STATIC_TYPED_ACCESSOR(return_type, Name) \
  static inline return_type Name(Instr instr) {          \
    char* temp = reinterpret_cast<char*>(&instr);        \
    return reinterpret_cast<Instruction*>(temp)->Name(); \
  }

#define DECLARE_STATIC_ACCESSOR(Name) DECLARE_STATIC_TYPED_ACCESSOR(int, Name)

  // Get the raw instruction bits.
  inline Instr InstructionBits() const {
    return *reinterpret_cast<const Instr*>(this);
  }

  // Set the raw instruction bits to value.
  inline void SetInstructionBits(Instr value) {
    *reinterpret_cast<Instr*>(this) = value;
  }

  // Read one particular bit out of the instruction bits.
  inline int Bit(int nr) const { return (InstructionBits() >> nr) & 1; }

  // Read a bit field's value out of the instruction bits.
  inline int Bits(int hi, int lo) const {
    return (InstructionBits() >> lo) & ((2 << (hi - lo)) - 1);
  }

  // Read a bit field out of the instruction bits.
  inline uint32_t BitField(int hi, int lo) const {
    return InstructionBits() & (((2 << (hi - lo)) - 1) << lo);
  }

  // Static support.

  // Read one particular bit out of the instruction bits.
  static inline int Bit(Instr instr, int nr) { return (instr >> nr) & 1; }

  // Read the value of a bit field out of the instruction bits.
  static inline int Bits(Instr instr, int hi, int lo) {
    return (instr >> lo) & ((2 << (hi - lo)) - 1);
  }

  // Read a bit field out of the instruction bits.
  static inline uint32_t BitField(Instr instr, int hi, int lo) {
    return instr & (((2 << (hi - lo)) - 1) << lo);
  }

  inline int RSValue() const { return Bits(25, 21); }
  inline int RTValue() const { return Bits(25, 21); }
  inline int RAValue() const { return Bits(20, 16); }
  DECLARE_STATIC_ACCESSOR(RAValue)
  inline int RBValue() const { return Bits(15, 11); }
  DECLARE_STATIC_ACCESSOR(RBValue)
  inline int RCValue() const { return Bits(10, 6); }
  DECLARE_STATIC_ACCESSOR(RCValue)

  inline int OpcodeValue() const { return static_cast<Opcode>(Bits(31, 26)); }
  inline uint32_t OpcodeField() const {
    return static_cast<Opcode>(BitField(31, 26));
  }
  inline uint32_t PrefixOpcodeField() const {
    return static_cast<Opcode>(BitField(31, 25));
  }

#define OPCODE_CASES(name, opcode_name, opcode_value) case opcode_name:

  inline Opcode OpcodeBase() const {
    uint32_t opcode = PrefixOpcodeField();
    uint32_t extcode = PrefixOpcodeField();
    // Check for prefix.
    switch (opcode) {
      PPC_PREFIX_OPCODE_TYPE_00_LIST(OPCODE_CASES)
      PPC_PREFIX_OPCODE_TYPE_10_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = OpcodeField();
    extcode = OpcodeField();
    // Check for suffix.
    switch (opcode) {
      PPC_PREFIX_OPCODE_TYPE_00_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    switch (opcode) {
      PPC_D_OPCODE_LIST(OPCODE_CASES)
      PPC_I_OPCODE_LIST(OPCODE_CASES)
      PPC_B_OPCODE_LIST(OPCODE_CASES)
      PPC_M_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(5, 0);
    switch (opcode) {
      PPC_VA_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    // Some VX opcodes have integers hard coded in the middle, handle those
    // first.
    opcode = extcode | BitField(20, 16) | BitField(10, 0);
    switch (opcode) {
      PPC_VX_OPCODE_D_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_F_FORM_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 0);
    switch (opcode) {
      PPC_VX_OPCODE_A_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_B_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_C_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_E_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_G_FORM_LIST(OPCODE_CASES)
      PPC_VX_OPCODE_UNUSED_LIST(OPCODE_CASES)
      PPC_X_OPCODE_EH_S_FORM_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(9, 0);
    switch (opcode) {
      PPC_VC_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 1) | BitField(20, 20);
    switch (opcode) {
      PPC_XFX_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    // Some XX2 opcodes have integers hard coded in the middle, handle those
    // first.
    opcode = extcode | BitField(20, 16) | BitField(10, 2);
    switch (opcode) {
      PPC_XX2_OPCODE_B_FORM_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 2);
    switch (opcode) {
      PPC_XX2_OPCODE_VECTOR_A_FORM_LIST(OPCODE_CASES)
      PPC_XX2_OPCODE_SCALAR_A_FORM_LIST(OPCODE_CASES)
      PPC_XX2_OPCODE_UNUSED_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 1);
    switch (opcode) {
      PPC_X_OPCODE_LIST(OPCODE_CASES)
      PPC_XL_OPCODE_LIST(OPCODE_CASES)
      PPC_XFL_OPCODE_LIST(OPCODE_CASES)
      PPC_XX1_OPCODE_LIST(OPCODE_CASES)
      PPC_EVX_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(9, 1);
    switch (opcode) {
      PPC_XO_OPCODE_LIST(OPCODE_CASES)
      PPC_Z22_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 2);
    switch (opcode) {
      PPC_XS_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(9, 3);
    switch (opcode) {
      PPC_XX3_OPCODE_VECTOR_A_FORM_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(10, 3);
    switch (opcode) {
      PPC_EVS_OPCODE_LIST(OPCODE_CASES)
      PPC_XX3_OPCODE_VECTOR_B_FORM_LIST(OPCODE_CASES)
      PPC_XX3_OPCODE_SCALAR_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(8, 1);
    switch (opcode) {
      PPC_Z23_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(5, 1);
    switch (opcode) {
      PPC_A_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(4, 1);
    switch (opcode) {
      PPC_MDS_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(4, 2);
    switch (opcode) {
      PPC_MD_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(5, 4);
    switch (opcode) {
      PPC_XX4_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(2, 0);
    switch (opcode) {
      PPC_DQ_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(1, 0);
    switch (opcode) {
      PPC_DS_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    opcode = extcode | BitField(1, 1);
    switch (opcode) {
      PPC_SC_OPCODE_LIST(OPCODE_CASES)
      return static_cast<Opcode>(opcode);
    }
    UNIMPLEMENTED();
    return static_cast<Opcode>(0);
  }

#undef OPCODE_CASES

  // Fields used in Software interrupt instructions
  inline SoftwareInterruptCodes SvcValue() const {
    return static_cast<SoftwareInterruptCodes>(Bits(23, 0));
  }

  // Instructions are read of out a code stream. The only way to get a
  // reference to an instruction is to convert a pointer. There is no way
  // to allocate or create instances of class Instruction.
  // Use the At(pc) function to create references to Instruction.
  static Instruction* At(uint8_t* pc) {
    return reinterpret_cast<Instruction*>(pc);
  }

 private:
  // We need to prevent the creation of instances of class Instruction.
  DISALLOW_IMPLICIT_CONSTRUCTORS(Instruction);
};

// Helper functions for converting between register numbers and names.
class Registers {
 public:
  // Lookup the register number for the name provided.
  static int Number(const char* name);

 private:
  static const char* names_[kNumRegisters];
};

// Helper functions for converting between FP register numbers and names.
class DoubleRegisters {
 public:
  // Lookup the register number for the name provided.
  static int Number(const char* name);

 private:
  static const char* names_[kNumDoubleRegisters];
};
}  // namespace internal
}  // namespace v8

static constexpr int kR0DwarfCode = 0;
static constexpr int kFpDwarfCode = 31;  // frame-pointer
static constexpr int kLrDwarfCode = 65;  // return-address(lr)
static constexpr int kSpDwarfCode = 1;   // stack-pointer (sp)

#endif  // V8_CODEGEN_PPC_CONSTANTS_PPC_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/codegen/ppc/cpu-ppc.cc                                                      0000664 0000000 0000000 00000002561 14746647661 0020704 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

// CPU specific code for ppc independent of OS goes here.

#if V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64

#include "src/codegen/cpu-features.h"

#define INSTR_AND_DATA_CACHE_COHERENCY PPC_6_PLUS

namespace v8 {
namespace internal {

void CpuFeatures::FlushICache(void* buffer, size_t size) {
#if !defined(USE_SIMULATOR)
  if (CpuFeatures::IsSupported(INSTR_AND_DATA_CACHE_COHERENCY)) {
    __asm__ __volatile__(
        "sync \n"
        "icbi 0, %0  \n"
        "isync  \n"
        : /* no output */
        : "r"(buffer)
        : "memory");
    return;
  }

  const int kCacheLineSize = CpuFeatures::icache_line_size();
  intptr_t mask = kCacheLineSize - 1;
  uint8_t* start =
      reinterpret_cast<uint8_t*>(reinterpret_cast<intptr_t>(buffer) & ~mask);
  uint8_t* end = static_cast<uint8_t*>(buffer) + size;
  for (uint8_t* pointer = start; pointer < end; pointer += kCacheLineSize) {
    __asm__(
        "dcbf 0, %0  \n"
        "sync        \n"
        "icbi 0, %0  \n"
        "isync       \n"
        : /* no output */
        : "r"(pointer));
  }

#endif  // !USE_SIMULATOR
}
}  // namespace internal
}  // namespace v8

#undef INSTR_AND_DATA_CACHE_COHERENCY
#endif  // V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
                                                                                                                                               node-23.7.0/deps/v8/src/codegen/ppc/interface-descriptors-ppc-inl.h                                 0000664 0000000 0000000 00000025372 14746647661 0025043 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2021 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_PPC_INTERFACE_DESCRIPTORS_PPC_INL_H_
#define V8_CODEGEN_PPC_INTERFACE_DESCRIPTORS_PPC_INL_H_

#if V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64

#include "src/codegen/interface-descriptors.h"
#include "src/execution/frames.h"

namespace v8 {
namespace internal {

constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
  auto registers = RegisterArray(r3, r4, r5, r6, r7);
  static_assert(registers.size() == kMaxBuiltinRegisterParams);
  return registers;
}

constexpr auto CallInterfaceDescriptor::DefaultDoubleRegisterArray() {
  auto registers = DoubleRegisterArray(d1, d2, d3, d4, d5, d6, d7);
  return registers;
}

constexpr auto CallInterfaceDescriptor::DefaultReturnRegisterArray() {
  auto registers =
      RegisterArray(kReturnRegister0, kReturnRegister1, kReturnRegister2);
  return registers;
}

constexpr auto CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray() {
  // Padding to have as many double return registers as GP return registers.
  auto registers = DoubleRegisterArray(kFPReturnRegister0, no_dreg, no_dreg);
  return registers;
}

#if DEBUG
template <typename DerivedDescriptor>
void StaticCallInterfaceDescriptor<DerivedDescriptor>::
    VerifyArgumentRegisterCount(CallInterfaceDescriptorData* data, int argc) {
  RegList allocatable_regs = data->allocatable_registers();
  if (argc >= 1) DCHECK(allocatable_regs.has(r3));
  if (argc >= 2) DCHECK(allocatable_regs.has(r4));
  if (argc >= 3) DCHECK(allocatable_regs.has(r5));
  if (argc >= 4) DCHECK(allocatable_regs.has(r6));
  if (argc >= 5) DCHECK(allocatable_regs.has(r7));
  if (argc >= 6) DCHECK(allocatable_regs.has(r8));
  if (argc >= 7) DCHECK(allocatable_regs.has(r9));
  if (argc >= 8) DCHECK(allocatable_regs.has(r10));
  // Additional arguments are passed on the stack.
}
#endif  // DEBUG

// static
constexpr auto WriteBarrierDescriptor::registers() {
  return RegisterArray(r4, r8, r7, r5, r3, r6, kContextRegister);
}

// static
constexpr Register LoadDescriptor::ReceiverRegister() { return r4; }
// static
constexpr Register LoadDescriptor::NameRegister() { return r5; }
// static
constexpr Register LoadDescriptor::SlotRegister() { return r3; }

// static
constexpr Register LoadWithVectorDescriptor::VectorRegister() { return r6; }

// static
constexpr Register KeyedLoadBaselineDescriptor::ReceiverRegister() {
  return r4;
}
// static
constexpr Register KeyedLoadBaselineDescriptor::NameRegister() {
  return kInterpreterAccumulatorRegister;
}
// static
constexpr Register KeyedLoadBaselineDescriptor::SlotRegister() { return r5; }

// static
constexpr Register KeyedLoadWithVectorDescriptor::VectorRegister() {
  return r6;
}

// static
constexpr Register EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister() {
  return r7;
}

// static
constexpr Register EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister() {
  return r8;
}

// static
constexpr Register EnumeratedKeyedLoadBaselineDescriptor::SlotRegister() {
  return r5;
}

// static
constexpr Register KeyedHasICBaselineDescriptor::ReceiverRegister() {
  return kInterpreterAccumulatorRegister;
}
// static
constexpr Register KeyedHasICBaselineDescriptor::NameRegister() { return r4; }
// static
constexpr Register KeyedHasICBaselineDescriptor::SlotRegister() { return r5; }

// static
constexpr Register KeyedHasICWithVectorDescriptor::VectorRegister() {
  return r6;
}

// static
constexpr Register
LoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() {
  return r7;
}

// static
constexpr Register StoreDescriptor::ReceiverRegister() { return r4; }
// static
constexpr Register StoreDescriptor::NameRegister() { return r5; }
// static
constexpr Register StoreDescriptor::ValueRegister() { return r3; }
// static
constexpr Register StoreDescriptor::SlotRegister() { return r7; }

// static
constexpr Register StoreWithVectorDescriptor::VectorRegister() { return r6; }

// static
constexpr Register DefineKeyedOwnDescriptor::FlagsRegister() { return r8; }

// static
constexpr Register StoreTransitionDescriptor::MapRegister() { return r8; }

// static
constexpr Register ApiGetterDescriptor::HolderRegister() { return r3; }
// static
constexpr Register ApiGetterDescriptor::CallbackRegister() { return r6; }

// static
constexpr Register GrowArrayElementsDescriptor::ObjectRegister() { return r3; }
// static
constexpr Register GrowArrayElementsDescriptor::KeyRegister() { return r6; }

// static
constexpr Register BaselineLeaveFrameDescriptor::ParamsSizeRegister() {
  return r6;
}
// static
constexpr Register BaselineLeaveFrameDescriptor::WeightRegister() { return r7; }

// static
// static
constexpr Register TypeConversionDescriptor::ArgumentRegister() { return r3; }

// static
constexpr auto TypeofDescriptor::registers() { return RegisterArray(r3); }

// static
constexpr auto CallTrampolineDescriptor::registers() {
  // r3 : number of arguments
  // r4 : the target to call
  return RegisterArray(r4, r3);
}

// static
constexpr auto CopyDataPropertiesWithExcludedPropertiesDescriptor::registers() {
  // r4 : the source
  // r3 : the excluded property count
  return RegisterArray(r4, r3);
}

// static
constexpr auto
CopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers() {
  // r4 : the source
  // r3 : the excluded property count
  // r5 : the excluded property base
  return RegisterArray(r4, r3, r5);
}

// static
constexpr auto CallVarargsDescriptor::registers() {
  // r3 : number of arguments (on the stack)
  // r4 : the target to call
  // r7 : arguments list length (untagged)
  // r5 : arguments list (FixedArray)
  return RegisterArray(r4, r3, r7, r5);
}

// static
constexpr auto CallForwardVarargsDescriptor::registers() {
  // r3 : number of arguments
  // r5 : start index (to support rest parameters)
  // r4 : the target to call
  return RegisterArray(r4, r3, r5);
}

// static
constexpr auto CallFunctionTemplateDescriptor::registers() {
  // r4 : function template info
  // r5 : number of arguments (on the stack)
  return RegisterArray(r4, r5);
}

// static
constexpr auto CallFunctionTemplateGenericDescriptor::registers() {
  // r4 : function template info
  // r5 : number of arguments (on the stack)
  // r6 : topmost script-having context
  return RegisterArray(r4, r5, r6);
}

// static
constexpr auto CallWithSpreadDescriptor::registers() {
  // r3 : number of arguments (on the stack)
  // r4 : the target to call
  // r5 : the object to spread
  return RegisterArray(r4, r3, r5);
}

// static
constexpr auto CallWithArrayLikeDescriptor::registers() {
  // r4 : the target to call
  // r5 : the arguments list
  return RegisterArray(r4, r5);
}

// static
constexpr auto ConstructVarargsDescriptor::registers() {
  // r3 : number of arguments (on the stack)
  // r4 : the target to call
  // r6 : the new target
  // r7 : arguments list length (untagged)
  // r5 : arguments list (FixedArray)
  return RegisterArray(r4, r6, r3, r7, r5);
}

// static
constexpr auto ConstructForwardVarargsDescriptor::registers() {
  // r3 : number of arguments
  // r6 : the new target
  // r5 : start index (to support rest parameters)
  // r4 : the target to call
  return RegisterArray(r4, r6, r3, r5);
}

// static
constexpr auto ConstructWithSpreadDescriptor::registers() {
  // r3 : number of arguments (on the stack)
  // r4 : the target to call
  // r6 : the new target
  // r5 : the object to spread
  return RegisterArray(r4, r6, r3, r5);
}

// static
constexpr auto ConstructWithArrayLikeDescriptor::registers() {
  // r4 : the target to call
  // r6 : the new target
  // r5 : the arguments list
  return RegisterArray(r4, r6, r5);
}

// static
constexpr auto ConstructStubDescriptor::registers() {
  // r3 : number of arguments
  // r4 : the target to call
  // r6 : the new target
  return RegisterArray(r4, r6, r3);
}

// static
constexpr auto AbortDescriptor::registers() { return RegisterArray(r4); }

// static
constexpr auto CompareDescriptor::registers() { return RegisterArray(r4, r3); }

// static
constexpr auto Compare_BaselineDescriptor::registers() {
  return RegisterArray(r4, r3, r5);
}

// static
constexpr auto BinaryOpDescriptor::registers() { return RegisterArray(r4, r3); }

// static
constexpr auto BinaryOp_BaselineDescriptor::registers() {
  return RegisterArray(r4, r3, r5);
}

// static
constexpr auto BinarySmiOp_BaselineDescriptor::registers() {
  return RegisterArray(r3, r4, r5);
}

// static
constexpr Register
CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister() {
  return r4;
}
// static
constexpr Register
CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister() {
  return r5;
}
// static
constexpr Register
CallApiCallbackOptimizedDescriptor::FunctionTemplateInfoRegister() {
  return r6;
}
// static
constexpr Register CallApiCallbackOptimizedDescriptor::HolderRegister() {
  return r3;
}
// static
constexpr Register
CallApiCallbackGenericDescriptor::ActualArgumentsCountRegister() {
  return r5;
}
// static
constexpr Register
CallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister() {
  return r4;
}
// static
constexpr Register
CallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister() {
  return r6;
}
// static
constexpr Register CallApiCallbackGenericDescriptor::HolderRegister() {
  return r3;
}

// static
constexpr auto InterpreterDispatchDescriptor::registers() {
  return RegisterArray(
      kInterpreterAccumulatorRegister, kInterpreterBytecodeOffsetRegister,
      kInterpreterBytecodeArrayRegister, kInterpreterDispatchTableRegister);
}

// static
constexpr auto InterpreterPushArgsThenCallDescriptor::registers() {
  return RegisterArray(r3,   // argument count
                       r5,   // address of first argument
                       r4);  // the target callable to be call
}

// static
constexpr auto InterpreterPushArgsThenConstructDescriptor::registers() {
  return RegisterArray(
      r3,   // argument count
      r7,   // address of the first argument
      r4,   // constructor to call
      r6,   // new target
      r5);  // allocation site feedback if available, undefined otherwise
}

// static
constexpr auto ConstructForwardAllArgsDescriptor::registers() {
  return RegisterArray(r4,   // constructor to call
                       r6);  // new target
}

// static
constexpr auto ResumeGeneratorDescriptor::registers() {
  return RegisterArray(r3,   // the value to pass to the generator
                       r4);  // the JSGeneratorObject to resume
}

// static
constexpr auto RunMicrotasksEntryDescriptor::registers() {
  return RegisterArray(r3, r4);
}

constexpr auto WasmJSToWasmWrapperDescriptor::registers() {
  // Arbitrarily picked register.
  return RegisterArray(r14);
}
}  // namespace internal
}  // namespace v8

#endif  // V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64

#endif  // V8_CODEGEN_PPC_INTERFACE_DESCRIPTORS_PPC_INL_H_
                                                                                                                                                                                                                                                                      node-23.7.0/deps/v8/src/codegen/ppc/macro-assembler-ppc.cc                                          0000664 0000000 0000000 00000642601 14746647661 0023176 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include <assert.h>  // For assert
#include <limits.h>  // For LONG_MIN, LONG_MAX.

#if V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64

#include <optional>

#include "src/base/bits.h"
#include "src/base/division-by-constant.h"
#include "src/builtins/builtins-inl.h"
#include "src/codegen/callable.h"
#include "src/codegen/code-factory.h"
#include "src/codegen/external-reference-table.h"
#include "src/codegen/interface-descriptors-inl.h"
#include "src/codegen/macro-assembler.h"
#include "src/codegen/register-configuration.h"
#include "src/codegen/register.h"
#include "src/debug/debug.h"
#include "src/deoptimizer/deoptimizer.h"
#include "src/execution/frames-inl.h"
#include "src/heap/mutable-page-metadata.h"
#include "src/init/bootstrapper.h"
#include "src/logging/counters.h"
#include "src/runtime/runtime.h"
#include "src/snapshot/snapshot.h"

// Satisfy cpplint check, but don't include platform-specific header. It is
// included recursively via macro-assembler.h.
#if 0
#include "src/codegen/ppc/macro-assembler-ppc.h"
#endif

#define __ ACCESS_MASM(masm)

namespace v8 {
namespace internal {

namespace {

// Simd and Floating Pointer registers are not shared. For WebAssembly we save
// both registers, If we are not running Wasm, we can get away with only saving
// FP registers.
#if V8_ENABLE_WEBASSEMBLY
constexpr int kStackSavedSavedFPSizeInBytes =
    (kNumCallerSavedDoubles * kSimd128Size) +
    (kNumCallerSavedDoubles * kDoubleSize);
#else
constexpr int kStackSavedSavedFPSizeInBytes =
    kNumCallerSavedDoubles * kDoubleSize;
#endif  // V8_ENABLE_WEBASSEMBLY

}  // namespace

int MacroAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
                                                    Register exclusion1,
                                                    Register exclusion2,
                                                    Register exclusion3) const {
  int bytes = 0;

  RegList exclusions = {exclusion1, exclusion2, exclusion3};
  RegList list = kJSCallerSaved - exclusions;
  bytes += list.Count() * kSystemPointerSize;

  if (fp_mode == SaveFPRegsMode::kSave) {
    bytes += kStackSavedSavedFPSizeInBytes;
  }

  return bytes;
}

int MacroAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register scratch1,
                                    Register scratch2, Register exclusion1,
                                    Register exclusion2, Register exclusion3) {
  int bytes = 0;

  RegList exclusions = {exclusion1, exclusion2, exclusion3};
  RegList list = kJSCallerSaved - exclusions;
  MultiPush(list);
  bytes += list.Count() * kSystemPointerSize;

  if (fp_mode == SaveFPRegsMode::kSave) {
    MultiPushF64AndV128(kCallerSavedDoubles, kCallerSavedSimd128s, scratch1,
                        scratch2);
    bytes += kStackSavedSavedFPSizeInBytes;
  }

  return bytes;
}

int MacroAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register scratch1,
                                   Register scratch2, Register exclusion1,
                                   Register exclusion2, Register exclusion3) {
  int bytes = 0;
  if (fp_mode == SaveFPRegsMode::kSave) {
    MultiPopF64AndV128(kCallerSavedDoubles, kCallerSavedSimd128s, scratch1,
                       scratch2);
    bytes += kStackSavedSavedFPSizeInBytes;
  }

  RegList exclusions = {exclusion1, exclusion2, exclusion3};
  RegList list = kJSCallerSaved - exclusions;
  MultiPop(list);
  bytes += list.Count() * kSystemPointerSize;

  return bytes;
}

void MacroAssembler::Jump(Register target) {
  mtctr(target);
  bctr();
}

void MacroAssembler::LoadFromConstantsTable(Register destination,
                                            int constant_index) {
  DCHECK(RootsTable::IsImmortalImmovable(RootIndex::kBuiltinsConstantsTable));

  DCHECK_NE(destination, r0);
  LoadRoot(destination, RootIndex::kBuiltinsConstantsTable);
  LoadTaggedField(destination,
                  FieldMemOperand(destination, FixedArray::OffsetOfElementAt(
                                                   constant_index)),
                  r0);
}

void MacroAssembler::LoadRootRelative(Register destination, int32_t offset) {
  LoadU64(destination, MemOperand(kRootRegister, offset), r0);
}

void MacroAssembler::StoreRootRelative(int32_t offset, Register value) {
  StoreU64(value, MemOperand(kRootRegister, offset));
}

void MacroAssembler::LoadRootRegisterOffset(Register destination,
                                            intptr_t offset) {
  if (offset == 0) {
    mr(destination, kRootRegister);
  } else {
    AddS64(destination, kRootRegister, Operand(offset), destination);
  }
}

MemOperand MacroAssembler::ExternalReferenceAsOperand(
    ExternalReference reference, Register scratch) {
  if (root_array_available()) {
    if (reference.IsIsolateFieldId()) {
      return MemOperand(kRootRegister, reference.offset_from_root_register());
    }
    if (options().enable_root_relative_access) {
      intptr_t offset =
          RootRegisterOffsetForExternalReference(isolate(), reference);
      if (is_int32(offset)) {
        return MemOperand(kRootRegister, static_cast<int32_t>(offset));
      }
    }
    if (options().isolate_independent_code) {
      if (IsAddressableThroughRootRegister(isolate(), reference)) {
        // Some external references can be efficiently loaded as an offset from
        // kRootRegister.
        intptr_t offset =
            RootRegisterOffsetForExternalReference(isolate(), reference);
        CHECK(is_int32(offset));
        return MemOperand(kRootRegister, static_cast<int32_t>(offset));
      } else {
        // Otherwise, do a memory load from the external reference table.
        LoadU64(scratch,
                MemOperand(kRootRegister,
                           RootRegisterOffsetForExternalReferenceTableEntry(
                               isolate(), reference)));
        return MemOperand(scratch, 0);
      }
    }
  }
  Move(scratch, reference);
  return MemOperand(scratch, 0);
}

void MacroAssembler::Jump(intptr_t target, RelocInfo::Mode rmode,
                          Condition cond, CRegister cr) {
  Label skip;

  if (cond != al) b(NegateCondition(cond), &skip, cr);

  mov(ip, Operand(target, rmode));
  mtctr(ip);
  bctr();

  bind(&skip);
}

void MacroAssembler::Jump(Address target, RelocInfo::Mode rmode, Condition cond,
                          CRegister cr) {
  DCHECK(!RelocInfo::IsCodeTarget(rmode));
  Jump(static_cast<intptr_t>(target), rmode, cond, cr);
}

void MacroAssembler::Jump(Handle<Code> code, RelocInfo::Mode rmode,
                          Condition cond, CRegister cr) {
  DCHECK(RelocInfo::IsCodeTarget(rmode));
  DCHECK_IMPLIES(options().isolate_independent_code,
                 Builtins::IsIsolateIndependentBuiltin(*code));

  Builtin builtin = Builtin::kNoBuiltinId;
  if (isolate()->builtins()->IsBuiltinHandle(code, &builtin)) {
    TailCallBuiltin(builtin, cond, cr);
    return;
  }
  int32_t target_index = AddCodeTarget(code);
  Jump(static_cast<intptr_t>(target_index), rmode, cond, cr);
}

void MacroAssembler::Jump(const ExternalReference& reference) {
  UseScratchRegisterScope temps(this);
  Register scratch = temps.Acquire();
  Move(scratch, reference);
  if (ABI_USES_FUNCTION_DESCRIPTORS) {
    // AIX uses a function descriptor. When calling C code be
    // aware of this descriptor and pick up values from it.
    LoadU64(ToRegister(ABI_TOC_REGISTER),
            MemOperand(scratch, kSystemPointerSize));
    LoadU64(scratch, MemOperand(scratch, 0));
  }
  Jump(scratch);
}

void MacroAssembler::Call(Register target) {
  BlockTrampolinePoolScope block_trampoline_pool(this);
  // branch via link register and set LK bit for return point
  mtctr(target);
  bctrl();
}

void MacroAssembler::CallJSEntry(Register target) {
  CHECK(target == r5);
  Call(target);
}

int MacroAssembler::CallSizeNotPredictableCodeSize(Address target,
                                                   RelocInfo::Mode rmode,
                                                   Condition cond) {
  return (2 + kMovInstructionsNoConstantPool) * kInstrSize;
}

void MacroAssembler::Call(Address target, RelocInfo::Mode rmode,
                          Condition cond) {
  BlockTrampolinePoolScope block_trampoline_pool(this);
  DCHECK(cond == al);

  // This can likely be optimized to make use of bc() with 24bit relative
  //
  // RecordRelocInfo(x.rmode_, x.immediate);
  // bc( BA, .... offset, LKset);
  //

  mov(ip, Operand(target, rmode));
  mtctr(ip);
  bctrl();
}

void MacroAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
                          Condition cond) {
  BlockTrampolinePoolScope block_trampoline_pool(this);
  DCHECK(RelocInfo::IsCodeTarget(rmode));
  DCHECK_IMPLIES(options().isolate_independent_code,
                 Builtins::IsIsolateIndependentBuiltin(*code));

  Builtin builtin = Builtin::kNoBuiltinId;
  if (isolate()->builtins()->IsBuiltinHandle(code, &builtin)) {
    CallBuiltin(builtin, cond);
    return;
  }
  int32_t target_index = AddCodeTarget(code);
  Call(static_cast<Address>(target_index), rmode, cond);
}

void MacroAssembler::CallBuiltin(Builtin builtin, Condition cond) {
  ASM_CODE_COMMENT_STRING(this, CommentForOffHeapTrampoline("call", builtin));
  // Use ip directly instead of using UseScratchRegisterScope, as we do not
  // preserve scratch registers across calls.
  switch (options().builtin_call_jump_mode) {
    case BuiltinCallJumpMode::kAbsolute: {
      Label skip;
      mov(ip, Operand(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET));
      if (cond != al) b(NegateCondition(cond), &skip);
      Call(ip);
      bind(&skip);
      break;
    }
    case BuiltinCallJumpMode::kPCRelative:
      UNREACHABLE();
    case BuiltinCallJumpMode::kIndirect: {
      Label skip;
      LoadU64(ip, EntryFromBuiltinAsOperand(builtin));
      if (cond != al) b(NegateCondition(cond), &skip);
      Call(ip);
      bind(&skip);
      break;
    }
    case BuiltinCallJumpMode::kForMksnapshot: {
      if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
        Handle<Code> code = isolate()->builtins()->code_handle(builtin);
        int32_t code_target_index = AddCodeTarget(code);
        Call(static_cast<Address>(code_target_index), RelocInfo::CODE_TARGET,
             cond);
      } else {
        Label skip;
        LoadU64(ip, EntryFromBuiltinAsOperand(builtin), r0);
        if (cond != al) b(NegateCondition(cond), &skip);
        Call(ip);
        bind(&skip);
      }
      break;
    }
  }
}

void MacroAssembler::TailCallBuiltin(Builtin builtin, Condition cond,
                                     CRegister cr) {
  ASM_CODE_COMMENT_STRING(this,
                          CommentForOffHeapTrampoline("tail call", builtin));
  // Use ip directly instead of using UseScratchRegisterScope, as we do not
  // preserve scratch registers across calls.
  switch (options().builtin_call_jump_mode) {
    case BuiltinCallJumpMode::kAbsolute: {
      Label skip;
      mov(ip, Operand(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET));
      if (cond != al) b(NegateCondition(cond), &skip, cr);
      Jump(ip);
      bind(&skip);
      break;
    }
    case BuiltinCallJumpMode::kPCRelative:
      UNREACHABLE();
    case BuiltinCallJumpMode::kIndirect: {
      Label skip;
      LoadU64(ip, EntryFromBuiltinAsOperand(builtin));
      if (cond != al) b(NegateCondition(cond), &skip, cr);
      Jump(ip);
      bind(&skip);
      break;
    }
    case BuiltinCallJumpMode::kForMksnapshot: {
      if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
        Handle<Code> code = isolate()->builtins()->code_handle(builtin);
        int32_t code_target_index = AddCodeTarget(code);
        Jump(static_cast<intptr_t>(code_target_index), RelocInfo::CODE_TARGET,
             cond, cr);
      } else {
        Label skip;
        LoadU64(ip, EntryFromBuiltinAsOperand(builtin), r0);
        if (cond != al) b(NegateCondition(cond), &skip, cr);
        Jump(ip);
        bind(&skip);
      }
      break;
    }
  }
}

void MacroAssembler::Drop(int count) {
  if (count > 0) {
    AddS64(sp, sp, Operand(count * kSystemPointerSize), r0);
  }
}

void MacroAssembler::Drop(Register count, Register scratch) {
  ShiftLeftU64(scratch, count, Operand(kSystemPointerSizeLog2));
  add(sp, sp, scratch);
}

void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code,
                                                       Register scratch1,
                                                       Register scratch2) {
  LoadU32(scratch1, FieldMemOperand(code, Code::kFlagsOffset), scratch2);
  TestBit(scratch1, Code::kMarkedForDeoptimizationBit, scratch2);
}

Operand MacroAssembler::ClearedValue() const {
  return Operand(static_cast<int32_t>(i::ClearedValue(isolate()).ptr()));
}

void MacroAssembler::Call(Label* target) { b(target, SetLK); }

void MacroAssembler::Push(Handle<HeapObject> handle) {
  mov(r0, Operand(handle));
  push(r0);
}

void MacroAssembler::Push(Tagged<Smi> smi) {
  mov(r0, Operand(smi));
  push(r0);
}

void MacroAssembler::PushArray(Register array, Register size, Register scratch,
                               Register scratch2, PushArrayOrder order) {
  Label loop, done;

  if (order == kNormal) {
    cmpi(size, Operand::Zero());
    beq(&done);
    ShiftLeftU64(scratch, size, Operand(kSystemPointerSizeLog2));
    add(scratch, array, scratch);
    mtctr(size);

    bind(&loop);
    LoadU64WithUpdate(scratch2, MemOperand(scratch, -kSystemPointerSize));
    StoreU64WithUpdate(scratch2, MemOperand(sp, -kSystemPointerSize));
    bdnz(&loop);

    bind(&done);
  } else {
    cmpi(size, Operand::Zero());
    beq(&done);

    mtctr(size);
    subi(scratch, array, Operand(kSystemPointerSize));

    bind(&loop);
    LoadU64WithUpdate(scratch2, MemOperand(scratch, kSystemPointerSize));
    StoreU64WithUpdate(scratch2, MemOperand(sp, -kSystemPointerSize));
    bdnz(&loop);
    bind(&done);
  }
}

void MacroAssembler::Move(Register dst, Handle<HeapObject> value,
                          RelocInfo::Mode rmode) {
  // TODO(jgruber,v8:8887): Also consider a root-relative load when generating
  // non-isolate-independent code. In many cases it might be cheaper than
  // embedding the relocatable value.
  if (root_array_available_ && options().isolate_independent_code) {
    IndirectLoadConstant(dst, value);
    return;
  } else if (RelocInfo::IsCompressedEmbeddedObject(rmode)) {
    EmbeddedObjectIndex index = AddEmbeddedObject(value);
    DCHECK(is_uint32(index));
    mov(dst, Operand(static_cast<int>(index), rmode));
  } else {
    DCHECK(RelocInfo::IsFullEmbeddedObject(rmode));
    mov(dst, Operand(value.address(), rmode));
  }
}

void MacroAssembler::Move(Register dst, ExternalReference reference) {
  if (root_array_available()) {
    if (reference.IsIsolateFieldId()) {
      AddS64(dst, kRootRegister,
             Operand(reference.offset_from_root_register()));
      return;
    }
    if (options().isolate_independent_code) {
      IndirectLoadExternalReference(dst, reference);
      return;
    }
  }

  // External references should not get created with IDs if
  // `!root_array_available()`.
  CHECK(!reference.IsIsolateFieldId());
  mov(dst, Operand(reference));
}

void MacroAssembler::LoadIsolateField(Register dst, IsolateFieldId id) {
  Move(dst, ExternalReference::Create(id));
}

void MacroAssembler::Move(Register dst, Register src, Condition cond) {
  DCHECK(cond == al);
  if (dst != src) {
    mr(dst, src);
  }
}

void MacroAssembler::Move(DoubleRegister dst, DoubleRegister src) {
  if (dst != src) {
    fmr(dst, src);
  }
}

void MacroAssembler::MultiPush(RegList regs, Register location) {
  int16_t num_to_push = regs.Count();
  int16_t stack_offset = num_to_push * kSystemPointerSize;

  subi(location, location, Operand(stack_offset));
  for (int16_t i = Register::kNumRegisters - 1; i >= 0; i--) {
    if ((regs.bits() & (1 << i)) != 0) {
      stack_offset -= kSystemPointerSize;
      StoreU64(ToRegister(i), MemOperand(location, stack_offset));
    }
  }
}

void MacroAssembler::MultiPop(RegList regs, Register location) {
  int16_t stack_offset = 0;

  for (int16_t i = 0; i < Register::kNumRegisters; i++) {
    if ((regs.bits() & (1 << i)) != 0) {
      LoadU64(ToRegister(i), MemOperand(location, stack_offset));
      stack_offset += kSystemPointerSize;
    }
  }
  addi(location, location, Operand(stack_offset));
}

void MacroAssembler::MultiPushDoubles(DoubleRegList dregs, Register location) {
  int16_t num_to_push = dregs.Count();
  int16_t stack_offset = num_to_push * kDoubleSize;

  subi(location, location, Operand(stack_offset));
  for (int16_t i = DoubleRegister::kNumRegisters - 1; i >= 0; i--) {
    if ((dregs.bits() & (1 << i)) != 0) {
      DoubleRegister dreg = DoubleRegister::from_code(i);
      stack_offset -= kDoubleSize;
      stfd(dreg, MemOperand(location, stack_offset));
    }
  }
}

void MacroAssembler::MultiPushV128(Simd128RegList simd_regs, Register scratch,
                                   Register location) {
  int16_t num_to_push = simd_regs.Count();
  int16_t stack_offset = num_to_push * kSimd128Size;

  subi(location, location, Operand(stack_offset));
  for (int16_t i = Simd128Register::kNumRegisters - 1; i >= 0; i--) {
    if ((simd_regs.bits() & (1 << i)) != 0) {
      Simd128Register simd_reg = Simd128Register::from_code(i);
      stack_offset -= kSimd128Size;
      StoreSimd128(simd_reg, MemOperand(location, stack_offset), scratch);
    }
  }
}

void MacroAssembler::MultiPopDoubles(DoubleRegList dregs, Register location) {
  int16_t stack_offset = 0;

  for (int16_t i = 0; i < DoubleRegister::kNumRegisters; i++) {
    if ((dregs.bits() & (1 << i)) != 0) {
      DoubleRegister dreg = DoubleRegister::from_code(i);
      lfd(dreg, MemOperand(location, stack_offset));
      stack_offset += kDoubleSize;
    }
  }
  addi(location, location, Operand(stack_offset));
}

void MacroAssembler::MultiPopV128(Simd128RegList simd_regs, Register scratch,
                                  Register location) {
  int16_t stack_offset = 0;

  for (int16_t i = 0; i < Simd128Register::kNumRegisters; i++) {
    if ((simd_regs.bits() & (1 << i)) != 0) {
      Simd128Register simd_reg = Simd128Register::from_code(i);
      LoadSimd128(simd_reg, MemOperand(location, stack_offset), scratch);
      stack_offset += kSimd128Size;
    }
  }
  addi(location, location, Operand(stack_offset));
}

void MacroAssembler::MultiPushF64AndV128(DoubleRegList dregs,
                                         Simd128RegList simd_regs,
                                         Register scratch1, Register scratch2,
                                         Register location) {
  MultiPushDoubles(dregs);
#if V8_ENABLE_WEBASSEMBLY
  bool generating_bultins =
      isolate() && isolate()->IsGeneratingEmbeddedBuiltins();
  if (generating_bultins) {
    // V8 uses the same set of fp param registers as Simd param registers.
    // As these registers are two different sets on ppc we must make
    // sure to also save them when Simd is enabled.
    // Check the comments under crrev.com/c/2645694 for more details.
    Label push_empty_simd, simd_pushed;
    Move(scratch1, ExternalReference::supports_wasm_simd_128_address());
    LoadU8(scratch1, MemOperand(scratch1), scratch2);
    cmpi(scratch1, Operand::Zero());  // If > 0 then simd is available.
    ble(&push_empty_simd);
    MultiPushV128(simd_regs, scratch1);
    b(&simd_pushed);
    bind(&push_empty_simd);
    // We still need to allocate empty space on the stack even if we
    // are not pushing Simd registers (see kFixedFrameSizeFromFp).
    addi(sp, sp,
         Operand(-static_cast<int8_t>(simd_regs.Count()) * kSimd128Size));
    bind(&simd_pushed);
  } else {
    if (CpuFeatures::SupportsWasmSimd128()) {
      MultiPushV128(simd_regs, scratch1);
    } else {
      addi(sp, sp,
           Operand(-static_cast<int8_t>(simd_regs.Count()) * kSimd128Size));
    }
  }
#endif
}

void MacroAssembler::MultiPopF64AndV128(DoubleRegList dregs,
                                        Simd128RegList simd_regs,
                                        Register scratch1, Register scratch2,
                                        Register location) {
#if V8_ENABLE_WEBASSEMBLY
  bool generating_bultins =
      isolate() && isolate()->IsGeneratingEmbeddedBuiltins();
  if (generating_bultins) {
    Label pop_empty_simd, simd_popped;
    Move(scratch1, ExternalReference::supports_wasm_simd_128_address());
    LoadU8(scratch1, MemOperand(scratch1), scratch2);
    cmpi(scratch1, Operand::Zero());  // If > 0 then simd is available.
    ble(&pop_empty_simd);
    MultiPopV128(simd_regs, scratch1);
    b(&simd_popped);
    bind(&pop_empty_simd);
    addi(sp, sp,
         Operand(static_cast<int8_t>(simd_regs.Count()) * kSimd128Size));
    bind(&simd_popped);
  } else {
    if (CpuFeatures::SupportsWasmSimd128()) {
      MultiPopV128(simd_regs, scratch1);
    } else {
      addi(sp, sp,
           Operand(static_cast<int8_t>(simd_regs.Count()) * kSimd128Size));
    }
  }
#endif
  MultiPopDoubles(dregs);
}

void MacroAssembler::LoadTaggedRoot(Register destination, RootIndex index) {
  ASM_CODE_COMMENT(this);
  if (CanBeImmediate(index)) {
    mov(destination, Operand(ReadOnlyRootPtr(index), RelocInfo::Mode::NO_INFO));
    return;
  }
  LoadRoot(destination, index);
}

void MacroAssembler::LoadRoot(Register destination, RootIndex index,
                              Condition cond) {
  DCHECK(cond == al);
  if (CanBeImmediate(index)) {
    DecompressTagged(destination, ReadOnlyRootPtr(index));
    return;
  }
  LoadU64(destination,
          MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)), r0);
}

void MacroAssembler::LoadTaggedField(const Register& destination,
                                     const MemOperand& field_operand,
                                     const Register& scratch) {
  if (COMPRESS_POINTERS_BOOL) {
    DecompressTagged(destination, field_operand);
  } else {
    LoadU64(destination, field_operand, scratch);
  }
}

void MacroAssembler::SmiUntag(Register dst, const MemOperand& src, RCBit rc,
                              Register scratch) {
  if (SmiValuesAre31Bits()) {
    LoadU32(dst, src, scratch);
  } else {
    LoadU64(dst, src, scratch);
  }

  SmiUntag(dst, rc);
}

void MacroAssembler::StoreTaggedField(const Register& value,
                                      const MemOperand& dst_field_operand,
                                      const Register& scratch) {
  if (COMPRESS_POINTERS_BOOL) {
    RecordComment("[ StoreTagged");
    StoreU32(value, dst_field_operand, scratch);
    RecordComment("]");
  } else {
    StoreU64(value, dst_field_operand, scratch);
  }
}

void MacroAssembler::DecompressTaggedSigned(Register destination,
                                            Register src) {
  RecordComment("[ DecompressTaggedSigned");
  ZeroExtWord32(destination, src);
  RecordComment("]");
}

void MacroAssembler::DecompressTaggedSigned(Register destination,
                                            MemOperand field_operand) {
  RecordComment("[ DecompressTaggedSigned");
  LoadU32(destination, field_operand, r0);
  RecordComment("]");
}

void MacroAssembler::DecompressTagged(Register destination, Register source) {
  RecordComment("[ DecompressTagged");
  ZeroExtWord32(destination, source);
  add(destination, destination, kPtrComprCageBaseRegister);
  RecordComment("]");
}

void MacroAssembler::DecompressTagged(Register destination,
                                      MemOperand field_operand) {
  RecordComment("[ DecompressTagged");
  LoadU32(destination, field_operand, r0);
  add(destination, destination, kPtrComprCageBaseRegister);
  RecordComment("]");
}

void MacroAssembler::DecompressTagged(const Register& destination,
                                      Tagged_t immediate) {
  ASM_CODE_COMMENT(this);
  AddS64(destination, kPtrComprCageBaseRegister,
         Operand(immediate, RelocInfo::Mode::NO_INFO));
}

void MacroAssembler::LoadTaggedSignedField(Register destination,
                                           MemOperand field_operand,
                                           Register scratch) {
  if (COMPRESS_POINTERS_BOOL) {
    DecompressTaggedSigned(destination, field_operand);
  } else {
    LoadU64(destination, field_operand, scratch);
  }
}

void MacroAssembler::RecordWriteField(Register object, int offset,
                                      Register value, Register slot_address,
                                      LinkRegisterStatus lr_status,
                                      SaveFPRegsMode save_fp,
                                      SmiCheck smi_check, SlotDescriptor slot) {
  // First, check if a write barrier is even needed. The tests below
  // catch stores of Smis.
  Label done;

  // Skip barrier if writing a smi.
  if (smi_check == SmiCheck::kInline) {
    JumpIfSmi(value, &done);
  }

  // Although the object register is tagged, the offset is relative to the start
  // of the object, so so offset must be a multiple of kSystemPointerSize.
  DCHECK(IsAligned(offset, kTaggedSize));

  AddS64(slot_address, object, Operand(offset - kHeapObjectTag), r0);
  if (v8_flags.debug_code) {
    Label ok;
    andi(r0, slot_address, Operand(kTaggedSize - 1));
    beq(&ok, cr0);
    stop();
    bind(&ok);
  }

  RecordWrite(object, slot_address, value, lr_status, save_fp, SmiCheck::kOmit,
              slot);

  bind(&done);

  // Clobber clobbered input registers when running with the debug-code flag
  // turned on to provoke errors.
  if (v8_flags.debug_code) {
    mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 4)));
    mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 8)));
  }
}

void MacroAssembler::DecodeSandboxedPointer(Register value) {
  ASM_CODE_COMMENT(this);
#ifdef V8_ENABLE_SANDBOX
  ShiftRightU64(value, value, Operand(kSandboxedPointerShift));
  AddS64(value, value, kPtrComprCageBaseRegister);
#else
  UNREACHABLE();
#endif
}

void MacroAssembler::LoadSandboxedPointerField(Register destination,
                                               const MemOperand& field_operand,
                                               Register scratch) {
  ASM_CODE_COMMENT(this);
#ifdef V8_ENABLE_SANDBOX
  LoadU64(destination, field_operand, scratch);
  DecodeSandboxedPointer(destination);
#else
  UNREACHABLE();
#endif
}

void MacroAssembler::StoreSandboxedPointerField(
    Register value, const MemOperand& dst_field_operand, Register scratch) {
  ASM_CODE_COMMENT(this);
#ifdef V8_ENABLE_SANDBOX
  UseScratchRegisterScope temps(this);
  Register scratch2 = temps.Acquire();
  DCHECK(!AreAliased(scratch, scratch2));
  SubS64(scratch2, value, kPtrComprCageBaseRegister);
  ShiftLeftU64(scratch2, scratch2, Operand(kSandboxedPointerShift));
  StoreU64(scratch2, dst_field_operand, scratch);
#else
  UNREACHABLE();
#endif
}

void MacroAssembler::LoadExternalPointerField(Register destination,
                                              MemOperand field_operand,
                                              ExternalPointerTag tag,
                                              Register isolate_root,
                                              Register scratch) {
  DCHECK(!AreAliased(destination, isolate_root));
  ASM_CODE_COMMENT(this);
#ifdef V8_ENABLE_SANDBOX
  DCHECK_NE(tag, kExternalPointerNullTag);
  DCHECK(!IsSharedExternalPointerType(tag));
  UseScratchRegisterScope temps(this);
  Register external_table = temps.Acquire();
  DCHECK(!AreAliased(scratch, external_table));
  if (isolate_root == no_reg) {
    DCHECK(root_array_available_);
    isolate_root = kRootRegister;
  }
  LoadU64(external_table,
          MemOperand(isolate_root,
                     IsolateData::external_pointer_table_offset() +
                         Internals::kExternalPointerTableBasePointerOffset),
          scratch);
  LoadU32(destination, field_operand, scratch);
  ShiftRightU64(destination, destination, Operand(kExternalPointerIndexShift));
  ShiftLeftU64(destination, destination,
               Operand(kExternalPointerTableEntrySizeLog2));
  LoadU64(destination, MemOperand(external_table, destination), scratch);
  mov(scratch, Operand(~tag));
  AndU64(destination, destination, scratch);
#else
  LoadU64(destination, field_operand, scratch);
#endif  // V8_ENABLE_SANDBOX
}

void MacroAssembler::LoadTrustedPointerField(Register destination,
                                             MemOperand field_operand,
                                             IndirectPointerTag tag,
                                             Register scratch) {
#ifdef V8_ENABLE_SANDBOX
  LoadIndirectPointerField(destination, field_operand, tag, scratch);
#else
  LoadTaggedField(destination, field_operand, scratch);
#endif
}

void MacroAssembler::StoreTrustedPointerField(Register value,
                                              MemOperand dst_field_operand,
                                              Register scratch) {
#ifdef V8_ENABLE_SANDBOX
  StoreIndirectPointerField(value, dst_field_operand, scratch);
#else
  StoreTaggedField(value, dst_field_operand, scratch);
#endif
}

void MacroAssembler::JumpIfJSAnyIsNotPrimitive(Register heap_object,
                                               Register scratch, Label* target,
                                               Label::Distance distance,
                                               Condition cc) {
  CHECK(cc == Condition::kUnsignedLessThan ||
        cc == Condition::kUnsignedGreaterThanEqual);
  if (V8_STATIC_ROOTS_BOOL) {
#ifdef DEBUG
    Label ok;
    LoadMap(scratch, heap_object);
    CompareInstanceTypeRange(scratch, scratch, r0, FIRST_JS_RECEIVER_TYPE,
                             LAST_JS_RECEIVER_TYPE);
    ble(&ok);
    LoadMap(scratch, heap_object);
    CompareInstanceTypeRange(scratch, scratch, r0,
                             FIRST_PRIMITIVE_HEAP_OBJECT_TYPE,
                             LAST_PRIMITIVE_HEAP_OBJECT_TYPE);
    ble(&ok);
    Abort(AbortReason::kInvalidReceiver);
    bind(&ok);
#endif  // DEBUG

    // All primitive object's maps are allocated at the start of the read only
    // heap. Thus JS_RECEIVER's must have maps with larger (compressed)
    // addresses.
    UseScratchRegisterScope temps(this);
    Register scratch2 = temps.Acquire();
    DCHECK(!AreAliased(scratch2, scratch));
    LoadCompressedMap(scratch, heap_object, scratch2);
    mov(scratch2, Operand(InstanceTypeChecker::kNonJsReceiverMapLimit));
    CompareTagged(scratch, scratch2);
  } else {
    static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
    CompareObjectType(heap_object, scratch, scratch, FIRST_JS_RECEIVER_TYPE);
  }
  b(to_condition(cc), target);
}

void MacroAssembler::LoadIndirectPointerField(Register destination,
                                              MemOperand field_operand,
                                              IndirectPointerTag tag,
                                              Register scratch) {
#ifdef V8_ENABLE_SANDBOX
  ASM_CODE_COMMENT(this);
  Register handle = scratch;
  DCHECK(!AreAliased(handle, destination));
  LoadU32(handle, field_operand, scratch);
  ResolveIndirectPointerHandle(destination, handle, tag, scratch);
#else
  UNREACHABLE();
#endif  // V8_ENABLE_SANDBOX
}

void MacroAssembler::StoreIndirectPointerField(Register value,
                                               MemOperand dst_field_operand,
                                               Register scratch) {
#ifdef V8_ENABLE_SANDBOX
  ASM_CODE_COMMENT(this);
  UseScratchRegisterScope temps(this);
  Register scratch2 = temps.Acquire();
  DCHECK(!AreAliased(scratch, scratch2));
  LoadU32(
      scratch2,
      FieldMemOperand(value, ExposedTrustedObject::kSelfIndirectPointerOffset),
      scratch);
  StoreU32(scratch2, dst_field_operand, scratch);
#else
  UNREACHABLE();
#endif  // V8_ENABLE_SANDBOX
}

#ifdef V8_ENABLE_SANDBOX
void MacroAssembler::ResolveIndirectPointerHandle(Register destination,
                                                  Register handle,
                                                  IndirectPointerTag tag,
                                                  Register scratch) {
  // Pointer resolution will fail in several paths if handle == ra
  DCHECK(!AreAliased(handle, r0));

  // The tag implies which pointer table to use.
  if (tag == kUnknownIndirectPointerTag) {
    // In this case we have to rely on the handle marking to determine which
    // pointer table to use.
    Label is_trusted_pointer_handle, done;
    mov(scratch, Operand(kCodePointerHandleMarker));
    AndU64(scratch, handle, scratch, SetRC);
    beq(&is_trusted_pointer_handle, cr0);
    ResolveCodePointerHandle(destination, handle, scratch);
    b(&done);
    bind(&is_trusted_pointer_handle);
    ResolveTrustedPointerHandle(destination, handle, kUnknownIndirectPointerTag,
                                scratch);
    bind(&done);
  } else if (tag == kCodeIndirectPointerTag) {
    ResolveCodePointerHandle(destination, handle, scratch);
  } else {
    ResolveTrustedPointerHandle(destination, handle, tag, scratch);
  }
}

void MacroAssembler::ResolveTrustedPointerHandle(Register destination,
                                                 Register handle,
                                                 IndirectPointerTag tag,
                                                 Register scratch) {
  DCHECK_NE(tag, kCodeIndirectPointerTag);
  DCHECK(!AreAliased(handle, destination));

  CHECK(root_array_available_);
  Register table = destination;
  Move(table, ExternalReference::trusted_pointer_table_base_address(isolate()));
  ShiftRightU64(handle, handle, Operand(kTrustedPointerHandleShift));
  ShiftLeftU64(handle, handle, Operand(kTrustedPointerTableEntrySizeLog2));
  LoadU64(destination, MemOperand(table, handle), scratch);
  // The LSB is used as marking bit by the trusted pointer table, so here we
  // have to set it using a bitwise OR as it may or may not be set.
  mov(handle, Operand(kHeapObjectTag));
  OrU64(destination, destination, handle);
}

void MacroAssembler::ResolveCodePointerHandle(Register destination,
                                              Register handle,
                                              Register scratch) {
  DCHECK(!AreAliased(handle, destination));

  Register table = destination;
  Move(table, ExternalReference::code_pointer_table_address());
  ShiftRightU64(handle, handle, Operand(kCodePointerHandleShift));
  ShiftLeftU64(handle, handle, Operand(kCodePointerTableEntrySizeLog2));
  AddS64(handle, table, handle);
  LoadU64(destination,
          MemOperand(handle, kCodePointerTableEntryCodeObjectOffset), scratch);
  // The LSB is used as marking bit by the code pointer table, so here we have
  // to set it using a bitwise OR as it may or may not be set.
  mov(handle, Operand(kHeapObjectTag));
  OrU64(destination, destination, handle);
}

void MacroAssembler::LoadCodeEntrypointViaCodePointer(Register destination,
                                                      MemOperand field_operand,
                                                      Register scratch) {
  ASM_CODE_COMMENT(this);

  // Due to register pressure, table is also used as a scratch register
  DCHECK(destination != r0);
  Register table = scratch;
  LoadU32(destination, field_operand, scratch);
  Move(table, ExternalReference::code_pointer_table_address());
  // TODO(tpearson): can the offset computation be done more efficiently?
  ShiftRightU64(destination, destination, Operand(kCodePointerHandleShift));
  ShiftLeftU64(destination, destination,
               Operand(kCodePointerTableEntrySizeLog2));
  LoadU64(destination, MemOperand(destination, table));
}
#endif  // V8_ENABLE_SANDBOX

void MacroAssembler::MaybeSaveRegisters(RegList registers) {
  if (registers.is_empty()) return;
  MultiPush(registers);
}

void MacroAssembler::MaybeRestoreRegisters(RegList registers) {
  if (registers.is_empty()) return;
  MultiPop(registers);
}

void MacroAssembler::CallEphemeronKeyBarrier(Register object,
                                             Register slot_address,
                                             SaveFPRegsMode fp_mode) {
  DCHECK(!AreAliased(object, slot_address));
  RegList registers =
      WriteBarrierDescriptor::ComputeSavedRegisters(object, slot_address);
  MaybeSaveRegisters(registers);

  Register object_parameter = WriteBarrierDescriptor::ObjectRegister();
  Register slot_address_parameter =
      WriteBarrierDescriptor::SlotAddressRegister();

  // TODO(tpearson): The following is equivalent to
  // MovePair(slot_address_parameter, slot_address, object_parameter, object);
  // Implement with MoveObjectAndSlot()
  push(object);
  push(slot_address);
  pop(slot_address_parameter);
  pop(object_parameter);

  CallBuiltin(Builtins::EphemeronKeyBarrier(fp_mode));
  MaybeRestoreRegisters(registers);
}

void MacroAssembler::CallIndirectPointerBarrier(Register object,
                                                Register slot_address,
                                                SaveFPRegsMode fp_mode,
                                                IndirectPointerTag tag) {
  ASM_CODE_COMMENT(this);
  DCHECK(!AreAliased(object, slot_address));
  RegList registers =
      IndirectPointerWriteBarrierDescriptor::ComputeSavedRegisters(
          object, slot_address);
  MaybeSaveRegisters(registers);

  Register object_parameter =
      IndirectPointerWriteBarrierDescriptor::ObjectRegister();
  Register slot_address_parameter =
      IndirectPointerWriteBarrierDescriptor::SlotAddressRegister();
  Register tag_parameter =
      IndirectPointerWriteBarrierDescriptor::IndirectPointerTagRegister();
  DCHECK(!AreAliased(object_parameter, slot_address_parameter, tag_parameter));

  // TODO(tpearson): The following is equivalent to
  // MovePair(slot_address_parameter, slot_address, object_parameter, object);
  // Implement with MoveObjectAndSlot()
  push(object);
  push(slot_address);
  pop(slot_address_parameter);
  pop(object_parameter);

  mov(tag_parameter, Operand(tag));

  CallBuiltin(Builtins::IndirectPointerBarrier(fp_mode));
  MaybeRestoreRegisters(registers);
}

void MacroAssembler::CallRecordWriteStubSaveRegisters(Register object,
                                                      Register slot_address,
                                                      SaveFPRegsMode fp_mode,
                                                      StubCallMode mode) {
  DCHECK(!AreAliased(object, slot_address));
  RegList registers =
      WriteBarrierDescriptor::ComputeSavedRegisters(object, slot_address);
  MaybeSaveRegisters(registers);

  Register object_parameter = WriteBarrierDescriptor::ObjectRegister();
  Register slot_address_parameter =
      WriteBarrierDescriptor::SlotAddressRegister();

  push(object);
  push(slot_address);
  pop(slot_address_parameter);
  pop(object_parameter);

  CallRecordWriteStub(object_parameter, slot_address_parameter, fp_mode, mode);

  MaybeRestoreRegisters(registers);
}

void MacroAssembler::CallRecordWriteStub(Register object, Register slot_address,
                                         SaveFPRegsMode fp_mode,
                                         StubCallMode mode) {
  // Use CallRecordWriteStubSaveRegisters if the object and slot registers
  // need to be caller saved.
  DCHECK_EQ(WriteBarrierDescriptor::ObjectRegister(), object);
  DCHECK_EQ(WriteBarrierDescriptor::SlotAddressRegister(), slot_address);
#if V8_ENABLE_WEBASSEMBLY
  if (mode == StubCallMode::kCallWasmRuntimeStub) {
    // Use {near_call} for direct Wasm call within a module.
    auto wasm_target =
        static_cast<Address>(wasm::WasmCode::GetRecordWriteBuiltin(fp_mode));
    Call(wasm_target, RelocInfo::WASM_STUB_CALL);
#else
  if (false) {
#endif
  } else {
    CallBuiltin(Builtins::RecordWrite(fp_mode), al);
  }
}

// Will clobber 4 registers: object, address, scratch, ip.  The
// register 'object' contains a heap object pointer.  The heap object
// tag is shifted away.
void MacroAssembler::RecordWrite(Register object, Register slot_address,
                                 Register value, LinkRegisterStatus lr_status,
                                 SaveFPRegsMode fp_mode, SmiCheck smi_check,
                                 SlotDescriptor slot) {
  ASM_CODE_COMMENT(this);
  DCHECK(!AreAliased(object, value, slot_address));
  if (v8_flags.debug_code) {
    Register value_check = r0;
    // TODO(tpearson): Figure out why ScratchRegisterScope returns a
    // register that is aliased with one of our other in-use registers
    // For now, use r11 (kScratchReg in the code generator)
    Register scratch = r11;
    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
    DCHECK(!AreAliased(object, value, value_check, scratch));
    if (slot.contains_indirect_pointer()) {
      LoadIndirectPointerField(value_check, MemOperand(slot_address),
                               slot.indirect_pointer_tag(), scratch);
    } else {
      DCHECK(slot.contains_direct_pointer());
      LoadTaggedField(value_check, MemOperand(slot_address));
    }
    CmpS64(value_check, value);
    Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);
  }

  if (v8_flags.disable_write_barriers) {
    return;
  }

  // First, check if a write barrier is even needed. The tests below
  // catch stores of smis and stores into the young generation.
  Label done;

  if (smi_check == SmiCheck::kInline) {
    JumpIfSmi(value, &done);
  }

  CheckPageFlag(value,
                value,  // Used as scratch.
                MemoryChunk::kPointersToHereAreInterestingMask, eq, &done);
  CheckPageFlag(object,
                value,  // Used as scratch.
                MemoryChunk::kPointersFromHereAreInterestingMask, eq, &done);

  // Record the actual write.
  if (lr_status == kLRHasNotBeenSaved) {
    mflr(r0);
    push(r0);
  }
  if (slot.contains_direct_pointer()) {
    CallRecordWriteStubSaveRegisters(object, slot_address, fp_mode,
                                     StubCallMode::kCallBuiltinPointer);
  } else {
    DCHECK(slot.contains_indirect_pointer());
    CallIndirectPointerBarrier(object, slot_address, fp_mode,
                               slot.indirect_pointer_tag());
  }
  if (lr_status == kLRHasNotBeenSaved) {
    pop(r0);
    mtlr(r0);
  }

  if (v8_flags.debug_code) mov(slot_address, Operand(kZapValue));

  bind(&done);

  // Clobber clobbered registers when running with the debug-code flag
  // turned on to provoke errors.
  if (v8_flags.debug_code) {
    mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 12)));
    mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 16)));
  }
}

void MacroAssembler::PushCommonFrame(Register marker_reg) {
  int fp_delta = 0;
  mflr(r0);
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    if (marker_reg.is_valid()) {
      Push(r0, fp, kConstantPoolRegister, marker_reg);
      fp_delta = 2;
    } else {
      Push(r0, fp, kConstantPoolRegister);
      fp_delta = 1;
    }
  } else {
    if (marker_reg.is_valid()) {
      Push(r0, fp, marker_reg);
      fp_delta = 1;
    } else {
      Push(r0, fp);
      fp_delta = 0;
    }
  }
  addi(fp, sp, Operand(fp_delta * kSystemPointerSize));
}

void MacroAssembler::PushStandardFrame(Register function_reg) {
  int fp_delta = 0;
  mflr(r0);
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    if (function_reg.is_valid()) {
      Push(r0, fp, kConstantPoolRegister, cp, function_reg);
      fp_delta = 3;
    } else {
      Push(r0, fp, kConstantPoolRegister, cp);
      fp_delta = 2;
    }
  } else {
    if (function_reg.is_valid()) {
      Push(r0, fp, cp, function_reg);
      fp_delta = 2;
    } else {
      Push(r0, fp, cp);
      fp_delta = 1;
    }
  }
  addi(fp, sp, Operand(fp_delta * kSystemPointerSize));
  Push(kJavaScriptCallArgCountRegister);
}

void MacroAssembler::RestoreFrameStateForTailCall() {
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    LoadU64(kConstantPoolRegister,
            MemOperand(fp, StandardFrameConstants::kConstantPoolOffset));
    set_constant_pool_available(false);
  }
  LoadU64(r0, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
  LoadU64(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
  mtlr(r0);
}

void MacroAssembler::CanonicalizeNaN(const DoubleRegister dst,
                                     const DoubleRegister src) {
  // Turn potential sNaN into qNaN.
  fsub(dst, src, kDoubleRegZero);
}

void MacroAssembler::ConvertIntToDouble(Register src, DoubleRegister dst) {
  MovIntToDouble(dst, src, r0);
  fcfid(dst, dst);
}

void MacroAssembler::ConvertUnsignedIntToDouble(Register src,
                                                DoubleRegister dst) {
  MovUnsignedIntToDouble(dst, src, r0);
  fcfid(dst, dst);
}

void MacroAssembler::ConvertIntToFloat(Register src, DoubleRegister dst) {
  MovIntToDouble(dst, src, r0);
  fcfids(dst, dst);
}

void MacroAssembler::ConvertUnsignedIntToFloat(Register src,
                                               DoubleRegister dst) {
  MovUnsignedIntToDouble(dst, src, r0);
  fcfids(dst, dst);
}

#if V8_TARGET_ARCH_PPC64
void MacroAssembler::ConvertInt64ToDouble(Register src,
                                          DoubleRegister double_dst) {
  MovInt64ToDouble(double_dst, src);
  fcfid(double_dst, double_dst);
}

void MacroAssembler::ConvertUnsignedInt64ToFloat(Register src,
                                                 DoubleRegister double_dst) {
  MovInt64ToDouble(double_dst, src);
  fcfidus(double_dst, double_dst);
}

void MacroAssembler::ConvertUnsignedInt64ToDouble(Register src,
                                                  DoubleRegister double_dst) {
  MovInt64ToDouble(double_dst, src);
  fcfidu(double_dst, double_dst);
}

void MacroAssembler::ConvertInt64ToFloat(Register src,
                                         DoubleRegister double_dst) {
  MovInt64ToDouble(double_dst, src);
  fcfids(double_dst, double_dst);
}
#endif

void MacroAssembler::ConvertDoubleToInt64(const DoubleRegister double_input,
#if !V8_TARGET_ARCH_PPC64
                                          const Register dst_hi,
#endif
                                          const Register dst,
                                          const DoubleRegister double_dst,
                                          FPRoundingMode rounding_mode) {
  if (rounding_mode == kRoundToZero) {
    fctidz(double_dst, double_input);
  } else {
    SetRoundingMode(rounding_mode);
    fctid(double_dst, double_input);
    ResetRoundingMode();
  }

  MovDoubleToInt64(
#if !V8_TARGET_ARCH_PPC64
      dst_hi,
#endif
      dst, double_dst);
}

#if V8_TARGET_ARCH_PPC64
void MacroAssembler::ConvertDoubleToUnsignedInt64(
    const DoubleRegister double_input, const Register dst,
    const DoubleRegister double_dst, FPRoundingMode rounding_mode) {
  if (rounding_mode == kRoundToZero) {
    fctiduz(double_dst, double_input);
  } else {
    SetRoundingMode(rounding_mode);
    fctidu(double_dst, double_input);
    ResetRoundingMode();
  }

  MovDoubleToInt64(dst, double_dst);
}
#endif

#if !V8_TARGET_ARCH_PPC64
void MacroAssembler::ShiftLeftPair(Register dst_low, Register dst_high,
                                   Register src_low, Register src_high,
                                   Register scratch, Register shift) {
  DCHECK(!AreAliased(dst_low, src_high));
  DCHECK(!AreAliased(dst_high, src_low));
  DCHECK(!AreAliased(dst_low, dst_high, shift));
  Label less_than_32;
  Label done;
  cmpi(shift, Operand(32));
  blt(&less_than_32);
  // If shift >= 32
  andi(scratch, shift, Operand(0x1F));
  ShiftLeftU32(dst_high, src_low, scratch);
  li(dst_low, Operand::Zero());
  b(&done);
  bind(&less_than_32);
  // If shift < 32
  subfic(scratch, shift, Operand(32));
  ShiftLeftU32(dst_high, src_high, shift);
  srw(scratch, src_low, scratch);
  orx(dst_high, dst_high, scratch);
  ShiftLeftU32(dst_low, src_low, shift);
  bind(&done);
}

void MacroAssembler::ShiftLeftPair(Register dst_low, Register dst_high,
                                   Register src_low, Register src_high,
                                   uint32_t shift) {
  DCHECK(!AreAliased(dst_low, src_high));
  DCHECK(!AreAliased(dst_high, src_low));
  if (shift == 32) {
    Move(dst_high, src_low);
    li(dst_low, Operand::Zero());
  } else if (shift > 32) {
    shift &= 0x1F;
    ShiftLeftU32(dst_high, src_low, Operand(shift));
    li(dst_low, Operand::Zero());
  } else if (shift == 0) {
    Move(dst_low, src_low);
    Move(dst_high, src_high);
  } else {
    ShiftLeftU32(dst_high, src_high, Operand(shift));
    rlwimi(dst_high, src_low, shift, 32 - shift, 31);
    ShiftLeftU32(dst_low, src_low, Operand(shift));
  }
}

void MacroAssembler::ShiftRightPair(Register dst_low, Register dst_high,
                                    Register src_low, Register src_high,
                                    Register scratch, Register shift) {
  DCHECK(!AreAliased(dst_low, src_high));
  DCHECK(!AreAliased(dst_high, src_low));
  DCHECK(!AreAliased(dst_low, dst_high, shift));
  Label less_than_32;
  Label done;
  cmpi(shift, Operand(32));
  blt(&less_than_32);
  // If shift >= 32
  andi(scratch, shift, Operand(0x1F));
  srw(dst_low, src_high, scratch);
  li(dst_high, Operand::Zero());
  b(&done);
  bind(&less_than_32);
  // If shift < 32
  subfic(scratch, shift, Operand(32));
  srw(dst_low, src_low, shift);
  ShiftLeftU32(scratch, src_high, scratch);
  orx(dst_low, dst_low, scratch);
  srw(dst_high, src_high, shift);
  bind(&done);
}

void MacroAssembler::ShiftRightPair(Register dst_low, Register dst_high,
                                    Register src_low, Register src_high,
                                    uint32_t shift) {
  DCHECK(!AreAliased(dst_low, src_high));
  DCHECK(!AreAliased(dst_high, src_low));
  if (shift == 32) {
    Move(dst_low, src_high);
    li(dst_high, Operand::Zero());
  } else if (shift > 32) {
    shift &= 0x1F;
    srwi(dst_low, src_high, Operand(shift));
    li(dst_high, Operand::Zero());
  } else if (shift == 0) {
    Move(dst_low, src_low);
    Move(dst_high, src_high);
  } else {
    srwi(dst_low, src_low, Operand(shift));
    rlwimi(dst_low, src_high, 32 - shift, 0, shift - 1);
    srwi(dst_high, src_high, Operand(shift));
  }
}

void MacroAssembler::ShiftRightAlgPair(Register dst_low, Register dst_high,
                                       Register src_low, Register src_high,
                                       Register scratch, Register shift) {
  DCHECK(!AreAliased(dst_low, src_high, shift));
  DCHECK(!AreAliased(dst_high, src_low, shift));
  Label less_than_32;
  Label done;
  cmpi(shift, Operand(32));
  blt(&less_than_32);
  // If shift >= 32
  andi(scratch, shift, Operand(0x1F));
  sraw(dst_low, src_high, scratch);
  srawi(dst_high, src_high, 31);
  b(&done);
  bind(&less_than_32);
  // If shift < 32
  subfic(scratch, shift, Operand(32));
  srw(dst_low, src_low, shift);
  ShiftLeftU32(scratch, src_high, scratch);
  orx(dst_low, dst_low, scratch);
  sraw(dst_high, src_high, shift);
  bind(&done);
}

void MacroAssembler::ShiftRightAlgPair(Register dst_low, Register dst_high,
                                       Register src_low, Register src_high,
                                       uint32_t shift) {
  DCHECK(!AreAliased(dst_low, src_high));
  DCHECK(!AreAliased(dst_high, src_low));
  if (shift == 32) {
    Move(dst_low, src_high);
    srawi(dst_high, src_high, 31);
  } else if (shift > 32) {
    shift &= 0x1F;
    srawi(dst_low, src_high, shift);
    srawi(dst_high, src_high, 31);
  } else if (shift == 0) {
    Move(dst_low, src_low);
    Move(dst_high, src_high);
  } else {
    srwi(dst_low, src_low, Operand(shift));
    rlwimi(dst_low, src_high, 32 - shift, 0, shift - 1);
    srawi(dst_high, src_high, shift);
  }
}
#endif

void MacroAssembler::LoadConstantPoolPointerRegisterFromCodeTargetAddress(
    Register code_target_address, Register scratch1, Register scratch2) {
  // Builtins do not use the constant pool (see is_constant_pool_available).
  static_assert(InstructionStream::kOnHeapBodyIsContiguous);

#ifdef V8_ENABLE_SANDBOX
  LoadCodeEntrypointViaCodePointer(
      scratch2,
      FieldMemOperand(code_target_address, Code::kSelfIndirectPointerOffset),
      scratch1);
#else
  LoadU64(scratch2,
          FieldMemOperand(code_target_address, Code::kInstructionStartOffset),
          scratch1);
#endif
  LoadU32(scratch1,
          FieldMemOperand(code_target_address, Code::kInstructionSizeOffset),
          scratch1);
  add(scratch2, scratch1, scratch2);
  LoadU32(kConstantPoolRegister,
          FieldMemOperand(code_target_address, Code::kConstantPoolOffsetOffset),
          scratch1);
  add(kConstantPoolRegister, scratch2, kConstantPoolRegister);
}

void MacroAssembler::LoadPC(Register dst) {
  b(4, SetLK);
  mflr(dst);
}

void MacroAssembler::ComputeCodeStartAddress(Register dst) {
  mflr(r0);
  LoadPC(dst);
  subi(dst, dst, Operand(pc_offset() - kInstrSize));
  mtlr(r0);
}

void MacroAssembler::LoadConstantPoolPointerRegister() {
  //
  // Builtins do not use the constant pool (see is_constant_pool_available).
  static_assert(InstructionStream::kOnHeapBodyIsContiguous);

  LoadPC(kConstantPoolRegister);
  int32_t delta = -pc_offset() + 4;
  add_label_offset(kConstantPoolRegister, kConstantPoolRegister,
                   ConstantPoolPosition(), delta);
}

void MacroAssembler::StubPrologue(StackFrame::Type type) {
  {
    ConstantPoolUnavailableScope constant_pool_unavailable(this);
    mov(r11, Operand(StackFrame::TypeToMarker(type)));
    PushCommonFrame(r11);
  }
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    LoadConstantPoolPointerRegister();
    set_constant_pool_available(true);
  }
}

void MacroAssembler::Prologue() {
  PushStandardFrame(r4);
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    // base contains prologue address
    LoadConstantPoolPointerRegister();
    set_constant_pool_available(true);
  }
}

void MacroAssembler::DropArguments(Register count) {
  ShiftLeftU64(ip, count, Operand(kSystemPointerSizeLog2));
  add(sp, sp, ip);
}

void MacroAssembler::DropArgumentsAndPushNewReceiver(Register argc,
                                                     Register receiver) {
  DCHECK(!AreAliased(argc, receiver));
  DropArguments(argc);
  push(receiver);
}

void MacroAssembler::EnterFrame(StackFrame::Type type,
                                bool load_constant_pool_pointer_reg) {
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL && load_constant_pool_pointer_reg) {
    // Push type explicitly so we can leverage the constant pool.
    // This path cannot rely on ip containing code entry.
    PushCommonFrame();
    LoadConstantPoolPointerRegister();
    if (!StackFrame::IsJavaScript(type)) {
      mov(ip, Operand(StackFrame::TypeToMarker(type)));
      push(ip);
    }
  } else {
    Register scratch = no_reg;
    if (!StackFrame::IsJavaScript(type)) {
      scratch = ip;
      mov(scratch, Operand(StackFrame::TypeToMarker(type)));
    }
    PushCommonFrame(scratch);
  }
#if V8_ENABLE_WEBASSEMBLY
  if (type == StackFrame::WASM) Push(kWasmInstanceRegister);
#endif  // V8_ENABLE_WEBASSEMBLY
}

int MacroAssembler::LeaveFrame(StackFrame::Type type, int stack_adjustment) {
  ConstantPoolUnavailableScope constant_pool_unavailable(this);
  // r3: preserved
  // r4: preserved
  // r5: preserved

  // Drop the execution stack down to the frame pointer and restore
  // the caller's state.
  int frame_ends;
  LoadU64(r0, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
  LoadU64(ip, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    LoadU64(kConstantPoolRegister,
            MemOperand(fp, StandardFrameConstants::kConstantPoolOffset));
  }
  mtlr(r0);
  frame_ends = pc_offset();
  AddS64(sp, fp,
         Operand(StandardFrameConstants::kCallerSPOffset + stack_adjustment),
         r0);
  mr(fp, ip);
  return frame_ends;
}

// ExitFrame layout (probably wrongish.. needs updating)
//
//  SP -> previousSP
//        LK reserved
//        sp_on_exit (for debug?)
// oldSP->prev SP
//        LK
//        <parameters on stack>

// Prior to calling EnterExitFrame, we've got a bunch of parameters
// on the stack that we need to wrap a real frame around.. so first
// we reserve a slot for LK and push the previous SP which is captured
// in the fp register (r31)
// Then - we buy a new frame

void MacroAssembler::EnterExitFrame(Register scratch, int stack_space,
                                    StackFrame::Type frame_type) {
  DCHECK(frame_type == StackFrame::EXIT ||
         frame_type == StackFrame::BUILTIN_EXIT ||
         frame_type == StackFrame::API_ACCESSOR_EXIT ||
         frame_type == StackFrame::API_CALLBACK_EXIT);

  using ER = ExternalReference;

  // Set up the frame structure on the stack.
  DCHECK_EQ(2 * kSystemPointerSize, ExitFrameConstants::kCallerSPDisplacement);
  DCHECK_EQ(1 * kSystemPointerSize, ExitFrameConstants::kCallerPCOffset);
  DCHECK_EQ(0 * kSystemPointerSize, ExitFrameConstants::kCallerFPOffset);

  // This is an opportunity to build a frame to wrap
  // all of the pushes that have happened inside of V8
  // since we were called from C code

  mov(ip, Operand(StackFrame::TypeToMarker(frame_type)));
  PushCommonFrame(ip);
  // Reserve room for saved entry sp.
  subi(sp, fp, Operand(ExitFrameConstants::kFixedFrameSizeFromFp));

  if (v8_flags.debug_code) {
    li(r8, Operand::Zero());
    StoreU64(r8, MemOperand(fp, ExitFrameConstants::kSPOffset));
  }
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL) {
    StoreU64(kConstantPoolRegister,
             MemOperand(fp, ExitFrameConstants::kConstantPoolOffset));
  }

  // Save the frame pointer and the context in top.
  ER c_entry_fp_address =
      ER::Create(IsolateAddressId::kCEntryFPAddress, isolate());
  StoreU64(fp, ExternalReferenceAsOperand(c_entry_fp_address, no_reg));

  ER context_address = ER::Create(IsolateAddressId::kContextAddress, isolate());
  StoreU64(cp, ExternalReferenceAsOperand(context_address, no_reg));

  AddS64(sp, sp, Operand(-(stack_space + 1) * kSystemPointerSize));

  // Allocate and align the frame preparing for calling the runtime
  // function.
  const int frame_alignment = ActivationFrameAlignment();
  if (frame_alignment > kSystemPointerSize) {
    DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
    ClearRightImm(sp, sp,
                  Operand(base::bits::WhichPowerOfTwo(frame_alignment)));
  }
  li(r0, Operand::Zero());
  StoreU64WithUpdate(
      r0, MemOperand(sp, -kNumRequiredStackFrameSlots * kSystemPointerSize));

  // Set the exit frame sp value to point just before the return address
  // location.
  AddS64(r8, sp, Operand((kStackFrameExtraParamSlot + 1) * kSystemPointerSize),
         r0);
  StoreU64(r8, MemOperand(fp, ExitFrameConstants::kSPOffset));
}

int MacroAssembler::ActivationFrameAlignment() {
#if !defined(USE_SIMULATOR)
  // Running on the real platform. Use the alignment as mandated by the local
  // environment.
  // Note: This will break if we ever start generating snapshots on one PPC
  // platform for another PPC platform with a different alignment.
  return base::OS::ActivationFrameAlignment();
#else  // Simulated
  // If we are using the simulator then we should always align to the expected
  // alignment. As the simulator is used to generate snapshots we do not know
  // if the target platform will need alignment, so this is controlled from a
  // flag.
  return v8_flags.sim_stack_alignment;
#endif
}

void MacroAssembler::LeaveExitFrame(Register scratch) {
  ConstantPoolUnavailableScope constant_pool_unavailable(this);

  using ER = ExternalReference;

  // Restore current context from top and clear it in debug mode.
  ER context_address = ER::Create(IsolateAddressId::kContextAddress, isolate());
  LoadU64(cp, ExternalReferenceAsOperand(context_address, no_reg));

#ifdef DEBUG
  mov(scratch, Operand(Context::kInvalidContext));
  StoreU64(scratch, ExternalReferenceAsOperand(context_address, no_reg));
#endif

  // Clear the top frame.
  ER c_entry_fp_address =
      ER::Create(IsolateAddressId::kCEntryFPAddress, isolate());
  mov(scratch, Operand::Zero());
  StoreU64(scratch, ExternalReferenceAsOperand(c_entry_fp_address, no_reg));

  // Tear down the exit frame, pop the arguments, and return.
  LeaveFrame(StackFrame::EXIT);
}

void MacroAssembler::MovFromFloatResult(const DoubleRegister dst) {
  Move(dst, d1);
}

void MacroAssembler::MovFromFloatParameter(const DoubleRegister dst) {
  Move(dst, d1);
}

void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind,
                                    Register scratch) {
  DCHECK(root_array_available());
  intptr_t offset = kind == StackLimitKind::kRealStackLimit
                        ? IsolateData::real_jslimit_offset()
                        : IsolateData::jslimit_offset();
  CHECK(is_int32(offset));
  LoadU64(destination, MemOperand(kRootRegister, offset), scratch);
}

void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch,
                                        Label* stack_overflow) {
  // Check the stack for overflow. We are not trying to catch
  // interruptions (e.g. debug break and preemption) here, so the "real stack
  // limit" is checked.
  LoadStackLimit(scratch, StackLimitKind::kRealStackLimit, r0);
  // Make scratch the space we have left. The stack might already be overflowed
  // here which will cause scratch to become negative.
  sub(scratch, sp, scratch);
  // Check if the arguments will overflow the stack.
  ShiftLeftU64(r0, num_args, Operand(kSystemPointerSizeLog2));
  CmpS64(scratch, r0);
  ble(stack_overflow);  // Signed comparison.
}

void MacroAssembler::InvokePrologue(Register expected_parameter_count,
                                    Register actual_parameter_count,
                                    Label* done, InvokeType type) {
  Label regular_invoke;

  //  r3: actual arguments count
  //  r4: function (passed through to callee)
  //  r5: expected arguments count

  DCHECK_EQ(actual_parameter_count, r3);
  DCHECK_EQ(expected_parameter_count, r5);

  // If overapplication or if the actual argument count is equal to the
  // formal parameter count, no need to push extra undefined values.
  sub(expected_parameter_count, expected_parameter_count,
      actual_parameter_count, LeaveOE, SetRC);
  ble(&regular_invoke, cr0);

  Label stack_overflow;
  Register scratch = r7;
  StackOverflowCheck(expected_parameter_count, scratch, &stack_overflow);

  // Underapplication. Move the arguments already in the stack, including the
  // receiver and the return address.
  {
    Label copy, skip;
    Register src = r9, dest = r8;
    addi(src, sp, Operand(-kSystemPointerSize));
    ShiftLeftU64(r0, expected_parameter_count, Operand(kSystemPointerSizeLog2));
    sub(sp, sp, r0);
    // Update stack pointer.
    addi(dest, sp, Operand(-kSystemPointerSize));
    mr(r0, actual_parameter_count);
    cmpi(r0, Operand::Zero());
    ble(&skip);
    mtctr(r0);

    bind(&copy);
    LoadU64WithUpdate(r0, MemOperand(src, kSystemPointerSize));
    StoreU64WithUpdate(r0, MemOperand(dest, kSystemPointerSize));
    bdnz(&copy);
    bind(&skip);
  }

  // Fill remaining expected arguments with undefined values.
  LoadRoot(scratch, RootIndex::kUndefinedValue);
  {
    mtctr(expected_parameter_count);

    Label loop;
    bind(&loop);
    StoreU64WithUpdate(scratch, MemOperand(r8, kSystemPointerSize));
    bdnz(&loop);
  }
  b(&regular_invoke);

  bind(&stack_overflow);
  {
    FrameScope frame(
        this, has_frame() ? StackFrame::NO_FRAME_TYPE : StackFrame::INTERNAL);
    CallRuntime(Runtime::kThrowStackOverflow);
    bkpt(0);
  }

  bind(&regular_invoke);
}

void MacroAssembler::CheckDebugHook(Register fun, Register new_target,
                                    Register expected_parameter_count,
                                    Register actual_parameter_count) {
  Label skip_hook;

  ExternalReference debug_hook_active =
      ExternalReference::debug_hook_on_function_call_address(isolate());
  Move(r7, debug_hook_active);
  LoadU8(r7, MemOperand(r7), r0);
  extsb(r7, r7);
  CmpSmiLiteral(r7, Smi::zero(), r0);
  beq(&skip_hook);

  {
    // Load receiver to pass it later to DebugOnFunctionCall hook.
    LoadReceiver(r7);
    FrameScope frame(
        this, has_frame() ? StackFrame::NO_FRAME_TYPE : StackFrame::INTERNAL);

    SmiTag(expected_parameter_count);
    Push(expected_parameter_count);

    SmiTag(actual_parameter_count);
    Push(actual_parameter_count);

    if (new_target.is_valid()) {
      Push(new_target);
    }
    Push(fun, fun, r7);
    CallRuntime(Runtime::kDebugOnFunctionCall);
    Pop(fun);
    if (new_target.is_valid()) {
      Pop(new_target);
    }

    Pop(actual_parameter_count);
    SmiUntag(actual_parameter_count);

    Pop(expected_parameter_count);
    SmiUntag(expected_parameter_count);
  }
  bind(&skip_hook);
}

void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
                                        Register expected_parameter_count,
                                        Register actual_parameter_count,
                                        InvokeType type) {
  // You can't call a function without a valid frame.
  DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
  DCHECK_EQ(function, r4);
  DCHECK_IMPLIES(new_target.is_valid(), new_target == r6);

  // On function call, call into the debugger if necessary.
  CheckDebugHook(function, new_target, expected_parameter_count,
                 actual_parameter_count);

  // Clear the new.target register if not given.
  if (!new_target.is_valid()) {
    LoadRoot(r6, RootIndex::kUndefinedValue);
  }

  Label done;
  InvokePrologue(expected_parameter_count, actual_parameter_count, &done, type);
  // We call indirectly through the code field in the function to
  // allow recompilation to take effect without changing any of the
  // call sites.
  switch (type) {
    case InvokeType::kCall:
      CallJSFunction(function, r0);
      break;
    case InvokeType::kJump:
      JumpJSFunction(function, r0);
      break;
  }

    // Continue here if InvokePrologue does handle the invocation due to
    // mismatched parameter counts.
    bind(&done);
}

void MacroAssembler::InvokeFunctionWithNewTarget(
    Register fun, Register new_target, Register actual_parameter_count,
    InvokeType type) {
  // You can't call a function without a valid frame.
  DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());

  // Contract with called JS functions requires that function is passed in r4.
  DCHECK_EQ(fun, r4);

  Register expected_reg = r5;
  Register temp_reg = r7;

  LoadTaggedField(
      temp_reg, FieldMemOperand(r4, JSFunction::kSharedFunctionInfoOffset), r0);
  LoadTaggedField(cp, FieldMemOperand(r4, JSFunction::kContextOffset), r0);
  LoadU16(expected_reg,
          FieldMemOperand(temp_reg,
                          SharedFunctionInfo::kFormalParameterCountOffset));

  InvokeFunctionCode(fun, new_target, expected_reg, actual_parameter_count,
                     type);
}

void MacroAssembler::InvokeFunction(Register function,
                                    Register expected_parameter_count,
                                    Register actual_parameter_count,
                                    InvokeType type) {
  // You can't call a function without a valid frame.
  DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());

  // Contract with called JS functions requires that function is passed in r4.
  DCHECK_EQ(function, r4);

  // Get the function and setup the context.
  LoadTaggedField(cp, FieldMemOperand(r4, JSFunction::kContextOffset), r0);

  InvokeFunctionCode(r4, no_reg, expected_parameter_count,
                     actual_parameter_count, type);
}

void MacroAssembler::PushStackHandler() {
  // Adjust this code if not the case.
  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);

  Push(Smi::zero());  // Padding.

  // Link the current handler as the next handler.
  // Preserve r4-r8.
  Move(r3,
       ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
  LoadU64(r0, MemOperand(r3));
  push(r0);

  // Set this new handler as the current one.
  StoreU64(sp, MemOperand(r3));
}

void MacroAssembler::PopStackHandler() {
  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
  static_assert(StackHandlerConstants::kNextOffset == 0);

  pop(r4);
  Move(ip,
       ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
  StoreU64(r4, MemOperand(ip));

  Drop(1);  // Drop padding.
}

#if V8_STATIC_ROOTS_BOOL
void MacroAssembler::CompareInstanceTypeWithUniqueCompressedMap(
    Register map, Register scratch, InstanceType type) {
  std::optional<RootIndex> expected =
      InstanceTypeChecker::UniqueMapOfInstanceType(type);
  CHECK(expected);
  Tagged_t expected_ptr = ReadOnlyRootPtr(*expected);
  DCHECK_NE(map, scratch);
  UseScratchRegisterScope temps(this);
  CHECK(scratch != Register::no_reg() || temps.CanAcquire());
  if (scratch == Register::no_reg()) {
    // TODO(tpearson): Figure out why ScratchRegisterScope returns a
    // register that is aliased with one of our other in-use registers
    // For now, use r11 (kScratchReg in the code generator)
    scratch = r11;
    DCHECK_NE(map, scratch);
  }
  mov(scratch, Operand(expected_ptr));
  CompareTagged(map, scratch);
}

void MacroAssembler::IsObjectTypeFast(Register object,
                                      Register compressed_map_scratch,
                                      InstanceType type, Register scratch) {
  ASM_CODE_COMMENT(this);
  CHECK(InstanceTypeChecker::UniqueMapOfInstanceType(type));
  LoadCompressedMap(compressed_map_scratch, object, scratch);
  CompareInstanceTypeWithUniqueCompressedMap(compressed_map_scratch,
                                             Register::no_reg(), type);
}
#endif  // V8_STATIC_ROOTS_BOOL

// Sets equality condition flags.
void MacroAssembler::IsObjectType(Register object, Register scratch1,
                                  Register scratch2, InstanceType type) {
  ASM_CODE_COMMENT(this);

#if V8_STATIC_ROOTS_BOOL
  if (InstanceTypeChecker::UniqueMapOfInstanceType(type)) {
    DCHECK((scratch1 != scratch2) || (scratch1 != r0));
    LoadCompressedMap(scratch1, object, scratch1 != scratch2 ? scratch2 : r0);
    CompareInstanceTypeWithUniqueCompressedMap(
        scratch1, scratch1 != scratch2 ? scratch2 : r0, type);
    return;
  }
#endif  // V8_STATIC_ROOTS_BOOL

  CompareObjectType(object, scratch1, scratch2, type);
}

void MacroAssembler::CompareObjectType(Register object, Register map,
                                       Register type_reg, InstanceType type) {
  const Register temp = type_reg == no_reg ? r0 : type_reg;

  LoadMap(map, object);
  CompareInstanceType(map, temp, type);
}

void MacroAssembler::CompareObjectTypeRange(Register object, Register map,
                                            Register type_reg, Register scratch,
                                            InstanceType lower_limit,
                                            InstanceType upper_limit) {
  ASM_CODE_COMMENT(this);
  LoadMap(map, object);
  CompareInstanceTypeRange(map, type_reg, scratch, lower_limit, upper_limit);
}

void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
                                         InstanceType type) {
  static_assert(Map::kInstanceTypeOffset < 4096);
  static_assert(LAST_TYPE <= 0xFFFF);
  lhz(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
  cmpi(type_reg, Operand(type));
}

void MacroAssembler::CompareRange(Register value, Register scratch,
                                  unsigned lower_limit, unsigned higher_limit) {
  ASM_CODE_COMMENT(this);
  DCHECK_LT(lower_limit, higher_limit);
  if (lower_limit != 0) {
    mov(scratch, Operand(lower_limit));
    sub(scratch, value, scratch);
    cmpli(scratch, Operand(higher_limit - lower_limit));
  } else {
    mov(scratch, Operand(higher_limit));
    CmpU64(value, scratch);
  }
}

void MacroAssembler::CompareInstanceTypeRange(Register map, Register type_reg,
                                              Register scratch,
                                              InstanceType lower_limit,
                                              InstanceType higher_limit) {
  DCHECK_LT(lower_limit, higher_limit);
  LoadU16(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
  CompareRange(type_reg, scratch, lower_limit, higher_limit);
}

void MacroAssembler::CompareTaggedRoot(const Register& obj, RootIndex index) {
  ASM_CODE_COMMENT(this);
  // Use r0 as a safe scratch register here, since temps.Acquire() tends
  // to spit back the register being passed as an argument in obj...
  Register temp = r0;
  DCHECK(!AreAliased(obj, temp));

  if (V8_STATIC_ROOTS_BOOL && RootsTable::IsReadOnly(index)) {
    mov(temp, Operand(ReadOnlyRootPtr(index)));
    CompareTagged(obj, temp);
    return;
  }
  // Some smi roots contain system pointer size values like stack limits.
  DCHECK(base::IsInRange(index, RootIndex::kFirstStrongOrReadOnlyRoot,
                         RootIndex::kLastStrongOrReadOnlyRoot));
  LoadRoot(temp, index);
  CompareTagged(obj, temp);
}

void MacroAssembler::CompareRoot(Register obj, RootIndex index) {
  ASM_CODE_COMMENT(this);
  // Use r0 as a safe scratch register here, since temps.Acquire() tends
  // to spit back the register being passed as an argument in obj...
  Register temp = r0;
  if (!base::IsInRange(index, RootIndex::kFirstStrongOrReadOnlyRoot,
                       RootIndex::kLastStrongOrReadOnlyRoot)) {
    // Some smi roots contain system pointer size values like stack limits.
    DCHECK(!AreAliased(obj, temp));
    LoadRoot(temp, index);
    CmpU64(obj, temp);
    return;
  }
  CompareTaggedRoot(obj, index);
}

void MacroAssembler::AddAndCheckForOverflow(Register dst, Register left,
                                            Register right,
                                            Register overflow_dst,
                                            Register scratch) {
  DCHECK(dst != overflow_dst);
  DCHECK(dst != scratch);
  DCHECK(overflow_dst != scratch);
  DCHECK(overflow_dst != left);
  DCHECK(overflow_dst != right);

  bool left_is_right = left == right;
  RCBit xorRC = left_is_right ? SetRC : LeaveRC;

  // C = A+B; C overflows if A/B have same sign and C has diff sign than A
  if (dst == left) {
    mr(scratch, left);                        // Preserve left.
    add(dst, left, right);                    // Left is overwritten.
    xor_(overflow_dst, dst, scratch, xorRC);  // Original left.
    if (!left_is_right) xor_(scratch, dst, right);
  } else if (dst == right) {
    mr(scratch, right);     // Preserve right.
    add(dst, left, right);  // Right is overwritten.
    xor_(overflow_dst, dst, left, xorRC);
    if (!left_is_right) xor_(scratch, dst, scratch);  // Original right.
  } else {
    add(dst, left, right);
    xor_(overflow_dst, dst, left, xorRC);
    if (!left_is_right) xor_(scratch, dst, right);
  }
  if (!left_is_right) and_(overflow_dst, scratch, overflow_dst, SetRC);
}

void MacroAssembler::AddAndCheckForOverflow(Register dst, Register left,
                                            intptr_t right,
                                            Register overflow_dst,
                                            Register scratch) {
  Register original_left = left;
  DCHECK(dst != overflow_dst);
  DCHECK(dst != scratch);
  DCHECK(overflow_dst != scratch);
  DCHECK(overflow_dst != left);

  // C = A+B; C overflows if A/B have same sign and C has diff sign than A
  if (dst == left) {
    // Preserve left.
    original_left = overflow_dst;
    mr(original_left, left);
  }
  AddS64(dst, left, Operand(right), scratch);
  xor_(overflow_dst, dst, original_left);
  if (right >= 0) {
    and_(overflow_dst, overflow_dst, dst, SetRC);
  } else {
    andc(overflow_dst, overflow_dst, dst, SetRC);
  }
}

void MacroAssembler::SubAndCheckForOverflow(Register dst, Register left,
                                            Register right,
                                            Register overflow_dst,
                                            Register scratch) {
  DCHECK(dst != overflow_dst);
  DCHECK(dst != scratch);
  DCHECK(overflow_dst != scratch);
  DCHECK(overflow_dst != left);
  DCHECK(overflow_dst != right);

  // C = A-B; C overflows if A/B have diff signs and C has diff sign than A
  if (dst == left) {
    mr(scratch, left);      // Preserve left.
    sub(dst, left, right);  // Left is overwritten.
    xor_(overflow_dst, dst, scratch);
    xor_(scratch, scratch, right);
    and_(overflow_dst, overflow_dst, scratch, SetRC);
  } else if (dst == right) {
    mr(scratch, right);     // Preserve right.
    sub(dst, left, right);  // Right is overwritten.
    xor_(overflow_dst, dst, left);
    xor_(scratch, left, scratch);
    and_(overflow_dst, overflow_dst, scratch, SetRC);
  } else {
    sub(dst, left, right);
    xor_(overflow_dst, dst, left);
    xor_(scratch, left, right);
    and_(overflow_dst, scratch, overflow_dst, SetRC);
  }
}

void MacroAssembler::MinF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, DoubleRegister scratch) {
  Label check_zero, return_left, return_right, return_nan, done;
  fcmpu(lhs, rhs);
  bunordered(&return_nan);
  if (CpuFeatures::IsSupported(PPC_7_PLUS)) {
    xsmindp(dst, lhs, rhs);
    b(&done);
  }
  beq(&check_zero);
  ble(&return_left);
  b(&return_right);

  bind(&check_zero);
  fcmpu(lhs, kDoubleRegZero);
  /* left == right != 0. */
  bne(&return_left);
  /* At this point, both left and right are either 0 or -0. */
  /* Min: The algorithm is: -((-L) + (-R)), which in case of L and R */
  /* being different registers is most efficiently expressed */
  /* as -((-L) - R). */
  fneg(scratch, lhs);
  if (scratch == rhs) {
    fadd(dst, scratch, rhs);
  } else {
    fsub(dst, scratch, rhs);
  }
  fneg(dst, dst);
  b(&done);

  bind(&return_nan);
  /* If left or right are NaN, fadd propagates the appropriate one.*/
  fadd(dst, lhs, rhs);
  b(&done);

  bind(&return_right);
  if (rhs != dst) {
    fmr(dst, rhs);
  }
  b(&done);

  bind(&return_left);
  if (lhs != dst) {
    fmr(dst, lhs);
  }
  bind(&done);
}

void MacroAssembler::MaxF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, DoubleRegister scratch) {
  Label check_zero, return_left, return_right, return_nan, done;
  fcmpu(lhs, rhs);
  bunordered(&return_nan);
  if (CpuFeatures::IsSupported(PPC_7_PLUS)) {
    xsmaxdp(dst, lhs, rhs);
    b(&done);
  }
  beq(&check_zero);
  bge(&return_left);
  b(&return_right);

  bind(&check_zero);
  fcmpu(lhs, kDoubleRegZero);
  /* left == right != 0. */
  bne(&return_left);
  /* At this point, both left and right are either 0 or -0. */
  fadd(dst, lhs, rhs);
  b(&done);

  bind(&return_nan);
  /* If left or right are NaN, fadd propagates the appropriate one.*/
  fadd(dst, lhs, rhs);
  b(&done);

  bind(&return_right);
  if (rhs != dst) {
    fmr(dst, rhs);
  }
  b(&done);

  bind(&return_left);
  if (lhs != dst) {
    fmr(dst, lhs);
  }
  bind(&done);
}

void MacroAssembler::JumpIfIsInRange(Register value, Register scratch,
                                     unsigned lower_limit,
                                     unsigned higher_limit,
                                     Label* on_in_range) {
  CompareRange(value, scratch, lower_limit, higher_limit);
  ble(on_in_range);
}

void MacroAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
                                       Register result,
                                       DoubleRegister double_input,
                                       StubCallMode stub_mode) {
  Label done;

  TryInlineTruncateDoubleToI(result, double_input, &done);

  // If we fell through then inline version didn't succeed - call stub instead.
  mflr(r0);
  push(r0);
  // Put input on stack.
  stfdu(double_input, MemOperand(sp, -kDoubleSize));

#if V8_ENABLE_WEBASSEMBLY
  if (stub_mode == StubCallMode::kCallWasmRuntimeStub) {
    Call(static_cast<Address>(Builtin::kDoubleToI), RelocInfo::WASM_STUB_CALL);
#else
  // For balance.
  if (false) {
#endif  // V8_ENABLE_WEBASSEMBLY
  } else {
    CallBuiltin(Builtin::kDoubleToI);
  }

  LoadU64(result, MemOperand(sp));
  addi(sp, sp, Operand(kDoubleSize));
  pop(r0);
  mtlr(r0);

  bind(&done);
}

void MacroAssembler::TryInlineTruncateDoubleToI(Register result,
                                                DoubleRegister double_input,
                                                Label* done) {
  DoubleRegister double_scratch = kScratchDoubleReg;
#if !V8_TARGET_ARCH_PPC64
  Register scratch = ip;
#endif

  ConvertDoubleToInt64(double_input,
#if !V8_TARGET_ARCH_PPC64
                       scratch,
#endif
                       result, double_scratch);

// Test for overflow
#if V8_TARGET_ARCH_PPC64
  TestIfInt32(result, r0);
#else
  TestIfInt32(scratch, result, r0);
#endif
  beq(done);
}

namespace {

void TailCallOptimizedCodeSlot(MacroAssembler* masm,
                               Register optimized_code_entry,
                               Register scratch) {
  // ----------- S t a t e -------------
  //  -- r3 : actual argument count
  //  -- r6 : new target (preserved for callee if needed, and caller)
  //  -- r4 : target function (preserved for callee if needed, and caller)
  // -----------------------------------
  DCHECK(!AreAliased(r4, r6, optimized_code_entry, scratch));

  Register closure = r4;
  Label heal_optimized_code_slot;

  // If the optimized code is cleared, go to runtime to update the optimization
  // marker field.
  __ LoadWeakValue(optimized_code_entry, optimized_code_entry,
                   &heal_optimized_code_slot);

  // The entry references a CodeWrapper object. Unwrap it now.
  __ LoadCodePointerField(
      optimized_code_entry,
      FieldMemOperand(optimized_code_entry, CodeWrapper::kCodeOffset), scratch);

  // Check if the optimized code is marked for deopt. If it is, call the
  // runtime to clear it.
  {
    UseScratchRegisterScope temps(masm);
    __ TestCodeIsMarkedForDeoptimization(optimized_code_entry, temps.Acquire(),
                                         scratch);
    __ bne(&heal_optimized_code_slot, cr0);
  }

  // Optimized code is good, get it into the closure and link the closure
  // into the optimized functions list, then tail call the optimized code.
  __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure, scratch,
                                         r8);
  static_assert(kJavaScriptCallCodeStartRegister == r5, "ABI mismatch");
  __ LoadCodeInstructionStart(r5, optimized_code_entry);
  __ Jump(r5);

  // Optimized code slot contains deoptimized code or code is cleared and
  // optimized code marker isn't updated. Evict the code, update the marker
  // and re-enter the closure's code.
  __ bind(&heal_optimized_code_slot);
  __ GenerateTailCallToReturnedCode(Runtime::kHealOptimizedCodeSlot);
}

}  // namespace

#ifdef V8_ENABLE_DEBUG_CODE
void MacroAssembler::AssertFeedbackCell(Register object, Register scratch) {
  if (v8_flags.debug_code) {
    CompareObjectType(object, scratch, scratch, FEEDBACK_CELL_TYPE);
    Assert(eq, AbortReason::kExpectedFeedbackCell);
  }
}
void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
  if (v8_flags.debug_code) {
    CompareObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);
    Assert(eq, AbortReason::kExpectedFeedbackVector);
  }
}
#endif  // V8_ENABLE_DEBUG_CODE

// Optimized code is good, get it into the closure and link the closure
// into the optimized functions list, then tail call the optimized code.
void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(
    Register optimized_code, Register closure, Register scratch1,
    Register slot_address) {
  DCHECK(!AreAliased(optimized_code, closure, scratch1, slot_address));
  DCHECK_EQ(closure, kJSFunctionRegister);
  DCHECK(!AreAliased(optimized_code, closure));
  // Store code entry in the closure.
  StoreCodePointerField(optimized_code,
                        FieldMemOperand(closure, JSFunction::kCodeOffset), r0);
  // Write barrier clobbers scratch1 below.
  Register value = scratch1;
  mr(value, optimized_code);

  RecordWriteField(closure, JSFunction::kCodeOffset, value, slot_address,
                   kLRHasNotBeenSaved, SaveFPRegsMode::kIgnore, SmiCheck::kOmit,
                   SlotDescriptor::ForCodePointerSlot());
}

void MacroAssembler::GenerateTailCallToReturnedCode(
    Runtime::FunctionId function_id) {
  // ----------- S t a t e -------------
  //  -- r3 : actual argument count
  //  -- r4 : target function (preserved for callee)
  //  -- r6 : new target (preserved for callee)
  // -----------------------------------
  {
    FrameAndConstantPoolScope scope(this, StackFrame::INTERNAL);
    // Push a copy of the target function, the new target and the actual
    // argument count.
    // Push function as parameter to the runtime call.
    SmiTag(kJavaScriptCallArgCountRegister);
    Push(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
         kJavaScriptCallArgCountRegister, kJavaScriptCallTargetRegister);

    CallRuntime(function_id, 1);
    mr(r5, r3);

    // Restore target function, new target and actual argument count.
    Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
        kJavaScriptCallArgCountRegister);
    SmiUntag(kJavaScriptCallArgCountRegister);
  }
  static_assert(kJavaScriptCallCodeStartRegister == r5, "ABI mismatch");
  JumpCodeObject(r5);
}

// Read off the flags in the feedback vector and check if there
// is optimized code or a tiering state that needs to be processed.
void MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(
    Register flags, Register feedback_vector, CodeKind current_code_kind,
    Label* flags_need_processing) {
  ASM_CODE_COMMENT(this);
  DCHECK(!AreAliased(flags, feedback_vector));
  DCHECK(CodeKindCanTierUp(current_code_kind));
  LoadU16(flags,
          FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));
  uint32_t kFlagsMask = FeedbackVector::kFlagsTieringStateIsAnyRequested |
                        FeedbackVector::kFlagsMaybeHasTurbofanCode |
                        FeedbackVector::kFlagsLogNextExecution;
  if (current_code_kind != CodeKind::MAGLEV) {
    kFlagsMask |= FeedbackVector::kFlagsMaybeHasMaglevCode;
  }
  CHECK(is_uint16(kFlagsMask));
  mov(r0, Operand(kFlagsMask));
  AndU32(r0, flags, r0, SetRC);
  bne(flags_need_processing, cr0);
}

void MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot(
    Register flags, Register feedback_vector) {
  DCHECK(!AreAliased(flags, feedback_vector));
  Label maybe_has_optimized_code, maybe_needs_logging;
  // Check if optimized code is available
  TestBitMask(flags, FeedbackVector::kFlagsTieringStateIsAnyRequested, r0);
  beq(&maybe_needs_logging, cr0);

  GenerateTailCallToReturnedCode(Runtime::kCompileOptimized);

  bind(&maybe_needs_logging);
  TestBitMask(flags, FeedbackVector::LogNextExecutionBit::kMask, r0);
  beq(&maybe_has_optimized_code, cr0);
  GenerateTailCallToReturnedCode(Runtime::kFunctionLogNextExecution);

  bind(&maybe_has_optimized_code);
  Register optimized_code_entry = flags;
  LoadTaggedField(optimized_code_entry,
                  FieldMemOperand(feedback_vector,
                                  FeedbackVector::kMaybeOptimizedCodeOffset),
                  r0);
  TailCallOptimizedCodeSlot(this, optimized_code_entry, r9);
}

void MacroAssembler::CallRuntime(const Runtime::Function* f,
                                 int num_arguments) {
  // All parameters are on the stack.  r3 has the return value after call.

  // If the expected number of arguments of the runtime function is
  // constant, we check that the actual number of arguments match the
  // expectation.
  CHECK(f->nargs < 0 || f->nargs == num_arguments);

  // TODO(1236192): Most runtime routines don't need the number of
  // arguments passed in because it is constant. At some point we
  // should remove this need and make the runtime routine entry code
  // smarter.
  mov(r3, Operand(num_arguments));
  Move(r4, ExternalReference::Create(f));
#if V8_TARGET_ARCH_PPC64
  CallBuiltin(Builtins::RuntimeCEntry(f->result_size));
#else
  CallBuiltin(Builtins::RuntimeCEntry(1));
#endif
}

void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
  const Runtime::Function* function = Runtime::FunctionForId(fid);
  DCHECK_EQ(1, function->result_size);
  if (function->nargs >= 0) {
    mov(r3, Operand(function->nargs));
  }
  JumpToExternalReference(ExternalReference::Create(fid));
}

void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                             bool builtin_exit_frame) {
  Move(r4, builtin);
  TailCallBuiltin(Builtins::CEntry(1, ArgvMode::kStack, builtin_exit_frame));
}

void MacroAssembler::LoadWeakValue(Register out, Register in,
                                   Label* target_if_cleared) {
  CmpS32(in, Operand(kClearedWeakHeapObjectLower32), r0);
  beq(target_if_cleared);

  mov(r0, Operand(~kWeakHeapObjectMask));
  and_(out, in, r0);
}

void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                          Register scratch1,
                                          Register scratch2) {
  DCHECK_GT(value, 0);
  if (v8_flags.native_code_counters && counter->Enabled()) {
    // This operation has to be exactly 32-bit wide in case the external
    // reference table redirects the counter to a uint32_t dummy_stats_counter_
    // field.
    Move(scratch2, ExternalReference::Create(counter));
    lwz(scratch1, MemOperand(scratch2));
    addi(scratch1, scratch1, Operand(value));
    stw(scratch1, MemOperand(scratch2));
  }
}

void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                          Register scratch1,
                                          Register scratch2) {
  DCHECK_GT(value, 0);
  if (v8_flags.native_code_counters && counter->Enabled()) {
    // This operation has to be exactly 32-bit wide in case the external
    // reference table redirects the counter to a uint32_t dummy_stats_counter_
    // field.
    Move(scratch2, ExternalReference::Create(counter));
    lwz(scratch1, MemOperand(scratch2));
    subi(scratch1, scratch1, Operand(value));
    stw(scratch1, MemOperand(scratch2));
  }
}

void MacroAssembler::Check(Condition cond, AbortReason reason, CRegister cr) {
  Label L;
  b(cond, &L, cr);
  Abort(reason);
  // will not return here
  bind(&L);
}

void MacroAssembler::Abort(AbortReason reason) {
  Label abort_start;
  bind(&abort_start);
  if (v8_flags.code_comments) {
    const char* msg = GetAbortReason(reason);
    RecordComment("Abort message: ");
    RecordComment(msg);
  }

  // Avoid emitting call to builtin if requested.
  if (trap_on_abort()) {
    stop();
    return;
  }

  if (should_abort_hard()) {
    // We don't care if we constructed a frame. Just pretend we did.
    FrameScope assume_frame(this, StackFrame::NO_FRAME_TYPE);
    mov(r3, Operand(static_cast<int>(reason)));
    PrepareCallCFunction(1, 0, r4);
    Register dst = ip;
    if (!ABI_CALL_VIA_IP) {
      dst = r4;
    }
    Move(dst, ExternalReference::abort_with_reason());
    // Use Call directly to avoid any unneeded overhead. The function won't
    // return anyway.
    Call(dst);
    return;
  }

  LoadSmiLiteral(r4, Smi::FromInt(static_cast<int>(reason)));

  {
    // We don't actually want to generate a pile of code for this, so just
    // claim there is a stack frame, without generating one.
    FrameScope scope(this, StackFrame::NO_FRAME_TYPE);
    if (root_array_available()) {
      // Generate an indirect call via builtins entry table here in order to
      // ensure that the interpreter_entry_return_pc_offset is the same for
      // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
      // when v8_flags.debug_code is enabled.
      LoadEntryFromBuiltin(Builtin::kAbort, ip);
      Call(ip);
    } else {
      CallBuiltin(Builtin::kAbort);
    }
  }
  // will not return here
}

void MacroAssembler::LoadMap(Register destination, Register object) {
  LoadTaggedField(destination, FieldMemOperand(object, HeapObject::kMapOffset),
                  r0);
}

void MacroAssembler::LoadFeedbackVector(Register dst, Register closure,
                                        Register scratch, Label* fbv_undef) {
  Label done;

  // Load the feedback vector from the closure.
  LoadTaggedField(
      dst, FieldMemOperand(closure, JSFunction::kFeedbackCellOffset), r0);
  LoadTaggedField(dst, FieldMemOperand(dst, FeedbackCell::kValueOffset), r0);

  // Check if feedback vector is valid.
  LoadTaggedField(scratch, FieldMemOperand(dst, HeapObject::kMapOffset), r0);
  LoadU16(scratch, FieldMemOperand(scratch, Map::kInstanceTypeOffset));
  CmpS32(scratch, Operand(FEEDBACK_VECTOR_TYPE), r0);
  b(eq, &done);

  // Not valid, load undefined.
  LoadRoot(dst, RootIndex::kUndefinedValue);
  b(fbv_undef);

  bind(&done);
}

void MacroAssembler::LoadCompressedMap(Register dst, Register object,
                                       Register scratch) {
  ASM_CODE_COMMENT(this);
  LoadU32(dst, FieldMemOperand(object, HeapObject::kMapOffset), scratch);
}

void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
  LoadMap(dst, cp);
  LoadTaggedField(
      dst,
      FieldMemOperand(dst, Map::kConstructorOrBackPointerOrNativeContextOffset),
      r0);
  LoadTaggedField(dst, MemOperand(dst, Context::SlotOffset(index)), r0);
}

#ifdef V8_ENABLE_DEBUG_CODE
void MacroAssembler::Assert(Condition cond, AbortReason reason, CRegister cr) {
  if (v8_flags.debug_code) Check(cond, reason, cr);
}

void MacroAssembler::AssertNotSmi(Register object) {
  if (v8_flags.debug_code) {
    static_assert(kSmiTag == 0);
    TestIfSmi(object, r0);
    Check(ne, AbortReason::kOperandIsASmi, cr0);
  }
}

void MacroAssembler::AssertSmi(Register object) {
  if (v8_flags.debug_code) {
    static_assert(kSmiTag == 0);
    TestIfSmi(object, r0);
    Check(eq, AbortReason::kOperandIsNotASmi, cr0);
  }
}

void MacroAssembler::AssertConstructor(Register object) {
  if (v8_flags.debug_code) {
    static_assert(kSmiTag == 0);
    TestIfSmi(object, r0);
    Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, cr0);
    push(object);
    LoadMap(object, object);
    lbz(object, FieldMemOperand(object, Map::kBitFieldOffset));
    andi(object, object, Operand(Map::Bits1::IsConstructorBit::kMask));
    pop(object);
    Check(ne, AbortReason::kOperandIsNotAConstructor, cr0);
  }
}

void MacroAssembler::AssertFunction(Register object) {
  if (v8_flags.debug_code) {
    static_assert(kSmiTag == 0);
    TestIfSmi(object, r0);
    Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
    push(object);
    LoadMap(object, object);
    CompareInstanceTypeRange(object, object, r0, FIRST_JS_FUNCTION_TYPE,
                             LAST_JS_FUNCTION_TYPE);
    pop(object);
    Check(le, AbortReason::kOperandIsNotAFunction);
  }
}

void MacroAssembler::AssertCallableFunction(Register object) {
  if (!v8_flags.debug_code) return;
  ASM_CODE_COMMENT(this);
  static_assert(kSmiTag == 0);
  TestIfSmi(object, r0);
  Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
  push(object);
  LoadMap(object, object);
  CompareInstanceTypeRange(object, object, r0, FIRST_CALLABLE_JS_FUNCTION_TYPE,
                           LAST_CALLABLE_JS_FUNCTION_TYPE);
  pop(object);
  Check(le, AbortReason::kOperandIsNotACallableFunction);
}

void MacroAssembler::AssertBoundFunction(Register object) {
  if (v8_flags.debug_code) {
    static_assert(kSmiTag == 0);
    TestIfSmi(object, r0);
    Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, cr0);
    push(object);
    CompareObjectType(object, object, object, JS_BOUND_FUNCTION_TYPE);
    pop(object);
    Check(eq, AbortReason::kOperandIsNotABoundFunction);
  }
}

void MacroAssembler::AssertGeneratorObject(Register object) {
  if (!v8_flags.debug_code) return;
  TestIfSmi(object, r0);
  Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, cr0);

  // Load map
  Register map = object;
  push(object);
  LoadMap(map, object);

  // Check if JSGeneratorObject
  Register instance_type = object;
  CompareInstanceTypeRange(map, instance_type, r0,
                           FIRST_JS_GENERATOR_OBJECT_TYPE,
                           LAST_JS_GENERATOR_OBJECT_TYPE);
  // Restore generator object to register and perform assertion
  pop(object);
  Check(le, AbortReason::kOperandIsNotAGeneratorObject);
}

void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                     Register scratch) {
  if (v8_flags.debug_code) {
    Label done_checking;
    AssertNotSmi(object);
    CompareRoot(object, RootIndex::kUndefinedValue);
    beq(&done_checking);
    LoadMap(scratch, object);
    CompareInstanceType(scratch, scratch, ALLOCATION_SITE_TYPE);
    Assert(eq, AbortReason::kExpectedUndefinedOrCell);
    bind(&done_checking);
  }
}

void MacroAssembler::AssertJSAny(Register object, Register map_tmp,
                                 Register tmp, AbortReason abort_reason) {
  if (!v8_flags.debug_code) return;

  ASM_CODE_COMMENT(this);
  DCHECK(!AreAliased(object, map_tmp, tmp));
  Label ok;

  JumpIfSmi(object, &ok);

  LoadMap(map_tmp, object);
  CompareInstanceType(map_tmp, tmp, LAST_NAME_TYPE);
  ble(&ok);

  CompareInstanceType(map_tmp, tmp, FIRST_JS_RECEIVER_TYPE);
  bge(&ok);

  CompareRoot(map_tmp, RootIndex::kHeapNumberMap);
  beq(&ok);

  CompareRoot(map_tmp, RootIndex::kBigIntMap);
  beq(&ok);

  CompareRoot(object, RootIndex::kUndefinedValue);
  beq(&ok);

  CompareRoot(object, RootIndex::kTrueValue);
  beq(&ok);

  CompareRoot(object, RootIndex::kFalseValue);
  beq(&ok);

  CompareRoot(object, RootIndex::kNullValue);
  beq(&ok);

  Abort(abort_reason);

  bind(&ok);
}

#endif  // V8_ENABLE_DEBUG_CODE

int MacroAssembler::CalculateStackPassedWords(int num_reg_arguments,
                                              int num_double_arguments) {
  int stack_passed_words = 0;
  if (num_double_arguments > DoubleRegister::kNumRegisters) {
    stack_passed_words +=
        2 * (num_double_arguments - DoubleRegister::kNumRegisters);
  }
  // Up to 8 simple arguments are passed in registers r3..r10.
  if (num_reg_arguments > kRegisterPassedArguments) {
    stack_passed_words += num_reg_arguments - kRegisterPassedArguments;
  }
  return stack_passed_words;
}

void MacroAssembler::PrepareCallCFunction(int num_reg_arguments,
                                          int num_double_arguments,
                                          Register scratch) {
  int frame_alignment = ActivationFrameAlignment();
  int stack_passed_arguments =
      CalculateStackPassedWords(num_reg_arguments, num_double_arguments);
  int stack_space = kNumRequiredStackFrameSlots;

  if (frame_alignment > kSystemPointerSize) {
    // Make stack end at alignment and make room for stack arguments
    // -- preserving original value of sp.
    mr(scratch, sp);
    AddS64(sp, sp, Operand(-(stack_passed_arguments + 1) * kSystemPointerSize),
           scratch);
    DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
    ClearRightImm(sp, sp,
                  Operand(base::bits::WhichPowerOfTwo(frame_alignment)));
    StoreU64(scratch,
             MemOperand(sp, stack_passed_arguments * kSystemPointerSize));
  } else {
    // Make room for stack arguments
    stack_space += stack_passed_arguments;
  }

  // Allocate frame with required slots to make ABI work.
  li(r0, Operand::Zero());
  StoreU64WithUpdate(r0, MemOperand(sp, -stack_space * kSystemPointerSize));
}

void MacroAssembler::PrepareCallCFunction(int num_reg_arguments,
                                          Register scratch) {
  PrepareCallCFunction(num_reg_arguments, 0, scratch);
}

void MacroAssembler::MovToFloatParameter(DoubleRegister src) { Move(d1, src); }

void MacroAssembler::MovToFloatResult(DoubleRegister src) { Move(d1, src); }

void MacroAssembler::MovToFloatParameters(DoubleRegister src1,
                                          DoubleRegister src2) {
  if (src2 == d1) {
    DCHECK(src1 != d2);
    Move(d2, src2);
    Move(d1, src1);
  } else {
    Move(d1, src1);
    Move(d2, src2);
  }
}

int MacroAssembler::CallCFunction(ExternalReference function,
                                  int num_reg_arguments,
                                  int num_double_arguments,
                                  SetIsolateDataSlots set_isolate_data_slots,
                                  bool has_function_descriptor) {
  Move(ip, function);
  return CallCFunction(ip, num_reg_arguments, num_double_arguments,
                       set_isolate_data_slots, has_function_descriptor);
}

int MacroAssembler::CallCFunction(Register function, int num_reg_arguments,
                                  int num_double_arguments,
                                  SetIsolateDataSlots set_isolate_data_slots,
                                  bool has_function_descriptor) {
  ASM_CODE_COMMENT(this);
  DCHECK_LE(num_reg_arguments + num_double_arguments, kMaxCParameters);
  DCHECK(has_frame());

  Label start_call;
  Register pc_scratch = r11;
  DCHECK(!AreAliased(pc_scratch, function));
  LoadPC(pc_scratch);
  bind(&start_call);
  int start_pc_offset = pc_offset();
  // We are going to patch this instruction after emitting
  // Call, using a zero offset here as placeholder for now.
  // patch_pc_address assumes `addi` is used here to
  // add the offset to pc.
  addi(pc_scratch, pc_scratch, Operand::Zero());

  if (set_isolate_data_slots == SetIsolateDataSlots::kYes) {
    // Save the frame pointer and PC so that the stack layout remains iterable,
    // even without an ExitFrame which normally exists between JS and C frames.
    Register scratch = r8;
    Push(scratch);
    mflr(scratch);
    CHECK(root_array_available());
    StoreU64(pc_scratch,
             ExternalReferenceAsOperand(IsolateFieldId::kFastCCallCallerPC));
    StoreU64(fp,
             ExternalReferenceAsOperand(IsolateFieldId::kFastCCallCallerFP));
    mtlr(scratch);
    Pop(scratch);
  }

  // Just call directly. The function called cannot cause a GC, or
  // allow preemption, so the return address in the link register
  // stays correct.
  Register dest = function;
  if (ABI_USES_FUNCTION_DESCRIPTORS && has_function_descriptor) {
    // AIX/PPC64BE Linux uses a function descriptor. When calling C code be
    // aware of this descriptor and pick up values from it
    LoadU64(ToRegister(ABI_TOC_REGISTER),
            MemOperand(function, kSystemPointerSize));
    LoadU64(ip, MemOperand(function, 0));
    dest = ip;
  } else if (ABI_CALL_VIA_IP) {
    // pLinux and Simualtor, not AIX
    Move(ip, function);
    dest = ip;
  }

  Call(dest);
  int call_pc_offset = pc_offset();
  int offset_since_start_call = SizeOfCodeGeneratedSince(&start_call);
  // Here we are going to patch the `addi` instruction above to use the
  // correct offset.
  // LoadPC emits two instructions and pc is the address of its second emitted
  // instruction. Add one more to the offset to point to after the Call.
  offset_since_start_call += kInstrSize;
  patch_pc_address(pc_scratch, start_pc_offset, offset_since_start_call);

  if (set_isolate_data_slots == SetIsolateDataSlots::kYes) {
    // We don't unset the PC; the FP is the source of truth.
    Register zero_scratch = r0;
    mov(zero_scratch, Operand::Zero());

    StoreU64(zero_scratch,
             ExternalReferenceAsOperand(IsolateFieldId::kFastCCallCallerFP));
  }

  // Remove frame bought in PrepareCallCFunction
  int stack_passed_arguments =
      CalculateStackPassedWords(num_reg_arguments, num_double_arguments);
  int stack_space = kNumRequiredStackFrameSlots + stack_passed_arguments;
  if (ActivationFrameAlignment() > kSystemPointerSize) {
    LoadU64(sp, MemOperand(sp, stack_space * kSystemPointerSize), r0);
  } else {
    AddS64(sp, sp, Operand(stack_space * kSystemPointerSize), r0);
  }

  return call_pc_offset;
}

int MacroAssembler::CallCFunction(ExternalReference function, int num_arguments,
                                  SetIsolateDataSlots set_isolate_data_slots,
                                  bool has_function_descriptor) {
  return CallCFunction(function, num_arguments, 0, set_isolate_data_slots,
                       has_function_descriptor);
}

int MacroAssembler::CallCFunction(Register function, int num_arguments,
                                  SetIsolateDataSlots set_isolate_data_slots,
                                  bool has_function_descriptor) {
  return CallCFunction(function, num_arguments, 0, set_isolate_data_slots,
                       has_function_descriptor);
}

void MacroAssembler::CheckPageFlag(
    Register object,
    Register scratch,  // scratch may be same register as object
    int mask, Condition cc, Label* condition_met) {
  DCHECK(cc == ne || cc == eq);
  DCHECK(scratch != r0);
  ClearRightImm(scratch, object, Operand(kPageSizeBits));
  LoadU64(scratch, MemOperand(scratch, MemoryChunkLayout::kFlagsOffset), r0);

  mov(r0, Operand(mask));
  and_(r0, scratch, r0, SetRC);

  if (cc == ne) {
    bne(condition_met, cr0);
  }
  if (cc == eq) {
    beq(condition_met, cr0);
  }
}

void MacroAssembler::SetRoundingMode(FPRoundingMode RN) { mtfsfi(7, RN); }

void MacroAssembler::ResetRoundingMode() {
  mtfsfi(7, kRoundToNearest);  // reset (default is kRoundToNearest)
}

////////////////////////////////////////////////////////////////////////////////
//
// New MacroAssembler Interfaces added for PPC
//
////////////////////////////////////////////////////////////////////////////////
void MacroAssembler::LoadIntLiteral(Register dst, int value) {
  mov(dst, Operand(value));
}

void MacroAssembler::LoadSmiLiteral(Register dst, Tagged<Smi> smi) {
  mov(dst, Operand(smi));
}

void MacroAssembler::LoadDoubleLiteral(DoubleRegister result,
                                       base::Double value, Register scratch) {
  if (V8_EMBEDDED_CONSTANT_POOL_BOOL && is_constant_pool_available() &&
      !(scratch == r0 && ConstantPoolAccessIsInOverflow())) {
    ConstantPoolEntry::Access access = ConstantPoolAddEntry(value);
    if (access == ConstantPoolEntry::OVERFLOWED) {
      addis(scratch, kConstantPoolRegister, Operand::Zero());
      lfd(result, MemOperand(scratch, 0));
    } else {
      lfd(result, MemOperand(kConstantPoolRegister, 0));
    }
    return;
  }

  // avoid gcc strict aliasing error using union cast
  union {
    uint64_t dval;
#if V8_TARGET_ARCH_PPC64
    intptr_t ival;
#else
    intptr_t ival[2];
#endif
  } litVal;

  litVal.dval = value.AsUint64();

#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mov(scratch, Operand(litVal.ival));
    mtfprd(result, scratch);
    return;
  }
#endif

  addi(sp, sp, Operand(-kDoubleSize));
#if V8_TARGET_ARCH_PPC64
  mov(scratch, Operand(litVal.ival));
  std(scratch, MemOperand(sp));
#else
  LoadIntLiteral(scratch, litVal.ival[0]);
  stw(scratch, MemOperand(sp, 0));
  LoadIntLiteral(scratch, litVal.ival[1]);
  stw(scratch, MemOperand(sp, 4));
#endif
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(result, MemOperand(sp, 0));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovIntToDouble(DoubleRegister dst, Register src,
                                    Register scratch) {
// sign-extend src to 64-bit
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mtfprwa(dst, src);
    return;
  }
#endif

  DCHECK(src != scratch);
  subi(sp, sp, Operand(kDoubleSize));
#if V8_TARGET_ARCH_PPC64
  extsw(scratch, src);
  std(scratch, MemOperand(sp, 0));
#else
  srawi(scratch, src, 31);
  stw(scratch, MemOperand(sp, Register::kExponentOffset));
  stw(src, MemOperand(sp, Register::kMantissaOffset));
#endif
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp, 0));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovUnsignedIntToDouble(DoubleRegister dst, Register src,
                                            Register scratch) {
// zero-extend src to 64-bit
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mtfprwz(dst, src);
    return;
  }
#endif

  DCHECK(src != scratch);
  subi(sp, sp, Operand(kDoubleSize));
#if V8_TARGET_ARCH_PPC64
  clrldi(scratch, src, Operand(32));
  std(scratch, MemOperand(sp, 0));
#else
  li(scratch, Operand::Zero());
  stw(scratch, MemOperand(sp, Register::kExponentOffset));
  stw(src, MemOperand(sp, Register::kMantissaOffset));
#endif
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp, 0));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovInt64ToDouble(DoubleRegister dst,
#if !V8_TARGET_ARCH_PPC64
                                      Register src_hi,
#endif
                                      Register src) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mtfprd(dst, src);
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
#if V8_TARGET_ARCH_PPC64
  std(src, MemOperand(sp, 0));
#else
  stw(src_hi, MemOperand(sp, Register::kExponentOffset));
  stw(src, MemOperand(sp, Register::kMantissaOffset));
#endif
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp, 0));
  addi(sp, sp, Operand(kDoubleSize));
}

#if V8_TARGET_ARCH_PPC64
void MacroAssembler::MovInt64ComponentsToDouble(DoubleRegister dst,
                                                Register src_hi,
                                                Register src_lo,
                                                Register scratch) {
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    ShiftLeftU64(scratch, src_hi, Operand(32));
    rldimi(scratch, src_lo, 0, 32);
    mtfprd(dst, scratch);
    return;
  }

  subi(sp, sp, Operand(kDoubleSize));
  stw(src_hi, MemOperand(sp, Register::kExponentOffset));
  stw(src_lo, MemOperand(sp, Register::kMantissaOffset));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp));
  addi(sp, sp, Operand(kDoubleSize));
}
#endif

void MacroAssembler::InsertDoubleLow(DoubleRegister dst, Register src,
                                     Register scratch) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mffprd(scratch, dst);
    rldimi(scratch, src, 0, 32);
    mtfprd(dst, scratch);
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
  stfd(dst, MemOperand(sp));
  stw(src, MemOperand(sp, Register::kMantissaOffset));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::InsertDoubleHigh(DoubleRegister dst, Register src,
                                      Register scratch) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mffprd(scratch, dst);
    rldimi(scratch, src, 32, 0);
    mtfprd(dst, scratch);
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
  stfd(dst, MemOperand(sp));
  stw(src, MemOperand(sp, Register::kExponentOffset));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfd(dst, MemOperand(sp));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovDoubleLowToInt(Register dst, DoubleRegister src) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mffprwz(dst, src);
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
  stfd(src, MemOperand(sp));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lwz(dst, MemOperand(sp, Register::kMantissaOffset));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovDoubleHighToInt(Register dst, DoubleRegister src) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mffprd(dst, src);
    srdi(dst, dst, Operand(32));
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
  stfd(src, MemOperand(sp));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lwz(dst, MemOperand(sp, Register::kExponentOffset));
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovDoubleToInt64(
#if !V8_TARGET_ARCH_PPC64
    Register dst_hi,
#endif
    Register dst, DoubleRegister src) {
#if V8_TARGET_ARCH_PPC64
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    mffprd(dst, src);
    return;
  }
#endif

  subi(sp, sp, Operand(kDoubleSize));
  stfd(src, MemOperand(sp));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
#if V8_TARGET_ARCH_PPC64
  ld(dst, MemOperand(sp, 0));
#else
  lwz(dst_hi, MemOperand(sp, Register::kExponentOffset));
  lwz(dst, MemOperand(sp, Register::kMantissaOffset));
#endif
  addi(sp, sp, Operand(kDoubleSize));
}

void MacroAssembler::MovIntToFloat(DoubleRegister dst, Register src,
                                   Register scratch) {
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    ShiftLeftU64(scratch, src, Operand(32));
    mtfprd(dst, scratch);
    xscvspdpn(dst, dst);
    return;
  }
  subi(sp, sp, Operand(kFloatSize));
  stw(src, MemOperand(sp, 0));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lfs(dst, MemOperand(sp, 0));
  addi(sp, sp, Operand(kFloatSize));
}

void MacroAssembler::MovFloatToInt(Register dst, DoubleRegister src,
                                   DoubleRegister scratch) {
  if (CpuFeatures::IsSupported(PPC_8_PLUS)) {
    xscvdpspn(scratch, src);
    mffprwz(dst, scratch);
    return;
  }
  subi(sp, sp, Operand(kFloatSize));
  stfs(src, MemOperand(sp, 0));
  nop(GROUP_ENDING_NOP);  // LHS/RAW optimization
  lwz(dst, MemOperand(sp, 0));
  addi(sp, sp, Operand(kFloatSize));
}

void MacroAssembler::AddS64(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  add(dst, src, value, s, r);
}

void MacroAssembler::AddS64(Register dst, Register src, const Operand& value,
                            Register scratch, OEBit s, RCBit r) {
  if (is_int16(value.immediate()) && s == LeaveOE && r == LeaveRC) {
    addi(dst, src, value);
  } else {
    mov(scratch, value);
    add(dst, src, scratch, s, r);
  }
}

void MacroAssembler::SubS64(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  sub(dst, src, value, s, r);
}

void MacroAssembler::SubS64(Register dst, Register src, const Operand& value,
                            Register scratch, OEBit s, RCBit r) {
  if (is_int16(value.immediate()) && s == LeaveOE && r == LeaveRC) {
    subi(dst, src, value);
  } else {
    mov(scratch, value);
    sub(dst, src, scratch, s, r);
  }
}

void MacroAssembler::AddS32(Register dst, Register src, Register value,
                            RCBit r) {
  AddS64(dst, src, value, LeaveOE, r);
  extsw(dst, dst, r);
}

void MacroAssembler::AddS32(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  AddS64(dst, src, value, scratch, LeaveOE, r);
  extsw(dst, dst, r);
}

void MacroAssembler::SubS32(Register dst, Register src, Register value,
                            RCBit r) {
  SubS64(dst, src, value, LeaveOE, r);
  extsw(dst, dst, r);
}

void MacroAssembler::SubS32(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  SubS64(dst, src, value, scratch, LeaveOE, r);
  extsw(dst, dst, r);
}

void MacroAssembler::MulS64(Register dst, Register src, const Operand& value,
                            Register scratch, OEBit s, RCBit r) {
  if (is_int16(value.immediate()) && s == LeaveOE && r == LeaveRC) {
    mulli(dst, src, value);
  } else {
    mov(scratch, value);
    mulld(dst, src, scratch, s, r);
  }
}

void MacroAssembler::MulS64(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  mulld(dst, src, value, s, r);
}

void MacroAssembler::MulS32(Register dst, Register src, const Operand& value,
                            Register scratch, OEBit s, RCBit r) {
  MulS64(dst, src, value, scratch, s, r);
  extsw(dst, dst, r);
}

void MacroAssembler::MulS32(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  MulS64(dst, src, value, s, r);
  extsw(dst, dst, r);
}

void MacroAssembler::DivS64(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  divd(dst, src, value, s, r);
}

void MacroAssembler::DivU64(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  divdu(dst, src, value, s, r);
}

void MacroAssembler::DivS32(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  divw(dst, src, value, s, r);
  extsw(dst, dst);
}
void MacroAssembler::DivU32(Register dst, Register src, Register value, OEBit s,
                            RCBit r) {
  divwu(dst, src, value, s, r);
  ZeroExtWord32(dst, dst);
}

void MacroAssembler::ModS64(Register dst, Register src, Register value) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    modsd(dst, src, value);
  } else {
    Register scratch = GetRegisterThatIsNotOneOf(dst, src, value);
    Push(scratch);
    divd(scratch, src, value);
    mulld(scratch, scratch, value);
    sub(dst, src, scratch);
    Pop(scratch);
  }
}

void MacroAssembler::ModU64(Register dst, Register src, Register value) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    modud(dst, src, value);
  } else {
    Register scratch = GetRegisterThatIsNotOneOf(dst, src, value);
    Push(scratch);
    divdu(scratch, src, value);
    mulld(scratch, scratch, value);
    sub(dst, src, scratch);
    Pop(scratch);
  }
}

void MacroAssembler::ModS32(Register dst, Register src, Register value) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    modsw(dst, src, value);
  } else {
    Register scratch = GetRegisterThatIsNotOneOf(dst, src, value);
    Push(scratch);
    divw(scratch, src, value);
    mullw(scratch, scratch, value);
    sub(dst, src, scratch);
    Pop(scratch);
  }
  extsw(dst, dst);
}
void MacroAssembler::ModU32(Register dst, Register src, Register value) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    moduw(dst, src, value);
  } else {
    Register scratch = GetRegisterThatIsNotOneOf(dst, src, value);
    Push(scratch);
    divwu(scratch, src, value);
    mullw(scratch, scratch, value);
    sub(dst, src, scratch);
    Pop(scratch);
  }
  ZeroExtWord32(dst, dst);
}

void MacroAssembler::AndU64(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  if (is_uint16(value.immediate()) && r == SetRC) {
    andi(dst, src, value);
  } else {
    mov(scratch, value);
    and_(dst, src, scratch, r);
  }
}

void MacroAssembler::AndU64(Register dst, Register src, Register value,
                            RCBit r) {
  and_(dst, src, value, r);
}

void MacroAssembler::OrU64(Register dst, Register src, const Operand& value,
                           Register scratch, RCBit r) {
  if (is_int16(value.immediate()) && r == LeaveRC) {
    ori(dst, src, value);
  } else {
    mov(scratch, value);
    orx(dst, src, scratch, r);
  }
}

void MacroAssembler::OrU64(Register dst, Register src, Register value,
                           RCBit r) {
  orx(dst, src, value, r);
}

void MacroAssembler::XorU64(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  if (is_int16(value.immediate()) && r == LeaveRC) {
    xori(dst, src, value);
  } else {
    mov(scratch, value);
    xor_(dst, src, scratch, r);
  }
}

void MacroAssembler::XorU64(Register dst, Register src, Register value,
                            RCBit r) {
  xor_(dst, src, value, r);
}

void MacroAssembler::AndU32(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  AndU64(dst, src, value, scratch, r);
  extsw(dst, dst, r);
}

void MacroAssembler::AndU32(Register dst, Register src, Register value,
                            RCBit r) {
  AndU64(dst, src, value, r);
  extsw(dst, dst, r);
}

void MacroAssembler::OrU32(Register dst, Register src, const Operand& value,
                           Register scratch, RCBit r) {
  OrU64(dst, src, value, scratch, r);
  extsw(dst, dst, r);
}

void MacroAssembler::OrU32(Register dst, Register src, Register value,
                           RCBit r) {
  OrU64(dst, src, value, r);
  extsw(dst, dst, r);
}

void MacroAssembler::XorU32(Register dst, Register src, const Operand& value,
                            Register scratch, RCBit r) {
  XorU64(dst, src, value, scratch, r);
  extsw(dst, dst, r);
}

void MacroAssembler::XorU32(Register dst, Register src, Register value,
                            RCBit r) {
  XorU64(dst, src, value, r);
  extsw(dst, dst, r);
}

void MacroAssembler::ShiftLeftU64(Register dst, Register src,
                                  const Operand& value, RCBit r) {
  sldi(dst, src, value, r);
}

void MacroAssembler::ShiftRightU64(Register dst, Register src,
                                   const Operand& value, RCBit r) {
  srdi(dst, src, value, r);
}

void MacroAssembler::ShiftRightS64(Register dst, Register src,
                                   const Operand& value, RCBit r) {
  sradi(dst, src, value.immediate(), r);
}

void MacroAssembler::ShiftLeftU32(Register dst, Register src,
                                  const Operand& value, RCBit r) {
  slwi(dst, src, value, r);
}

void MacroAssembler::ShiftRightU32(Register dst, Register src,
                                   const Operand& value, RCBit r) {
  srwi(dst, src, value, r);
}

void MacroAssembler::ShiftRightS32(Register dst, Register src,
                                   const Operand& value, RCBit r) {
  srawi(dst, src, value.immediate(), r);
}

void MacroAssembler::ShiftLeftU64(Register dst, Register src, Register value,
                                  RCBit r) {
  sld(dst, src, value, r);
}

void MacroAssembler::ShiftRightU64(Register dst, Register src, Register value,
                                   RCBit r) {
  srd(dst, src, value, r);
}

void MacroAssembler::ShiftRightS64(Register dst, Register src, Register value,
                                   RCBit r) {
  srad(dst, src, value, r);
}

void MacroAssembler::ShiftLeftU32(Register dst, Register src, Register value,
                                  RCBit r) {
  slw(dst, src, value, r);
}

void MacroAssembler::ShiftRightU32(Register dst, Register src, Register value,
                                   RCBit r) {
  srw(dst, src, value, r);
}

void MacroAssembler::ShiftRightS32(Register dst, Register src, Register value,
                                   RCBit r) {
  sraw(dst, src, value, r);
}

void MacroAssembler::CmpS64(Register src1, Register src2, CRegister cr) {
  cmp(src1, src2, cr);
}

void MacroAssembler::CmpS64(Register src1, const Operand& src2,
                            Register scratch, CRegister cr) {
  intptr_t value = src2.immediate();
  if (is_int16(value)) {
    cmpi(src1, src2, cr);
  } else {
    mov(scratch, src2);
    CmpS64(src1, scratch, cr);
  }
}

void MacroAssembler::CmpU64(Register src1, const Operand& src2,
                            Register scratch, CRegister cr) {
  intptr_t value = src2.immediate();
  if (is_uint16(value)) {
    cmpli(src1, src2, cr);
  } else {
    mov(scratch, src2);
    CmpU64(src1, scratch, cr);
  }
}

void MacroAssembler::CmpU64(Register src1, Register src2, CRegister cr) {
  cmpl(src1, src2, cr);
}

void MacroAssembler::CmpS32(Register src1, const Operand& src2,
                            Register scratch, CRegister cr) {
  intptr_t value = src2.immediate();
  if (is_int16(value)) {
    cmpwi(src1, src2, cr);
  } else {
    mov(scratch, src2);
    CmpS32(src1, scratch, cr);
  }
}

void MacroAssembler::CmpS32(Register src1, Register src2, CRegister cr) {
  cmpw(src1, src2, cr);
}

void MacroAssembler::CmpU32(Register src1, const Operand& src2,
                            Register scratch, CRegister cr) {
  intptr_t value = src2.immediate();
  if (is_uint16(value)) {
    cmplwi(src1, src2, cr);
  } else {
    mov(scratch, src2);
    cmplw(src1, scratch, cr);
  }
}

void MacroAssembler::CmpU32(Register src1, Register src2, CRegister cr) {
  cmplw(src1, src2, cr);
}

void MacroAssembler::AddF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fadd(dst, lhs, rhs, r);
}

void MacroAssembler::SubF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fsub(dst, lhs, rhs, r);
}

void MacroAssembler::MulF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fmul(dst, lhs, rhs, r);
}

void MacroAssembler::DivF64(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fdiv(dst, lhs, rhs, r);
}

void MacroAssembler::AddF32(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fadd(dst, lhs, rhs, r);
  frsp(dst, dst, r);
}

void MacroAssembler::SubF32(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fsub(dst, lhs, rhs, r);
  frsp(dst, dst, r);
}

void MacroAssembler::MulF32(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fmul(dst, lhs, rhs, r);
  frsp(dst, dst, r);
}

void MacroAssembler::DivF32(DoubleRegister dst, DoubleRegister lhs,
                            DoubleRegister rhs, RCBit r) {
  fdiv(dst, lhs, rhs, r);
  frsp(dst, dst, r);
}

void MacroAssembler::CopySignF64(DoubleRegister dst, DoubleRegister lhs,
                                 DoubleRegister rhs, RCBit r) {
  fcpsgn(dst, rhs, lhs, r);
}

void MacroAssembler::CmpSmiLiteral(Register src1, Tagged<Smi> smi,
                                   Register scratch, CRegister cr) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  CmpS32(src1, Operand(smi), scratch, cr);
#else
  LoadSmiLiteral(scratch, smi);
  CmpS64(src1, scratch, cr);
#endif
}

void MacroAssembler::CmplSmiLiteral(Register src1, Tagged<Smi> smi,
                                    Register scratch, CRegister cr) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  CmpU64(src1, Operand(smi), scratch, cr);
#else
  LoadSmiLiteral(scratch, smi);
  CmpU64(src1, scratch, cr);
#endif
}

void MacroAssembler::AddSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                                   Register scratch) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  AddS64(dst, src, Operand(smi.ptr()), scratch);
#else
  LoadSmiLiteral(scratch, smi);
  add(dst, src, scratch);
#endif
}

void MacroAssembler::SubSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                                   Register scratch) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  AddS64(dst, src, Operand(-(static_cast<intptr_t>(smi.ptr()))), scratch);
#else
  LoadSmiLiteral(scratch, smi);
  sub(dst, src, scratch);
#endif
}

void MacroAssembler::AndSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                                   Register scratch, RCBit rc) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  AndU64(dst, src, Operand(smi), scratch, rc);
#else
  LoadSmiLiteral(scratch, smi);
  and_(dst, src, scratch, rc);
#endif
}

#define GenerateMemoryOperation(reg, mem, ri_op, rr_op) \
  {                                                     \
    int64_t offset = mem.offset();                      \
                                                        \
    if (mem.rb() == no_reg) {                           \
      if (!is_int16(offset)) {                          \
        /* cannot use d-form */                         \
        CHECK_NE(scratch, no_reg);                      \
        mov(scratch, Operand(offset));                  \
        rr_op(reg, MemOperand(mem.ra(), scratch));      \
      } else {                                          \
        ri_op(reg, mem);                                \
      }                                                 \
    } else {                                            \
      if (offset == 0) {                                \
        rr_op(reg, mem);                                \
      } else if (is_int16(offset)) {                    \
        CHECK_NE(scratch, no_reg);                      \
        addi(scratch, mem.rb(), Operand(offset));       \
        rr_op(reg, MemOperand(mem.ra(), scratch));      \
      } else {                                          \
        CHECK_NE(scratch, no_reg);                      \
        mov(scratch, Operand(offset));                  \
        add(scratch, scratch, mem.rb());                \
        rr_op(reg, MemOperand(mem.ra(), scratch));      \
      }                                                 \
    }                                                   \
  }

#define GenerateMemoryOperationRR(reg, mem, op)                \
  {                                                            \
    if (mem.offset() == 0) {                                   \
      if (mem.rb() != no_reg)                                  \
        op(reg, mem);                                          \
      else                                                     \
        op(reg, MemOperand(r0, mem.ra()));                     \
    } else if (is_int16(mem.offset())) {                       \
      if (mem.rb() != no_reg)                                  \
        addi(scratch, mem.rb(), Operand(mem.offset()));        \
      else                                                     \
        mov(scratch, Operand(mem.offset()));                   \
      op(reg, MemOperand(mem.ra(), scratch));                  \
    } else {                                                   \
      mov(scratch, Operand(mem.offset()));                     \
      if (mem.rb() != no_reg) add(scratch, scratch, mem.rb()); \
      op(reg, MemOperand(mem.ra(), scratch));                  \
    }                                                          \
  }

#define GenerateMemoryOperationPrefixed(reg, mem, ri_op, rip_op, rr_op)       \
  {                                                                           \
    int64_t offset = mem.offset();                                            \
                                                                              \
    if (mem.rb() == no_reg) {                                                 \
      if (is_int16(offset)) {                                                 \
        ri_op(reg, mem);                                                      \
      } else if (is_int34(offset) && CpuFeatures::IsSupported(PPC_10_PLUS)) { \
        rip_op(reg, mem);                                                     \
      } else {                                                                \
        /* cannot use d-form */                                               \
        CHECK_NE(scratch, no_reg);                                            \
        mov(scratch, Operand(offset));                                        \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      }                                                                       \
    } else {                                                                  \
      if (offset == 0) {                                                      \
        rr_op(reg, mem);                                                      \
      } else if (is_int16(offset)) {                                          \
        CHECK_NE(scratch, no_reg);                                            \
        addi(scratch, mem.rb(), Operand(offset));                             \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      } else {                                                                \
        CHECK_NE(scratch, no_reg);                                            \
        mov(scratch, Operand(offset));                                        \
        add(scratch, scratch, mem.rb());                                      \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      }                                                                       \
    }                                                                         \
  }

#define GenerateMemoryOperationWithAlign(reg, mem, ri_op, rr_op) \
  {                                                              \
    int64_t offset = mem.offset();                               \
    int misaligned = (offset & 3);                               \
                                                                 \
    if (mem.rb() == no_reg) {                                    \
      if (!is_int16(offset) || misaligned) {                     \
        /* cannot use d-form */                                  \
        CHECK_NE(scratch, no_reg);                               \
        mov(scratch, Operand(offset));                           \
        rr_op(reg, MemOperand(mem.ra(), scratch));               \
      } else {                                                   \
        ri_op(reg, mem);                                         \
      }                                                          \
    } else {                                                     \
      if (offset == 0) {                                         \
        rr_op(reg, mem);                                         \
      } else if (is_int16(offset)) {                             \
        CHECK_NE(scratch, no_reg);                               \
        addi(scratch, mem.rb(), Operand(offset));                \
        rr_op(reg, MemOperand(mem.ra(), scratch));               \
      } else {                                                   \
        CHECK_NE(scratch, no_reg);                               \
        mov(scratch, Operand(offset));                           \
        add(scratch, scratch, mem.rb());                         \
        rr_op(reg, MemOperand(mem.ra(), scratch));               \
      }                                                          \
    }                                                            \
  }

#define GenerateMemoryOperationWithAlignPrefixed(reg, mem, ri_op, rip_op,     \
                                                 rr_op)                       \
  {                                                                           \
    int64_t offset = mem.offset();                                            \
    int misaligned = (offset & 3);                                            \
                                                                              \
    if (mem.rb() == no_reg) {                                                 \
      if (is_int16(offset) && !misaligned) {                                  \
        ri_op(reg, mem);                                                      \
      } else if (is_int34(offset) && CpuFeatures::IsSupported(PPC_10_PLUS)) { \
        rip_op(reg, mem);                                                     \
      } else {                                                                \
        /* cannot use d-form */                                               \
        CHECK_NE(scratch, no_reg);                                            \
        mov(scratch, Operand(offset));                                        \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      }                                                                       \
    } else {                                                                  \
      if (offset == 0) {                                                      \
        rr_op(reg, mem);                                                      \
      } else if (is_int16(offset)) {                                          \
        CHECK_NE(scratch, no_reg);                                            \
        addi(scratch, mem.rb(), Operand(offset));                             \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      } else {                                                                \
        CHECK_NE(scratch, no_reg);                                            \
        mov(scratch, Operand(offset));                                        \
        add(scratch, scratch, mem.rb());                                      \
        rr_op(reg, MemOperand(mem.ra(), scratch));                            \
      }                                                                       \
    }                                                                         \
  }

#define MEM_OP_WITH_ALIGN_LIST(V) \
  V(LoadU64WithUpdate, ldu, ldux) \
  V(StoreU64WithUpdate, stdu, stdux)

#define MEM_OP_WITH_ALIGN_FUNCTION(name, ri_op, rr_op)           \
  void MacroAssembler::name(Register reg, const MemOperand& mem, \
                            Register scratch) {                  \
    GenerateMemoryOperationWithAlign(reg, mem, ri_op, rr_op);    \
  }
MEM_OP_WITH_ALIGN_LIST(MEM_OP_WITH_ALIGN_FUNCTION)
#undef MEM_OP_WITH_ALIGN_LIST
#undef MEM_OP_WITH_ALIGN_FUNCTION

#define MEM_OP_WITH_ALIGN_PREFIXED_LIST(V) \
  V(LoadS32, lwa, plwa, lwax)              \
  V(LoadU64, ld, pld, ldx)                 \
  V(StoreU64, std, pstd, stdx)

#define MEM_OP_WITH_ALIGN_PREFIXED_FUNCTION(name, ri_op, rip_op, rr_op)       \
  void MacroAssembler::name(Register reg, const MemOperand& mem,              \
                            Register scratch) {                               \
    GenerateMemoryOperationWithAlignPrefixed(reg, mem, ri_op, rip_op, rr_op); \
  }
MEM_OP_WITH_ALIGN_PREFIXED_LIST(MEM_OP_WITH_ALIGN_PREFIXED_FUNCTION)
#undef MEM_OP_WITH_ALIGN_PREFIXED_LIST
#undef MEM_OP_WITH_ALIGN_PREFIXED_FUNCTION

#define MEM_OP_LIST(V)                                 \
  V(LoadF64WithUpdate, DoubleRegister, lfdu, lfdux)    \
  V(LoadF32WithUpdate, DoubleRegister, lfsu, lfsux)    \
  V(StoreF64WithUpdate, DoubleRegister, stfdu, stfdux) \
  V(StoreF32WithUpdate, DoubleRegister, stfsu, stfsux)

#define MEM_OP_FUNCTION(name, result_t, ri_op, rr_op)            \
  void MacroAssembler::name(result_t reg, const MemOperand& mem, \
                            Register scratch) {                  \
    GenerateMemoryOperation(reg, mem, ri_op, rr_op);             \
  }
MEM_OP_LIST(MEM_OP_FUNCTION)
#undef MEM_OP_LIST
#undef MEM_OP_FUNCTION

#define MEM_OP_PREFIXED_LIST(V)                   \
  V(LoadU32, Register, lwz, plwz, lwzx)           \
  V(LoadS16, Register, lha, plha, lhax)           \
  V(LoadU16, Register, lhz, plhz, lhzx)           \
  V(LoadU8, Register, lbz, plbz, lbzx)            \
  V(StoreU32, Register, stw, pstw, stwx)          \
  V(StoreU16, Register, sth, psth, sthx)          \
  V(StoreU8, Register, stb, pstb, stbx)           \
  V(LoadF64, DoubleRegister, lfd, plfd, lfdx)     \
  V(LoadF32, DoubleRegister, lfs, plfs, lfsx)     \
  V(StoreF64, DoubleRegister, stfd, pstfd, stfdx) \
  V(StoreF32, DoubleRegister, stfs, pstfs, stfsx)

#define MEM_OP_PREFIXED_FUNCTION(name, result_t, ri_op, rip_op, rr_op) \
  void MacroAssembler::name(result_t reg, const MemOperand& mem,       \
                            Register scratch) {                        \
    GenerateMemoryOperationPrefixed(reg, mem, ri_op, rip_op, rr_op);   \
  }
MEM_OP_PREFIXED_LIST(MEM_OP_PREFIXED_FUNCTION)
#undef MEM_OP_PREFIXED_LIST
#undef MEM_OP_PREFIXED_FUNCTION

#define MEM_OP_SIMD_LIST(V)      \
  V(LoadSimd128, lxvx)           \
  V(StoreSimd128, stxvx)         \
  V(LoadSimd128Uint64, lxsdx)    \
  V(LoadSimd128Uint32, lxsiwzx)  \
  V(LoadSimd128Uint16, lxsihzx)  \
  V(LoadSimd128Uint8, lxsibzx)   \
  V(StoreSimd128Uint64, stxsdx)  \
  V(StoreSimd128Uint32, stxsiwx) \
  V(StoreSimd128Uint16, stxsihx) \
  V(StoreSimd128Uint8, stxsibx)

#define MEM_OP_SIMD_FUNCTION(name, rr_op)                               \
  void MacroAssembler::name(Simd128Register reg, const MemOperand& mem, \
                            Register scratch) {                         \
    GenerateMemoryOperationRR(reg, mem, rr_op);                         \
  }
MEM_OP_SIMD_LIST(MEM_OP_SIMD_FUNCTION)
#undef MEM_OP_SIMD_LIST
#undef MEM_OP_SIMD_FUNCTION

void MacroAssembler::LoadS8(Register dst, const MemOperand& mem,
                            Register scratch) {
  LoadU8(dst, mem, scratch);
  extsb(dst, dst);
}

#define MEM_LE_OP_LIST(V) \
  V(LoadU64, ldbrx)       \
  V(LoadU32, lwbrx)       \
  V(LoadU16, lhbrx)       \
  V(StoreU64, stdbrx)     \
  V(StoreU32, stwbrx)     \
  V(StoreU16, sthbrx)

#ifdef V8_TARGET_BIG_ENDIAN
#define MEM_LE_OP_FUNCTION(name, op)                                 \
  void MacroAssembler::name##LE(Register reg, const MemOperand& mem, \
                                Register scratch) {                  \
    GenerateMemoryOperationRR(reg, mem, op);                         \
  }
#else
#define MEM_LE_OP_FUNCTION(name, op)                                 \
  void MacroAssembler::name##LE(Register reg, const MemOperand& mem, \
                                Register scratch) {                  \
    name(reg, mem, scratch);                                         \
  }
#endif

MEM_LE_OP_LIST(MEM_LE_OP_FUNCTION)
#undef MEM_LE_OP_FUNCTION
#undef MEM_LE_OP_LIST

void MacroAssembler::LoadS32LE(Register dst, const MemOperand& mem,
                               Register scratch) {
#ifdef V8_TARGET_BIG_ENDIAN
  LoadU32LE(dst, mem, scratch);
  extsw(dst, dst);
#else
  LoadS32(dst, mem, scratch);
#endif
}

void MacroAssembler::LoadS16LE(Register dst, const MemOperand& mem,
                               Register scratch) {
#ifdef V8_TARGET_BIG_ENDIAN
  LoadU16LE(dst, mem, scratch);
  extsh(dst, dst);
#else
  LoadS16(dst, mem, scratch);
#endif
}

void MacroAssembler::LoadF64LE(DoubleRegister dst, const MemOperand& mem,
                               Register scratch, Register scratch2) {
#ifdef V8_TARGET_BIG_ENDIAN
  LoadU64LE(scratch, mem, scratch2);
  push(scratch);
  LoadF64(dst, MemOperand(sp), scratch2);
  pop(scratch);
#else
  LoadF64(dst, mem, scratch);
#endif
}

void MacroAssembler::LoadF32LE(DoubleRegister dst, const MemOperand& mem,
                               Register scratch, Register scratch2) {
#ifdef V8_TARGET_BIG_ENDIAN
  LoadU32LE(scratch, mem, scratch2);
  push(scratch);
  LoadF32(dst, MemOperand(sp, 4), scratch2);
  pop(scratch);
#else
  LoadF32(dst, mem, scratch);
#endif
}

void MacroAssembler::StoreF64LE(DoubleRegister dst, const MemOperand& mem,
                                Register scratch, Register scratch2) {
#ifdef V8_TARGET_BIG_ENDIAN
  StoreF64(dst, mem, scratch2);
  LoadU64(scratch, mem, scratch2);
  StoreU64LE(scratch, mem, scratch2);
#else
  StoreF64(dst, mem, scratch);
#endif
}

void MacroAssembler::StoreF32LE(DoubleRegister dst, const MemOperand& mem,
                                Register scratch, Register scratch2) {
#ifdef V8_TARGET_BIG_ENDIAN
  StoreF32(dst, mem, scratch2);
  LoadU32(scratch, mem, scratch2);
  StoreU32LE(scratch, mem, scratch2);
#else
  StoreF32(dst, mem, scratch);
#endif
}

// Simd Support.
#define SIMD_BINOP_LIST(V)         \
  V(F64x2Add, xvadddp)             \
  V(F64x2Sub, xvsubdp)             \
  V(F64x2Mul, xvmuldp)             \
  V(F64x2Div, xvdivdp)             \
  V(F64x2Eq, xvcmpeqdp)            \
  V(F32x4Add, vaddfp)              \
  V(F32x4Sub, vsubfp)              \
  V(F32x4Mul, xvmulsp)             \
  V(F32x4Div, xvdivsp)             \
  V(F32x4Min, vminfp)              \
  V(F32x4Max, vmaxfp)              \
  V(F32x4Eq, xvcmpeqsp)            \
  V(I64x2Add, vaddudm)             \
  V(I64x2Sub, vsubudm)             \
  V(I64x2Eq, vcmpequd)             \
  V(I64x2GtS, vcmpgtsd)            \
  V(I32x4Add, vadduwm)             \
  V(I32x4Sub, vsubuwm)             \
  V(I32x4Mul, vmuluwm)             \
  V(I32x4MinS, vminsw)             \
  V(I32x4MinU, vminuw)             \
  V(I32x4MaxS, vmaxsw)             \
  V(I32x4MaxU, vmaxuw)             \
  V(I32x4Eq, vcmpequw)             \
  V(I32x4GtS, vcmpgtsw)            \
  V(I32x4GtU, vcmpgtuw)            \
  V(I16x8Add, vadduhm)             \
  V(I16x8Sub, vsubuhm)             \
  V(I16x8MinS, vminsh)             \
  V(I16x8MinU, vminuh)             \
  V(I16x8MaxS, vmaxsh)             \
  V(I16x8MaxU, vmaxuh)             \
  V(I16x8Eq, vcmpequh)             \
  V(I16x8GtS, vcmpgtsh)            \
  V(I16x8GtU, vcmpgtuh)            \
  V(I16x8AddSatS, vaddshs)         \
  V(I16x8SubSatS, vsubshs)         \
  V(I16x8AddSatU, vadduhs)         \
  V(I16x8SubSatU, vsubuhs)         \
  V(I16x8RoundingAverageU, vavguh) \
  V(I8x16Add, vaddubm)             \
  V(I8x16Sub, vsububm)             \
  V(I8x16MinS, vminsb)             \
  V(I8x16MinU, vminub)             \
  V(I8x16MaxS, vmaxsb)             \
  V(I8x16MaxU, vmaxub)             \
  V(I8x16Eq, vcmpequb)             \
  V(I8x16GtS, vcmpgtsb)            \
  V(I8x16GtU, vcmpgtub)            \
  V(I8x16AddSatS, vaddsbs)         \
  V(I8x16SubSatS, vsubsbs)         \
  V(I8x16AddSatU, vaddubs)         \
  V(I8x16SubSatU, vsububs)         \
  V(I8x16RoundingAverageU, vavgub) \
  V(S128And, vand)                 \
  V(S128Or, vor)                   \
  V(S128Xor, vxor)                 \
  V(S128AndNot, vandc)

#define EMIT_SIMD_BINOP(name, op)                                      \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src1, \
                            Simd128Register src2) {                    \
    op(dst, src1, src2);                                               \
  }
SIMD_BINOP_LIST(EMIT_SIMD_BINOP)
#undef EMIT_SIMD_BINOP
#undef SIMD_BINOP_LIST

#define SIMD_SHIFT_LIST(V) \
  V(I64x2Shl, vsld)        \
  V(I64x2ShrS, vsrad)      \
  V(I64x2ShrU, vsrd)       \
  V(I32x4Shl, vslw)        \
  V(I32x4ShrS, vsraw)      \
  V(I32x4ShrU, vsrw)       \
  V(I16x8Shl, vslh)        \
  V(I16x8ShrS, vsrah)      \
  V(I16x8ShrU, vsrh)       \
  V(I8x16Shl, vslb)        \
  V(I8x16ShrS, vsrab)      \
  V(I8x16ShrU, vsrb)

#define EMIT_SIMD_SHIFT(name, op)                                      \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src1, \
                            Register src2, Simd128Register scratch) {  \
    mtvsrd(scratch, src2);                                             \
    vspltb(scratch, scratch, Operand(7));                              \
    op(dst, src1, scratch);                                            \
  }                                                                    \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src1, \
                            const Operand& src2, Register scratch1,    \
                            Simd128Register scratch2) {                \
    mov(scratch1, src2);                                               \
    name(dst, src1, scratch1, scratch2);                               \
  }
SIMD_SHIFT_LIST(EMIT_SIMD_SHIFT)
#undef EMIT_SIMD_SHIFT
#undef SIMD_SHIFT_LIST

#define SIMD_UNOP_LIST(V)            \
  V(F64x2Abs, xvabsdp)               \
  V(F64x2Neg, xvnegdp)               \
  V(F64x2Sqrt, xvsqrtdp)             \
  V(F64x2Ceil, xvrdpip)              \
  V(F64x2Floor, xvrdpim)             \
  V(F64x2Trunc, xvrdpiz)             \
  V(F32x4Abs, xvabssp)               \
  V(F32x4Neg, xvnegsp)               \
  V(F32x4Sqrt, xvsqrtsp)             \
  V(F32x4Ceil, xvrspip)              \
  V(F32x4Floor, xvrspim)             \
  V(F32x4Trunc, xvrspiz)             \
  V(F32x4SConvertI32x4, xvcvsxwsp)   \
  V(F32x4UConvertI32x4, xvcvuxwsp)   \
  V(I64x2Neg, vnegd)                 \
  V(I64x2SConvertI32x4Low, vupklsw)  \
  V(I64x2SConvertI32x4High, vupkhsw) \
  V(I32x4Neg, vnegw)                 \
  V(I32x4SConvertI16x8Low, vupklsh)  \
  V(I32x4SConvertI16x8High, vupkhsh) \
  V(I32x4UConvertF32x4, xvcvspuxws)  \
  V(I16x8SConvertI8x16Low, vupklsb)  \
  V(I16x8SConvertI8x16High, vupkhsb) \
  V(I8x16Popcnt, vpopcntb)

#define EMIT_SIMD_UNOP(name, op)                                        \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src) { \
    op(dst, src);                                                       \
  }
SIMD_UNOP_LIST(EMIT_SIMD_UNOP)
#undef EMIT_SIMD_UNOP
#undef SIMD_UNOP_LIST

#define EXT_MUL(dst_even, dst_odd, mul_even, mul_odd) \
  mul_even(dst_even, src1, src2);                     \
  mul_odd(dst_odd, src1, src2);
#define SIMD_EXT_MUL_LIST(V)                         \
  V(I32x4ExtMulLowI16x8S, vmulesh, vmulosh, vmrglw)  \
  V(I32x4ExtMulHighI16x8S, vmulesh, vmulosh, vmrghw) \
  V(I32x4ExtMulLowI16x8U, vmuleuh, vmulouh, vmrglw)  \
  V(I32x4ExtMulHighI16x8U, vmuleuh, vmulouh, vmrghw) \
  V(I16x8ExtMulLowI8x16S, vmulesb, vmulosb, vmrglh)  \
  V(I16x8ExtMulHighI8x16S, vmulesb, vmulosb, vmrghh) \
  V(I16x8ExtMulLowI8x16U, vmuleub, vmuloub, vmrglh)  \
  V(I16x8ExtMulHighI8x16U, vmuleub, vmuloub, vmrghh)

#define EMIT_SIMD_EXT_MUL(name, mul_even, mul_odd, merge)                    \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src1,       \
                            Simd128Register src2, Simd128Register scratch) { \
    EXT_MUL(scratch, dst, mul_even, mul_odd)                                 \
    merge(dst, scratch, dst);                                                \
  }
SIMD_EXT_MUL_LIST(EMIT_SIMD_EXT_MUL)
#undef EMIT_SIMD_EXT_MUL
#undef SIMD_EXT_MUL_LIST

#define SIMD_ALL_TRUE_LIST(V) \
  V(I64x2AllTrue, vcmpgtud)   \
  V(I32x4AllTrue, vcmpgtuw)   \
  V(I16x8AllTrue, vcmpgtuh)   \
  V(I8x16AllTrue, vcmpgtub)

#define EMIT_SIMD_ALL_TRUE(name, op)                              \
  void MacroAssembler::name(Register dst, Simd128Register src,    \
                            Register scratch1, Register scratch2, \
                            Simd128Register scratch3) {           \
    constexpr uint8_t fxm = 0x2; /* field mask. */                \
    constexpr int bit_number = 24;                                \
    li(scratch1, Operand(0));                                     \
    li(scratch2, Operand(1));                                     \
    /* Check if all lanes > 0, if not then return false.*/        \
    vxor(scratch3, scratch3, scratch3);                           \
    mtcrf(scratch1, fxm); /* Clear cr6.*/                         \
    op(scratch3, src, scratch3, SetRC);                           \
    isel(dst, scratch2, scratch1, bit_number);                    \
  }
SIMD_ALL_TRUE_LIST(EMIT_SIMD_ALL_TRUE)
#undef EMIT_SIMD_ALL_TRUE
#undef SIMD_ALL_TRUE_LIST

#define SIMD_BITMASK_LIST(V)                      \
  V(I64x2BitMask, vextractdm, 0x8080808080800040) \
  V(I32x4BitMask, vextractwm, 0x8080808000204060) \
  V(I16x8BitMask, vextracthm, 0x10203040506070)

#define EMIT_SIMD_BITMASK(name, op, indicies)                              \
  void MacroAssembler::name(Register dst, Simd128Register src,             \
                            Register scratch1, Simd128Register scratch2) { \
    if (CpuFeatures::IsSupported(PPC_10_PLUS)) {                           \
      op(dst, src);                                                        \
    } else {                                                               \
      mov(scratch1, Operand(indicies)); /* Select 0 for the high bits. */  \
      mtvsrd(scratch2, scratch1);                                          \
      vbpermq(scratch2, src, scratch2);                                    \
      vextractub(scratch2, scratch2, Operand(6));                          \
      mfvsrd(dst, scratch2);                                               \
    }                                                                      \
  }
SIMD_BITMASK_LIST(EMIT_SIMD_BITMASK)
#undef EMIT_SIMD_BITMASK
#undef SIMD_BITMASK_LIST

#define SIMD_QFM_LIST(V)   \
  V(F64x2Qfma, xvmaddmdp)  \
  V(F64x2Qfms, xvnmsubmdp) \
  V(F32x4Qfma, xvmaddmsp)  \
  V(F32x4Qfms, xvnmsubmsp)

#define EMIT_SIMD_QFM(name, op)                                         \
  void MacroAssembler::name(Simd128Register dst, Simd128Register src1,  \
                            Simd128Register src2, Simd128Register src3, \
                            Simd128Register scratch) {                  \
    Simd128Register dest = dst;                                         \
    if (dst != src1) {                                                  \
      vor(scratch, src1, src1);                                         \
      dest = scratch;                                                   \
    }                                                                   \
    op(dest, src2, src3);                                               \
    if (dest != dst) {                                                  \
      vor(dst, dest, dest);                                             \
    }                                                                   \
  }
SIMD_QFM_LIST(EMIT_SIMD_QFM)
#undef EMIT_SIMD_QFM
#undef SIMD_QFM_LIST

void MacroAssembler::I64x2ExtMulLowI32x4S(Simd128Register dst,
                                          Simd128Register src1,
                                          Simd128Register src2,
                                          Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  EXT_MUL(scratch, dst, vmulesw, vmulosw)
  vextractd(scratch, scratch, Operand(1 * lane_width_in_bytes));
  vinsertd(dst, scratch, Operand(0));
}

void MacroAssembler::I64x2ExtMulHighI32x4S(Simd128Register dst,
                                           Simd128Register src1,
                                           Simd128Register src2,
                                           Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  EXT_MUL(scratch, dst, vmulesw, vmulosw)
  vinsertd(scratch, dst, Operand(1 * lane_width_in_bytes));
  vor(dst, scratch, scratch);
}

void MacroAssembler::I64x2ExtMulLowI32x4U(Simd128Register dst,
                                          Simd128Register src1,
                                          Simd128Register src2,
                                          Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  EXT_MUL(scratch, dst, vmuleuw, vmulouw)
  vextractd(scratch, scratch, Operand(1 * lane_width_in_bytes));
  vinsertd(dst, scratch, Operand(0));
}

void MacroAssembler::I64x2ExtMulHighI32x4U(Simd128Register dst,
                                           Simd128Register src1,
                                           Simd128Register src2,
                                           Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  EXT_MUL(scratch, dst, vmuleuw, vmulouw)
  vinsertd(scratch, dst, Operand(1 * lane_width_in_bytes));
  vor(dst, scratch, scratch);
}
#undef EXT_MUL

void MacroAssembler::LoadSimd128LE(Simd128Register dst, const MemOperand& mem,
                                   Register scratch) {
#ifdef V8_TARGET_BIG_ENDIAN
  LoadSimd128(dst, mem, scratch);
  xxbrq(dst, dst);
#else
  LoadSimd128(dst, mem, scratch);
#endif
}

void MacroAssembler::StoreSimd128LE(Simd128Register src, const MemOperand& mem,
                                    Register scratch1,
                                    Simd128Register scratch2) {
#ifdef V8_TARGET_BIG_ENDIAN
  xxbrq(scratch2, src);
  StoreSimd128(scratch2, mem, scratch1);
#else
  StoreSimd128(src, mem, scratch1);
#endif
}

void MacroAssembler::F64x2Splat(Simd128Register dst, DoubleRegister src,
                                Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  MovDoubleToInt64(scratch, src);
  mtvsrd(dst, scratch);
  vinsertd(dst, dst, Operand(1 * lane_width_in_bytes));
}

void MacroAssembler::F32x4Splat(Simd128Register dst, DoubleRegister src,
                                DoubleRegister scratch1, Register scratch2) {
  MovFloatToInt(scratch2, src, scratch1);
  mtvsrd(dst, scratch2);
  vspltw(dst, dst, Operand(1));
}

void MacroAssembler::I64x2Splat(Simd128Register dst, Register src) {
  constexpr int lane_width_in_bytes = 8;
  mtvsrd(dst, src);
  vinsertd(dst, dst, Operand(1 * lane_width_in_bytes));
}

void MacroAssembler::I32x4Splat(Simd128Register dst, Register src) {
  mtvsrd(dst, src);
  vspltw(dst, dst, Operand(1));
}

void MacroAssembler::I16x8Splat(Simd128Register dst, Register src) {
  mtvsrd(dst, src);
  vsplth(dst, dst, Operand(3));
}

void MacroAssembler::I8x16Splat(Simd128Register dst, Register src) {
  mtvsrd(dst, src);
  vspltb(dst, dst, Operand(7));
}

void MacroAssembler::F64x2ExtractLane(DoubleRegister dst, Simd128Register src,
                                      uint8_t imm_lane_idx,
                                      Simd128Register scratch1,
                                      Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  vextractd(scratch1, src, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  mfvsrd(scratch2, scratch1);
  MovInt64ToDouble(dst, scratch2);
}

void MacroAssembler::F32x4ExtractLane(DoubleRegister dst, Simd128Register src,
                                      uint8_t imm_lane_idx,
                                      Simd128Register scratch1,
                                      Register scratch2, Register scratch3) {
  constexpr int lane_width_in_bytes = 4;
  vextractuw(scratch1, src, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  mfvsrd(scratch2, scratch1);
  MovIntToFloat(dst, scratch2, scratch3);
}

void MacroAssembler::I64x2ExtractLane(Register dst, Simd128Register src,
                                      uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  vextractd(scratch, src, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  mfvsrd(dst, scratch);
}

void MacroAssembler::I32x4ExtractLane(Register dst, Simd128Register src,
                                      uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 4;
  vextractuw(scratch, src, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  mfvsrd(dst, scratch);
}

void MacroAssembler::I16x8ExtractLaneU(Register dst, Simd128Register src,
                                       uint8_t imm_lane_idx,
                                       Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 2;
  vextractuh(scratch, src, Operand((7 - imm_lane_idx) * lane_width_in_bytes));
  mfvsrd(dst, scratch);
}

void MacroAssembler::I16x8ExtractLaneS(Register dst, Simd128Register src,
                                       uint8_t imm_lane_idx,
                                       Simd128Register scratch) {
  I16x8ExtractLaneU(dst, src, imm_lane_idx, scratch);
  extsh(dst, dst);
}

void MacroAssembler::I8x16ExtractLaneU(Register dst, Simd128Register src,
                                       uint8_t imm_lane_idx,
                                       Simd128Register scratch) {
  vextractub(scratch, src, Operand(15 - imm_lane_idx));
  mfvsrd(dst, scratch);
}

void MacroAssembler::I8x16ExtractLaneS(Register dst, Simd128Register src,
                                       uint8_t imm_lane_idx,
                                       Simd128Register scratch) {
  I8x16ExtractLaneU(dst, src, imm_lane_idx, scratch);
  extsb(dst, dst);
}

void MacroAssembler::F64x2ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      DoubleRegister src2, uint8_t imm_lane_idx,
                                      Register scratch1,
                                      Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  MovDoubleToInt64(scratch1, src2);
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vinsd(dst, scratch1, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  } else {
    mtvsrd(scratch2, scratch1);
    vinsertd(dst, scratch2, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  }
}

void MacroAssembler::F32x4ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      DoubleRegister src2, uint8_t imm_lane_idx,
                                      Register scratch1,
                                      DoubleRegister scratch2,
                                      Simd128Register scratch3) {
  constexpr int lane_width_in_bytes = 4;
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  MovFloatToInt(scratch1, src2, scratch2);
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vinsw(dst, scratch1, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  } else {
    mtvsrd(scratch3, scratch1);
    vinsertw(dst, scratch3, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  }
}

void MacroAssembler::I64x2ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      Register src2, uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vinsd(dst, src2, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  } else {
    mtvsrd(scratch, src2);
    vinsertd(dst, scratch, Operand((1 - imm_lane_idx) * lane_width_in_bytes));
  }
}

void MacroAssembler::I32x4ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      Register src2, uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 4;
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vinsw(dst, src2, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  } else {
    mtvsrd(scratch, src2);
    vinsertw(dst, scratch, Operand((3 - imm_lane_idx) * lane_width_in_bytes));
  }
}

void MacroAssembler::I16x8ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      Register src2, uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  constexpr int lane_width_in_bytes = 2;
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  mtvsrd(scratch, src2);
  vinserth(dst, scratch, Operand((7 - imm_lane_idx) * lane_width_in_bytes));
}

void MacroAssembler::I8x16ReplaceLane(Simd128Register dst, Simd128Register src1,
                                      Register src2, uint8_t imm_lane_idx,
                                      Simd128Register scratch) {
  if (src1 != dst) {
    vor(dst, src1, src1);
  }
  mtvsrd(scratch, src2);
  vinsertb(dst, scratch, Operand(15 - imm_lane_idx));
}

void MacroAssembler::I64x2Mul(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Register scratch1,
                              Register scratch2, Register scratch3,
                              Simd128Register scratch4) {
  constexpr int lane_width_in_bytes = 8;
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vmulld(dst, src1, src2);
  } else {
    Register scratch_1 = scratch1;
    Register scratch_2 = scratch2;
    for (int i = 0; i < 2; i++) {
      if (i > 0) {
        vextractd(scratch4, src1, Operand(1 * lane_width_in_bytes));
        vextractd(dst, src2, Operand(1 * lane_width_in_bytes));
        src1 = scratch4;
        src2 = dst;
      }
      mfvsrd(scratch_1, src1);
      mfvsrd(scratch_2, src2);
      mulld(scratch_1, scratch_1, scratch_2);
      scratch_1 = scratch2;
      scratch_2 = scratch3;
    }
    mtvsrdd(dst, scratch1, scratch2);
  }
}

void MacroAssembler::I16x8Mul(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2) {
  vxor(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
  vmladduhm(dst, src1, src2, kSimd128RegZero);
}

#define F64X2_MIN_MAX_NAN(result)                        \
  xvcmpeqdp(scratch2, src1, src1);                       \
  vsel(result, src1, result, scratch2);                  \
  xvcmpeqdp(scratch2, src2, src2);                       \
  vsel(dst, src2, result, scratch2);                     \
  /* Use xvmindp to turn any selected SNANs to QNANs. */ \
  xvmindp(dst, dst, dst);
void MacroAssembler::F64x2Min(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch1,
                              Simd128Register scratch2) {
  xvmindp(scratch1, src1, src2);
  // We need to check if an input is NAN and preserve it.
  F64X2_MIN_MAX_NAN(scratch1)
}

void MacroAssembler::F64x2Max(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch1,
                              Simd128Register scratch2) {
  xvmaxdp(scratch1, src1, src2);
  // We need to check if an input is NAN and preserve it.
  F64X2_MIN_MAX_NAN(scratch1)
}
#undef F64X2_MIN_MAX_NAN

void MacroAssembler::F64x2Lt(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2) {
  xvcmpgtdp(dst, src2, src1);
}

void MacroAssembler::F64x2Le(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2) {
  xvcmpgedp(dst, src2, src1);
}

void MacroAssembler::F64x2Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  xvcmpeqdp(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::F32x4Lt(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2) {
  xvcmpgtsp(dst, src2, src1);
}

void MacroAssembler::F32x4Le(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2) {
  xvcmpgesp(dst, src2, src1);
}

void MacroAssembler::F32x4Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  xvcmpeqsp(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I64x2Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  vcmpequd(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I64x2GeS(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpgtsd(scratch, src2, src1);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I32x4Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  vcmpequw(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I32x4GeS(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpgtsw(scratch, src2, src1);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I32x4GeU(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpequw(scratch, src1, src2);
  vcmpgtuw(dst, src1, src2);
  vor(dst, dst, scratch);
}

void MacroAssembler::I16x8Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  vcmpequh(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I16x8GeS(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpgtsh(scratch, src2, src1);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I16x8GeU(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpequh(scratch, src1, src2);
  vcmpgtuh(dst, src1, src2);
  vor(dst, dst, scratch);
}

void MacroAssembler::I8x16Ne(Simd128Register dst, Simd128Register src1,
                             Simd128Register src2, Simd128Register scratch) {
  vcmpequb(scratch, src1, src2);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I8x16GeS(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpgtsb(scratch, src2, src1);
  vnor(dst, scratch, scratch);
}

void MacroAssembler::I8x16GeU(Simd128Register dst, Simd128Register src1,
                              Simd128Register src2, Simd128Register scratch) {
  vcmpequb(scratch, src1, src2);
  vcmpgtub(dst, src1, src2);
  vor(dst, dst, scratch);
}

void MacroAssembler::I64x2Abs(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  constexpr int shift_bits = 63;
  xxspltib(scratch, Operand(shift_bits));
  vsrad(scratch, src, scratch);
  vxor(dst, src, scratch);
  vsubudm(dst, dst, scratch);
}
void MacroAssembler::I32x4Abs(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  constexpr int shift_bits = 31;
  xxspltib(scratch, Operand(shift_bits));
  vsraw(scratch, src, scratch);
  vxor(dst, src, scratch);
  vsubuwm(dst, dst, scratch);
}
void MacroAssembler::I16x8Abs(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  constexpr int shift_bits = 15;
  xxspltib(scratch, Operand(shift_bits));
  vsrah(scratch, src, scratch);
  vxor(dst, src, scratch);
  vsubuhm(dst, dst, scratch);
}
void MacroAssembler::I16x8Neg(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  vspltish(scratch, Operand(1));
  vnor(dst, src, src);
  vadduhm(dst, scratch, dst);
}
void MacroAssembler::I8x16Abs(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  constexpr int shift_bits = 7;
  xxspltib(scratch, Operand(shift_bits));
  vsrab(scratch, src, scratch);
  vxor(dst, src, scratch);
  vsububm(dst, dst, scratch);
}
void MacroAssembler::I8x16Neg(Simd128Register dst, Simd128Register src,
                              Simd128Register scratch) {
  xxspltib(scratch, Operand(1));
  vnor(dst, src, src);
  vaddubm(dst, scratch, dst);
}

void MacroAssembler::F64x2Pmin(Simd128Register dst, Simd128Register src1,
                               Simd128Register src2, Simd128Register scratch) {
  xvcmpgtdp(kScratchSimd128Reg, src1, src2);
  vsel(dst, src1, src2, kScratchSimd128Reg);
}

void MacroAssembler::F64x2Pmax(Simd128Register dst, Simd128Register src1,
                               Simd128Register src2, Simd128Register scratch) {
  xvcmpgtdp(kScratchSimd128Reg, src2, src1);
  vsel(dst, src1, src2, kScratchSimd128Reg);
}

void MacroAssembler::F32x4Pmin(Simd128Register dst, Simd128Register src1,
                               Simd128Register src2, Simd128Register scratch) {
  xvcmpgtsp(kScratchSimd128Reg, src1, src2);
  vsel(dst, src1, src2, kScratchSimd128Reg);
}

void MacroAssembler::F32x4Pmax(Simd128Register dst, Simd128Register src1,
                               Simd128Register src2, Simd128Register scratch) {
  xvcmpgtsp(kScratchSimd128Reg, src2, src1);
  vsel(dst, src1, src2, kScratchSimd128Reg);
}

void MacroAssembler::I32x4SConvertF32x4(Simd128Register dst,
                                        Simd128Register src,
                                        Simd128Register scratch) {
  // NaN to 0
  xvcmpeqsp(scratch, src, src);
  vand(scratch, src, scratch);
  xvcvspsxws(dst, scratch);
}

void MacroAssembler::I16x8SConvertI32x4(Simd128Register dst,
                                        Simd128Register src1,
                                        Simd128Register src2) {
  vpkswss(dst, src2, src1);
}

void MacroAssembler::I16x8UConvertI32x4(Simd128Register dst,
                                        Simd128Register src1,
                                        Simd128Register src2) {
  vpkswus(dst, src2, src1);
}

void MacroAssembler::I8x16SConvertI16x8(Simd128Register dst,
                                        Simd128Register src1,
                                        Simd128Register src2) {
  vpkshss(dst, src2, src1);
}

void MacroAssembler::I8x16UConvertI16x8(Simd128Register dst,
                                        Simd128Register src1,
                                        Simd128Register src2) {
  vpkshus(dst, src2, src1);
}

void MacroAssembler::F64x2ConvertLowI32x4S(Simd128Register dst,
                                           Simd128Register src) {
  vupklsw(dst, src);
  xvcvsxddp(dst, dst);
}

void MacroAssembler::F64x2ConvertLowI32x4U(Simd128Register dst,
                                           Simd128Register src,
                                           Register scratch1,
                                           Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  vupklsw(dst, src);
  // Zero extend.
  mov(scratch1, Operand(0xFFFFFFFF));
  mtvsrd(scratch2, scratch1);
  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
  vand(dst, scratch2, dst);
  xvcvuxddp(dst, dst);
}

void MacroAssembler::I64x2UConvertI32x4Low(Simd128Register dst,
                                           Simd128Register src,
                                           Register scratch1,
                                           Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  vupklsw(dst, src);
  // Zero extend.
  mov(scratch1, Operand(0xFFFFFFFF));
  mtvsrd(scratch2, scratch1);
  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I64x2UConvertI32x4High(Simd128Register dst,
                                            Simd128Register src,
                                            Register scratch1,
                                            Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  vupkhsw(dst, src);
  // Zero extend.
  mov(scratch1, Operand(0xFFFFFFFF));
  mtvsrd(scratch2, scratch1);
  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I32x4UConvertI16x8Low(Simd128Register dst,
                                           Simd128Register src,
                                           Register scratch1,
                                           Simd128Register scratch2) {
  vupklsh(dst, src);
  // Zero extend.
  mov(scratch1, Operand(0xFFFF));
  mtvsrd(scratch2, scratch1);
  vspltw(scratch2, scratch2, Operand(1));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I32x4UConvertI16x8High(Simd128Register dst,
                                            Simd128Register src,
                                            Register scratch1,
                                            Simd128Register scratch2) {
  vupkhsh(dst, src);
  // Zero extend.
  mov(scratch1, Operand(0xFFFF));
  mtvsrd(scratch2, scratch1);
  vspltw(scratch2, scratch2, Operand(1));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I16x8UConvertI8x16Low(Simd128Register dst,
                                           Simd128Register src,
                                           Register scratch1,
                                           Simd128Register scratch2) {
  vupklsb(dst, src);
  // Zero extend.
  li(scratch1, Operand(0xFF));
  mtvsrd(scratch2, scratch1);
  vsplth(scratch2, scratch2, Operand(3));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I16x8UConvertI8x16High(Simd128Register dst,
                                            Simd128Register src,
                                            Register scratch1,
                                            Simd128Register scratch2) {
  vupkhsb(dst, src);
  // Zero extend.
  li(scratch1, Operand(0xFF));
  mtvsrd(scratch2, scratch1);
  vsplth(scratch2, scratch2, Operand(3));
  vand(dst, scratch2, dst);
}

void MacroAssembler::I8x16BitMask(Register dst, Simd128Register src,
                                  Register scratch1, Register scratch2,
                                  Simd128Register scratch3) {
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    vextractbm(dst, src);
  } else {
    mov(scratch1, Operand(0x8101820283038));
    mov(scratch2, Operand(0x4048505860687078));
    mtvsrdd(scratch3, scratch1, scratch2);
    vbpermq(scratch3, src, scratch3);
    mfvsrd(dst, scratch3);
  }
}

void MacroAssembler::I32x4DotI16x8S(Simd128Register dst, Simd128Register src1,
                                    Simd128Register src2) {
  vxor(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
  vmsumshm(dst, src1, src2, kSimd128RegZero);
}

void MacroAssembler::I32x4DotI8x16AddS(Simd128Register dst,
                                       Simd128Register src1,
                                       Simd128Register src2,
                                       Simd128Register src3) {
  vmsummbm(dst, src1, src2, src3);
}

void MacroAssembler::I16x8DotI8x16S(Simd128Register dst, Simd128Register src1,
                                    Simd128Register src2,
                                    Simd128Register scratch) {
  vmulesb(scratch, src1, src2);
  vmulosb(dst, src1, src2);
  vadduhm(dst, scratch, dst);
}

void MacroAssembler::I16x8Q15MulRSatS(Simd128Register dst, Simd128Register src1,
                                      Simd128Register src2) {
  vxor(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
  vmhraddshs(dst, src1, src2, kSimd128RegZero);
}

void MacroAssembler::I8x16Swizzle(Simd128Register dst, Simd128Register src1,
                                  Simd128Register src2,
                                  Simd128Register scratch) {
  // Saturate the indices to 5 bits. Input indices more than 31 should
  // return 0.
  xxspltib(scratch, Operand(31));
  vminub(scratch, src2, scratch);
  // Input needs to be reversed.
  xxbrq(dst, src1);
  vxor(kSimd128RegZero, kSimd128RegZero, kSimd128RegZero);
  vperm(dst, dst, kSimd128RegZero, scratch);
}

void MacroAssembler::I8x16Shuffle(Simd128Register dst, Simd128Register src1,
                                  Simd128Register src2, uint64_t high,
                                  uint64_t low, Register scratch1,
                                  Register scratch2, Simd128Register scratch3) {
  mov(scratch1, Operand(low));
  mov(scratch2, Operand(high));
  mtvsrdd(scratch3, scratch2, scratch1);
  vperm(dst, src1, src2, scratch3);
}

#define EXT_ADD_PAIRWISE(splat, mul_even, mul_odd, add) \
  splat(scratch1, Operand(1));                          \
  mul_even(scratch2, src, scratch1);                    \
  mul_odd(scratch1, src, scratch1);                     \
  add(dst, scratch2, scratch1);
void MacroAssembler::I32x4ExtAddPairwiseI16x8S(Simd128Register dst,
                                               Simd128Register src,
                                               Simd128Register scratch1,
                                               Simd128Register scratch2) {
  EXT_ADD_PAIRWISE(vspltish, vmulesh, vmulosh, vadduwm)
}
void MacroAssembler::I32x4ExtAddPairwiseI16x8U(Simd128Register dst,
                                               Simd128Register src,
                                               Simd128Register scratch1,
                                               Simd128Register scratch2) {
  EXT_ADD_PAIRWISE(vspltish, vmuleuh, vmulouh, vadduwm)
}
void MacroAssembler::I16x8ExtAddPairwiseI8x16S(Simd128Register dst,
                                               Simd128Register src,
                                               Simd128Register scratch1,
                                               Simd128Register scratch2) {
  EXT_ADD_PAIRWISE(xxspltib, vmulesb, vmulosb, vadduhm)
}
void MacroAssembler::I16x8ExtAddPairwiseI8x16U(Simd128Register dst,
                                               Simd128Register src,
                                               Simd128Register scratch1,
                                               Simd128Register scratch2) {
  EXT_ADD_PAIRWISE(xxspltib, vmuleub, vmuloub, vadduhm)
}
#undef EXT_ADD_PAIRWISE

void MacroAssembler::F64x2PromoteLowF32x4(Simd128Register dst,
                                          Simd128Register src) {
  constexpr int lane_number = 8;
  vextractd(dst, src, Operand(lane_number));
  vinsertw(dst, dst, Operand(lane_number));
  xvcvspdp(dst, dst);
}

void MacroAssembler::F32x4DemoteF64x2Zero(Simd128Register dst,
                                          Simd128Register src,
                                          Simd128Register scratch) {
  constexpr int lane_number = 8;
  xvcvdpsp(scratch, src);
  vextractuw(dst, scratch, Operand(lane_number));
  vinsertw(scratch, dst, Operand(4));
  vxor(dst, dst, dst);
  vinsertd(dst, scratch, Operand(lane_number));
}

void MacroAssembler::I32x4TruncSatF64x2SZero(Simd128Register dst,
                                             Simd128Register src,
                                             Simd128Register scratch) {
  constexpr int lane_number = 8;
  // NaN to 0.
  xvcmpeqdp(scratch, src, src);
  vand(scratch, src, scratch);
  xvcvdpsxws(scratch, scratch);
  vextractuw(dst, scratch, Operand(lane_number));
  vinsertw(scratch, dst, Operand(4));
  vxor(dst, dst, dst);
  vinsertd(dst, scratch, Operand(lane_number));
}

void MacroAssembler::I32x4TruncSatF64x2UZero(Simd128Register dst,
                                             Simd128Register src,
                                             Simd128Register scratch) {
  constexpr int lane_number = 8;
  xvcvdpuxws(scratch, src);
  vextractuw(dst, scratch, Operand(lane_number));
  vinsertw(scratch, dst, Operand(4));
  vxor(dst, dst, dst);
  vinsertd(dst, scratch, Operand(lane_number));
}

#if V8_TARGET_BIG_ENDIAN
#define MAYBE_REVERSE_BYTES(reg, instr) instr(reg, reg);
#else
#define MAYBE_REVERSE_BYTES(reg, instr)
#endif
void MacroAssembler::LoadLane64LE(Simd128Register dst, const MemOperand& mem,
                                  int lane, Register scratch1,
                                  Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  LoadSimd128Uint64(scratch2, mem, scratch1);
  MAYBE_REVERSE_BYTES(scratch2, xxbrd)
  vinsertd(dst, scratch2, Operand((1 - lane) * lane_width_in_bytes));
}

void MacroAssembler::LoadLane32LE(Simd128Register dst, const MemOperand& mem,
                                  int lane, Register scratch1,
                                  Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 4;
  LoadSimd128Uint32(scratch2, mem, scratch1);
  MAYBE_REVERSE_BYTES(scratch2, xxbrw)
  vinsertw(dst, scratch2, Operand((3 - lane) * lane_width_in_bytes));
}

void MacroAssembler::LoadLane16LE(Simd128Register dst, const MemOperand& mem,
                                  int lane, Register scratch1,
                                  Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 2;
  LoadSimd128Uint16(scratch2, mem, scratch1);
  MAYBE_REVERSE_BYTES(scratch2, xxbrh)
  vinserth(dst, scratch2, Operand((7 - lane) * lane_width_in_bytes));
}

void MacroAssembler::LoadLane8LE(Simd128Register dst, const MemOperand& mem,
                                 int lane, Register scratch1,
                                 Simd128Register scratch2) {
  LoadSimd128Uint8(scratch2, mem, scratch1);
  vinsertb(dst, scratch2, Operand((15 - lane)));
}

void MacroAssembler::StoreLane64LE(Simd128Register src, const MemOperand& mem,
                                   int lane, Register scratch1,
                                   Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  vextractd(scratch2, src, Operand((1 - lane) * lane_width_in_bytes));
  MAYBE_REVERSE_BYTES(scratch2, xxbrd)
  StoreSimd128Uint64(scratch2, mem, scratch1);
}

void MacroAssembler::StoreLane32LE(Simd128Register src, const MemOperand& mem,
                                   int lane, Register scratch1,
                                   Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 4;
  vextractuw(scratch2, src, Operand((3 - lane) * lane_width_in_bytes));
  MAYBE_REVERSE_BYTES(scratch2, xxbrw)
  StoreSimd128Uint32(scratch2, mem, scratch1);
}

void MacroAssembler::StoreLane16LE(Simd128Register src, const MemOperand& mem,
                                   int lane, Register scratch1,
                                   Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 2;
  vextractuh(scratch2, src, Operand((7 - lane) * lane_width_in_bytes));
  MAYBE_REVERSE_BYTES(scratch2, xxbrh)
  StoreSimd128Uint16(scratch2, mem, scratch1);
}

void MacroAssembler::StoreLane8LE(Simd128Register src, const MemOperand& mem,
                                  int lane, Register scratch1,
                                  Simd128Register scratch2) {
  vextractub(scratch2, src, Operand(15 - lane));
  StoreSimd128Uint8(scratch2, mem, scratch1);
}

void MacroAssembler::LoadAndSplat64x2LE(Simd128Register dst,
                                        const MemOperand& mem,
                                        Register scratch) {
  constexpr int lane_width_in_bytes = 8;
  LoadSimd128Uint64(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrd)
  vinsertd(dst, dst, Operand(1 * lane_width_in_bytes));
}

void MacroAssembler::LoadAndSplat32x4LE(Simd128Register dst,
                                        const MemOperand& mem,
                                        Register scratch) {
  LoadSimd128Uint32(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrw)
  vspltw(dst, dst, Operand(1));
}

void MacroAssembler::LoadAndSplat16x8LE(Simd128Register dst,
                                        const MemOperand& mem,
                                        Register scratch) {
  LoadSimd128Uint16(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrh)
  vsplth(dst, dst, Operand(3));
}

void MacroAssembler::LoadAndSplat8x16LE(Simd128Register dst,
                                        const MemOperand& mem,
                                        Register scratch) {
  LoadSimd128Uint8(dst, mem, scratch);
  vspltb(dst, dst, Operand(7));
}

void MacroAssembler::LoadAndExtend32x2SLE(Simd128Register dst,
                                          const MemOperand& mem,
                                          Register scratch) {
  LoadSimd128Uint64(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrd)
  vupkhsw(dst, dst);
}

void MacroAssembler::LoadAndExtend32x2ULE(Simd128Register dst,
                                          const MemOperand& mem,
                                          Register scratch1,
                                          Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  LoadAndExtend32x2SLE(dst, mem, scratch1);
  // Zero extend.
  mov(scratch1, Operand(0xFFFFFFFF));
  mtvsrd(scratch2, scratch1);
  vinsertd(scratch2, scratch2, Operand(1 * lane_width_in_bytes));
  vand(dst, scratch2, dst);
}

void MacroAssembler::LoadAndExtend16x4SLE(Simd128Register dst,
                                          const MemOperand& mem,
                                          Register scratch) {
  LoadSimd128Uint64(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrd)
  vupkhsh(dst, dst);
}

void MacroAssembler::LoadAndExtend16x4ULE(Simd128Register dst,
                                          const MemOperand& mem,
                                          Register scratch1,
                                          Simd128Register scratch2) {
  LoadAndExtend16x4SLE(dst, mem, scratch1);
  // Zero extend.
  mov(scratch1, Operand(0xFFFF));
  mtvsrd(scratch2, scratch1);
  vspltw(scratch2, scratch2, Operand(1));
  vand(dst, scratch2, dst);
}

void MacroAssembler::LoadAndExtend8x8SLE(Simd128Register dst,
                                         const MemOperand& mem,
                                         Register scratch) {
  LoadSimd128Uint64(dst, mem, scratch);
  MAYBE_REVERSE_BYTES(dst, xxbrd)
  vupkhsb(dst, dst);
}

void MacroAssembler::LoadAndExtend8x8ULE(Simd128Register dst,
                                         const MemOperand& mem,
                                         Register scratch1,
                                         Simd128Register scratch2) {
  LoadAndExtend8x8SLE(dst, mem, scratch1);
  // Zero extend.
  li(scratch1, Operand(0xFF));
  mtvsrd(scratch2, scratch1);
  vsplth(scratch2, scratch2, Operand(3));
  vand(dst, scratch2, dst);
}

void MacroAssembler::LoadV64ZeroLE(Simd128Register dst, const MemOperand& mem,
                                   Register scratch1,
                                   Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 8;
  LoadSimd128Uint64(scratch2, mem, scratch1);
  MAYBE_REVERSE_BYTES(scratch2, xxbrd)
  vxor(dst, dst, dst);
  vinsertd(dst, scratch2, Operand(1 * lane_width_in_bytes));
}

void MacroAssembler::LoadV32ZeroLE(Simd128Register dst, const MemOperand& mem,
                                   Register scratch1,
                                   Simd128Register scratch2) {
  constexpr int lane_width_in_bytes = 4;
  LoadSimd128Uint32(scratch2, mem, scratch1);
  MAYBE_REVERSE_BYTES(scratch2, xxbrw)
  vxor(dst, dst, dst);
  vinsertw(dst, scratch2, Operand(3 * lane_width_in_bytes));
}
#undef MAYBE_REVERSE_BYTES

void MacroAssembler::V128AnyTrue(Register dst, Simd128Register src,
                                 Register scratch1, Register scratch2,
                                 Simd128Register scratch3) {
  constexpr uint8_t fxm = 0x2;  // field mask.
  constexpr int bit_number = 24;
  li(scratch1, Operand(0));
  li(scratch2, Operand(1));
  // Check if both lanes are 0, if so then return false.
  vxor(scratch3, scratch3, scratch3);
  mtcrf(scratch1, fxm);  // Clear cr6.
  vcmpequd(scratch3, src, scratch3, SetRC);
  isel(dst, scratch1, scratch2, bit_number);
}

void MacroAssembler::S128Not(Simd128Register dst, Simd128Register src) {
  vnor(dst, src, src);
}

void MacroAssembler::S128Const(Simd128Register dst, uint64_t high, uint64_t low,
                               Register scratch1, Register scratch2) {
  mov(scratch1, Operand(low));
  mov(scratch2, Operand(high));
  mtvsrdd(dst, scratch2, scratch1);
}

void MacroAssembler::S128Select(Simd128Register dst, Simd128Register src1,
                                Simd128Register src2, Simd128Register mask) {
  vsel(dst, src2, src1, mask);
}

Register GetRegisterThatIsNotOneOf(Register reg1, Register reg2, Register reg3,
                                   Register reg4, Register reg5,
                                   Register reg6) {
  RegList regs = {reg1, reg2, reg3, reg4, reg5, reg6};

  const RegisterConfiguration* config = RegisterConfiguration::Default();
  for (int i = 0; i < config->num_allocatable_general_registers(); ++i) {
    int code = config->GetAllocatableGeneralCode(i);
    Register candidate = Register::from_code(code);
    if (regs.has(candidate)) continue;
    return candidate;
  }
  UNREACHABLE();
}

void MacroAssembler::SwapP(Register src, Register dst, Register scratch) {
  if (src == dst) return;
  DCHECK(!AreAliased(src, dst, scratch));
  mr(scratch, src);
  mr(src, dst);
  mr(dst, scratch);
}

void MacroAssembler::SwapP(Register src, MemOperand dst, Register scratch) {
  if (dst.ra() != r0 && dst.ra().is_valid())
    DCHECK(!AreAliased(src, dst.ra(), scratch));
  if (dst.rb() != r0 && dst.rb().is_valid())
    DCHECK(!AreAliased(src, dst.rb(), scratch));
  DCHECK(!AreAliased(src, scratch));
  mr(scratch, src);
  LoadU64(src, dst, r0);
  StoreU64(scratch, dst, r0);
}

void MacroAssembler::SwapP(MemOperand src, MemOperand dst, Register scratch_0,
                           Register scratch_1) {
  if (src.ra() != r0 && src.ra().is_valid())
    DCHECK(!AreAliased(src.ra(), scratch_0, scratch_1));
  if (src.rb() != r0 && src.rb().is_valid())
    DCHECK(!AreAliased(src.rb(), scratch_0, scratch_1));
  if (dst.ra() != r0 && dst.ra().is_valid())
    DCHECK(!AreAliased(dst.ra(), scratch_0, scratch_1));
  if (dst.rb() != r0 && dst.rb().is_valid())
    DCHECK(!AreAliased(dst.rb(), scratch_0, scratch_1));
  DCHECK(!AreAliased(scratch_0, scratch_1));
  if (is_int16(src.offset()) || is_int16(dst.offset())) {
    if (!is_int16(src.offset())) {
      // swap operand
      MemOperand temp = src;
      src = dst;
      dst = temp;
    }
    LoadU64(scratch_1, dst, scratch_0);
    LoadU64(scratch_0, src);
    StoreU64(scratch_1, src);
    StoreU64(scratch_0, dst, scratch_1);
  } else {
    LoadU64(scratch_1, dst, scratch_0);
    push(scratch_1);
    LoadU64(scratch_0, src, scratch_1);
    StoreU64(scratch_0, dst, scratch_1);
    pop(scratch_1);
    StoreU64(scratch_1, src, scratch_0);
  }
}

void MacroAssembler::SwapFloat32(DoubleRegister src, DoubleRegister dst,
                                 DoubleRegister scratch) {
  if (src == dst) return;
  DCHECK(!AreAliased(src, dst, scratch));
  fmr(scratch, src);
  fmr(src, dst);
  fmr(dst, scratch);
}

void MacroAssembler::SwapFloat32(DoubleRegister src, MemOperand dst,
                                 DoubleRegister scratch) {
  DCHECK(!AreAliased(src, scratch));
  fmr(scratch, src);
  LoadF32(src, dst, r0);
  StoreF32(scratch, dst, r0);
}

void MacroAssembler::SwapFloat32(MemOperand src, MemOperand dst,
                                 DoubleRegister scratch_0,
                                 DoubleRegister scratch_1) {
  DCHECK(!AreAliased(scratch_0, scratch_1));
  LoadF32(scratch_0, src, r0);
  LoadF32(scratch_1, dst, r0);
  StoreF32(scratch_0, dst, r0);
  StoreF32(scratch_1, src, r0);
}

void MacroAssembler::SwapDouble(DoubleRegister src, DoubleRegister dst,
                                DoubleRegister scratch) {
  if (src == dst) return;
  DCHECK(!AreAliased(src, dst, scratch));
  fmr(scratch, src);
  fmr(src, dst);
  fmr(dst, scratch);
}

void MacroAssembler::SwapDouble(DoubleRegister src, MemOperand dst,
                                DoubleRegister scratch) {
  DCHECK(!AreAliased(src, scratch));
  fmr(scratch, src);
  LoadF64(src, dst, r0);
  StoreF64(scratch, dst, r0);
}

void MacroAssembler::SwapDouble(MemOperand src, MemOperand dst,
                                DoubleRegister scratch_0,
                                DoubleRegister scratch_1) {
  DCHECK(!AreAliased(scratch_0, scratch_1));
  LoadF64(scratch_0, src, r0);
  LoadF64(scratch_1, dst, r0);
  StoreF64(scratch_0, dst, r0);
  StoreF64(scratch_1, src, r0);
}

void MacroAssembler::SwapSimd128(Simd128Register src, Simd128Register dst,
                                 Simd128Register scratch) {
  if (src == dst) return;
  vor(scratch, src, src);
  vor(src, dst, dst);
  vor(dst, scratch, scratch);
}

void MacroAssembler::SwapSimd128(Simd128Register src, MemOperand dst,
                                 Simd128Register scratch1, Register scratch2) {
  DCHECK(src != scratch1);
  LoadSimd128(scratch1, dst, scratch2);
  StoreSimd128(src, dst, scratch2);
  vor(src, scratch1, scratch1);
}

void MacroAssembler::SwapSimd128(MemOperand src, MemOperand dst,
                                 Simd128Register scratch1,
                                 Simd128Register scratch2, Register scratch3) {
  LoadSimd128(scratch1, src, scratch3);
  LoadSimd128(scratch2, dst, scratch3);

  StoreSimd128(scratch1, dst, scratch3);
  StoreSimd128(scratch2, src, scratch3);
}

void MacroAssembler::ByteReverseU16(Register dst, Register val,
                                    Register scratch) {
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    brh(dst, val);
    ZeroExtHalfWord(dst, dst);
    return;
  }
  rlwinm(scratch, val, 8, 16, 23);
  rlwinm(dst, val, 24, 24, 31);
  orx(dst, scratch, dst);
  ZeroExtHalfWord(dst, dst);
}

void MacroAssembler::ByteReverseU32(Register dst, Register val,
                                    Register scratch) {
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    brw(dst, val);
    ZeroExtWord32(dst, dst);
    return;
  }
  rotlwi(scratch, val, 8);
  rlwimi(scratch, val, 24, 0, 7);
  rlwimi(scratch, val, 24, 16, 23);
  ZeroExtWord32(dst, scratch);
}

void MacroAssembler::ByteReverseU64(Register dst, Register val, Register) {
  if (CpuFeatures::IsSupported(PPC_10_PLUS)) {
    brd(dst, val);
    return;
  }
  subi(sp, sp, Operand(kSystemPointerSize));
  std(val, MemOperand(sp));
  ldbrx(dst, MemOperand(r0, sp));
  addi(sp, sp, Operand(kSystemPointerSize));
}

void MacroAssembler::JumpIfEqual(Register x, int32_t y, Label* dest) {
  CmpS32(x, Operand(y), r0);
  beq(dest);
}

void MacroAssembler::JumpIfLessThan(Register x, int32_t y, Label* dest) {
  CmpS32(x, Operand(y), r0);
  blt(dest);
}

void MacroAssembler::LoadEntryFromBuiltinIndex(Register builtin_index,
                                               Register target) {
  static_assert(kSystemPointerSize == 8);
  static_assert(kSmiTagSize == 1);
  static_assert(kSmiTag == 0);

  // The builtin_index register contains the builtin index as a Smi.
  if (SmiValuesAre32Bits()) {
    ShiftRightS64(target, builtin_index,
                  Operand(kSmiShift - kSystemPointerSizeLog2));
  } else {
    DCHECK(SmiValuesAre31Bits());
    ShiftLeftU64(target, builtin_index,
                 Operand(kSystemPointerSizeLog2 - kSmiShift));
  }
  AddS64(target, target, Operand(IsolateData::builtin_entry_table_offset()));
  LoadU64(target, MemOperand(kRootRegister, target));
}

void MacroAssembler::CallBuiltinByIndex(Register builtin_index,
                                        Register target) {
  LoadEntryFromBuiltinIndex(builtin_index, target);
  Call(target);
}

void MacroAssembler::LoadEntryFromBuiltin(Builtin builtin,
                                          Register destination) {
  ASM_CODE_COMMENT(this);
  LoadU64(destination, EntryFromBuiltinAsOperand(builtin));
}

MemOperand MacroAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
  ASM_CODE_COMMENT(this);
  DCHECK(root_array_available());
  return MemOperand(kRootRegister,
                    IsolateData::BuiltinEntrySlotOffset(builtin));
}

void MacroAssembler::LoadCodeInstructionStart(Register destination,
                                              Register code_object,
                                              CodeEntrypointTag tag) {
  ASM_CODE_COMMENT(this);
#ifdef V8_ENABLE_SANDBOX
  LoadCodeEntrypointViaCodePointer(
      destination,
      FieldMemOperand(code_object, Code::kSelfIndirectPointerOffset), r0);
#else
  LoadU64(destination,
          FieldMemOperand(code_object, Code::kInstructionStartOffset), r0);
#endif
}

void MacroAssembler::CallCodeObject(Register code_object) {
  ASM_CODE_COMMENT(this);
  LoadCodeInstructionStart(code_object, code_object);
  Call(code_object);
}

void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
  ASM_CODE_COMMENT(this);
  DCHECK_EQ(JumpMode::kJump, jump_mode);
  LoadCodeInstructionStart(code_object, code_object);
  Jump(code_object);
}

void MacroAssembler::CallJSFunction(Register function_object,
                                    Register scratch) {
  Register code = kJavaScriptCallCodeStartRegister;
#ifdef V8_ENABLE_SANDBOX
  // When the sandbox is enabled, we can directly fetch the entrypoint pointer
  // from the code pointer table instead of going through the Code object. In
  // this way, we avoid one memory load on this code path.
  LoadCodeEntrypointViaCodePointer(
      code, FieldMemOperand(function_object, JSFunction::kCodeOffset), scratch);
  Call(code);
#else
  LoadTaggedField(
      code, FieldMemOperand(function_object, JSFunction::kCodeOffset), scratch);
  CallCodeObject(code);
#endif
}

void MacroAssembler::JumpJSFunction(Register function_object, Register scratch,
                                    JumpMode jump_mode) {
  Register code = kJavaScriptCallCodeStartRegister;
#ifdef V8_ENABLE_SANDBOX
  // When the sandbox is enabled, we can directly fetch the entrypoint pointer
  // from the code pointer table instead of going through the Code object. In
  // this way, we avoid one memory load on this code path.
  LoadCodeEntrypointViaCodePointer(
      code, FieldMemOperand(function_object, JSFunction::kCodeOffset), scratch);
  DCHECK_EQ(jump_mode, JumpMode::kJump);
  DCHECK_EQ(code, r5);
  Jump(code);
#else
  LoadTaggedField(
      code, FieldMemOperand(function_object, JSFunction::kCodeOffset), scratch);
  JumpCodeObject(code, jump_mode);
#endif
}

void MacroAssembler::StoreReturnAddressAndCall(Register target) {
  // This generates the final instruction sequence for calls to C functions
  // once an exit frame has been constructed.
  //
  // Note that this assumes the caller code (i.e. the InstructionStream object
  // currently being generated) is immovable or that the callee function cannot
  // trigger GC, since the callee function will return to it.

  static constexpr int after_call_offset = 5 * kInstrSize;
  Label start_call;
  Register dest = target;

  if (ABI_USES_FUNCTION_DESCRIPTORS) {
    // AIX/PPC64BE Linux uses a function descriptor. When calling C code be
    // aware of this descriptor and pick up values from it
    LoadU64(ToRegister(ABI_TOC_REGISTER),
            MemOperand(target, kSystemPointerSize));
    LoadU64(ip, MemOperand(target, 0));
    dest = ip;
  } else if (ABI_CALL_VIA_IP && dest != ip) {
    Move(ip, target);
    dest = ip;
  }

  LoadPC(r7);
  bind(&start_call);
  addi(r7, r7, Operand(after_call_offset));
  StoreU64(r7, MemOperand(sp, kStackFrameExtraParamSlot * kSystemPointerSize));
  Call(dest);

  DCHECK_EQ(after_call_offset - kInstrSize,
            SizeOfCodeGeneratedSince(&start_call));
}

// Check if the code object is marked for deoptimization. If it is, then it
// jumps to the CompileLazyDeoptimizedCode builtin. In order to do this we need
// to:
//    1. read from memory the word that contains that bit, which can be found in
//       the flags in the referenced {Code} object;
//    2. test kMarkedForDeoptimizationBit in those flags; and
//    3. if it is not zero then it jumps to the builtin.
void MacroAssembler::BailoutIfDeoptimized() {
  int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;
  LoadTaggedField(r11, MemOperand(kJavaScriptCallCodeStartRegister, offset),
                     r0);
  LoadU32(r11, FieldMemOperand(r11, Code::kFlagsOffset), r0);
  TestBit(r11, Code::kMarkedForDeoptimizationBit);
  TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, cr0);
}

void MacroAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                           DeoptimizeKind kind, Label* ret,
                                           Label*) {
  BlockTrampolinePoolScope block_trampoline_pool(this);
  CHECK_LE(target, Builtins::kLastTier0);
  LoadU64(ip, MemOperand(kRootRegister,
                         IsolateData::BuiltinEntrySlotOffset(target)));
  Call(ip);
  DCHECK_EQ(SizeOfCodeGeneratedSince(exit),
            (kind == DeoptimizeKind::kLazy) ? Deoptimizer::kLazyDeoptExitSize
                                            : Deoptimizer::kEagerDeoptExitSize);
}

void MacroAssembler::ZeroExtByte(Register dst, Register src) {
  clrldi(dst, src, Operand(56));
}

void MacroAssembler::ZeroExtHalfWord(Register dst, Register src) {
  clrldi(dst, src, Operand(48));
}

void MacroAssembler::ZeroExtWord32(Register dst, Register src) {
  clrldi(dst, src, Operand(32));
}

void MacroAssembler::Trap() { stop(); }
void MacroAssembler::DebugBreak() { stop(); }

void MacroAssembler::Popcnt32(Register dst, Register src) { popcntw(dst, src); }

void MacroAssembler::Popcnt64(Register dst, Register src) { popcntd(dst, src); }

void MacroAssembler::CountLeadingZerosU32(Register dst, Register src, RCBit r) {
  cntlzw(dst, src, r);
}

void MacroAssembler::CountLeadingZerosU64(Register dst, Register src, RCBit r) {
  cntlzd(dst, src, r);
}

#define COUNT_TRAILING_ZEROES_SLOW(max_count, scratch1, scratch2) \
  Label loop, done;                                               \
  li(scratch1, Operand(max_count));                               \
  mtctr(scratch1);                                                \
  mr(scratch1, src);                                              \
  li(dst, Operand::Zero());                                       \
  bind(&loop); /* while ((src & 1) == 0) */                       \
  andi(scratch2, scratch1, Operand(1));                           \
  bne(&done, cr0);                                                \
  srdi(scratch1, scratch1, Operand(1)); /* src >>= 1;*/           \
  addi(dst, dst, Operand(1));           /* dst++ */               \
  bdnz(&loop);                                                    \
  bind(&done);
void MacroAssembler::CountTrailingZerosU32(Register dst, Register src,
                                           Register scratch1, Register scratch2,
                                           RCBit r) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    cnttzw(dst, src, r);
  } else {
    COUNT_TRAILING_ZEROES_SLOW(32, scratch1, scratch2);
  }
}

void MacroAssembler::CountTrailingZerosU64(Register dst, Register src,
                                           Register scratch1, Register scratch2,
                                           RCBit r) {
  if (CpuFeatures::IsSupported(PPC_9_PLUS)) {
    cnttzd(dst, src, r);
  } else {
    COUNT_TRAILING_ZEROES_SLOW(64, scratch1, scratch2);
  }
}
#undef COUNT_TRAILING_ZEROES_SLOW

void MacroAssembler::ClearByteU64(Register dst, int byte_idx) {
  CHECK(0 <= byte_idx && byte_idx <= 7);
  int shift = byte_idx*8;
  rldicl(dst, dst, shift, 8);
  rldicl(dst, dst, 64-shift, 0);
}

void MacroAssembler::ReverseBitsU64(Register dst, Register src,
                                    Register scratch1, Register scratch2) {
  ByteReverseU64(dst, src);
  for (int i = 0; i < 8; i++) {
    ReverseBitsInSingleByteU64(dst, dst, scratch1, scratch2, i);
  }
}

void MacroAssembler::ReverseBitsU32(Register dst, Register src,
                                    Register scratch1, Register scratch2) {
  ByteReverseU32(dst, src, scratch1);
  for (int i = 4; i < 8; i++) {
    ReverseBitsInSingleByteU64(dst, dst, scratch1, scratch2, i);
  }
}

// byte_idx=7 refers to least significant byte
void MacroAssembler::ReverseBitsInSingleByteU64(Register dst, Register src,
                                                Register scratch1,
                                                Register scratch2,
                                                int byte_idx) {
  CHECK(0 <= byte_idx && byte_idx <= 7);
  int j = byte_idx;
  // zero all bits of scratch1
  li(scratch2, Operand(0));
  for (int i = 0; i <= 7; i++) {
    // zero all bits of scratch1
    li(scratch1, Operand(0));
    // move bit (j+1)*8-i-1 of src to bit j*8+i of scratch1, erase bits
    // (j*8+i+1):end of scratch1
    int shift = 7 - (2*i);
    if (shift < 0) shift += 64;
    rldicr(scratch1, src, shift, j*8+i);
    // erase bits start:(j*8-1+i) of scratch1 (inclusive)
    rldicl(scratch1, scratch1, 0, j*8+i);
    // scratch2 = scratch2|scratch1
    orx(scratch2, scratch2, scratch1);
  }
  // clear jth byte of dst and insert jth byte of scratch2
  ClearByteU64(dst, j);
  orx(dst, dst, scratch2);
}

// Calls an API function. Allocates HandleScope, extracts returned value
// from handle and propagates exceptions. Clobbers C argument registers
// and C caller-saved registers. Restores context. On return removes
//   (*argc_operand + slots_to_drop_on_return) * kSystemPointerSize
// (GCed, includes the call JS arguments space and the additional space
// allocated for the fast call).
void CallApiFunctionAndReturn(MacroAssembler* masm, bool with_profiling,
                              Register function_address,
                              ExternalReference thunk_ref, Register thunk_arg,
                              int slots_to_drop_on_return,
                              MemOperand* argc_operand,
                              MemOperand return_value_operand) {
  using ER = ExternalReference;

  Isolate* isolate = masm->isolate();
  MemOperand next_mem_op = __ ExternalReferenceAsOperand(
      ER::handle_scope_next_address(isolate), no_reg);
  MemOperand limit_mem_op = __ ExternalReferenceAsOperand(
      ER::handle_scope_limit_address(isolate), no_reg);
  MemOperand level_mem_op = __ ExternalReferenceAsOperand(
      ER::handle_scope_level_address(isolate), no_reg);

  // Additional parameter is the address of the actual callback.
  Register return_value = r3;
  Register scratch = ip;
  Register scratch2 = r0;

  // Allocate HandleScope in callee-saved registers.
  // We will need to restore the HandleScope after the call to the API function,
  // by allocating it in callee-saved registers it'll be preserved by C code.
  Register prev_next_address_reg = r14;
  Register prev_limit_reg = r15;
  Register prev_level_reg = r16;

  // C arguments (kCArgRegs[0/1]) are expected to be initialized outside, so
  // this function must not corrupt them (return_value overlaps with
  // kCArgRegs[0] but that's ok because we start using it only after the C
  // call).
  DCHECK(!AreAliased(kCArgRegs[0], kCArgRegs[1],  // C args
                     scratch, scratch2, prev_next_address_reg, prev_limit_reg));
  // function_address and thunk_arg might overlap but this function must not
  // corrupted them until the call is made (i.e. overlap with return_value is
  // fine).
  DCHECK(!AreAliased(function_address,  // incoming parameters
                     scratch, scratch2, prev_next_address_reg, prev_limit_reg));
  DCHECK(!AreAliased(thunk_arg,  // incoming parameters
                     scratch, scratch2, prev_next_address_reg, prev_limit_reg));
  {
    ASM_CODE_COMMENT_STRING(masm,
                            "Allocate HandleScope in callee-save registers.");
    __ LoadU64(prev_next_address_reg, next_mem_op);
    __ LoadU64(prev_limit_reg, limit_mem_op);
    __ lwz(prev_level_reg, level_mem_op);
    __ addi(scratch, prev_level_reg, Operand(1));
    __ stw(scratch, level_mem_op);
  }

  Label profiler_or_side_effects_check_enabled, done_api_call;
  if (with_profiling) {
    __ RecordComment("Check if profiler or side effects check is enabled");
    __ lbz(scratch,
           __ ExternalReferenceAsOperand(IsolateFieldId::kExecutionMode));
    __ cmpi(scratch, Operand::Zero());
    __ bne(&profiler_or_side_effects_check_enabled);
#ifdef V8_RUNTIME_CALL_STATS
    __ RecordComment("Check if RCS is enabled");
    __ Move(scratch, ER::address_of_runtime_stats_flag());
    __ lwz(scratch, MemOperand(scratch, 0));
    __ cmpi(scratch, Operand::Zero());
    __ bne(&profiler_or_side_effects_check_enabled);
#endif  // V8_RUNTIME_CALL_STATS
  }

  __ RecordComment("Call the api function directly.");
  __ StoreReturnAddressAndCall(function_address);
  __ bind(&done_api_call);

  Label propagate_exception;
  Label delete_allocated_handles;
  Label leave_exit_frame;

  // load value from ReturnValue
  __ RecordComment("Load the value from ReturnValue");
  __ LoadU64(r3, return_value_operand);

  {
    ASM_CODE_COMMENT_STRING(
        masm,
        "No more valid handles (the result handle was the last one)."
        "Restore previous handle scope.");
    __ StoreU64(prev_next_address_reg, next_mem_op);
    if (v8_flags.debug_code) {
      __ lwz(scratch, level_mem_op);
      __ subi(scratch, scratch, Operand(1));
      __ CmpS64(scratch, prev_level_reg);
      __ Check(eq, AbortReason::kUnexpectedLevelAfterReturnFromApiCall);
    }
    __ stw(prev_level_reg, level_mem_op);
    __ LoadU64(scratch, limit_mem_op);
    __ CmpS64(scratch, prev_limit_reg);
    __ bne(&delete_allocated_handles);
  }

  __ RecordComment("Leave the API exit frame.");
  __ bind(&leave_exit_frame);
  Register argc_reg = prev_limit_reg;
  if (argc_operand != nullptr) {
    // Load the number of stack slots to drop before LeaveExitFrame modifies sp.
    __ LoadU64(argc_reg, *argc_operand);
  }
  __ LeaveExitFrame(scratch);

  {
    ASM_CODE_COMMENT_STRING(masm,
                            "Check if the function scheduled an exception.");
    __ LoadRoot(scratch, RootIndex::kTheHoleValue);
    __ LoadU64(scratch2, __ ExternalReferenceAsOperand(
                             ER::exception_address(isolate), no_reg));
    __ CmpS64(scratch, scratch2);
    __ bne(&propagate_exception);
  }

  __ AssertJSAny(return_value, scratch, scratch2,
                 AbortReason::kAPICallReturnedInvalidObject);

  if (argc_operand == nullptr) {
    DCHECK_NE(slots_to_drop_on_return, 0);
    __ AddS64(sp, sp, Operand(slots_to_drop_on_return * kSystemPointerSize));

  } else {
    // {argc_operand} was loaded into {argc_reg} above.
    __ AddS64(sp, sp, Operand(slots_to_drop_on_return * kSystemPointerSize));
    __ ShiftLeftU64(r0, argc_reg, Operand(kSystemPointerSizeLog2));
    __ AddS64(sp, sp, r0);
  }

  __ blr();

  if (with_profiling) {
    ASM_CODE_COMMENT_STRING(masm, "Call the api function via the thunk.");
    __ bind(&profiler_or_side_effects_check_enabled);
    // Additional parameter is the address of the actual callback function.
    if (thunk_arg.is_valid()) {
      MemOperand thunk_arg_mem_op = __ ExternalReferenceAsOperand(
          IsolateFieldId::kApiCallbackThunkArgument);
      __ StoreU64(thunk_arg, thunk_arg_mem_op);
    }
    __ Move(scratch, thunk_ref);
    __ StoreReturnAddressAndCall(scratch);
    __ b(&done_api_call);
  }

  __ RecordComment("An exception was thrown. Propagate it.");
  __ bind(&propagate_exception);
  __ TailCallRuntime(Runtime::kPropagateException);

  {
    ASM_CODE_COMMENT_STRING(
        masm, "HandleScope limit has changed. Delete allocated extensions.");
    __ bind(&delete_allocated_handles);
    __ StoreU64(prev_limit_reg, limit_mem_op);
    // Save the return value in a callee-save register.
    Register saved_result = prev_limit_reg;
    __ mr(saved_result, return_value);
    __ PrepareCallCFunction(1, scratch);
    __ Move(kCArgRegs[0], ER::isolate_address());
    __ CallCFunction(ER::delete_handle_scope_extensions(), 1);
    __ mr(return_value, saved_result);
    __ b(&leave_exit_frame);
  }
}

}  // namespace internal
}  // namespace v8

#undef __

#endif  // V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
                                                                                                                               node-23.7.0/deps/v8/src/codegen/ppc/macro-assembler-ppc.h                                           0000664 0000000 0000000 00000246325 14746647661 0023043 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef INCLUDED_FROM_MACRO_ASSEMBLER_H
#error This header must be included via macro-assembler.h
#endif

#ifndef V8_CODEGEN_PPC_MACRO_ASSEMBLER_PPC_H_
#define V8_CODEGEN_PPC_MACRO_ASSEMBLER_PPC_H_

#include "src/base/numbers/double.h"
#include "src/base/platform/platform.h"
#include "src/codegen/bailout-reason.h"
#include "src/codegen/ppc/assembler-ppc.h"
#include "src/common/globals.h"
#include "src/execution/frame-constants.h"
#include "src/execution/isolate-data.h"
#include "src/objects/contexts.h"

namespace v8 {
namespace internal {

enum class StackLimitKind { kInterruptStackLimit, kRealStackLimit };

// ----------------------------------------------------------------------------
// Static helper functions

// Generate a MemOperand for loading a field from an object.
inline MemOperand FieldMemOperand(Register object, int offset) {
  return MemOperand(object, offset - kHeapObjectTag);
}

enum LinkRegisterStatus { kLRHasNotBeenSaved, kLRHasBeenSaved };

Register GetRegisterThatIsNotOneOf(Register reg1, Register reg2 = no_reg,
                                   Register reg3 = no_reg,
                                   Register reg4 = no_reg,
                                   Register reg5 = no_reg,
                                   Register reg6 = no_reg);

// These exist to provide portability between 32 and 64bit
#if V8_TARGET_ARCH_PPC64
#define ClearLeftImm clrldi
#define ClearRightImm clrrdi
#else
#define ClearLeftImm clrlwi
#define ClearRightImm clrrwi
#endif

class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 public:
  using MacroAssemblerBase::MacroAssemblerBase;

  void CallBuiltin(Builtin builtin, Condition cond = al);
  void TailCallBuiltin(Builtin builtin, Condition cond = al,
                       CRegister cr = cr7);
  void Popcnt32(Register dst, Register src);
  void Popcnt64(Register dst, Register src);
  // Converts the integer (untagged smi) in |src| to a double, storing
  // the result to |dst|
  void ConvertIntToDouble(Register src, DoubleRegister dst);

  // Converts the unsigned integer (untagged smi) in |src| to
  // a double, storing the result to |dst|
  void ConvertUnsignedIntToDouble(Register src, DoubleRegister dst);

  // Converts the integer (untagged smi) in |src| to
  // a float, storing the result in |dst|
  void ConvertIntToFloat(Register src, DoubleRegister dst);

  // Converts the unsigned integer (untagged smi) in |src| to
  // a float, storing the result in |dst|
  void ConvertUnsignedIntToFloat(Register src, DoubleRegister dst);

#if V8_TARGET_ARCH_PPC64
  void ConvertInt64ToFloat(Register src, DoubleRegister double_dst);
  void ConvertInt64ToDouble(Register src, DoubleRegister double_dst);
  void ConvertUnsignedInt64ToFloat(Register src, DoubleRegister double_dst);
  void ConvertUnsignedInt64ToDouble(Register src, DoubleRegister double_dst);
#endif

  // Converts the double_input to an integer.  Note that, upon return,
  // the contents of double_dst will also hold the fixed point representation.
  void ConvertDoubleToInt64(const DoubleRegister double_input,
#if !V8_TARGET_ARCH_PPC64
                            const Register dst_hi,
#endif
                            const Register dst, const DoubleRegister double_dst,
                            FPRoundingMode rounding_mode = kRoundToZero);

#if V8_TARGET_ARCH_PPC64
  // Converts the double_input to an unsigned integer.  Note that, upon return,
  // the contents of double_dst will also hold the fixed point representation.
  void ConvertDoubleToUnsignedInt64(
      const DoubleRegister double_input, const Register dst,
      const DoubleRegister double_dst,
      FPRoundingMode rounding_mode = kRoundToZero);
#endif

  // Activation support.
  void EnterFrame(StackFrame::Type type,
                  bool load_constant_pool_pointer_reg = false);

  // Returns the pc offset at which the frame ends.
  int LeaveFrame(StackFrame::Type type, int stack_adjustment = 0);

  void AllocateStackSpace(int bytes) {
    DCHECK_GE(bytes, 0);
    if (bytes == 0) return;
    AddS64(sp, sp, Operand(-bytes), r0);
  }

  void AllocateStackSpace(Register bytes) { sub(sp, sp, bytes); }

  // Push a fixed frame, consisting of lr, fp, constant pool.
  void PushCommonFrame(Register marker_reg = no_reg);

  // Generates function and stub prologue code.
  void StubPrologue(StackFrame::Type type);
  void Prologue();

  void DropArguments(Register count);
  void DropArgumentsAndPushNewReceiver(Register argc, Register receiver);

  // Push a standard frame, consisting of lr, fp, constant pool,
  // context and JS function
  void PushStandardFrame(Register function_reg);

  // Restore caller's frame pointer and return address prior to being
  // overwritten by tail call stack preparation.
  void RestoreFrameStateForTailCall();

  // Get the actual activation frame alignment for target environment.
  static int ActivationFrameAlignment();

  void InitializeRootRegister() {
    ExternalReference isolate_root = ExternalReference::isolate_root(isolate());
    mov(kRootRegister, Operand(isolate_root));
#ifdef V8_COMPRESS_POINTERS
    LoadRootRelative(kPtrComprCageBaseRegister,
                     IsolateData::cage_base_offset());
#endif
  }

  void LoadDoubleLiteral(DoubleRegister result, base::Double value,
                         Register scratch);

  // load a literal signed int value <value> to GPR <dst>
  void LoadIntLiteral(Register dst, int value);
  // load an SMI value <value> to GPR <dst>
  void LoadSmiLiteral(Register dst, Tagged<Smi> smi);

  void LoadPC(Register dst);
  void ComputeCodeStartAddress(Register dst);

  void CmpS64(Register src1, const Operand& src2, Register scratch,
              CRegister cr = cr7);
  void CmpS64(Register src1, Register src2, CRegister cr = cr7);
  void CmpU64(Register src1, const Operand& src2, Register scratch,
              CRegister cr = cr7);
  void CmpU64(Register src1, Register src2, CRegister cr = cr7);
  void CmpS32(Register src1, const Operand& src2, Register scratch,
              CRegister cr = cr7);
  void CmpS32(Register src1, Register src2, CRegister cr = cr7);
  void CmpU32(Register src1, const Operand& src2, Register scratch,
              CRegister cr = cr7);
  void CmpU32(Register src1, Register src2, CRegister cr = cr7);
  void CompareTagged(Register src1, Register src2, CRegister cr = cr7) {
    if (COMPRESS_POINTERS_BOOL) {
      CmpS32(src1, src2, cr);
    } else {
      CmpS64(src1, src2, cr);
    }
  }

  void MinF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              DoubleRegister scratch = kScratchDoubleReg);
  void MaxF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              DoubleRegister scratch = kScratchDoubleReg);

  // Set new rounding mode RN to FPSCR
  void SetRoundingMode(FPRoundingMode RN);

  // reset rounding mode to default (kRoundToNearest)
  void ResetRoundingMode();

  void AddS64(Register dst, Register src, const Operand& value,
              Register scratch = r0, OEBit s = LeaveOE, RCBit r = LeaveRC);
  void AddS64(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void SubS64(Register dst, Register src, const Operand& value,
              Register scratch = r0, OEBit s = LeaveOE, RCBit r = LeaveRC);
  void SubS64(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void AddS32(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = LeaveRC);
  void AddS32(Register dst, Register src, Register value, RCBit r = LeaveRC);
  void SubS32(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = LeaveRC);
  void SubS32(Register dst, Register src, Register value, RCBit r = LeaveRC);
  void MulS64(Register dst, Register src, const Operand& value,
              Register scratch = r0, OEBit s = LeaveOE, RCBit r = LeaveRC);
  void MulS64(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void MulS32(Register dst, Register src, const Operand& value,
              Register scratch = r0, OEBit s = LeaveOE, RCBit r = LeaveRC);
  void MulS32(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void DivS64(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void DivU64(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void DivS32(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void DivU32(Register dst, Register src, Register value, OEBit s = LeaveOE,
              RCBit r = LeaveRC);
  void ModS64(Register dst, Register src, Register value);
  void ModU64(Register dst, Register src, Register value);
  void ModS32(Register dst, Register src, Register value);
  void ModU32(Register dst, Register src, Register value);

  void AndU64(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = SetRC);
  void AndU64(Register dst, Register src, Register value, RCBit r = SetRC);
  void OrU64(Register dst, Register src, const Operand& value,
             Register scratch = r0, RCBit r = SetRC);
  void OrU64(Register dst, Register src, Register value, RCBit r = LeaveRC);
  void XorU64(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = SetRC);
  void XorU64(Register dst, Register src, Register value, RCBit r = LeaveRC);
  void AndU32(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = SetRC);
  void AndU32(Register dst, Register src, Register value, RCBit r = SetRC);
  void OrU32(Register dst, Register src, const Operand& value,
             Register scratch = r0, RCBit r = SetRC);
  void OrU32(Register dst, Register src, Register value, RCBit r = LeaveRC);
  void XorU32(Register dst, Register src, const Operand& value,
              Register scratch = r0, RCBit r = SetRC);
  void XorU32(Register dst, Register src, Register value, RCBit r = LeaveRC);

  void ShiftLeftU64(Register dst, Register src, const Operand& value,
                    RCBit r = LeaveRC);
  void ShiftRightU64(Register dst, Register src, const Operand& value,
                     RCBit r = LeaveRC);
  void ShiftRightS64(Register dst, Register src, const Operand& value,
                     RCBit r = LeaveRC);
  void ShiftLeftU32(Register dst, Register src, const Operand& value,
                    RCBit r = LeaveRC);
  void ShiftRightU32(Register dst, Register src, const Operand& value,
                     RCBit r = LeaveRC);
  void ShiftRightS32(Register dst, Register src, const Operand& value,
                     RCBit r = LeaveRC);
  void ShiftLeftU64(Register dst, Register src, Register value,
                    RCBit r = LeaveRC);
  void ShiftRightU64(Register dst, Register src, Register value,
                     RCBit r = LeaveRC);
  void ShiftRightS64(Register dst, Register src, Register value,
                     RCBit r = LeaveRC);
  void ShiftLeftU32(Register dst, Register src, Register value,
                    RCBit r = LeaveRC);
  void ShiftRightU32(Register dst, Register src, Register value,
                     RCBit r = LeaveRC);
  void ShiftRightS32(Register dst, Register src, Register value,
                     RCBit r = LeaveRC);

  void CountLeadingZerosU32(Register dst, Register src, RCBit r = LeaveRC);
  void CountLeadingZerosU64(Register dst, Register src, RCBit r = LeaveRC);
  void CountTrailingZerosU32(Register dst, Register src, Register scratch1 = ip,
                             Register scratch2 = r0, RCBit r = LeaveRC);
  void CountTrailingZerosU64(Register dst, Register src, Register scratch1 = ip,
                             Register scratch2 = r0, RCBit r = LeaveRC);

  void ClearByteU64(Register dst, int byte_idx);
  void ReverseBitsU64(Register dst, Register src, Register scratch1,
                      Register scratch2);
  void ReverseBitsU32(Register dst, Register src, Register scratch1,
                      Register scratch2);
  void ReverseBitsInSingleByteU64(Register dst, Register src,
                                  Register scratch1, Register scratch2,
                                  int byte_idx);

  void AddF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void SubF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void MulF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void DivF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void AddF32(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void SubF32(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void MulF32(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void DivF32(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
              RCBit r = LeaveRC);
  void CopySignF64(DoubleRegister dst, DoubleRegister lhs, DoubleRegister rhs,
                   RCBit r = LeaveRC);

  template <class _type>
  void SignedExtend(Register dst, Register value) {
    switch (sizeof(_type)) {
      case 1:
        extsb(dst, value);
        break;
      case 2:
        extsh(dst, value);
        break;
      case 4:
        extsw(dst, value);
        break;
      case 8:
        if (dst != value) mr(dst, value);
        break;
      default:
        UNREACHABLE();
    }
  }

  template <class _type>
  void ZeroExtend(Register dst, Register value) {
    switch (sizeof(_type)) {
      case 1:
        ZeroExtByte(dst, value);
        break;
      case 2:
        ZeroExtHalfWord(dst, value);
        break;
      case 4:
        ZeroExtWord32(dst, value);
        break;
      case 8:
        if (dst != value) mr(dst, value);
        break;
      default:
        UNREACHABLE();
    }
  }
  template <class _type>
  void ExtendValue(Register dst, Register value) {
    if (std::is_signed<_type>::value) {
      SignedExtend<_type>(dst, value);
    } else {
      ZeroExtend<_type>(dst, value);
    }
  }

  template <class _type>
  void LoadReserve(Register output, MemOperand dst) {
    switch (sizeof(_type)) {
      case 1:
        lbarx(output, dst);
        break;
      case 2:
        lharx(output, dst);
        break;
      case 4:
        lwarx(output, dst);
        break;
      case 8:
        ldarx(output, dst);
        break;
      default:
        UNREACHABLE();
    }
    if (std::is_signed<_type>::value) {
      SignedExtend<_type>(output, output);
    }
  }

  template <class _type>
  void StoreConditional(Register value, MemOperand dst) {
    switch (sizeof(_type)) {
      case 1:
        stbcx(value, dst);
        break;
      case 2:
        sthcx(value, dst);
        break;
      case 4:
        stwcx(value, dst);
        break;
      case 8:
        stdcx(value, dst);
        break;
      default:
        UNREACHABLE();
    }
  }

  template <class _type>
  void AtomicCompareExchange(MemOperand dst, Register old_value,
                             Register new_value, Register output,
                             Register scratch) {
    Label loop;
    Label exit;
    if (sizeof(_type) != 8) {
      ExtendValue<_type>(scratch, old_value);
      old_value = scratch;
    }
    lwsync();
    bind(&loop);
    LoadReserve<_type>(output, dst);
    cmp(output, old_value, cr0);
    bne(&exit, cr0);
    StoreConditional<_type>(new_value, dst);
    bne(&loop, cr0);
    bind(&exit);
    sync();
  }

  template <class _type>
  void AtomicExchange(MemOperand dst, Register new_value, Register output) {
    Label exchange;
    lwsync();
    bind(&exchange);
    LoadReserve<_type>(output, dst);
    StoreConditional<_type>(new_value, dst);
    bne(&exchange, cr0);
    sync();
  }

  template <class _type, class bin_op>
  void AtomicOps(MemOperand dst, Register value, Register output,
                 Register result, bin_op op) {
    Label binop;
    lwsync();
    bind(&binop);
    switch (sizeof(_type)) {
      case 1:
        lbarx(output, dst);
        break;
      case 2:
        lharx(output, dst);
        break;
      case 4:
        lwarx(output, dst);
        break;
      case 8:
        ldarx(output, dst);
        break;
      default:
        UNREACHABLE();
    }
    op(result, output, value);
    switch (sizeof(_type)) {
      case 1:
        stbcx(result, dst);
        break;
      case 2:
        sthcx(result, dst);
        break;
      case 4:
        stwcx(result, dst);
        break;
      case 8:
        stdcx(result, dst);
        break;
      default:
        UNREACHABLE();
    }
    bne(&binop, cr0);
    sync();
  }

  void Push(Register src) { push(src); }
  // Push a handle.
  void Push(Handle<HeapObject> handle);
  void Push(Tagged<Smi> smi);

  // Push two registers.  Pushes leftmost register first (to highest address).
  void Push(Register src1, Register src2) {
    StoreU64WithUpdate(src2, MemOperand(sp, -2 * kSystemPointerSize));
    StoreU64(src1, MemOperand(sp, kSystemPointerSize));
  }

  // Push three registers.  Pushes leftmost register first (to highest address).
  void Push(Register src1, Register src2, Register src3) {
    StoreU64WithUpdate(src3, MemOperand(sp, -3 * kSystemPointerSize));
    StoreU64(src2, MemOperand(sp, kSystemPointerSize));
    StoreU64(src1, MemOperand(sp, 2 * kSystemPointerSize));
  }

  // Push four registers.  Pushes leftmost register first (to highest address).
  void Push(Register src1, Register src2, Register src3, Register src4) {
    StoreU64WithUpdate(src4, MemOperand(sp, -4 * kSystemPointerSize));
    StoreU64(src3, MemOperand(sp, kSystemPointerSize));
    StoreU64(src2, MemOperand(sp, 2 * kSystemPointerSize));
    StoreU64(src1, MemOperand(sp, 3 * kSystemPointerSize));
  }

  // Push five registers.  Pushes leftmost register first (to highest address).
  void Push(Register src1, Register src2, Register src3, Register src4,
            Register src5) {
    StoreU64WithUpdate(src5, MemOperand(sp, -5 * kSystemPointerSize));
    StoreU64(src4, MemOperand(sp, kSystemPointerSize));
    StoreU64(src3, MemOperand(sp, 2 * kSystemPointerSize));
    StoreU64(src2, MemOperand(sp, 3 * kSystemPointerSize));
    StoreU64(src1, MemOperand(sp, 4 * kSystemPointerSize));
  }

  enum PushArrayOrder { kNormal, kReverse };
  void PushArray(Register array, Register size, Register scratch,
                 Register scratch2, PushArrayOrder order = kNormal);

  void Pop(Register dst) { pop(dst); }

  // Pop two registers. Pops rightmost register first (from lower address).
  void Pop(Register src1, Register src2) {
    LoadU64(src2, MemOperand(sp, 0));
    LoadU64(src1, MemOperand(sp, kSystemPointerSize));
    addi(sp, sp, Operand(2 * kSystemPointerSize));
  }

  // Pop three registers.  Pops rightmost register first (from lower address).
  void Pop(Register src1, Register src2, Register src3) {
    LoadU64(src3, MemOperand(sp, 0));
    LoadU64(src2, MemOperand(sp, kSystemPointerSize));
    LoadU64(src1, MemOperand(sp, 2 * kSystemPointerSize));
    addi(sp, sp, Operand(3 * kSystemPointerSize));
  }

  // Pop four registers.  Pops rightmost register first (from lower address).
  void Pop(Register src1, Register src2, Register src3, Register src4) {
    LoadU64(src4, MemOperand(sp, 0));
    LoadU64(src3, MemOperand(sp, kSystemPointerSize));
    LoadU64(src2, MemOperand(sp, 2 * kSystemPointerSize));
    LoadU64(src1, MemOperand(sp, 3 * kSystemPointerSize));
    addi(sp, sp, Operand(4 * kSystemPointerSize));
  }

  // Pop five registers.  Pops rightmost register first (from lower address).
  void Pop(Register src1, Register src2, Register src3, Register src4,
           Register src5) {
    LoadU64(src5, MemOperand(sp, 0));
    LoadU64(src4, MemOperand(sp, kSystemPointerSize));
    LoadU64(src3, MemOperand(sp, 2 * kSystemPointerSize));
    LoadU64(src2, MemOperand(sp, 3 * kSystemPointerSize));
    LoadU64(src1, MemOperand(sp, 4 * kSystemPointerSize));
    addi(sp, sp, Operand(5 * kSystemPointerSize));
  }

  void MaybeSaveRegisters(RegList registers);
  void MaybeRestoreRegisters(RegList registers);

  void CallEphemeronKeyBarrier(Register object, Register slot_address,
                               SaveFPRegsMode fp_mode);

  void CallIndirectPointerBarrier(Register object, Register slot_address,
                                  SaveFPRegsMode fp_mode,
                                  IndirectPointerTag tag);

  void CallRecordWriteStubSaveRegisters(
      Register object, Register slot_address, SaveFPRegsMode fp_mode,
      StubCallMode mode = StubCallMode::kCallBuiltinPointer);
  void CallRecordWriteStub(
      Register object, Register slot_address, SaveFPRegsMode fp_mode,
      StubCallMode mode = StubCallMode::kCallBuiltinPointer);

  void MultiPush(RegList regs, Register location = sp);
  void MultiPop(RegList regs, Register location = sp);

  void MultiPushDoubles(DoubleRegList dregs, Register location = sp);
  void MultiPopDoubles(DoubleRegList dregs, Register location = sp);

  void MultiPushV128(Simd128RegList dregs, Register scratch,
                     Register location = sp);
  void MultiPopV128(Simd128RegList dregs, Register scratch,
                    Register location = sp);

  void MultiPushF64AndV128(DoubleRegList dregs, Simd128RegList simd_regs,
                           Register scratch1, Register scratch2,
                           Register location = sp);
  void MultiPopF64AndV128(DoubleRegList dregs, Simd128RegList simd_regs,
                          Register scratch1, Register scratch2,
                          Register location = sp);

  // Calculate how much stack space (in bytes) are required to store caller
  // registers excluding those specified in the arguments.
  int RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
                                      Register exclusion1 = no_reg,
                                      Register exclusion2 = no_reg,
                                      Register exclusion3 = no_reg) const;

  // Push caller saved registers on the stack, and return the number of bytes
  // stack pointer is adjusted.
  int PushCallerSaved(SaveFPRegsMode fp_mode, Register scratch1,
                      Register scratch2, Register exclusion1 = no_reg,
                      Register exclusion2 = no_reg,
                      Register exclusion3 = no_reg);
  // Restore caller saved registers from the stack, and return the number of
  // bytes stack pointer is adjusted.
  int PopCallerSaved(SaveFPRegsMode fp_mode, Register scratch1,
                     Register scratch2, Register exclusion1 = no_reg,
                     Register exclusion2 = no_reg,
                     Register exclusion3 = no_reg);

  // Load an object from the root table.
  void LoadRoot(Register destination, RootIndex index) final {
    LoadRoot(destination, index, al);
  }
  void LoadRoot(Register destination, RootIndex index, Condition cond);
  void LoadTaggedRoot(Register destination, RootIndex index);

  void SwapP(Register src, Register dst, Register scratch);
  void SwapP(Register src, MemOperand dst, Register scratch);
  void SwapP(MemOperand src, MemOperand dst, Register scratch_0,
             Register scratch_1);
  void SwapFloat32(DoubleRegister src, DoubleRegister dst,
                   DoubleRegister scratch);
  void SwapFloat32(DoubleRegister src, MemOperand dst, DoubleRegister scratch);
  void SwapFloat32(MemOperand src, MemOperand dst, DoubleRegister scratch_0,
                   DoubleRegister scratch_1);
  void SwapDouble(DoubleRegister src, DoubleRegister dst,
                  DoubleRegister scratch);
  void SwapDouble(DoubleRegister src, MemOperand dst, DoubleRegister scratch);
  void SwapDouble(MemOperand src, MemOperand dst, DoubleRegister scratch_0,
                  DoubleRegister scratch_1);
  void SwapSimd128(Simd128Register src, Simd128Register dst,
                   Simd128Register scratch);
  void SwapSimd128(Simd128Register src, MemOperand dst,
                   Simd128Register scratch1, Register scratch2);
  void SwapSimd128(MemOperand src, MemOperand dst, Simd128Register scratch1,
                   Simd128Register scratch2, Register scratch3);

  void ByteReverseU16(Register dst, Register val, Register scratch);
  void ByteReverseU32(Register dst, Register val, Register scratch);
  void ByteReverseU64(Register dst, Register val, Register = r0);

  // Before calling a C-function from generated code, align arguments on stack.
  // After aligning the frame, non-register arguments must be stored in
  // sp[0], sp[4], etc., not pushed. The argument count assumes all arguments
  // are word sized. If double arguments are used, this function assumes that
  // all double arguments are stored before core registers; otherwise the
  // correct alignment of the double values is not guaranteed.
  // Some compilers/platforms require the stack to be aligned when calling
  // C++ code.
  // Needs a scratch register to do some arithmetic. This register will be
  // trashed.
  void PrepareCallCFunction(int num_reg_arguments, int num_double_registers,
                            Register scratch);
  void PrepareCallCFunction(int num_reg_arguments, Register scratch);

  // There are two ways of passing double arguments on ARM, depending on
  // whether soft or hard floating point ABI is used. These functions
  // abstract parameter passing for the three different ways we call
  // C functions from generated code.
  void MovToFloatParameter(DoubleRegister src);
  void MovToFloatParameters(DoubleRegister src1, DoubleRegister src2);
  void MovToFloatResult(DoubleRegister src);

  // Calls a C function and cleans up the space for arguments allocated
  // by PrepareCallCFunction. The called function is not allowed to trigger a
  // garbage collection, since that might move the code and invalidate the
  // return address (unless this is somehow accounted for by the called
  // function).
  int CallCFunction(
      ExternalReference function, int num_arguments,
      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,
      bool has_function_descriptor = true);
  int CallCFunction(
      Register function, int num_arguments,
      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,
      bool has_function_descriptor = true);
  int CallCFunction(
      ExternalReference function, int num_reg_arguments,
      int num_double_arguments,
      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,
      bool has_function_descriptor = true);
  int CallCFunction(
      Register function, int num_reg_arguments, int num_double_arguments,
      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,
      bool has_function_descriptor = true);

  void MovFromFloatParameter(DoubleRegister dst);
  void MovFromFloatResult(DoubleRegister dst);

  void Trap();
  void DebugBreak();

  // Calls Abort(msg) if the condition cond is not satisfied.
  // Use --debug_code to enable.
  void Assert(Condition cond, AbortReason reason,
              CRegister cr = cr7) NOOP_UNLESS_DEBUG_CODE;

  // Like Assert(), but always enabled.
  void Check(Condition cond, AbortReason reason, CRegister cr = cr7);

  // Print a message to stdout and abort execution.
  void Abort(AbortReason reason);

#if !V8_TARGET_ARCH_PPC64
  void ShiftLeftPair(Register dst_low, Register dst_high, Register src_low,
                     Register src_high, Register scratch, Register shift);
  void ShiftLeftPair(Register dst_low, Register dst_high, Register src_low,
                     Register src_high, uint32_t shift);
  void ShiftRightPair(Register dst_low, Register dst_high, Register src_low,
                      Register src_high, Register scratch, Register shift);
  void ShiftRightPair(Register dst_low, Register dst_high, Register src_low,
                      Register src_high, uint32_t shift);
  void ShiftRightAlgPair(Register dst_low, Register dst_high, Register src_low,
                         Register src_high, Register scratch, Register shift);
  void ShiftRightAlgPair(Register dst_low, Register dst_high, Register src_low,
                         Register src_high, uint32_t shift);
#endif

  void LoadFromConstantsTable(Register destination, int constant_index) final;
  void LoadRootRegisterOffset(Register destination, intptr_t offset) final;
  void LoadRootRelative(Register destination, int32_t offset) final;
  void StoreRootRelative(int32_t offset, Register value) final;

  // Operand pointing to an external reference.
  // May emit code to set up the scratch register. The operand is
  // only guaranteed to be correct as long as the scratch register
  // isn't changed.
  // If the operand is used more than once, use a scratch register
  // that is guaranteed not to be clobbered.
  MemOperand ExternalReferenceAsOperand(ExternalReference reference,
                                        Register scratch);
  MemOperand ExternalReferenceAsOperand(IsolateFieldId id) {
    return ExternalReferenceAsOperand(ExternalReference::Create(id), no_reg);
  }

  // Jump, Call, and Ret pseudo instructions implementing inter-working.
  void Jump(Register target);
  void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al,
            CRegister cr = cr7);
  void Jump(Handle<Code> code, RelocInfo::Mode rmode, Condition cond = al,
            CRegister cr = cr7);
  void Jump(const ExternalReference& reference);
  void Jump(intptr_t target, RelocInfo::Mode rmode, Condition cond = al,
            CRegister cr = cr7);
  void Call(Register target);
  void Call(Address target, RelocInfo::Mode rmode, Condition cond = al);
  void Call(Handle<Code> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
            Condition cond = al);
  void Call(Label* target);

  // Load the builtin given by the Smi in |builtin_index| into |target|.
  void LoadEntryFromBuiltinIndex(Register builtin_index, Register target);
  void LoadEntryFromBuiltin(Builtin builtin, Register destination);
  MemOperand EntryFromBuiltinAsOperand(Builtin builtin);

  // Load the code entry point from the Code object.
  void LoadCodeInstructionStart(
      Register destination, Register code_object,
      CodeEntrypointTag tag = kDefaultCodeEntrypointTag);
  void CallCodeObject(Register code_object);
  void JumpCodeObject(Register code_object,
                      JumpMode jump_mode = JumpMode::kJump);

  void CallBuiltinByIndex(Register builtin_index, Register target);
  void BailoutIfDeoptimized();
  void CallForDeoptimization(Builtin target, int deopt_id, Label* exit,
                             DeoptimizeKind kind, Label* ret,
                             Label* jump_deoptimization_entry_label);

  // Emit code to discard a non-negative number of pointer-sized elements
  // from the stack, clobbering only the sp register.
  void Drop(int count);
  void Drop(Register count, Register scratch = r0);

  void Ret() { blr(); }
  void Ret(Condition cond, CRegister cr = cr7) { bclr(cond, cr); }
  void Ret(int drop) {
    Drop(drop);
    blr();
  }

  // If the value is a NaN, canonicalize the value else, do nothing.
  void CanonicalizeNaN(const DoubleRegister dst, const DoubleRegister src);
  void CanonicalizeNaN(const DoubleRegister value) {
    CanonicalizeNaN(value, value);
  }
  void CheckPageFlag(Register object, Register scratch, int mask, Condition cc,
                     Label* condition_met);

  // Move values between integer and floating point registers.
  void MovIntToDouble(DoubleRegister dst, Register src, Register scratch);
  void MovUnsignedIntToDouble(DoubleRegister dst, Register src,
                              Register scratch);
  void MovInt64ToDouble(DoubleRegister dst,
#if !V8_TARGET_ARCH_PPC64
                        Register src_hi,
#endif
                        Register src);
#if V8_TARGET_ARCH_PPC64
  void MovInt64ComponentsToDouble(DoubleRegister dst, Register src_hi,
                                  Register src_lo, Register scratch);
#endif
  void InsertDoubleLow(DoubleRegister dst, Register src, Register scratch);
  void InsertDoubleHigh(DoubleRegister dst, Register src, Register scratch);
  void MovDoubleLowToInt(Register dst, DoubleRegister src);
  void MovDoubleHighToInt(Register dst, DoubleRegister src);
  void MovDoubleToInt64(
#if !V8_TARGET_ARCH_PPC64
      Register dst_hi,
#endif
      Register dst, DoubleRegister src);
  void MovIntToFloat(DoubleRegister dst, Register src, Register scratch);
  void MovFloatToInt(Register dst, DoubleRegister src, DoubleRegister scratch);
  // Register move. May do nothing if the registers are identical.
  void Move(Register dst, Tagged<Smi> smi) { LoadSmiLiteral(dst, smi); }
  void Move(Register dst, Handle<HeapObject> value,
            RelocInfo::Mode rmode = RelocInfo::FULL_EMBEDDED_OBJECT);
  void Move(Register dst, ExternalReference reference);
  void LoadIsolateField(Register dst, IsolateFieldId id);
  void Move(Register dst, Register src, Condition cond = al);
  void Move(DoubleRegister dst, DoubleRegister src);
  void Move(Register dst, const MemOperand& src) {
    // TODO: use scratch register scope instead of r0
    LoadU64(dst, src, r0);
  }

  void SmiUntag(Register dst, const MemOperand& src, RCBit rc = LeaveRC,
                Register scratch = no_reg);
  void SmiUntag(Register reg, RCBit rc = LeaveRC) { SmiUntag(reg, reg, rc); }

  void SmiUntag(Register dst, Register src, RCBit rc = LeaveRC) {
    if (COMPRESS_POINTERS_BOOL) {
      srawi(dst, src, kSmiShift, rc);
    } else {
      ShiftRightS64(dst, src, Operand(kSmiShift), rc);
    }
  }
  void SmiToInt32(Register smi) {
    if (v8_flags.enable_slow_asserts) {
      AssertSmi(smi);
    }
    DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
    SmiUntag(smi);
  }

  // Shift left by kSmiShift
  void SmiTag(Register reg, RCBit rc = LeaveRC) { SmiTag(reg, reg, rc); }
  void SmiTag(Register dst, Register src, RCBit rc = LeaveRC) {
    ShiftLeftU64(dst, src, Operand(kSmiShift), rc);
  }

  // Abort execution if argument is a smi, enabled via --debug-code.
  void AssertNotSmi(Register object) NOOP_UNLESS_DEBUG_CODE;
  void AssertSmi(Register object) NOOP_UNLESS_DEBUG_CODE;

  void ZeroExtByte(Register dst, Register src);
  void ZeroExtHalfWord(Register dst, Register src);
  void ZeroExtWord32(Register dst, Register src);

  // ---------------------------------------------------------------------------
  // Bit testing/extraction
  //
  // Bit numbering is such that the least significant bit is bit 0
  // (for consistency between 32/64-bit).

  // Extract consecutive bits (defined by rangeStart - rangeEnd) from src
  // and, if !test, shift them into the least significant bits of dst.
  inline void ExtractBitRange(Register dst, Register src, int rangeStart,
                              int rangeEnd, RCBit rc = LeaveRC,
                              bool test = false) {
    DCHECK(rangeStart >= rangeEnd && rangeStart < kBitsPerSystemPointer);
    int rotate = (rangeEnd == 0) ? 0 : kBitsPerSystemPointer - rangeEnd;
    int width = rangeStart - rangeEnd + 1;
    if (rc == SetRC && rangeStart < 16 && (rangeEnd == 0 || test)) {
      // Prefer faster andi when applicable.
      andi(dst, src, Operand(((1 << width) - 1) << rangeEnd));
    } else {
#if V8_TARGET_ARCH_PPC64
      rldicl(dst, src, rotate, kBitsPerSystemPointer - width, rc);
#else
      rlwinm(dst, src, rotate, kBitsPerSystemPointer - width,
             kBitsPerSystemPointer - 1, rc);
#endif
    }
  }

  inline void ExtractBit(Register dst, Register src, uint32_t bitNumber,
                         RCBit rc = LeaveRC, bool test = false) {
    ExtractBitRange(dst, src, bitNumber, bitNumber, rc, test);
  }

  // Extract consecutive bits (defined by mask) from src and place them
  // into the least significant bits of dst.
  inline void ExtractBitMask(Register dst, Register src, uintptr_t mask,
                             RCBit rc = LeaveRC, bool test = false) {
    int start = kBitsPerSystemPointer - 1;
    int end;
    uintptr_t bit = (1L << start);

    while (bit && (mask & bit) == 0) {
      start--;
      bit >>= 1;
    }
    end = start;
    bit >>= 1;

    while (bit && (mask & bit)) {
      end--;
      bit >>= 1;
    }

    // 1-bits in mask must be contiguous
    DCHECK(bit == 0 || (mask & ((bit << 1) - 1)) == 0);

    ExtractBitRange(dst, src, start, end, rc, test);
  }

  // Test single bit in value.
  inline void TestBit(Register value, int bitNumber, Register scratch = r0) {
    ExtractBitRange(scratch, value, bitNumber, bitNumber, SetRC, true);
  }

  // Test consecutive bit range in value.  Range is defined by mask.
  inline void TestBitMask(Register value, uintptr_t mask,
                          Register scratch = r0) {
    ExtractBitMask(scratch, value, mask, SetRC, true);
  }
  // Test consecutive bit range in value.  Range is defined by
  // rangeStart - rangeEnd.
  inline void TestBitRange(Register value, int rangeStart, int rangeEnd,
                           Register scratch = r0) {
    ExtractBitRange(scratch, value, rangeStart, rangeEnd, SetRC, true);
  }

  inline void TestIfSmi(Register value, Register scratch) {
    TestBitRange(value, kSmiTagSize - 1, 0, scratch);
  }
  // Jump the register contains a smi.
  inline void JumpIfSmi(Register value, Label* smi_label) {
    TestIfSmi(value, r0);
    beq(smi_label, cr0);  // branch if SMI
  }
  void JumpIfEqual(Register x, int32_t y, Label* dest);
  void JumpIfLessThan(Register x, int32_t y, Label* dest);

  void LoadMap(Register destination, Register object);
  void LoadCompressedMap(Register dst, Register object, Register scratch);

  void LoadFeedbackVector(Register dst, Register closure, Register scratch,
                          Label* fbv_undef);

#if V8_TARGET_ARCH_PPC64
  inline void TestIfInt32(Register value, Register scratch,
                          CRegister cr = cr7) {
    // High bits must be identical to fit into an 32-bit integer
    extsw(scratch, value);
    CmpS64(scratch, value, cr);
  }
#else
  inline void TestIfInt32(Register hi_word, Register lo_word, Register scratch,
                          CRegister cr = cr7) {
    // High bits must be identical to fit into an 32-bit integer
    srawi(scratch, lo_word, 31);
    CmpS64(scratch, hi_word, cr);
  }
#endif

  // Overflow handling functions.
  // Usage: call the appropriate arithmetic function and then call one of the
  // flow control functions with the corresponding label.

  // Compute dst = left + right, setting condition codes. dst may be same as
  // either left or right (or a unique register). left and right must not be
  // the same register.
  void AddAndCheckForOverflow(Register dst, Register left, Register right,
                              Register overflow_dst, Register scratch = r0);
  void AddAndCheckForOverflow(Register dst, Register left, intptr_t right,
                              Register overflow_dst, Register scratch = r0);

  // Compute dst = left - right, setting condition codes. dst may be same as
  // either left or right (or a unique register). left and right must not be
  // the same register.
  void SubAndCheckForOverflow(Register dst, Register left, Register right,
                              Register overflow_dst, Register scratch = r0);

  // Performs a truncating conversion of a floating point number as used by
  // the JS bitwise operations. See ECMA-262 9.5: ToInt32. Goes to 'done' if it
  // succeeds, otherwise falls through if result is saturated. On return
  // 'result' either holds answer, or is clobbered on fall through.
  void TryInlineTruncateDoubleToI(Register result, DoubleRegister input,
                                  Label* done);
  void TruncateDoubleToI(Isolate* isolate, Zone* zone, Register result,
                         DoubleRegister double_input, StubCallMode stub_mode);

  void LoadConstantPoolPointerRegister();

  // Loads the constant pool pointer (kConstantPoolRegister).
  void LoadConstantPoolPointerRegisterFromCodeTargetAddress(
      Register code_target_address, Register scratch1, Register scratch2);
  void AbortConstantPoolBuilding() {
#ifdef DEBUG
    // Avoid DCHECK(!is_linked()) failure in ~Label()
    bind(ConstantPoolPosition());
#endif
  }

  // Convenience functions to call/jmp to the code of a JSFunction object.
  void CallJSFunction(Register function_object, Register scratch);
  void JumpJSFunction(Register function_object, Register scratch,
                      JumpMode jump_mode = JumpMode::kJump);

  // Generates an instruction sequence s.t. the return address points to the
  // instruction following the call.
  // The return address on the stack is used by frame iteration.
  void StoreReturnAddressAndCall(Register target);

  // Control-flow integrity:

  // Define a function entrypoint. This doesn't emit any code for this
  // architecture, as control-flow integrity is not supported for it.
  void CodeEntry() {}
  // Define an exception handler.
  void ExceptionHandler() {}
  // Define an exception handler and bind a label.
  void BindExceptionHandler(Label* label) { bind(label); }

  // ---------------------------------------------------------------------------
  // V8 Sandbox support

  // Transform a SandboxedPointer from/to its encoded form, which is used when
  // the pointer is stored on the heap and ensures that the pointer will always
  // point into the sandbox.
  void DecodeSandboxedPointer(Register value);
  void LoadSandboxedPointerField(Register destination,
                                 const MemOperand& field_operand,
                                 Register scratch = no_reg);
  void StoreSandboxedPointerField(Register value,
                                  const MemOperand& dst_field_operand,
                                  Register scratch = no_reg);

  // Loads a field containing off-heap pointer and does necessary decoding
  // if sandboxed external pointers are enabled.
  void LoadExternalPointerField(Register destination, MemOperand field_operand,
                                ExternalPointerTag tag,
                                Register isolate_root = no_reg,
                                Register scratch = no_reg);

  // Load a trusted pointer field.
  // When the sandbox is enabled, these are indirect pointers using the trusted
  // pointer table. Otherwise they are regular tagged fields.
  void LoadTrustedPointerField(Register destination, MemOperand field_operand,
                               IndirectPointerTag tag,
                               Register scratch = no_reg);

  // Store a trusted pointer field.
  // When the sandbox is enabled, these are indirect pointers using the trusted
  // pointer table. Otherwise they are regular tagged fields.
  void StoreTrustedPointerField(Register value, MemOperand dst_field_operand,
                                Register scratch = no_reg);

  // Load a code pointer field.
  // These are special versions of trusted pointers that, when the sandbox is
  // enabled, reference code objects through the code pointer table.
  void LoadCodePointerField(Register destination, MemOperand field_operand,
                            Register scratch) {
    LoadTrustedPointerField(destination, field_operand, kCodeIndirectPointerTag,
                            scratch);
  }
  // Store a code pointer field.
  void StoreCodePointerField(Register value, MemOperand dst_field_operand,
                             Register scratch = no_reg) {
    StoreTrustedPointerField(value, dst_field_operand, scratch);
  }

  // Load an indirect pointer field.
  // Only available when the sandbox is enabled.
  void LoadIndirectPointerField(Register destination, MemOperand field_operand,
                                IndirectPointerTag tag, Register scratch);

  // Store an indirect pointer field.
  // Only available when the sandbox is enabled.
  void StoreIndirectPointerField(Register value, MemOperand dst_field_operand,
                                 Register scratch);

#ifdef V8_ENABLE_SANDBOX
  // Retrieve the heap object referenced by the given indirect pointer handle,
  // which can either be a trusted pointer handle or a code pointer handle.
  void ResolveIndirectPointerHandle(Register destination, Register handle,
                                    IndirectPointerTag tag,
                                    Register scratch = no_reg);

  // Retrieve the heap object referenced by the given trusted pointer handle.
  void ResolveTrustedPointerHandle(Register destination, Register handle,
                                   IndirectPointerTag tag,
                                   Register scratch = no_reg);

  // Retrieve the Code object referenced by the given code pointer handle.
  void ResolveCodePointerHandle(Register destination, Register handle,
                                Register scratch = no_reg);

  // Load the pointer to a Code's entrypoint via a code pointer.
  // Only available when the sandbox is enabled as it requires the code pointer
  // table.
  void LoadCodeEntrypointViaCodePointer(Register destination,
                                        MemOperand field_operand,
                                        Register scratch = no_reg);

#endif

  // ---------------------------------------------------------------------------
  // Pointer compression Support

  void SmiToPtrArrayOffset(Register dst, Register src) {
#if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
    static_assert(kSmiTag == 0 && kSmiShift < kSystemPointerSizeLog2);
    ShiftLeftU64(dst, src, Operand(kSystemPointerSizeLog2 - kSmiShift));
#else
    static_assert(kSmiTag == 0 && kSmiShift > kSystemPointerSizeLog2);
    ShiftRightS64(dst, src, Operand(kSmiShift - kSystemPointerSizeLog2));
#endif
  }

  // Loads a field containing any tagged value and decompresses it if necessary.
  void LoadTaggedField(const Register& destination,
                       const MemOperand& field_operand,
                       const Register& scratch = no_reg);
  void LoadTaggedSignedField(Register destination, MemOperand field_operand,
                             Register scratch);

  // Compresses and stores tagged value to given on-heap location.
  void StoreTaggedField(const Register& value,
                        const MemOperand& dst_field_operand,
                        const Register& scratch = no_reg);

  void DecompressTaggedSigned(Register destination, MemOperand field_operand);
  void DecompressTaggedSigned(Register destination, Register src);
  void DecompressTagged(Register destination, MemOperand field_operand);
  void DecompressTagged(Register destination, Register source);
  void DecompressTagged(const Register& destination, Tagged_t immediate);

  void LoadF64(DoubleRegister dst, const MemOperand& mem,
               Register scratch = no_reg);
  void LoadF32(DoubleRegister dst, const MemOperand& mem,
               Register scratch = no_reg);

  void StoreF32(DoubleRegister src, const MemOperand& mem,
                Register scratch = no_reg);
  void StoreF64(DoubleRegister src, const MemOperand& mem,
                Register scratch = no_reg);

  void LoadF32WithUpdate(DoubleRegister dst, const MemOperand& mem,
                         Register scratch = no_reg);
  void LoadF64WithUpdate(DoubleRegister dst, const MemOperand& mem,
                         Register scratch = no_reg);

  void StoreF32WithUpdate(DoubleRegister src, const MemOperand& mem,
                          Register scratch = no_reg);
  void StoreF64WithUpdate(DoubleRegister src, const MemOperand& mem,
                          Register scratch = no_reg);

  void LoadU64(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadU32(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadS32(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadU16(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadS16(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadU8(Register dst, const MemOperand& mem, Register scratch = no_reg);
  void LoadS8(Register dst, const MemOperand& mem, Register scratch = no_reg);

  void StoreU64(Register src, const MemOperand& mem, Register scratch = no_reg);
  void StoreU32(Register src, const MemOperand& mem, Register scratch);
  void StoreU16(Register src, const MemOperand& mem, Register scratch);
  void StoreU8(Register src, const MemOperand& mem, Register scratch);

  void LoadU64WithUpdate(Register dst, const MemOperand& mem,
                         Register scratch = no_reg);
  void StoreU64WithUpdate(Register src, const MemOperand& mem,
                          Register scratch = no_reg);

  void LoadU64LE(Register dst, const MemOperand& mem, Register scratch);
  void LoadU32LE(Register dst, const MemOperand& mem, Register scratch);
  void LoadU16LE(Register dst, const MemOperand& mem, Register scratch);
  void StoreU64LE(Register src, const MemOperand& mem, Register scratch);
  void StoreU32LE(Register src, const MemOperand& mem, Register scratch);
  void StoreU16LE(Register src, const MemOperand& mem, Register scratch);

  void LoadS32LE(Register dst, const MemOperand& mem, Register scratch);
  void LoadS16LE(Register dst, const MemOperand& mem, Register scratch);

  void LoadF64LE(DoubleRegister dst, const MemOperand& mem, Register scratch,
                 Register scratch2);
  void LoadF32LE(DoubleRegister dst, const MemOperand& mem, Register scratch,
                 Register scratch2);

  void StoreF32LE(DoubleRegister src, const MemOperand& mem, Register scratch,
                  Register scratch2);
  void StoreF64LE(DoubleRegister src, const MemOperand& mem, Register scratch,
                  Register scratch2);

  // Simd Support.
#define SIMD_BINOP_LIST(V) \
  V(F64x2Add)              \
  V(F64x2Sub)              \
  V(F64x2Mul)              \
  V(F64x2Div)              \
  V(F64x2Eq)               \
  V(F64x2Lt)               \
  V(F64x2Le)               \
  V(F32x4Add)              \
  V(F32x4Sub)              \
  V(F32x4Mul)              \
  V(F32x4Div)              \
  V(F32x4Min)              \
  V(F32x4Max)              \
  V(F32x4Eq)               \
  V(F32x4Lt)               \
  V(F32x4Le)               \
  V(I64x2Add)              \
  V(I64x2Sub)              \
  V(I64x2Eq)               \
  V(I64x2GtS)              \
  V(I32x4MinS)             \
  V(I32x4MinU)             \
  V(I32x4MaxS)             \
  V(I32x4MaxU)             \
  V(I32x4Add)              \
  V(I32x4Sub)              \
  V(I32x4Mul)              \
  V(I32x4Eq)               \
  V(I32x4GtS)              \
  V(I32x4GtU)              \
  V(I32x4DotI16x8S)        \
  V(I16x8Add)              \
  V(I16x8Sub)              \
  V(I16x8Mul)              \
  V(I16x8MinS)             \
  V(I16x8MinU)             \
  V(I16x8MaxS)             \
  V(I16x8MaxU)             \
  V(I16x8Eq)               \
  V(I16x8GtS)              \
  V(I16x8GtU)              \
  V(I16x8AddSatS)          \
  V(I16x8SubSatS)          \
  V(I16x8AddSatU)          \
  V(I16x8SubSatU)          \
  V(I16x8SConvertI32x4)    \
  V(I16x8UConvertI32x4)    \
  V(I16x8RoundingAverageU) \
  V(I16x8Q15MulRSatS)      \
  V(I8x16Add)              \
  V(I8x16Sub)              \
  V(I8x16MinS)             \
  V(I8x16MinU)             \
  V(I8x16MaxS)             \
  V(I8x16MaxU)             \
  V(I8x16Eq)               \
  V(I8x16GtS)              \
  V(I8x16GtU)              \
  V(I8x16AddSatS)          \
  V(I8x16SubSatS)          \
  V(I8x16AddSatU)          \
  V(I8x16SubSatU)          \
  V(I8x16SConvertI16x8)    \
  V(I8x16UConvertI16x8)    \
  V(I8x16RoundingAverageU) \
  V(S128And)               \
  V(S128Or)                \
  V(S128Xor)               \
  V(S128AndNot)

#define PROTOTYPE_SIMD_BINOP(name) \
  void name(Simd128Register dst, Simd128Register src1, Simd128Register src2);
  SIMD_BINOP_LIST(PROTOTYPE_SIMD_BINOP)
#undef PROTOTYPE_SIMD_BINOP
#undef SIMD_BINOP_LIST

#define SIMD_BINOP_WITH_SCRATCH_LIST(V) \
  V(F64x2Ne)                            \
  V(F64x2Pmin)                          \
  V(F64x2Pmax)                          \
  V(F32x4Ne)                            \
  V(F32x4Pmin)                          \
  V(F32x4Pmax)                          \
  V(I64x2Ne)                            \
  V(I64x2GeS)                           \
  V(I64x2ExtMulLowI32x4S)               \
  V(I64x2ExtMulHighI32x4S)              \
  V(I64x2ExtMulLowI32x4U)               \
  V(I64x2ExtMulHighI32x4U)              \
  V(I32x4Ne)                            \
  V(I32x4GeS)                           \
  V(I32x4GeU)                           \
  V(I32x4ExtMulLowI16x8S)               \
  V(I32x4ExtMulHighI16x8S)              \
  V(I32x4ExtMulLowI16x8U)               \
  V(I32x4ExtMulHighI16x8U)              \
  V(I16x8Ne)                            \
  V(I16x8GeS)                           \
  V(I16x8GeU)                           \
  V(I16x8ExtMulLowI8x16S)               \
  V(I16x8ExtMulHighI8x16S)              \
  V(I16x8ExtMulLowI8x16U)               \
  V(I16x8ExtMulHighI8x16U)              \
  V(I16x8DotI8x16S)                     \
  V(I8x16Ne)                            \
  V(I8x16GeS)                           \
  V(I8x16GeU)                           \
  V(I8x16Swizzle)

#define PROTOTYPE_SIMD_BINOP_WITH_SCRATCH(name)                              \
  void name(Simd128Register dst, Simd128Register src1, Simd128Register src2, \
            Simd128Register scratch);
  SIMD_BINOP_WITH_SCRATCH_LIST(PROTOTYPE_SIMD_BINOP_WITH_SCRATCH)
#undef PROTOTYPE_SIMD_BINOP_WITH_SCRATCH
#undef SIMD_BINOP_WITH_SCRATCH_LIST

#define SIMD_SHIFT_LIST(V) \
  V(I64x2Shl)              \
  V(I64x2ShrS)             \
  V(I64x2ShrU)             \
  V(I32x4Shl)              \
  V(I32x4ShrS)             \
  V(I32x4ShrU)             \
  V(I16x8Shl)              \
  V(I16x8ShrS)             \
  V(I16x8ShrU)             \
  V(I8x16Shl)              \
  V(I8x16ShrS)             \
  V(I8x16ShrU)

#define PROTOTYPE_SIMD_SHIFT(name)                                          \
  void name(Simd128Register dst, Simd128Register src1, Register src2,       \
            Simd128Register scratch);                                       \
  void name(Simd128Register dst, Simd128Register src1, const Operand& src2, \
            Register scratch1, Simd128Register scratch2);
  SIMD_SHIFT_LIST(PROTOTYPE_SIMD_SHIFT)
#undef PROTOTYPE_SIMD_SHIFT
#undef SIMD_SHIFT_LIST

#define SIMD_BITMASK_LIST(V) \
  V(I64x2BitMask)            \
  V(I32x4BitMask)            \
  V(I16x8BitMask)

#define PROTOTYPE_SIMD_BITMASK(name)                              \
  void name(Register dst, Simd128Register src, Register scratch1, \
            Simd128Register scratch2);
  SIMD_BITMASK_LIST(PROTOTYPE_SIMD_BITMASK)
#undef PROTOTYPE_SIMD_BITMASK
#undef SIMD_BITMASK_LIST

#define SIMD_UNOP_LIST(V)   \
  V(F64x2Abs)               \
  V(F64x2Neg)               \
  V(F64x2Sqrt)              \
  V(F64x2Ceil)              \
  V(F64x2Floor)             \
  V(F64x2Trunc)             \
  V(F64x2PromoteLowF32x4)   \
  V(F32x4Abs)               \
  V(F32x4Neg)               \
  V(F32x4Sqrt)              \
  V(F32x4Ceil)              \
  V(F32x4Floor)             \
  V(F32x4Trunc)             \
  V(F32x4SConvertI32x4)     \
  V(F32x4UConvertI32x4)     \
  V(I64x2Neg)               \
  V(F64x2ConvertLowI32x4S)  \
  V(I64x2SConvertI32x4Low)  \
  V(I64x2SConvertI32x4High) \
  V(I32x4Neg)               \
  V(I32x4SConvertI16x8Low)  \
  V(I32x4SConvertI16x8High) \
  V(I32x4UConvertF32x4)     \
  V(I16x8SConvertI8x16Low)  \
  V(I16x8SConvertI8x16High) \
  V(I8x16Popcnt)            \
  V(S128Not)

#define PROTOTYPE_SIMD_UNOP(name) \
  void name(Simd128Register dst, Simd128Register src);
  SIMD_UNOP_LIST(PROTOTYPE_SIMD_UNOP)
#undef PROTOTYPE_SIMD_UNOP
#undef SIMD_UNOP_LIST

#define SIMD_UNOP_WITH_SCRATCH_LIST(V) \
  V(F32x4DemoteF64x2Zero)              \
  V(I64x2Abs)                          \
  V(I32x4Abs)                          \
  V(I32x4SConvertF32x4)                \
  V(I32x4TruncSatF64x2SZero)           \
  V(I32x4TruncSatF64x2UZero)           \
  V(I16x8Abs)                          \
  V(I16x8Neg)                          \
  V(I8x16Abs)                          \
  V(I8x16Neg)

#define PROTOTYPE_SIMD_UNOP_WITH_SCRATCH(name) \
  void name(Simd128Register dst, Simd128Register src, Simd128Register scratch);
  SIMD_UNOP_WITH_SCRATCH_LIST(PROTOTYPE_SIMD_UNOP_WITH_SCRATCH)
#undef PROTOTYPE_SIMD_UNOP_WITH_SCRATCH
#undef SIMD_UNOP_WITH_SCRATCH_LIST

#define SIMD_ALL_TRUE_LIST(V) \
  V(I64x2AllTrue)             \
  V(I32x4AllTrue)             \
  V(I16x8AllTrue)             \
  V(I8x16AllTrue)

#define PROTOTYPE_SIMD_ALL_TRUE(name)                             \
  void name(Register dst, Simd128Register src, Register scratch1, \
            Register scratch2, Simd128Register scratch3);
  SIMD_ALL_TRUE_LIST(PROTOTYPE_SIMD_ALL_TRUE)
#undef PROTOTYPE_SIMD_ALL_TRUE
#undef SIMD_ALL_TRUE_LIST

#define SIMD_QFM_LIST(V) \
  V(F64x2Qfma)           \
  V(F64x2Qfms)           \
  V(F32x4Qfma)           \
  V(F32x4Qfms)
#define PROTOTYPE_SIMD_QFM(name)                                             \
  void name(Simd128Register dst, Simd128Register src1, Simd128Register src2, \
            Simd128Register src3, Simd128Register scratch);
  SIMD_QFM_LIST(PROTOTYPE_SIMD_QFM)
#undef PROTOTYPE_SIMD_QFM
#undef SIMD_QFM_LIST

#define SIMD_EXT_ADD_PAIRWISE_LIST(V) \
  V(I32x4ExtAddPairwiseI16x8S)        \
  V(I32x4ExtAddPairwiseI16x8U)        \
  V(I16x8ExtAddPairwiseI8x16S)        \
  V(I16x8ExtAddPairwiseI8x16U)
#define PROTOTYPE_SIMD_EXT_ADD_PAIRWISE(name)         \
  void name(Simd128Register dst, Simd128Register src, \
            Simd128Register scratch1, Simd128Register scratch2);
  SIMD_EXT_ADD_PAIRWISE_LIST(PROTOTYPE_SIMD_EXT_ADD_PAIRWISE)
#undef PROTOTYPE_SIMD_EXT_ADD_PAIRWISE
#undef SIMD_EXT_ADD_PAIRWISE_LIST

  void LoadSimd128(Simd128Register dst, const MemOperand& mem,
                   Register scratch);
  void StoreSimd128(Simd128Register src, const MemOperand& mem,
                    Register scratch);
  void LoadSimd128LE(Simd128Register dst, const MemOperand& mem,
                     Register scratch);
  void StoreSimd128LE(Simd128Register src, const MemOperand& mem,
                      Register scratch1, Simd128Register scratch2);
  void LoadSimd128Uint64(Simd128Register reg, const MemOperand& mem,
                         Register scratch);
  void LoadSimd128Uint32(Simd128Register reg, const MemOperand& mem,
                         Register scratch);
  void LoadSimd128Uint16(Simd128Register reg, const MemOperand& mem,
                         Register scratch);
  void LoadSimd128Uint8(Simd128Register reg, const MemOperand& mem,
                        Register scratch);
  void StoreSimd128Uint64(Simd128Register reg, const MemOperand& mem,
                          Register scratch);
  void StoreSimd128Uint32(Simd128Register reg, const MemOperand& mem,
                          Register scratch);
  void StoreSimd128Uint16(Simd128Register reg, const MemOperand& mem,
                          Register scratch);
  void StoreSimd128Uint8(Simd128Register reg, const MemOperand& mem,
                         Register scratch);
  void LoadLane64LE(Simd128Register dst, const MemOperand& mem, int lane,
                    Register scratch1, Simd128Register scratch2);
  void LoadLane32LE(Simd128Register dst, const MemOperand& mem, int lane,
                    Register scratch1, Simd128Register scratch2);
  void LoadLane16LE(Simd128Register dst, const MemOperand& mem, int lane,
                    Register scratch1, Simd128Register scratch2);
  void LoadLane8LE(Simd128Register dst, const MemOperand& mem, int lane,
                   Register scratch1, Simd128Register scratch2);
  void StoreLane64LE(Simd128Register src, const MemOperand& mem, int lane,
                     Register scratch1, Simd128Register scratch2);
  void StoreLane32LE(Simd128Register src, const MemOperand& mem, int lane,
                     Register scratch1, Simd128Register scratch2);
  void StoreLane16LE(Simd128Register src, const MemOperand& mem, int lane,
                     Register scratch1, Simd128Register scratch2);
  void StoreLane8LE(Simd128Register src, const MemOperand& mem, int lane,
                    Register scratch1, Simd128Register scratch2);
  void LoadAndSplat64x2LE(Simd128Register dst, const MemOperand& mem,
                          Register scratch);
  void LoadAndSplat32x4LE(Simd128Register dst, const MemOperand& mem,
                          Register scratch);
  void LoadAndSplat16x8LE(Simd128Register dst, const MemOperand& me,
                          Register scratch);
  void LoadAndSplat8x16LE(Simd128Register dst, const MemOperand& mem,
                          Register scratch);
  void LoadAndExtend32x2SLE(Simd128Register dst, const MemOperand& mem,
                            Register scratch);
  void LoadAndExtend32x2ULE(Simd128Register dst, const MemOperand& mem,
                            Register scratch1, Simd128Register scratch2);
  void LoadAndExtend16x4SLE(Simd128Register dst, const MemOperand& mem,
                            Register scratch);
  void LoadAndExtend16x4ULE(Simd128Register dst, const MemOperand& mem,
                            Register scratch1, Simd128Register scratch2);
  void LoadAndExtend8x8SLE(Simd128Register dst, const MemOperand& mem,
                           Register scratch);
  void LoadAndExtend8x8ULE(Simd128Register dst, const MemOperand& mem,
                           Register scratch1, Simd128Register scratch2);
  void LoadV64ZeroLE(Simd128Register dst, const MemOperand& mem,
                     Register scratch1, Simd128Register scratch2);
  void LoadV32ZeroLE(Simd128Register dst, const MemOperand& mem,
                     Register scratch1, Simd128Register scratch2);
  void F64x2Splat(Simd128Register dst, DoubleRegister src, Register scratch);
  void F32x4Splat(Simd128Register dst, DoubleRegister src,
                  DoubleRegister scratch1, Register scratch2);
  void I64x2Splat(Simd128Register dst, Register src);
  void I32x4Splat(Simd128Register dst, Register src);
  void I16x8Splat(Simd128Register dst, Register src);
  void I8x16Splat(Simd128Register dst, Register src);
  void F64x2ExtractLane(DoubleRegister dst, Simd128Register src,
                        uint8_t imm_lane_idx, Simd128Register scratch1,
                        Register scratch2);
  void F32x4ExtractLane(DoubleRegister dst, Simd128Register src,
                        uint8_t imm_lane_idx, Simd128Register scratch1,
                        Register scratch2, Register scratch3);
  void I64x2ExtractLane(Register dst, Simd128Register src, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I32x4ExtractLane(Register dst, Simd128Register src, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I16x8ExtractLaneU(Register dst, Simd128Register src,
                         uint8_t imm_lane_idx, Simd128Register scratch);
  void I16x8ExtractLaneS(Register dst, Simd128Register src,
                         uint8_t imm_lane_idx, Simd128Register scratch);
  void I8x16ExtractLaneU(Register dst, Simd128Register src,
                         uint8_t imm_lane_idx, Simd128Register scratch);
  void I8x16ExtractLaneS(Register dst, Simd128Register src,
                         uint8_t imm_lane_idx, Simd128Register scratch);
  void F64x2ReplaceLane(Simd128Register dst, Simd128Register src1,
                        DoubleRegister src2, uint8_t imm_lane_idx,
                        Register scratch1, Simd128Register scratch2);
  void F32x4ReplaceLane(Simd128Register dst, Simd128Register src1,
                        DoubleRegister src2, uint8_t imm_lane_idx,
                        Register scratch1, DoubleRegister scratch2,
                        Simd128Register scratch3);
  void I64x2ReplaceLane(Simd128Register dst, Simd128Register src1,
                        Register src2, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I32x4ReplaceLane(Simd128Register dst, Simd128Register src1,
                        Register src2, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I16x8ReplaceLane(Simd128Register dst, Simd128Register src1,
                        Register src2, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I8x16ReplaceLane(Simd128Register dst, Simd128Register src1,
                        Register src2, uint8_t imm_lane_idx,
                        Simd128Register scratch);
  void I64x2Mul(Simd128Register dst, Simd128Register src1, Simd128Register src2,
                Register scratch1, Register scrahc2, Register scratch3,
                Simd128Register scratch4);
  void F64x2Min(Simd128Register dst, Simd128Register src1, Simd128Register src2,
                Simd128Register scratch1, Simd128Register scratch2);
  void F64x2Max(Simd128Register dst, Simd128Register src1, Simd128Register src2,
                Simd128Register scratch1, Simd128Register scratch2);
  void F64x2ConvertLowI32x4U(Simd128Register dst, Simd128Register src,
                             Register scratch1, Simd128Register scratch2);
  void I64x2UConvertI32x4Low(Simd128Register dst, Simd128Register src,
                             Register scratch1, Simd128Register scratch2);
  void I64x2UConvertI32x4High(Simd128Register dst, Simd128Register src,
                              Register scratch1, Simd128Register scratch2);
  void I32x4UConvertI16x8Low(Simd128Register dst, Simd128Register src,
                             Register scratch1, Simd128Register scratch2);
  void I32x4UConvertI16x8High(Simd128Register dst, Simd128Register src,
                              Register scratch1, Simd128Register scratch2);
  void I16x8UConvertI8x16Low(Simd128Register dst, Simd128Register src,
                             Register scratch1, Simd128Register scratch2);
  void I16x8UConvertI8x16High(Simd128Register dst, Simd128Register src,
                              Register scratch1, Simd128Register scratch2);
  void I8x16BitMask(Register dst, Simd128Register src, Register scratch1,
                    Register scratch2, Simd128Register scratch3);
  void I8x16Shuffle(Simd128Register dst, Simd128Register src1,
                    Simd128Register src2, uint64_t high, uint64_t low,
                    Register scratch1, Register scratch2,
                    Simd128Register scratch3);
  void I32x4DotI8x16AddS(Simd128Register dst, Simd128Register src1,
                         Simd128Register src2, Simd128Register src3);
  void V128AnyTrue(Register dst, Simd128Register src, Register scratch1,
                   Register scratch2, Simd128Register scratch3);
  void S128Const(Simd128Register dst, uint64_t high, uint64_t low,
                 Register scratch1, Register scratch2);
  void S128Select(Simd128Register dst, Simd128Register src1,
                  Simd128Register src2, Simd128Register mask);

  // It assumes that the arguments are located below the stack pointer.
  void LoadReceiver(Register dest) { LoadU64(dest, MemOperand(sp, 0)); }
  void StoreReceiver(Register rec) { StoreU64(rec, MemOperand(sp, 0)); }

  // ---------------------------------------------------------------------------
  // GC Support

  // Notify the garbage collector that we wrote a pointer into an object.
  // |object| is the object being stored into, |value| is the object being
  // stored.  value and scratch registers are clobbered by the operation.
  // The offset is the offset from the start of the object, not the offset from
  // the tagged HeapObject pointer.  For use with FieldMemOperand(reg, off).
  void RecordWriteField(
      Register object, int offset, Register value, Register slot_address,
      LinkRegisterStatus lr_status, SaveFPRegsMode save_fp,
      SmiCheck smi_check = SmiCheck::kInline,
      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());

  // For a given |object| notify the garbage collector that the slot |address|
  // has been written.  |value| is the object being stored. The value and
  // address registers are clobbered by the operation.
  void RecordWrite(
      Register object, Register slot_address, Register value,
      LinkRegisterStatus lr_status, SaveFPRegsMode save_fp,
      SmiCheck smi_check = SmiCheck::kInline,
      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());

  // Enter exit frame.
  // stack_space - extra stack space, used for parameters before call to C.
  void EnterExitFrame(Register scratch, int stack_space,
                      StackFrame::Type frame_type);

  // Leave the current exit frame.
  void LeaveExitFrame(Register scratch);

  // Load the global proxy from the current context.
  void LoadGlobalProxy(Register dst) {
    LoadNativeContextSlot(dst, Context::GLOBAL_PROXY_INDEX);
  }

  void LoadNativeContextSlot(Register dst, int index);

  // ----------------------------------------------------------------
  // new PPC macro-assembler interfaces that are slightly higher level
  // than assembler-ppc and may generate variable length sequences

  // load a literal double value <value> to FPR <result>

  void AddSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                     Register scratch);
  void SubSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                     Register scratch);
  void CmpSmiLiteral(Register src1, Tagged<Smi> smi, Register scratch,
                     CRegister cr = cr7);
  void CmplSmiLiteral(Register src1, Tagged<Smi> smi, Register scratch,
                      CRegister cr = cr7);
  void AndSmiLiteral(Register dst, Register src, Tagged<Smi> smi,
                     Register scratch, RCBit rc = LeaveRC);

  // ---------------------------------------------------------------------------
  // JavaScript invokes

  // Removes current frame and its arguments from the stack preserving
  // the arguments and a return address pushed to the stack for the next call.
  // Both |callee_args_count| and |caller_args_countg| do not include
  // receiver. |callee_args_count| is not modified. |caller_args_count|
  // is trashed.

  // Invoke the JavaScript function code by either calling or jumping.
  void InvokeFunctionCode(Register function, Register new_target,
                          Register expected_parameter_count,
                          Register actual_parameter_count, InvokeType type);

  // On function call, call into the debugger if necessary.
  void CheckDebugHook(Register fun, Register new_target,
                      Register expected_parameter_count,
                      Register actual_parameter_count);

  // Invoke the JavaScript function in the given register. Changes the
  // current context to the context in the function before invoking.
  void InvokeFunctionWithNewTarget(Register function, Register new_target,
                                   Register actual_parameter_count,
                                   InvokeType type);
  void InvokeFunction(Register function, Register expected_parameter_count,
                      Register actual_parameter_count, InvokeType type);

  // Exception handling

  // Push a new stack handler and link into stack handler chain.
  void PushStackHandler();

  // Unlink the stack handler on top of the stack from the stack handler chain.
  // Must preserve the result register.
  void PopStackHandler();

  // ---------------------------------------------------------------------------
  // Support functions.

  // Compare object type for heap object.  heap_object contains a non-Smi
  // whose object type should be compared with the given type.  This both
  // sets the flags and leaves the object type in the type_reg register.
  // It leaves the map in the map register (unless the type_reg and map register
  // are the same register).  It leaves the heap object in the heap_object
  // register unless the heap_object register is the same register as one of the
  // other registers.
  // Type_reg can be no_reg. In that case ip is used.
  void CompareObjectType(Register heap_object, Register map, Register type_reg,
                         InstanceType type);
  // Variant of the above, which compares against a type range rather than a
  // single type (lower_limit and higher_limit are inclusive).
  //
  // Always use unsigned comparisons: ls for a positive result.
  void CompareObjectTypeRange(Register heap_object, Register map,
                              Register type_reg, Register scratch,
                              InstanceType lower_limit,
                              InstanceType higher_limit);

  // Variant of the above, which only guarantees to set the correct eq/ne flag.
  // Neither map, nor type_reg might be set to any particular value.
  void IsObjectType(Register heap_object, Register scratch1, Register scratch2,
                    InstanceType type);

#if V8_STATIC_ROOTS_BOOL
  // Fast variant which is guaranteed to not actually load the instance type
  // from the map.
  void IsObjectTypeFast(Register heap_object, Register compressed_map_scratch,
                        InstanceType type, Register scratch);
  void CompareInstanceTypeWithUniqueCompressedMap(Register map,
                                                  Register scratch,
                                                  InstanceType type);
#endif  // V8_STATIC_ROOTS_BOOL

  // Compare object type for heap object, and branch if equal (or not.)
  // heap_object contains a non-Smi whose object type should be compared with
  // the given type.  This both sets the flags and leaves the object type in
  // the type_reg register. It leaves the map in the map register (unless the
  // type_reg and map register are the same register).  It leaves the heap
  // object in the heap_object register unless the heap_object register is the
  // same register as one of the other registers.
  void JumpIfObjectType(Register object, Register map, Register type_reg,
                        InstanceType type, Label* if_cond_pass,
                        Condition cond = eq);

  // Compare instance type in a map.  map contains a valid map object whose
  // object type should be compared with the given type.  This both
  // sets the flags and leaves the object type in the type_reg register.
  void CompareInstanceType(Register map, Register type_reg, InstanceType type);

  // Compare instance type ranges for a map (lower_limit and higher_limit
  // inclusive).
  //
  // Always use unsigned comparisons: ls for a positive result.
  void CompareInstanceTypeRange(Register map, Register type_reg,
                                Register scratch, InstanceType lower_limit,
                                InstanceType higher_limit);

  // Compare the object in a register to a value from the root list.
  // Uses the ip register as scratch.
  void CompareRoot(Register obj, RootIndex index);
  void CompareTaggedRoot(const Register& with, RootIndex index);

  void PushRoot(RootIndex index) {
    LoadRoot(r0, index);
    Push(r0);
  }

  // Compare the object in a register to a value and jump if they are equal.
  void JumpIfRoot(Register with, RootIndex index, Label* if_equal) {
    CompareRoot(with, index);
    beq(if_equal);
  }

  // Compare the object in a register to a value and jump if they are not equal.
  void JumpIfNotRoot(Register with, RootIndex index, Label* if_not_equal) {
    CompareRoot(with, index);
    bne(if_not_equal);
  }

  // Checks if value is in range [lower_limit, higher_limit] using a single
  // comparison.
  void CompareRange(Register value, Register scratch, unsigned lower_limit,
                    unsigned higher_limit);
  void JumpIfIsInRange(Register value, Register scratch, unsigned lower_limit,
                       unsigned higher_limit, Label* on_in_range);

  void JumpIfJSAnyIsNotPrimitive(
      Register heap_object, Register scratch, Label* target,
      Label::Distance distance = Label::kFar,
      Condition condition = Condition::kUnsignedGreaterThanEqual);
  void JumpIfJSAnyIsPrimitive(Register heap_object, Register scratch,
                              Label* target,
                              Label::Distance distance = Label::kFar) {
    return JumpIfJSAnyIsNotPrimitive(heap_object, scratch, target, distance,
                                     Condition::kUnsignedLessThan);
  }

  // Tiering support.
  void AssertFeedbackCell(Register object,
                          Register scratch) NOOP_UNLESS_DEBUG_CODE;
  void AssertFeedbackVector(Register object,
                            Register scratch) NOOP_UNLESS_DEBUG_CODE;
  void ReplaceClosureCodeWithOptimizedCode(Register optimized_code,
                                           Register closure, Register scratch1,
                                           Register slot_address);
  void GenerateTailCallToReturnedCode(Runtime::FunctionId function_id);
  void LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(
      Register flags, Register feedback_vector, CodeKind current_code_kind,
      Label* flags_need_processing);
  void OptimizeCodeOrTailCallOptimizedCodeSlot(Register flags,
                                               Register feedback_vector);

  // ---------------------------------------------------------------------------
  // Runtime calls

  static int CallSizeNotPredictableCodeSize(Address target,
                                            RelocInfo::Mode rmode,
                                            Condition cond = al);
  void CallJSEntry(Register target);

  // Call a runtime routine.
  void CallRuntime(const Runtime::Function* f, int num_arguments);

  // Convenience function: Same as above, but takes the fid instead.
  void CallRuntime(Runtime::FunctionId fid) {
    const Runtime::Function* function = Runtime::FunctionForId(fid);
    CallRuntime(function, function->nargs);
  }

  // Convenience function: Same as above, but takes the fid instead.
  void CallRuntime(Runtime::FunctionId fid, int num_arguments) {
    CallRuntime(Runtime::FunctionForId(fid), num_arguments);
  }

  // Convenience function: tail call a runtime routine (jump).
  void TailCallRuntime(Runtime::FunctionId fid);

  // Jump to a runtime routine.
  void JumpToExternalReference(const ExternalReference& builtin,
                               bool builtin_exit_frame = false);

  // ---------------------------------------------------------------------------
  // In-place weak references.
  void LoadWeakValue(Register out, Register in, Label* target_if_cleared);

  // ---------------------------------------------------------------------------
  // StatsCounter support

  void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                        Register scratch2) {
    if (!v8_flags.native_code_counters) return;
    EmitIncrementCounter(counter, value, scratch1, scratch2);
  }
  void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                            Register scratch2);
  void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                        Register scratch2) {
    if (!v8_flags.native_code_counters) return;
    EmitDecrementCounter(counter, value, scratch1, scratch2);
  }
  void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
                            Register scratch2);

  // ---------------------------------------------------------------------------
  // Stack limit utilities

  void StackOverflowCheck(Register num_args, Register scratch,
                          Label* stack_overflow);
  void LoadStackLimit(Register destination, StackLimitKind kind,
                      Register scratch);

  // ---------------------------------------------------------------------------
  // Smi utilities

  // Jump if either of the registers contain a non-smi.
  inline void JumpIfNotSmi(Register value, Label* not_smi_label) {
    TestIfSmi(value, r0);
    bne(not_smi_label, cr0);
  }

#if !defined(V8_COMPRESS_POINTERS) && !defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
  // Ensure it is permissible to read/write int value directly from
  // upper half of the smi.
  static_assert(kSmiTag == 0);
  static_assert(kSmiTagSize + kSmiShiftSize == 32);
#endif
#if V8_TARGET_ARCH_PPC64 && V8_TARGET_LITTLE_ENDIAN
#define SmiWordOffset(offset) (offset + kSystemPointerSize / 2)
#else
#define SmiWordOffset(offset) offset
#endif

  // Abort execution if argument is not a Constructor, enabled via --debug-code.
  void AssertConstructor(Register object) NOOP_UNLESS_DEBUG_CODE;

  // Abort execution if argument is not a JSFunction, enabled via --debug-code.
  void AssertFunction(Register object) NOOP_UNLESS_DEBUG_CODE;

  // Abort execution if argument is not a callable JSFunction, enabled via
  // --debug-code.
  void AssertCallableFunction(Register object) NOOP_UNLESS_DEBUG_CODE;

  // Abort execution if argument is not a JSBoundFunction,
  // enabled via --debug-code.
  void AssertBoundFunction(Register object) NOOP_UNLESS_DEBUG_CODE;

  // Abort execution if argument is not a JSGeneratorObject (or subclass),
  // enabled via --debug-code.
  void AssertGeneratorObject(Register object) NOOP_UNLESS_DEBUG_CODE;

  // Abort execution if argument is not undefined or an AllocationSite, enabled
  // via --debug-code.
  void AssertUndefinedOrAllocationSite(Register object,
                                       Register scratch) NOOP_UNLESS_DEBUG_CODE;

  void AssertJSAny(Register object, Register map_tmp, Register tmp,
                   AbortReason abort_reason) NOOP_UNLESS_DEBUG_CODE;
  // ---------------------------------------------------------------------------
  // Patching helpers.

  template <typename Field>
  void DecodeField(Register dst, Register src, RCBit rc = LeaveRC) {
    ExtractBitRange(dst, src, Field::kShift + Field::kSize - 1, Field::kShift,
                    rc);
  }

  template <typename Field>
  void DecodeField(Register reg, RCBit rc = LeaveRC) {
    DecodeField<Field>(reg, reg, rc);
  }

  void TestCodeIsMarkedForDeoptimization(Register code, Register scratch1,
                                         Register scratch2);
  Operand ClearedValue() const;

 private:
  static const int kSmiShift = kSmiTagSize + kSmiShiftSize;

  int CalculateStackPassedWords(int num_reg_arguments,
                                int num_double_arguments);

  // Helper functions for generating invokes.
  void InvokePrologue(Register expected_parameter_count,
                      Register actual_parameter_count, Label* done,
                      InvokeType type);

  DISALLOW_IMPLICIT_CONSTRUCTORS(MacroAssembler);
};

struct MoveCycleState {
  // Whether a move in the cycle needs a double scratch register.
  bool pending_double_scratch_register_use = false;
};

// Provides access to exit frame parameters (GC-ed).
inline MemOperand ExitFrameStackSlotOperand(int offset) {
  // The slot at [sp] is reserved in all ExitFrames for storing the return
  // address before doing the actual call, it's necessary for frame iteration
  // (see StoreReturnAddressAndCall for details).
  static constexpr int kSPOffset = 1 * kSystemPointerSize;
  return MemOperand(sp, (kStackFrameExtraParamSlot * kSystemPointerSize) +
                            offset + kSPOffset);
}

// Provides access to exit frame stack space (not GC-ed).
inline MemOperand ExitFrameCallerStackSlotOperand(int index) {
  return MemOperand(
      fp, (BuiltinExitFrameConstants::kFixedSlotCountAboveFp + index) *
              kSystemPointerSize);
}

// Calls an API function. Allocates HandleScope, extracts returned value
// from handle and propagates exceptions. Clobbers C argument registers
// and C caller-saved registers. Restores context. On return removes
//   (*argc_operand + slots_to_drop_on_return) * kSystemPointerSize
// (GCed, includes the call JS arguments space and the additional space
// allocated for the fast call).
void CallApiFunctionAndReturn(MacroAssembler* masm, bool with_profiling,
                              Register function_address,
                              ExternalReference thunk_ref, Register thunk_arg,
                              int slots_to_drop_on_return,
                              MemOperand* argc_operand,
                              MemOperand return_value_operand);

#define ACCESS_MASM(masm) masm->

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_PPC_MACRO_ASSEMBLER_PPC_H_
                                                                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/codegen/ppc/register-ppc.h                                                  0000664 0000000 0000000 00000027042 14746647661 0021604 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_PPC_REGISTER_PPC_H_
#define V8_CODEGEN_PPC_REGISTER_PPC_H_

#include "src/codegen/register-base.h"

namespace v8 {
namespace internal {

// clang-format off
#define GENERAL_REGISTERS(V)                              \
  V(r0)  V(sp)  V(r2)  V(r3)  V(r4)  V(r5)  V(r6)  V(r7)  \
  V(r8)  V(r9)  V(r10) V(r11) V(ip) V(r13) V(r14) V(r15)  \
  V(r16) V(r17) V(r18) V(r19) V(r20) V(r21) V(r22) V(r23) \
  V(r24) V(r25) V(r26) V(r27) V(r28) V(r29) V(r30) V(fp)

#define ALWAYS_ALLOCATABLE_GENERAL_REGISTERS(V)                  \
  V(r3)  V(r4)  V(r5)  V(r6)  V(r7)                       \
  V(r8)  V(r9)  V(r10) V(r14) V(r15)                      \
  V(r16) V(r17) V(r18) V(r19) V(r20) V(r21) V(r22) V(r23) \
  V(r24) V(r25) V(r26) V(r30)

#if V8_EMBEDDED_CONSTANT_POOL_BOOL
#define MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V)
#else
#define MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V) V(r28)
#endif

#ifdef V8_COMPRESS_POINTERS
#define MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)
#else
#define MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)  V(r27)
#endif

#define ALLOCATABLE_GENERAL_REGISTERS(V)  \
  ALWAYS_ALLOCATABLE_GENERAL_REGISTERS(V) \
  MAYBE_ALLOCATEABLE_CONSTANT_POOL_REGISTER(V) \
  MAYBE_ALLOCATABLE_CAGE_REGISTERS(V)

#define LOW_DOUBLE_REGISTERS(V)                           \
  V(d0)  V(d1)  V(d2)  V(d3)  V(d4)  V(d5)  V(d6)  V(d7)  \
  V(d8)  V(d9)  V(d10) V(d11) V(d12) V(d13) V(d14) V(d15)

#define NON_LOW_DOUBLE_REGISTERS(V)                       \
  V(d16) V(d17) V(d18) V(d19) V(d20) V(d21) V(d22) V(d23) \
  V(d24) V(d25) V(d26) V(d27) V(d28) V(d29) V(d30) V(d31)

#define DOUBLE_REGISTERS(V) \
  LOW_DOUBLE_REGISTERS(V) NON_LOW_DOUBLE_REGISTERS(V)

#define ALLOCATABLE_DOUBLE_REGISTERS(V)                   \
  V(d1)  V(d2)  V(d3)  V(d4)  V(d5)  V(d6)  V(d7)         \
  V(d8)  V(d9)  V(d10) V(d11) V(d12) V(d15)               \
  V(d16) V(d17) V(d18) V(d19) V(d20) V(d21) V(d22) V(d23) \
  V(d24) V(d25) V(d26) V(d27) V(d28) V(d29) V(d30) V(d31)

#define FLOAT_REGISTERS DOUBLE_REGISTERS
#define SIMD128_REGISTERS(V)                              \
  V(v0)  V(v1)  V(v2)  V(v3)  V(v4)  V(v5)  V(v6)  V(v7)  \
  V(v8)  V(v9)  V(v10) V(v11) V(v12) V(v13) V(v14) V(v15) \
  V(v16) V(v17) V(v18) V(v19) V(v20) V(v21) V(v22) V(v23) \
  V(v24) V(v25) V(v26) V(v27) V(v28) V(v29) V(v30) V(v31)

#define ALLOCATABLE_SIMD128_REGISTERS(V)                  \
  V(v0)  V(v1)  V(v2)  V(v3)  V(v4)  V(v5)  V(v6)  V(v7)  \
  V(v8)  V(v9)  V(v10) V(v11) V(v12)                      \
  V(v16) V(v17) V(v18) V(v19) V(v20) V(v21) V(v22) V(v23) \
  V(v24) V(v25) V(v26) V(v27) V(v28) V(v29) V(v30) V(v31)

#define C_REGISTERS(V)                                            \
  V(cr0)  V(cr1)  V(cr2)  V(cr3)  V(cr4)  V(cr5)  V(cr6)  V(cr7)  \
  V(cr8)  V(cr9)  V(cr10) V(cr11) V(cr12) V(cr15)
// clang-format on

// The following constants describe the stack frame linkage area as
// defined by the ABI.  Note that kNumRequiredStackFrameSlots must
// satisfy alignment requirements (rounding up if required).
#if V8_TARGET_ARCH_PPC64 &&     \
    (V8_TARGET_LITTLE_ENDIAN || \
     (defined(_CALL_ELF) && _CALL_ELF == 2))  // ELFv2 ABI
// [0] back chain
// [1] condition register save area
// [2] link register save area
// [3] TOC save area
// [4] Parameter1 save area
// ...
// [11] Parameter8 save area
// [12] Parameter9 slot (if necessary)
// ...
const int kNumRequiredStackFrameSlots = 12;
const int kStackFrameLRSlot = 2;
const int kStackFrameExtraParamSlot = 12;
#else  // AIX
// [0] back chain
// [1] condition register save area
// [2] link register save area
// [3] reserved for compiler
// [4] reserved by binder
// [5] TOC save area
// [6] Parameter1 save area
// ...
// [13] Parameter8 save area
// [14] Parameter9 slot (if necessary)
// ...
const int kNumRequiredStackFrameSlots = 14;
const int kStackFrameLRSlot = 2;
const int kStackFrameExtraParamSlot = 14;
#endif

enum RegisterCode {
#define REGISTER_CODE(R) kRegCode_##R,
  GENERAL_REGISTERS(REGISTER_CODE)
#undef REGISTER_CODE
      kRegAfterLast
};

class Register : public RegisterBase<Register, kRegAfterLast> {
 public:
#if V8_TARGET_LITTLE_ENDIAN
  static constexpr int kMantissaOffset = 0;
  static constexpr int kExponentOffset = 4;
#else
  static constexpr int kMantissaOffset = 4;
  static constexpr int kExponentOffset = 0;
#endif

 private:
  friend class RegisterBase;
  explicit constexpr Register(int code) : RegisterBase(code) {}
};

ASSERT_TRIVIALLY_COPYABLE(Register);
static_assert(sizeof(Register) <= sizeof(int),
              "Register can efficiently be passed by value");

// Assign |source| value to |no_reg| and return the |source|'s previous value.
inline Register ReassignRegister(Register& source) {
  Register result = source;
  source = Register::no_reg();
  return result;
}

#define DEFINE_REGISTER(R) \
  constexpr Register R = Register::from_code(kRegCode_##R);
GENERAL_REGISTERS(DEFINE_REGISTER)
#undef DEFINE_REGISTER
constexpr Register no_reg = Register::no_reg();

// Aliases
constexpr Register kConstantPoolRegister = r28;  // Constant pool.
constexpr Register kRootRegister = r29;          // Roots array pointer.
constexpr Register cp = r30;                     // JavaScript context pointer.
#ifdef V8_COMPRESS_POINTERS
constexpr Register kPtrComprCageBaseRegister = r27;  // callee save
#else
constexpr Register kPtrComprCageBaseRegister = no_reg;
#endif

// PPC64 calling convention
constexpr Register kCArgRegs[] = {r3, r4, r5, r6, r7, r8, r9, r10};
static const int kRegisterPassedArguments = arraysize(kCArgRegs);

// Returns the number of padding slots needed for stack pointer alignment.
constexpr int ArgumentPaddingSlots(int argument_count) {
  // No argument padding required.
  return 0;
}

constexpr AliasingKind kFPAliasing = AliasingKind::kIndependent;
constexpr bool kSimdMaskRegisters = false;

//     |      | 0
//     |      | 1
//     |      | 2
//     |      | ...
//     |      | 31
// VSX |
//     |      | 32
//     |      | 33
//     |  VMX | 34
//     |      | ...
//     |      | 63
//
// VSX registers (0 to 63) can be used by VSX vector instructions, which are
// mainly focused on Floating Point arithmetic. They do have few Integer
// Instructions such as logical operations, merge and select. The main Simd
// integer instructions such as add/sub/mul/ extract_lane/replace_lane,
// comparisons etc. are only available with VMX instructions and can only access
// the VMX set of vector registers (which is a subset of VSX registers). So to
// assure access to all Simd instructions in V8 and avoid moving data between
// registers, we are only using the upper 32 registers (VMX set) for Simd
// operations and only use the lower set for scalar (non simd) floating point
// operations which makes our Simd register set separate from Floating Point
// ones.
enum Simd128RegisterCode {
#define REGISTER_CODE(R) kSimd128Code_##R,
  SIMD128_REGISTERS(REGISTER_CODE)
#undef REGISTER_CODE
      kSimd128AfterLast
};

// Simd128 register.
class Simd128Register
    : public RegisterBase<Simd128Register, kSimd128AfterLast> {
  friend class RegisterBase;

 public:
  explicit constexpr Simd128Register(int code) : RegisterBase(code) {}
};
ASSERT_TRIVIALLY_COPYABLE(Simd128Register);
static_assert(sizeof(Simd128Register) <= sizeof(int),
              "Simd128Register can efficiently be passed by value");

enum DoubleRegisterCode {
#define REGISTER_CODE(R) kDoubleCode_##R,
  DOUBLE_REGISTERS(REGISTER_CODE)
#undef REGISTER_CODE
      kDoubleAfterLast
};

// Double word FP register.
class DoubleRegister : public RegisterBase<DoubleRegister, kDoubleAfterLast> {
 public:
  // A few double registers are reserved: one as a scratch register and one to
  // hold 0.0, that does not fit in the immediate field of vmov instructions.
  // d14: 0.0
  // d15: scratch register.
  static constexpr int kSizeInBytes = 8;

  // This function differs from kNumRegisters by returning the number of double
  // registers supported by the current CPU, while kNumRegisters always returns
  // 32.
  inline static int SupportedRegisterCount();

  // On PPC Simdi128 registers are separate from Double registers.
  // More details can be found here: https://crrev.com/c/2718472 . This is a
  // helper function to cast a Double to a Simdi128 register.
  Simd128Register toSimd() const {
    int reg_code = code();
    V8_ASSUME(reg_code >= 0 && reg_code < kSimd128AfterLast);
    return Simd128Register(reg_code);
  }

 private:
  friend class RegisterBase;
  explicit constexpr DoubleRegister(int code) : RegisterBase(code) {}
};

ASSERT_TRIVIALLY_COPYABLE(DoubleRegister);
static_assert(sizeof(DoubleRegister) <= sizeof(int),
              "DoubleRegister can efficiently be passed by value");

using FloatRegister = DoubleRegister;

#define DECLARE_SIMD128_REGISTER(R) \
  constexpr Simd128Register R = Simd128Register::from_code(kSimd128Code_##R);
SIMD128_REGISTERS(DECLARE_SIMD128_REGISTER)
#undef DECLARE_SIMD128_REGISTER
const Simd128Register no_simdreg = Simd128Register::no_reg();

#define DEFINE_REGISTER(R) \
  constexpr DoubleRegister R = DoubleRegister::from_code(kDoubleCode_##R);
DOUBLE_REGISTERS(DEFINE_REGISTER)
#undef DEFINE_REGISTER
constexpr DoubleRegister no_dreg = DoubleRegister::no_reg();

constexpr DoubleRegister kFirstCalleeSavedDoubleReg = d14;
constexpr DoubleRegister kLastCalleeSavedDoubleReg = d31;
constexpr DoubleRegister kDoubleRegZero = d14;
constexpr DoubleRegister kScratchDoubleReg = d13;
constexpr Simd128Register kSimd128RegZero = v14;
constexpr Simd128Register kScratchSimd128Reg = v13;
constexpr Simd128Register kScratchSimd128Reg2 = v15;

Register ToRegister(int num);

enum CRegisterCode {
#define REGISTER_CODE(R) kCCode_##R,
  C_REGISTERS(REGISTER_CODE)
#undef REGISTER_CODE
      kCAfterLast
};

// Coprocessor register
class CRegister : public RegisterBase<CRegister, kCAfterLast> {
  friend class RegisterBase;
  explicit constexpr CRegister(int code) : RegisterBase(code) {}
};

constexpr CRegister no_creg = CRegister::no_reg();
#define DECLARE_C_REGISTER(R) \
  constexpr CRegister R = CRegister::from_code(kCCode_##R);
C_REGISTERS(DECLARE_C_REGISTER)
#undef DECLARE_C_REGISTER

// Define {RegisterName} methods for the register types.
DEFINE_REGISTER_NAMES(Register, GENERAL_REGISTERS)
DEFINE_REGISTER_NAMES(DoubleRegister, DOUBLE_REGISTERS)
DEFINE_REGISTER_NAMES(Simd128Register, SIMD128_REGISTERS)

// Give alias names to registers for calling conventions.
constexpr Register kReturnRegister0 = r3;
constexpr Register kReturnRegister1 = r4;
constexpr Register kReturnRegister2 = r5;
constexpr Register kJSFunctionRegister = r4;
constexpr Register kContextRegister = r30;
constexpr Register kAllocateSizeRegister = r4;
constexpr Register kInterpreterAccumulatorRegister = r3;
constexpr Register kInterpreterBytecodeOffsetRegister = r15;
constexpr Register kInterpreterBytecodeArrayRegister = r16;
constexpr Register kInterpreterDispatchTableRegister = r17;

constexpr Register kJavaScriptCallArgCountRegister = r3;
constexpr Register kJavaScriptCallCodeStartRegister = r5;
constexpr Register kJavaScriptCallTargetRegister = kJSFunctionRegister;
constexpr Register kJavaScriptCallNewTargetRegister = r6;
constexpr Register kJavaScriptCallExtraArg1Register = r5;

constexpr Register kRuntimeCallFunctionRegister = r4;
constexpr Register kRuntimeCallArgCountRegister = r3;
constexpr Register kRuntimeCallArgvRegister = r5;
constexpr Register kWasmInstanceRegister = r10;
constexpr Register kWasmCompileLazyFuncIndexRegister = r15;

constexpr DoubleRegister kFPReturnRegister0 = d1;

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_PPC_REGISTER_PPC_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              node-23.7.0/deps/v8/src/codegen/ppc/reglist-ppc.h                                                   0000664 0000000 0000000 00000004466 14746647661 0021436 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_PPC_REGLIST_PPC_H_
#define V8_CODEGEN_PPC_REGLIST_PPC_H_

#include "src/codegen/register-arch.h"
#include "src/codegen/reglist-base.h"

namespace v8 {
namespace internal {

using RegList = RegListBase<Register>;
using DoubleRegList = RegListBase<DoubleRegister>;
using Simd128RegList = RegListBase<Simd128Register>;
ASSERT_TRIVIALLY_COPYABLE(RegList);
ASSERT_TRIVIALLY_COPYABLE(DoubleRegList);

// Register list in load/store instructions
// Note that the bit values must match those used in actual instruction encoding

// Caller-saved/arguments registers
const RegList kJSCallerSaved = {r3,   // a1
                                r4,   // a2
                                r5,   // a3
                                r6,   // a4
                                r7,   // a5
                                r8,   // a6
                                r9,   // a7
                                r10,  // a8
                                r11};

const int kNumJSCallerSaved = 9;

// Return the code of the n-th caller-saved register available to JavaScript
// e.g. JSCallerSavedReg(0) returns r0.code() == 0
int JSCallerSavedCode(int n);

// Callee-saved registers preserved when switching from C to JavaScript
const RegList kCalleeSaved = {r14, r15, r16, r17, r18, r19, r20, r21, r22,
                              r23, r24, r25, r26, r27, r28, r29, r30, fp};

const int kNumCalleeSaved = 18;

const DoubleRegList kCallerSavedDoubles = {d0, d1, d2, d3,  d4,  d5,  d6,
                                           d7, d8, d9, d10, d11, d12, d13};

const Simd128RegList kCallerSavedSimd128s = {v0,  v1,  v2,  v3,  v4,  v5,  v6,
                                             v7,  v8,  v9,  v10, v11, v12, v13,
                                             v14, v15, v16, v17, v18, v19};

const int kNumCallerSavedDoubles = 14;

const DoubleRegList kCalleeSavedDoubles = {d14, d15, d16, d17, d18, d19,
                                           d20, d21, d22, d23, d24, d25,
                                           d26, d27, d28, d29, d30, d31};

const int kNumCalleeSavedDoubles = 18;

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_PPC_REGLIST_PPC_H_
                                                                                                                                                                                                          node-23.7.0/deps/v8/src/codegen/register-arch.h                                                     0000664 0000000 0000000 00000002037 14746647661 0021152 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGISTER_ARCH_H_
#define V8_CODEGEN_REGISTER_ARCH_H_

#include "src/codegen/register-base.h"

#if V8_TARGET_ARCH_IA32
#include "src/codegen/ia32/register-ia32.h"
#elif V8_TARGET_ARCH_X64
#include "src/codegen/x64/register-x64.h"
#elif V8_TARGET_ARCH_ARM64
#include "src/codegen/arm64/register-arm64.h"
#elif V8_TARGET_ARCH_ARM
#include "src/codegen/arm/register-arm.h"
#elif V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
#include "src/codegen/ppc/register-ppc.h"
#elif V8_TARGET_ARCH_MIPS64
#include "src/codegen/mips64/register-mips64.h"
#elif V8_TARGET_ARCH_LOONG64
#include "src/codegen/loong64/register-loong64.h"
#elif V8_TARGET_ARCH_S390
#include "src/codegen/s390/register-s390.h"
#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
#include "src/codegen/riscv/register-riscv.h"
#else
#error Unknown architecture.
#endif

#endif  // V8_CODEGEN_REGISTER_ARCH_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/codegen/register-base.h                                                     0000664 0000000 0000000 00000006151 14746647661 0021150 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2012 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGISTER_BASE_H_
#define V8_CODEGEN_REGISTER_BASE_H_

#include "src/base/bits.h"
#include "src/base/bounds.h"
#include "src/common/globals.h"

namespace v8 {

namespace internal {

// Base type for CPU Registers.
//
// 1) We would prefer to use an enum for registers, but enum values are
// assignment-compatible with int, which has caused code-generation bugs.
//
// 2) By not using an enum, we are possibly preventing the compiler from
// doing certain constant folds, which may significantly reduce the
// code generated for some assembly instructions (because they boil down
// to a few constants). If this is a problem, we could change the code
// such that we use an enum in optimized mode, and the class in debug
// mode. This way we get the compile-time error checking in debug mode
// and best performance in optimized code.
template <typename SubType, int kAfterLastRegister>
class RegisterBase {
 public:
  static constexpr int8_t kCode_no_reg = -1;
  static constexpr int8_t kNumRegisters = kAfterLastRegister;

  static constexpr SubType no_reg() { return SubType{kCode_no_reg}; }

  static constexpr SubType from_code(int8_t code) {
    V8_ASSUME(code >= 0 && code < kNumRegisters);
    return SubType{code};
  }

  constexpr bool is_valid() const { return reg_code_ != kCode_no_reg; }

  constexpr int8_t code() const {
#if V8_TARGET_ARCH_ARM64
    // Arm64 uses kSPRegInternalCode which is > kNumRegisters.
    V8_ASSUME(reg_code_ >= 0);
#else
    V8_ASSUME(reg_code_ >= 0 && reg_code_ < kNumRegisters);
#endif
    return reg_code_;
  }

  inline constexpr bool operator==(
      const RegisterBase<SubType, kAfterLastRegister>& other) const {
    return reg_code_ == other.reg_code_;
  }
  inline constexpr bool operator!=(
      const RegisterBase<SubType, kAfterLastRegister>& other) const {
    return reg_code_ != other.reg_code_;
  }

  // Used to print the name of some special registers.
  static const char* GetSpecialRegisterName(int code) { return "UNKNOWN"; }

 protected:
  explicit constexpr RegisterBase(int code) : reg_code_(code) {}

 private:
  int8_t reg_code_;
  static_assert(kAfterLastRegister <= kMaxInt8);
};

template <typename RegType,
          typename = decltype(RegisterName(std::declval<RegType>()))>
inline std::ostream& operator<<(std::ostream& os, RegType reg) {
  return os << RegisterName(reg);
}

// Helper macros to define a {RegisterName} method based on a macro list
// containing all names.
#define DEFINE_REGISTER_NAMES_NAME(name) #name,
#define DEFINE_REGISTER_NAMES(RegType, LIST)                                   \
  inline const char* RegisterName(RegType reg) {                               \
    static constexpr const char* Names[] = {LIST(DEFINE_REGISTER_NAMES_NAME)}; \
    static_assert(arraysize(Names) == RegType::kNumRegisters);                 \
    return reg.is_valid() ? Names[reg.code()] : "invalid";                     \
  }

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_REGISTER_BASE_H_
                                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/v8/src/codegen/register-configuration.cc                                           0000664 0000000 0000000 00000035646 14746647661 0023256 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/codegen/register-configuration.h"

#include "src/base/lazy-instance.h"
#include "src/codegen/cpu-features.h"
#include "src/codegen/register.h"
#include "src/common/globals.h"

namespace v8 {
namespace internal {

namespace {

#define REGISTER_COUNT(R) 1 +
static const int kMaxAllocatableGeneralRegisterCount =
    ALLOCATABLE_GENERAL_REGISTERS(REGISTER_COUNT) 0;
static const int kMaxAllocatableDoubleRegisterCount =
    ALLOCATABLE_DOUBLE_REGISTERS(REGISTER_COUNT) 0;
#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
static const int kMaxAllocatableSIMD128RegisterCount =
    ALLOCATABLE_SIMD128_REGISTERS(REGISTER_COUNT) 0;
#endif

static const int kAllocatableGeneralCodes[] = {
#define REGISTER_CODE(R) kRegCode_##R,
    ALLOCATABLE_GENERAL_REGISTERS(REGISTER_CODE)};
#undef REGISTER_CODE

#define REGISTER_CODE(R) kDoubleCode_##R,
static const int kAllocatableDoubleCodes[] = {
    ALLOCATABLE_DOUBLE_REGISTERS(REGISTER_CODE)};
#if V8_TARGET_ARCH_ARM
static const int kAllocatableNoVFP32DoubleCodes[] = {
    ALLOCATABLE_NO_VFP32_DOUBLE_REGISTERS(REGISTER_CODE)};
#endif  // V8_TARGET_ARCH_ARM
#undef REGISTER_CODE

#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
static const int kAllocatableSIMD128Codes[] = {
#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_RISCV32
#define REGISTER_CODE(R) kVRCode_##R,
#else
#define REGISTER_CODE(R) kSimd128Code_##R,
#endif
    ALLOCATABLE_SIMD128_REGISTERS(REGISTER_CODE)};
#undef REGISTER_CODE
#endif  // V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 ||
        // V8_TARGET_ARCH_PPC64

static_assert(RegisterConfiguration::kMaxGeneralRegisters >=
              Register::kNumRegisters);
static_assert(RegisterConfiguration::kMaxFPRegisters >=
              FloatRegister::kNumRegisters);
static_assert(RegisterConfiguration::kMaxFPRegisters >=
              DoubleRegister::kNumRegisters);
static_assert(RegisterConfiguration::kMaxFPRegisters >=
              Simd128Register::kNumRegisters);
#if V8_TARGET_ARCH_X64
static_assert(RegisterConfiguration::kMaxFPRegisters >=
              Simd256Register::kNumRegisters);
#endif

static int get_num_simd128_registers() {
  return
#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
      Simd128Register::kNumRegisters;
#else
      0;
#endif  // V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 ||
        // V8_TARGET_ARCH_PPC64
}

static int get_num_simd256_registers() { return 0; }

// Callers on architectures other than Arm expect this to be be constant
// between build and runtime. Avoid adding variability on other platforms.
static int get_num_allocatable_double_registers() {
  return
#if V8_TARGET_ARCH_IA32
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_X64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_ARM
      CpuFeatures::IsSupported(VFP32DREGS)
          ? kMaxAllocatableDoubleRegisterCount
          : (ALLOCATABLE_NO_VFP32_DOUBLE_REGISTERS(REGISTER_COUNT) 0);
#elif V8_TARGET_ARCH_ARM64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_MIPS
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_MIPS64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_LOONG64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_PPC
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_PPC64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_S390
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_RISCV64
      kMaxAllocatableDoubleRegisterCount;
#elif V8_TARGET_ARCH_RISCV32
      kMaxAllocatableDoubleRegisterCount;
#else
#error Unsupported target architecture.
#endif
}

#undef REGISTER_COUNT

static int get_num_allocatable_simd128_registers() {
  return
#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
      kMaxAllocatableSIMD128RegisterCount;
#else
      0;
#endif
}

static int get_num_allocatable_simd256_registers() { return 0; }

// Callers on architectures other than Arm expect this to be be constant
// between build and runtime. Avoid adding variability on other platforms.
static const int* get_allocatable_double_codes() {
  return
#if V8_TARGET_ARCH_ARM
      CpuFeatures::IsSupported(VFP32DREGS) ? kAllocatableDoubleCodes
                                           : kAllocatableNoVFP32DoubleCodes;
#else
      kAllocatableDoubleCodes;
#endif
}

static const int* get_allocatable_simd128_codes() {
  return
#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
      kAllocatableSIMD128Codes;
#else
      kAllocatableDoubleCodes;
#endif
}

class ArchDefaultRegisterConfiguration : public RegisterConfiguration {
 public:
  ArchDefaultRegisterConfiguration()
      : RegisterConfiguration(
            kFPAliasing, Register::kNumRegisters, DoubleRegister::kNumRegisters,
            get_num_simd128_registers(), get_num_simd256_registers(),
            kMaxAllocatableGeneralRegisterCount,
            get_num_allocatable_double_registers(),
            get_num_allocatable_simd128_registers(),
            get_num_allocatable_simd256_registers(), kAllocatableGeneralCodes,
            get_allocatable_double_codes(), get_allocatable_simd128_codes()) {}
};

DEFINE_LAZY_LEAKY_OBJECT_GETTER(ArchDefaultRegisterConfiguration,
                                GetDefaultRegisterConfiguration)

// RestrictedRegisterConfiguration uses the subset of allocatable general
// registers the architecture support, which results into generating assembly
// to use less registers. Currently, it's only used by RecordWrite code stub.
class RestrictedRegisterConfiguration : public RegisterConfiguration {
 public:
  RestrictedRegisterConfiguration(
      int num_allocatable_general_registers,
      std::unique_ptr<int[]> allocatable_general_register_codes,
      std::unique_ptr<char const*[]> allocatable_general_register_names)
      : RegisterConfiguration(
            kFPAliasing, Register::kNumRegisters, DoubleRegister::kNumRegisters,
            get_num_simd128_registers(), get_num_simd256_registers(),
            num_allocatable_general_registers,
            get_num_allocatable_double_registers(),
            get_num_allocatable_simd128_registers(),
            get_num_allocatable_simd256_registers(),
            allocatable_general_register_codes.get(),
            get_allocatable_double_codes(), get_allocatable_simd128_codes()),
        allocatable_general_register_codes_(
            std::move(allocatable_general_register_codes)),
        allocatable_general_register_names_(
            std::move(allocatable_general_register_names)) {
    for (int i = 0; i < num_allocatable_general_registers; ++i) {
      DCHECK(
          IsAllocatableGeneralRegister(allocatable_general_register_codes_[i]));
    }
  }

  bool IsAllocatableGeneralRegister(int code) {
    for (int i = 0; i < kMaxAllocatableGeneralRegisterCount; ++i) {
      if (code == kAllocatableGeneralCodes[i]) {
        return true;
      }
    }
    return false;
  }

 private:
  std::unique_ptr<int[]> allocatable_general_register_codes_;
  std::unique_ptr<char const*[]> allocatable_general_register_names_;
};

}  // namespace

const RegisterConfiguration* RegisterConfiguration::Default() {
  return GetDefaultRegisterConfiguration();
}

const RegisterConfiguration* RegisterConfiguration::RestrictGeneralRegisters(
    RegList registers) {
  int num = registers.Count();
  std::unique_ptr<int[]> codes{new int[num]};
  std::unique_ptr<char const* []> names { new char const*[num] };
  int counter = 0;
  for (int i = 0; i < Default()->num_allocatable_general_registers(); ++i) {
    auto reg = Register::from_code(Default()->GetAllocatableGeneralCode(i));
    if (registers.has(reg)) {
      DCHECK(counter < num);
      codes[counter] = reg.code();
      names[counter] = RegisterName(Register::from_code(i));
      counter++;
    }
  }

  return new RestrictedRegisterConfiguration(num, std::move(codes),
                                             std::move(names));
}

RegisterConfiguration::RegisterConfiguration(
    AliasingKind fp_aliasing_kind, int num_general_registers,
    int num_double_registers, int num_simd128_registers,
    int num_simd256_registers, int num_allocatable_general_registers,
    int num_allocatable_double_registers, int num_allocatable_simd128_registers,
    int num_allocatable_simd256_registers, const int* allocatable_general_codes,
    const int* allocatable_double_codes,
    const int* independent_allocatable_simd128_codes)
    : num_general_registers_(num_general_registers),
      num_float_registers_(0),
      num_double_registers_(num_double_registers),
      num_simd128_registers_(num_simd128_registers),
      num_simd256_registers_(num_simd256_registers),
      num_allocatable_general_registers_(num_allocatable_general_registers),
      num_allocatable_float_registers_(0),
      num_allocatable_double_registers_(num_allocatable_double_registers),
      num_allocatable_simd128_registers_(num_allocatable_simd128_registers),
      num_allocatable_simd256_registers_(num_allocatable_simd256_registers),
      allocatable_general_codes_mask_(0),
      allocatable_float_codes_mask_(0),
      allocatable_double_codes_mask_(0),
      allocatable_simd128_codes_mask_(0),
      allocatable_simd256_codes_mask_(0),
      allocatable_general_codes_(allocatable_general_codes),
      allocatable_double_codes_(allocatable_double_codes),
      fp_aliasing_kind_(fp_aliasing_kind) {
  DCHECK_LE(num_general_registers_,
            RegisterConfiguration::kMaxGeneralRegisters);
  DCHECK_LE(num_double_registers_, RegisterConfiguration::kMaxFPRegisters);
  for (int i = 0; i < num_allocatable_general_registers_; ++i) {
    allocatable_general_codes_mask_ |= (1 << allocatable_general_codes_[i]);
  }
  for (int i = 0; i < num_allocatable_double_registers_; ++i) {
    allocatable_double_codes_mask_ |= (1 << allocatable_double_codes_[i]);
  }

  if (fp_aliasing_kind_ == AliasingKind::kCombine) {
    num_float_registers_ = num_double_registers_ * 2 <= kMaxFPRegisters
                               ? num_double_registers_ * 2
                               : kMaxFPRegisters;
    num_allocatable_float_registers_ = 0;
    for (int i = 0; i < num_allocatable_double_registers_; i++) {
      int base_code = allocatable_double_codes_[i] * 2;
      if (base_code >= kMaxFPRegisters) continue;
      allocatable_float_codes_[num_allocatable_float_registers_++] = base_code;
      allocatable_float_codes_[num_allocatable_float_registers_++] =
          base_code + 1;
      allocatable_float_codes_mask_ |= (0x3 << base_code);
    }
    num_simd128_registers_ = num_double_registers_ / 2;
    num_allocatable_simd128_registers_ = 0;
    int last_simd128_code = allocatable_double_codes_[0] / 2;
    for (int i = 1; i < num_allocatable_double_registers_; i++) {
      int next_simd128_code = allocatable_double_codes_[i] / 2;
      // This scheme assumes allocatable_double_codes_ are strictly increasing.
      DCHECK_GE(next_simd128_code, last_simd128_code);
      if (last_simd128_code == next_simd128_code) {
        allocatable_simd128_codes_[num_allocatable_simd128_registers_++] =
            next_simd128_code;
        allocatable_simd128_codes_mask_ |= (0x1 << next_simd128_code);
      }
      last_simd128_code = next_simd128_code;
    }
  } else if (fp_aliasing_kind_ == AliasingKind::kOverlap) {
    num_float_registers_ = num_simd128_registers_ = num_double_registers_;
    num_allocatable_float_registers_ = num_allocatable_simd128_registers_ =
        num_allocatable_double_registers_;
    for (int i = 0; i < num_allocatable_float_registers_; ++i) {
      allocatable_float_codes_[i] = allocatable_simd128_codes_[i] =
          allocatable_double_codes_[i];
#if V8_TARGET_ARCH_X64
      allocatable_simd256_codes_[i] = allocatable_double_codes_[i];
#endif
    }
    allocatable_float_codes_mask_ = allocatable_simd128_codes_mask_ =
        allocatable_double_codes_mask_;
#if V8_TARGET_ARCH_X64
    num_simd256_registers_ = num_double_registers_;
    num_allocatable_simd256_registers_ = num_allocatable_double_registers_;
    allocatable_simd256_codes_mask_ = allocatable_double_codes_mask_;
#endif
  } else {
    DCHECK_EQ(fp_aliasing_kind_, AliasingKind::kIndependent);
    DCHECK_NE(independent_allocatable_simd128_codes, nullptr);
    num_float_registers_ = num_double_registers_;
    num_allocatable_float_registers_ = num_allocatable_double_registers_;
    for (int i = 0; i < num_allocatable_float_registers_; ++i) {
      allocatable_float_codes_[i] = allocatable_double_codes_[i];
    }
    allocatable_float_codes_mask_ = allocatable_double_codes_mask_;
    for (int i = 0; i < num_allocatable_simd128_registers; i++) {
      allocatable_simd128_codes_[i] = independent_allocatable_simd128_codes[i];
    }
    for (int i = 0; i < num_allocatable_simd128_registers_; ++i) {
      allocatable_simd128_codes_mask_ |= (1 << allocatable_simd128_codes_[i]);
    }
  }
}

// Assert that kFloat32, kFloat64, kSimd128 and kSimd256 are consecutive values.
static_assert(static_cast<int>(MachineRepresentation::kSimd256) ==
              static_cast<int>(MachineRepresentation::kSimd128) + 1);
static_assert(static_cast<int>(MachineRepresentation::kSimd128) ==
              static_cast<int>(MachineRepresentation::kFloat64) + 1);
static_assert(static_cast<int>(MachineRepresentation::kFloat64) ==
              static_cast<int>(MachineRepresentation::kFloat32) + 1);

int RegisterConfiguration::GetAliases(MachineRepresentation rep, int index,
                                      MachineRepresentation other_rep,
                                      int* alias_base_index) const {
  DCHECK(fp_aliasing_kind_ == AliasingKind::kCombine);
  DCHECK(IsFloatingPoint(rep) && IsFloatingPoint(other_rep));
  if (rep == other_rep) {
    *alias_base_index = index;
    return 1;
  }
  int rep_int = static_cast<int>(rep);
  int other_rep_int = static_cast<int>(other_rep);
  if (rep_int > other_rep_int) {
    int shift = rep_int - other_rep_int;
    int base_index = index << shift;
    if (base_index >= kMaxFPRegisters) {
      // Alias indices would be out of FP register range.
      return 0;
    }
    *alias_base_index = base_index;
    return 1 << shift;
  }
  int shift = other_rep_int - rep_int;
  *alias_base_index = index >> shift;
  return 1;
}

bool RegisterConfiguration::AreAliases(MachineRepresentation rep, int index,
                                       MachineRepresentation other_rep,
                                       int other_index) const {
  DCHECK(fp_aliasing_kind_ == AliasingKind::kCombine);
  DCHECK(IsFloatingPoint(rep) && IsFloatingPoint(other_rep));
  if (rep == other_rep) {
    return index == other_index;
  }
  int rep_int = static_cast<int>(rep);
  int other_rep_int = static_cast<int>(other_rep);
  if (rep_int > other_rep_int) {
    int shift = rep_int - other_rep_int;
    return index == other_index >> shift;
  }
  int shift = other_rep_int - rep_int;
  return index >> shift == other_index;
}

}  // namespace internal
}  // namespace v8
                                                                                          node-23.7.0/deps/v8/src/codegen/register-configuration.h                                            0000664 0000000 0000000 00000015064 14746647661 0023110 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2014 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGISTER_CONFIGURATION_H_
#define V8_CODEGEN_REGISTER_CONFIGURATION_H_

#include "src/base/macros.h"
#include "src/codegen/machine-type.h"
#include "src/codegen/reglist.h"
#include "src/common/globals.h"
#include "src/utils/utils.h"

namespace v8 {
namespace internal {

class V8_EXPORT_PRIVATE RegisterConfiguration {
 public:
  // Architecture independent maxes.
  static constexpr int kMaxGeneralRegisters = 32;
  static constexpr int kMaxFPRegisters = 32;
  static constexpr int kMaxRegisters =
      std::max(kMaxFPRegisters, kMaxGeneralRegisters);

  // Default RegisterConfigurations for the target architecture.
  static const RegisterConfiguration* Default();

  // Register configuration with reserved masking register.
  static const RegisterConfiguration* Poisoning();

  static const RegisterConfiguration* RestrictGeneralRegisters(
      RegList registers);

  RegisterConfiguration(
      AliasingKind fp_aliasing_kind, int num_general_registers,
      int num_double_registers, int num_simd128_registers,
      int num_simd256_registers, int num_allocatable_general_registers,
      int num_allocatable_double_registers,
      int num_allocatable_simd128_registers,
      int num_allocatable_simd256_registers,
      const int* allocatable_general_codes, const int* allocatable_double_codes,
      const int* independent_allocatable_simd128_codes = nullptr);

  int num_general_registers() const { return num_general_registers_; }
  int num_float_registers() const { return num_float_registers_; }
  int num_double_registers() const { return num_double_registers_; }
  int num_simd128_registers() const { return num_simd128_registers_; }
  int num_simd256_registers() const { return num_simd256_registers_; }
  int num_allocatable_general_registers() const {
    return num_allocatable_general_registers_;
  }
  int num_allocatable_float_registers() const {
    return num_allocatable_float_registers_;
  }
  // Caution: this value depends on the current cpu and may change between
  // build and runtime. At the time of writing, the only architecture with a
  // variable allocatable double register set is Arm.
  int num_allocatable_double_registers() const {
    return num_allocatable_double_registers_;
  }
  int num_allocatable_simd128_registers() const {
    return num_allocatable_simd128_registers_;
  }
  int num_allocatable_simd256_registers() const {
    return num_allocatable_simd256_registers_;
  }

  AliasingKind fp_aliasing_kind() const { return fp_aliasing_kind_; }
  int32_t allocatable_general_codes_mask() const {
    return allocatable_general_codes_mask_;
  }
  int32_t allocatable_double_codes_mask() const {
    return allocatable_double_codes_mask_;
  }
  int32_t allocatable_float_codes_mask() const {
    return allocatable_float_codes_mask_;
  }
  int32_t allocatable_simd128_codes_mask() const {
    return allocatable_simd128_codes_mask_;
  }
  int GetAllocatableGeneralCode(int index) const {
    DCHECK(index >= 0 && index < num_allocatable_general_registers());
    return allocatable_general_codes_[index];
  }
  bool IsAllocatableGeneralCode(int index) const {
    return ((1 << index) & allocatable_general_codes_mask_) != 0;
  }
  int GetAllocatableFloatCode(int index) const {
    DCHECK(index >= 0 && index < num_allocatable_float_registers());
    return allocatable_float_codes_[index];
  }
  bool IsAllocatableFloatCode(int index) const {
    return ((1 << index) & allocatable_float_codes_mask_) != 0;
  }
  int GetAllocatableDoubleCode(int index) const {
    DCHECK(index >= 0 && index < num_allocatable_double_registers());
    return allocatable_double_codes_[index];
  }
  bool IsAllocatableDoubleCode(int index) const {
    return ((1 << index) & allocatable_double_codes_mask_) != 0;
  }
  int GetAllocatableSimd128Code(int index) const {
    DCHECK(index >= 0 && index < num_allocatable_simd128_registers());
    return allocatable_simd128_codes_[index];
  }
  bool IsAllocatableSimd128Code(int index) const {
    return ((1 << index) & allocatable_simd128_codes_mask_) != 0;
  }
  int GetAllocatableSimd256Code(int index) const {
    DCHECK(index >= 0 && index < num_allocatable_simd256_registers());
    return allocatable_simd256_codes_[index];
  }
  bool IsAllocatableSimd256Code(int index) const {
    return ((1 << index) & allocatable_simd256_codes_mask_) != 0;
  }

  const int* allocatable_general_codes() const {
    return allocatable_general_codes_;
  }
  const int* allocatable_float_codes() const {
    return allocatable_float_codes_;
  }
  const int* allocatable_double_codes() const {
    return allocatable_double_codes_;
  }
  const int* allocatable_simd128_codes() const {
    return allocatable_simd128_codes_;
  }
  const int* allocatable_simd256_codes() const {
    return allocatable_simd256_codes_;
  }

  // Aliasing calculations for floating point registers, when fp_aliasing_kind()
  // is COMBINE. Currently only implemented for kFloat32, kFloat64, or kSimd128
  // reps. Returns the number of aliases, and if > 0, alias_base_index is set to
  // the index of the first alias.
  int GetAliases(MachineRepresentation rep, int index,
                 MachineRepresentation other_rep, int* alias_base_index) const;
  // Returns a value indicating whether two registers alias each other, when
  // fp_aliasing_kind() is COMBINE. Currently implemented for kFloat32,
  // kFloat64, or kSimd128 reps.
  bool AreAliases(MachineRepresentation rep, int index,
                  MachineRepresentation other_rep, int other_index) const;

  virtual ~RegisterConfiguration() = default;

 private:
  const int num_general_registers_;
  int num_float_registers_;
  const int num_double_registers_;
  int num_simd128_registers_;
  int num_simd256_registers_;
  int num_allocatable_general_registers_;
  int num_allocatable_float_registers_;
  int num_allocatable_double_registers_;
  int num_allocatable_simd128_registers_;
  int num_allocatable_simd256_registers_;
  int32_t allocatable_general_codes_mask_;
  int32_t allocatable_float_codes_mask_;
  int32_t allocatable_double_codes_mask_;
  int32_t allocatable_simd128_codes_mask_;
  int32_t allocatable_simd256_codes_mask_;
  const int* allocatable_general_codes_;
  int allocatable_float_codes_[kMaxFPRegisters];
  const int* allocatable_double_codes_;
  int allocatable_simd128_codes_[kMaxFPRegisters];
  int allocatable_simd256_codes_[kMaxFPRegisters];
  AliasingKind fp_aliasing_kind_;
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_REGISTER_CONFIGURATION_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/v8/src/codegen/register.h                                                          0000664 0000000 0000000 00000002602 14746647661 0020235 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGISTER_H_
#define V8_CODEGEN_REGISTER_H_

#include "src/codegen/register-arch.h"
#include "src/codegen/reglist.h"

namespace v8 {
namespace internal {

constexpr int AddArgumentPaddingSlots(int argument_count) {
  return argument_count + ArgumentPaddingSlots(argument_count);
}

constexpr bool ShouldPadArguments(int argument_count) {
  return ArgumentPaddingSlots(argument_count) != 0;
}

template <typename... RegTypes,
          // All arguments must be either Register or DoubleRegister.
          typename = typename std::enable_if_t<
              std::conjunction_v<std::is_same<Register, RegTypes>...> ||
              std::conjunction_v<std::is_same<DoubleRegister, RegTypes>...>
#ifdef V8_TARGET_ARCH_X64
              || std::conjunction_v<std::is_same<YMMRegister, RegTypes>...>
#endif  // V8_TARGET_ARCH_X64
              >>
inline constexpr bool AreAliased(RegTypes... regs) {
  using FirstRegType = std::tuple_element_t<0, std::tuple<RegTypes...>>;
  int num_different_regs = RegListBase<FirstRegType>{regs...}.Count();
  int num_given_regs = (... + (regs.is_valid() ? 1 : 0));
  return num_different_regs < num_given_regs;
}

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_REGISTER_H_
                                                                                                                              node-23.7.0/deps/v8/src/codegen/reglist-base.h                                                      0000664 0000000 0000000 00000014361 14746647661 0020777 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGLIST_BASE_H_
#define V8_CODEGEN_REGLIST_BASE_H_

#include <cstdint>
#include <initializer_list>

#include "src/base/bits.h"
#include "src/base/iterator.h"
#include "src/base/template-utils.h"

namespace v8 {
namespace internal {

class Register;

template <typename RegisterT>
class RegListBase {
  using num_registers_sized_storage_t = typename std::conditional<
      RegisterT::kNumRegisters <= 16, uint16_t,
      typename std::conditional<RegisterT::kNumRegisters <= 32, uint32_t,
                                uint64_t>::type>::type;
  static_assert(RegisterT::kNumRegisters <= 64);

 public:
  class Iterator;
  class ReverseIterator;

#ifdef V8_TARGET_ARCH_ARM64
  // On ARM64 the sp register has the special value 63 (kSPRegInternalCode)
  using storage_t = typename std::conditional<
      std::is_same<RegisterT, v8::internal::Register>::value, uint64_t,
      num_registers_sized_storage_t>::type;
#else
  using storage_t = num_registers_sized_storage_t;
#endif

  constexpr RegListBase() = default;
  constexpr RegListBase(std::initializer_list<RegisterT> regs) {
    for (RegisterT reg : regs) {
      set(reg);
    }
  }

  constexpr void set(RegisterT reg) {
    if (!reg.is_valid()) return;
    regs_ |= storage_t{1} << reg.code();
  }

  constexpr void clear(RegisterT reg) {
    if (!reg.is_valid()) return;
    regs_ &= ~(storage_t{1} << reg.code());
  }

  constexpr bool has(RegisterT reg) const {
    if (!reg.is_valid()) return false;
    return (regs_ & (storage_t{1} << reg.code())) != 0;
  }

  constexpr void clear(RegListBase other) { regs_ &= ~other.regs_; }

  constexpr bool is_empty() const { return regs_ == 0; }

  constexpr unsigned Count() const {
    return base::bits::CountPopulation(regs_);
  }

  constexpr RegListBase operator&(const RegListBase other) const {
    return RegListBase(regs_ & other.regs_);
  }

  constexpr RegListBase operator|(const RegListBase other) const {
    return RegListBase(regs_ | other.regs_);
  }

  constexpr RegListBase operator^(const RegListBase other) const {
    return RegListBase(regs_ ^ other.regs_);
  }

  constexpr RegListBase operator-(const RegListBase other) const {
    return RegListBase(regs_ & ~other.regs_);
  }

  constexpr RegListBase operator|(const RegisterT reg) const {
    return *this | RegListBase{reg};
  }

  constexpr RegListBase operator-(const RegisterT reg) const {
    return *this - RegListBase{reg};
  }

  constexpr RegListBase& operator&=(const RegListBase other) {
    regs_ &= other.regs_;
    return *this;
  }

  constexpr RegListBase& operator|=(const RegListBase other) {
    regs_ |= other.regs_;
    return *this;
  }

  constexpr bool operator==(const RegListBase other) const {
    return regs_ == other.regs_;
  }
  constexpr bool operator!=(const RegListBase other) const {
    return regs_ != other.regs_;
  }

  constexpr RegisterT first() const {
    DCHECK(!is_empty());
    int first_code = base::bits::CountTrailingZerosNonZero(regs_);
    return RegisterT::from_code(first_code);
  }

  constexpr RegisterT last() const {
    DCHECK(!is_empty());
    int last_code =
        8 * sizeof(regs_) - 1 - base::bits::CountLeadingZeros(regs_);
    return RegisterT::from_code(last_code);
  }

  constexpr RegisterT PopFirst() {
    RegisterT reg = first();
    clear(reg);
    return reg;
  }

  constexpr storage_t bits() const { return regs_; }

  inline Iterator begin() const;
  inline Iterator end() const;

  inline ReverseIterator rbegin() const;
  inline ReverseIterator rend() const;

  static RegListBase FromBits(storage_t bits) { return RegListBase(bits); }

  template <storage_t bits>
  static constexpr RegListBase FromBits() {
    return RegListBase{bits};
  }

 private:
  // Unchecked constructor. Only use for valid bits.
  explicit constexpr RegListBase(storage_t bits) : regs_(bits) {}

  storage_t regs_ = 0;
};

template <typename RegisterT>
class RegListBase<RegisterT>::Iterator
    : public base::iterator<std::forward_iterator_tag, RegisterT> {
 public:
  RegisterT operator*() { return remaining_.first(); }
  Iterator& operator++() {
    remaining_.clear(remaining_.first());
    return *this;
  }
  bool operator==(Iterator other) { return remaining_ == other.remaining_; }
  bool operator!=(Iterator other) { return remaining_ != other.remaining_; }

 private:
  explicit Iterator(RegListBase<RegisterT> remaining) : remaining_(remaining) {}
  friend class RegListBase;

  RegListBase<RegisterT> remaining_;
};

template <typename RegisterT>
class RegListBase<RegisterT>::ReverseIterator
    : public base::iterator<std::forward_iterator_tag, RegisterT> {
 public:
  RegisterT operator*() { return remaining_.last(); }
  ReverseIterator& operator++() {
    remaining_.clear(remaining_.last());
    return *this;
  }
  bool operator==(ReverseIterator other) {
    return remaining_ == other.remaining_;
  }
  bool operator!=(ReverseIterator other) {
    return remaining_ != other.remaining_;
  }

 private:
  explicit ReverseIterator(RegListBase<RegisterT> remaining)
      : remaining_(remaining) {}
  friend class RegListBase;

  RegListBase<RegisterT> remaining_;
};

template <typename RegisterT>
typename RegListBase<RegisterT>::Iterator RegListBase<RegisterT>::begin()
    const {
  return Iterator{*this};
}
template <typename RegisterT>
typename RegListBase<RegisterT>::Iterator RegListBase<RegisterT>::end() const {
  return Iterator{RegListBase<RegisterT>{}};
}

template <typename RegisterT>
typename RegListBase<RegisterT>::ReverseIterator
RegListBase<RegisterT>::rbegin() const {
  return ReverseIterator{*this};
}
template <typename RegisterT>
typename RegListBase<RegisterT>::ReverseIterator RegListBase<RegisterT>::rend()
    const {
  return ReverseIterator{RegListBase<RegisterT>{}};
}

template <typename RegisterT>
inline std::ostream& operator<<(std::ostream& os,
                                RegListBase<RegisterT> reglist) {
  os << "{";
  for (bool first = true; !reglist.is_empty(); first = false) {
    RegisterT reg = reglist.first();
    reglist.clear(reg);
    os << (first ? "" : ", ") << reg;
  }
  return os << "}";
}

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_REGLIST_BASE_H_
                                                                                                                                                                                                                                                                               node-23.7.0/deps/v8/src/codegen/reglist.h                                                           0000664 0000000 0000000 00000002746 14746647661 0020073 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2017 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_REGLIST_H_
#define V8_CODEGEN_REGLIST_H_

#if V8_TARGET_ARCH_IA32
#include "src/codegen/ia32/reglist-ia32.h"
#elif V8_TARGET_ARCH_X64
#include "src/codegen/x64/reglist-x64.h"
#elif V8_TARGET_ARCH_ARM64
#include "src/codegen/arm64/reglist-arm64.h"
#elif V8_TARGET_ARCH_ARM
#include "src/codegen/arm/reglist-arm.h"
#elif V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64
#include "src/codegen/ppc/reglist-ppc.h"
#elif V8_TARGET_ARCH_MIPS64
#include "src/codegen/mips64/reglist-mips64.h"
#elif V8_TARGET_ARCH_LOONG64
#include "src/codegen/loong64/reglist-loong64.h"
#elif V8_TARGET_ARCH_S390
#include "src/codegen/s390/reglist-s390.h"
#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
#include "src/codegen/riscv/reglist-riscv.h"
#else
#error Unknown architecture.
#endif

namespace v8 {
namespace internal {

static constexpr RegList kEmptyRegList = {};

#define LIST_REG(V) V,
static constexpr RegList kAllocatableGeneralRegisters = {
    ALLOCATABLE_GENERAL_REGISTERS(LIST_REG) Register::no_reg()};
#undef LIST_REG

static constexpr DoubleRegList kEmptyDoubleRegList = {};

#define LIST_REG(V) V,
static constexpr DoubleRegList kAllocatableDoubleRegisters = {
    ALLOCATABLE_DOUBLE_REGISTERS(LIST_REG) DoubleRegister::no_reg()};
#undef LIST_REG

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_REGLIST_H_
                          node-23.7.0/deps/v8/src/codegen/reloc-info-inl.h                                                    0000664 0000000 0000000 00000002770 14746647661 0021234 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_RELOC_INFO_INL_H_
#define V8_CODEGEN_RELOC_INFO_INL_H_

#include "src/codegen/assembler-inl.h"
#include "src/codegen/reloc-info.h"
#include "src/heap/heap-write-barrier-inl.h"

namespace v8 {
namespace internal {

void WritableRelocInfo::set_target_object(Tagged<InstructionStream> host,
                                          Tagged<HeapObject> target,
                                          WriteBarrierMode write_barrier_mode,
                                          ICacheFlushMode icache_flush_mode) {
  set_target_object(target, icache_flush_mode);
  if (!v8_flags.disable_write_barriers) {
    WriteBarrierForCode(host, this, target, write_barrier_mode);
  }
}

template <typename RelocInfoT>
RelocIteratorBase<RelocInfoT>::RelocIteratorBase(RelocInfoT reloc_info,
                                                 const uint8_t* pos,
                                                 const uint8_t* end,
                                                 int mode_mask)
    : pos_(pos), end_(end), rinfo_(reloc_info), mode_mask_(mode_mask) {
  DCHECK_EQ(reloc_info.rmode(), RelocInfo::NO_INFO);
  DCHECK_EQ(reloc_info.data(), 0);
  // Relocation info is read backwards.
  DCHECK_GE(pos_, end_);
  if (mode_mask_ == 0) pos_ = end_;
  next();
}

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RELOC_INFO_INL_H_
        node-23.7.0/deps/v8/src/codegen/reloc-info.cc                                                       0000664 0000000 0000000 00000044614 14746647661 0020615 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/codegen/reloc-info.h"

#include "src/base/vlq.h"
#include "src/codegen/assembler-inl.h"
#include "src/codegen/code-reference.h"
#include "src/codegen/external-reference-encoder.h"
#include "src/codegen/reloc-info-inl.h"
#include "src/deoptimizer/deoptimize-reason.h"
#include "src/deoptimizer/deoptimizer.h"
#include "src/heap/heap-write-barrier-inl.h"
#include "src/objects/code-inl.h"
#include "src/snapshot/embedded/embedded-data-inl.h"

namespace v8 {
namespace internal {

using namespace detail;

uint32_t RelocInfoWriter::WriteLongPCJump(uint32_t pc_delta) {
  // Return if the pc_delta can fit in kSmallPCDeltaBits bits.
  // Otherwise write a variable length PC jump for the bits that do
  // not fit in the kSmallPCDeltaBits bits.
  if (is_uintn(pc_delta, kSmallPCDeltaBits)) return pc_delta;
  WriteMode(RelocInfo::PC_JUMP);
  uint32_t pc_jump = pc_delta >> kSmallPCDeltaBits;
  DCHECK_GT(pc_jump, 0);
  base::VLQEncodeUnsigned([this](uint8_t byte) { *--pos_ = byte; }, pc_jump);
  // Return the remaining kSmallPCDeltaBits of the pc_delta.
  return pc_delta & kSmallPCDeltaMask;
}

void RelocInfoWriter::WriteShortTaggedPC(uint32_t pc_delta, int tag) {
  // Write a byte of tagged pc-delta, possibly preceded by an explicit pc-jump.
  pc_delta = WriteLongPCJump(pc_delta);
  *--pos_ = pc_delta << kTagBits | tag;
}

void RelocInfoWriter::WriteShortData(intptr_t data_delta) {
  *--pos_ = static_cast<uint8_t>(data_delta);
}

void RelocInfoWriter::WriteMode(RelocInfo::Mode rmode) {
  static_assert(RelocInfo::NUMBER_OF_MODES <= (1 << kLongTagBits));
  *--pos_ = static_cast<int>((rmode << kTagBits) | kDefaultTag);
}

void RelocInfoWriter::WriteModeAndPC(uint32_t pc_delta, RelocInfo::Mode rmode) {
  // Write two-byte tagged pc-delta, possibly preceded by var. length pc-jump.
  pc_delta = WriteLongPCJump(pc_delta);
  WriteMode(rmode);
  *--pos_ = pc_delta;
}

void RelocInfoWriter::WriteIntData(int number) {
  for (int i = 0; i < kIntSize; i++) {
    *--pos_ = static_cast<uint8_t>(number);
    // Signed right shift is arithmetic shift.  Tested in test-utils.cc.
    number = number >> kBitsPerByte;
  }
}

void RelocInfoWriter::Write(const RelocInfo* rinfo) {
  RelocInfo::Mode rmode = rinfo->rmode();
#ifdef DEBUG
  uint8_t* begin_pos = pos_;
#endif
  DCHECK(rinfo->rmode() < RelocInfo::NUMBER_OF_MODES);
  DCHECK_GE(rinfo->pc() - reinterpret_cast<Address>(last_pc_), 0);
  // Use unsigned delta-encoding for pc.
  uint32_t pc_delta =
      static_cast<uint32_t>(rinfo->pc() - reinterpret_cast<Address>(last_pc_));

  // The two most common modes are given small tags, and usually fit in a byte.
  if (rmode == RelocInfo::FULL_EMBEDDED_OBJECT) {
    WriteShortTaggedPC(pc_delta, kEmbeddedObjectTag);
  } else if (rmode == RelocInfo::CODE_TARGET) {
    WriteShortTaggedPC(pc_delta, kCodeTargetTag);
    DCHECK_LE(begin_pos - pos_, RelocInfo::kMaxCallSize);
  } else if (rmode == RelocInfo::WASM_STUB_CALL) {
    WriteShortTaggedPC(pc_delta, kWasmStubCallTag);
  } else {
    WriteModeAndPC(pc_delta, rmode);
    if (RelocInfo::IsDeoptReason(rmode)) {
      DCHECK_LT(rinfo->data(), 1 << kBitsPerByte);
      WriteShortData(rinfo->data());
    } else if (RelocInfo::IsConstPool(rmode) ||
               RelocInfo::IsVeneerPool(rmode) || RelocInfo::IsDeoptId(rmode) ||
               RelocInfo::IsDeoptPosition(rmode) ||
               RelocInfo::IsDeoptNodeId(rmode) ||
               RelocInfo::IsRelativeSwitchTableEntry(rmode)) {
      WriteIntData(static_cast<int>(rinfo->data()));
    }
  }
  last_pc_ = reinterpret_cast<uint8_t*>(rinfo->pc());
#ifdef DEBUG
  DCHECK_LE(begin_pos - pos_, kMaxSize);
#endif
}

template <typename RelocInfoT>
void RelocIteratorBase<RelocInfoT>::AdvanceReadInt() {
  int x = 0;
  for (int i = 0; i < kIntSize; i++) {
    x |= static_cast<int>(*--pos_) << i * kBitsPerByte;
  }
  rinfo_.data_ = x;
}

template <typename RelocInfoT>
void RelocIteratorBase<RelocInfoT>::AdvanceReadLongPCJump() {
  // Read the 32-kSmallPCDeltaBits most significant bits of the
  // pc jump as a VLQ encoded integer.
  uint32_t pc_jump = base::VLQDecodeUnsigned([this] { return *--pos_; });
  // The least significant kSmallPCDeltaBits bits will be added
  // later.
  rinfo_.pc_ += pc_jump << kSmallPCDeltaBits;
}

template <typename RelocInfoT>
inline void RelocIteratorBase<RelocInfoT>::ReadShortData() {
  uint8_t unsigned_b = *pos_;
  rinfo_.data_ = unsigned_b;
}

template <typename RelocInfoT>
void RelocIteratorBase<RelocInfoT>::next() {
  DCHECK(!done());
  // Basically, do the opposite of RelocInfoWriter::Write.
  // Reading of data is as far as possible avoided for unwanted modes,
  // but we must always update the pc.
  //
  // We exit this loop by returning when we find a mode we want.
  while (pos_ > end_) {
    int tag = AdvanceGetTag();
    if (tag == kEmbeddedObjectTag) {
      ReadShortTaggedPC();
      if (SetMode(RelocInfo::FULL_EMBEDDED_OBJECT)) return;
    } else if (tag == kCodeTargetTag) {
      ReadShortTaggedPC();
      if (SetMode(RelocInfo::CODE_TARGET)) return;
    } else if (tag == kWasmStubCallTag) {
      ReadShortTaggedPC();
      if (SetMode(RelocInfo::WASM_STUB_CALL)) return;
    } else {
      DCHECK_EQ(tag, kDefaultTag);
      RelocInfo::Mode rmode = GetMode();
      if (rmode == RelocInfo::PC_JUMP) {
        AdvanceReadLongPCJump();
      } else {
        AdvanceReadPC();
        if (RelocInfo::IsDeoptReason(rmode)) {
          Advance();
          if (SetMode(rmode)) {
            ReadShortData();
            return;
          }
        } else if (RelocInfo::IsConstPool(rmode) ||
                   RelocInfo::IsVeneerPool(rmode) ||
                   RelocInfo::IsDeoptId(rmode) ||
                   RelocInfo::IsDeoptPosition(rmode) ||
                   RelocInfo::IsDeoptNodeId(rmode) ||
                   RelocInfo::IsRelativeSwitchTableEntry(rmode)) {
          if (SetMode(rmode)) {
            AdvanceReadInt();
            return;
          }
          Advance(kIntSize);
        } else if (SetMode(static_cast<RelocInfo::Mode>(rmode))) {
          return;
        }
      }
    }
  }
  done_ = true;
}

RelocIterator::RelocIterator(Tagged<Code> code, int mode_mask)
    : RelocIterator(code->instruction_stream(), mode_mask) {}

RelocIterator::RelocIterator(Tagged<InstructionStream> istream, int mode_mask)
    : RelocIterator(
          istream->instruction_start(), istream->constant_pool(),
          // Use unchecked accessors since this can be called during GC
          istream->unchecked_relocation_info()->end(),
          istream->unchecked_relocation_info()->begin(), mode_mask) {}

RelocIterator::RelocIterator(const CodeReference code_reference)
    : RelocIterator(code_reference.instruction_start(),
                    code_reference.constant_pool(),
                    code_reference.relocation_end(),
                    code_reference.relocation_start(), kAllModesMask) {}

RelocIterator::RelocIterator(EmbeddedData* embedded_data, Tagged<Code> code,
                             int mode_mask)
    : RelocIterator(embedded_data->InstructionStartOf(code->builtin_id()),
                    code->constant_pool(), code->relocation_end(),
                    code->relocation_start(), mode_mask) {}

RelocIterator::RelocIterator(base::Vector<uint8_t> instructions,
                             base::Vector<const uint8_t> reloc_info,
                             Address const_pool, int mode_mask)
    : RelocIterator(reinterpret_cast<Address>(instructions.begin()), const_pool,
                    reloc_info.begin() + reloc_info.size(), reloc_info.begin(),
                    mode_mask) {}

RelocIterator::RelocIterator(Address pc, Address constant_pool,
                             const uint8_t* pos, const uint8_t* end,
                             int mode_mask)
    : RelocIteratorBase<RelocInfo>(
          RelocInfo(pc, RelocInfo::NO_INFO, 0, constant_pool), pos, end,
          mode_mask) {}

WritableRelocIterator::WritableRelocIterator(
    WritableJitAllocation& jit_allocation, Tagged<InstructionStream> istream,
    Address constant_pool, int mode_mask)
    : RelocIteratorBase<WritableRelocInfo>(
          WritableRelocInfo(jit_allocation, istream->instruction_start(),
                            RelocInfo::NO_INFO, 0, constant_pool),
          // Use unchecked accessors since this can be called during GC
          istream->unchecked_relocation_info()->end(),
          istream->unchecked_relocation_info()->begin(), mode_mask) {}

WritableRelocIterator::WritableRelocIterator(
    WritableJitAllocation& jit_allocation, base::Vector<uint8_t> instructions,
    base::Vector<const uint8_t> reloc_info, Address constant_pool,
    int mode_mask)
    : RelocIteratorBase<WritableRelocInfo>(
          WritableRelocInfo(jit_allocation,
                            reinterpret_cast<Address>(instructions.begin()),
                            RelocInfo::NO_INFO, 0, constant_pool),
          reloc_info.begin() + reloc_info.size(), reloc_info.begin(),
          mode_mask) {}

// -----------------------------------------------------------------------------
// Implementation of RelocInfo

// static
bool RelocInfo::OffHeapTargetIsCodedSpecially() {
#if defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_ARM64) || \
    defined(V8_TARGET_ARCH_X64)
  return false;
#elif defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_MIPS64) || \
    defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_PPC64) ||     \
    defined(V8_TARGET_ARCH_S390) || defined(V8_TARGET_ARCH_RISCV64) ||  \
    defined(V8_TARGET_ARCH_LOONG64) || defined(V8_TARGET_ARCH_RISCV32)
  return true;
#endif
}

Address RelocInfo::wasm_call_address() const {
  DCHECK_EQ(rmode_, WASM_CALL);
  return Assembler::target_address_at(pc_, constant_pool_);
}

void WritableRelocInfo::set_wasm_call_address(Address address) {
  DCHECK_EQ(rmode_, WASM_CALL);
  Assembler::set_target_address_at(pc_, constant_pool_, address,
                                   SKIP_ICACHE_FLUSH);
}

Address RelocInfo::wasm_stub_call_address() const {
  DCHECK_EQ(rmode_, WASM_STUB_CALL);
  return Assembler::target_address_at(pc_, constant_pool_);
}

void WritableRelocInfo::set_wasm_stub_call_address(Address address) {
  DCHECK_EQ(rmode_, WASM_STUB_CALL);
  Assembler::set_target_address_at(pc_, constant_pool_, address,
                                   SKIP_ICACHE_FLUSH);
}

uint32_t RelocInfo::wasm_canonical_sig_id() const {
  DCHECK_EQ(rmode_, WASM_CANONICAL_SIG_ID);
  return Assembler::uint32_constant_at(pc_, constant_pool_);
}

void WritableRelocInfo::set_wasm_canonical_sig_id(uint32_t canonical_sig_id) {
  DCHECK_EQ(rmode_, WASM_CANONICAL_SIG_ID);
  Assembler::set_uint32_constant_at(pc_, constant_pool_, canonical_sig_id,
                                    SKIP_ICACHE_FLUSH);
}

void WritableRelocInfo::set_target_address(Address target,
                                           ICacheFlushMode icache_flush_mode) {
  DCHECK(IsCodeTargetMode(rmode_) || IsNearBuiltinEntry(rmode_) ||
         IsWasmCall(rmode_));
  Assembler::set_target_address_at(pc_, constant_pool_, target,
                                   icache_flush_mode);
}

void WritableRelocInfo::set_target_address(Tagged<InstructionStream> host,
                                           Address target,
                                           WriteBarrierMode write_barrier_mode,
                                           ICacheFlushMode icache_flush_mode) {
  set_target_address(target, icache_flush_mode);
  if (IsCodeTargetMode(rmode_) && !v8_flags.disable_write_barriers) {
    Tagged<InstructionStream> target_code =
        InstructionStream::FromTargetAddress(target);
    WriteBarrierForCode(host, this, target_code, write_barrier_mode);
  }
}

void RelocInfo::set_off_heap_target_address(Address target,
                                            ICacheFlushMode icache_flush_mode) {
  DCHECK(IsCodeTargetMode(rmode_));
  Assembler::set_target_address_at(pc_, constant_pool_, target,
                                   icache_flush_mode);
}

bool RelocInfo::HasTargetAddressAddress() const {
  // TODO(jgruber): Investigate whether WASM_CALL is still appropriate on
  // non-intel platforms now that wasm code is no longer on the heap.
#if defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_X64)
  static constexpr int kTargetAddressAddressModeMask =
      ModeMask(CODE_TARGET) | ModeMask(FULL_EMBEDDED_OBJECT) |
      ModeMask(COMPRESSED_EMBEDDED_OBJECT) | ModeMask(EXTERNAL_REFERENCE) |
      ModeMask(OFF_HEAP_TARGET) | ModeMask(WASM_CALL) |
      ModeMask(WASM_STUB_CALL);
#else
  static constexpr int kTargetAddressAddressModeMask =
      ModeMask(CODE_TARGET) | ModeMask(RELATIVE_CODE_TARGET) |
      ModeMask(FULL_EMBEDDED_OBJECT) | ModeMask(EXTERNAL_REFERENCE) |
      ModeMask(OFF_HEAP_TARGET) | ModeMask(WASM_CALL);
#endif
  return (ModeMask(rmode_) & kTargetAddressAddressModeMask) != 0;
}

#ifdef ENABLE_DISASSEMBLER
const char* RelocInfo::RelocModeName(RelocInfo::Mode rmode) {
  switch (rmode) {
    case NO_INFO:
      return "no reloc";
    case COMPRESSED_EMBEDDED_OBJECT:
      return "compressed embedded object";
    case FULL_EMBEDDED_OBJECT:
      return "full embedded object";
    case CODE_TARGET:
      return "code target";
    case RELATIVE_CODE_TARGET:
      return "relative code target";
    case EXTERNAL_REFERENCE:
      return "external reference";
    case INTERNAL_REFERENCE:
      return "internal reference";
    case INTERNAL_REFERENCE_ENCODED:
      return "encoded internal reference";
    case RELATIVE_SWITCH_TABLE_ENTRY:
      return "relative switch table entry";
    case OFF_HEAP_TARGET:
      return "off heap target";
    case NEAR_BUILTIN_ENTRY:
      return "near builtin entry";
    case DEOPT_SCRIPT_OFFSET:
      return "deopt script offset";
    case DEOPT_INLINING_ID:
      return "deopt inlining id";
    case DEOPT_REASON:
      return "deopt reason";
    case DEOPT_ID:
      return "deopt index";
    case DEOPT_NODE_ID:
      return "deopt node id";
    case CONST_POOL:
      return "constant pool";
    case VENEER_POOL:
      return "veneer pool";
    case WASM_CALL:
      return "internal wasm call";
    case WASM_STUB_CALL:
      return "wasm stub call";
    case WASM_CANONICAL_SIG_ID:
      return "wasm canonical signature id";
    case NUMBER_OF_MODES:
    case PC_JUMP:
      UNREACHABLE();
  }
  return "unknown relocation type";
}

void RelocInfo::Print(Isolate* isolate, std::ostream& os) {
  os << reinterpret_cast<const void*>(pc_) << "  " << RelocModeName(rmode_);
  if (rmode_ == DEOPT_SCRIPT_OFFSET || rmode_ == DEOPT_INLINING_ID) {
    os << "  (" << data() << ")";
  } else if (rmode_ == DEOPT_REASON) {
    os << "  ("
       << DeoptimizeReasonToString(static_cast<DeoptimizeReason>(data_)) << ")";
  } else if (rmode_ == FULL_EMBEDDED_OBJECT) {
    os << "  (" << Brief(target_object(isolate)) << ")";
  } else if (rmode_ == COMPRESSED_EMBEDDED_OBJECT) {
    os << "  (" << Brief(target_object(isolate)) << " compressed)";
  } else if (rmode_ == EXTERNAL_REFERENCE) {
    if (isolate) {
      ExternalReferenceEncoder ref_encoder(isolate);
      os << " ("
         << ref_encoder.NameOfAddress(isolate, target_external_reference())
         << ") ";
    }
    os << " (" << reinterpret_cast<const void*>(target_external_reference())
       << ")";
  } else if (IsCodeTargetMode(rmode_)) {
    const Address code_target = target_address();
    Tagged<Code> target_code = Code::FromTargetAddress(code_target);
    os << " (" << CodeKindToString(target_code->kind());
    if (Builtins::IsBuiltin(target_code)) {
      os << " " << Builtins::name(target_code->builtin_id());
    }
    os << ")  (" << reinterpret_cast<const void*>(target_address()) << ")";
  } else if (IsConstPool(rmode_)) {
    os << " (size " << static_cast<int>(data_) << ")";
  } else if (IsWasmStubCall(rmode_)) {
    os << "  (";
    Address addr = target_address();
    if (isolate != nullptr) {
      Builtin builtin = OffHeapInstructionStream::TryLookupCode(isolate, addr);
      os << (Builtins::IsBuiltinId(builtin) ? Builtins::name(builtin)
                                            : "<UNRECOGNIZED>")
         << ")  (";
    }
    os << reinterpret_cast<const void*>(addr) << ")";
  }

  os << "\n";
}
#endif  // ENABLE_DISASSEMBLER

#ifdef VERIFY_HEAP
void RelocInfo::Verify(Isolate* isolate) {
  switch (rmode_) {
    case COMPRESSED_EMBEDDED_OBJECT:
      Object::VerifyPointer(isolate, target_object(isolate));
      break;
    case FULL_EMBEDDED_OBJECT:
      Object::VerifyAnyTagged(isolate, target_object(isolate));
      break;
    case CODE_TARGET:
    case RELATIVE_CODE_TARGET: {
      // convert inline target address to code object
      Address addr = target_address();
      CHECK_NE(addr, kNullAddress);
      // Check that we can find the right code object.
      Tagged<InstructionStream> code =
          InstructionStream::FromTargetAddress(addr);
      Tagged<Code> lookup_result =
          isolate->heap()->FindCodeForInnerPointer(addr);
      CHECK_EQ(code.address(), lookup_result->instruction_stream().address());
      break;
    }
    case INTERNAL_REFERENCE:
    case INTERNAL_REFERENCE_ENCODED: {
      Address target = target_internal_reference();
      Address pc = target_internal_reference_address();
      Tagged<Code> lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
      CHECK_GE(target, lookup_result->instruction_start());
      CHECK_LT(target, lookup_result->instruction_end());
      break;
    }
    case OFF_HEAP_TARGET: {
      Address addr = target_off_heap_target();
      CHECK_NE(addr, kNullAddress);
      CHECK(Builtins::IsBuiltinId(
          OffHeapInstructionStream::TryLookupCode(isolate, addr)));
      break;
    }
    case WASM_STUB_CALL:
    case NEAR_BUILTIN_ENTRY: {
      Address addr = target_address();
      CHECK_NE(addr, kNullAddress);
      CHECK(Builtins::IsBuiltinId(
          OffHeapInstructionStream::TryLookupCode(isolate, addr)));
      break;
    }
    case EXTERNAL_REFERENCE:
    case DEOPT_SCRIPT_OFFSET:
    case DEOPT_INLINING_ID:
    case DEOPT_REASON:
    case DEOPT_ID:
    case DEOPT_NODE_ID:
    case CONST_POOL:
    case VENEER_POOL:
    case WASM_CALL:
    case NO_INFO:
    case RELATIVE_SWITCH_TABLE_ENTRY:
    case WASM_CANONICAL_SIG_ID:
      break;
    case NUMBER_OF_MODES:
    case PC_JUMP:
      UNREACHABLE();
  }
}
#endif  // VERIFY_HEAP

template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
    RelocIteratorBase<RelocInfo>;
template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
    RelocIteratorBase<WritableRelocInfo>;

}  // namespace internal
}  // namespace v8
                                                                                                                    node-23.7.0/deps/v8/src/codegen/reloc-info.h                                                        0000664 0000000 0000000 00000054764 14746647661 0020466 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2018 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_RELOC_INFO_H_
#define V8_CODEGEN_RELOC_INFO_H_

#include "src/base/export-template.h"
#include "src/common/code-memory-access.h"
#include "src/common/globals.h"
#include "src/objects/code.h"
#include "src/objects/instruction-stream.h"

namespace v8 {
namespace internal {

class CodeReference;
class EmbeddedData;

// Specifies whether to perform icache flush operations on RelocInfo updates.
// If FLUSH_ICACHE_IF_NEEDED, the icache will always be flushed if an
// instruction was modified. If SKIP_ICACHE_FLUSH the flush will always be
// skipped (only use this if you will flush the icache manually before it is
// executed).
enum ICacheFlushMode { FLUSH_ICACHE_IF_NEEDED, SKIP_ICACHE_FLUSH };

namespace detail {
// -----------------------------------------------------------------------------
// Implementation of RelocInfoWriter and RelocIterator
//
// Relocation information is written backwards in memory, from high addresses
// towards low addresses, byte by byte.  Therefore, in the encodings listed
// below, the first byte listed it at the highest address, and successive
// bytes in the record are at progressively lower addresses.
//
// Encoding
//
// The most common modes are given single-byte encodings.  Also, it is
// easy to identify the type of reloc info and skip unwanted modes in
// an iteration.
//
// The encoding relies on the fact that there are fewer than 14
// different relocation modes using standard non-compact encoding.
//
// The first byte of a relocation record has a tag in its low 2 bits:
// Here are the record schemes, depending on the low tag and optional higher
// tags.
//
// Low tag:
//   00: embedded_object:      [6-bit pc delta] 00
//
//   01: code_target:          [6-bit pc delta] 01
//
//   10: wasm_stub_call:       [6-bit pc delta] 10
//
//   11: long_record           [6 bit reloc mode] 11
//                             followed by pc delta
//                             followed by optional data depending on type.
//
//  If a pc delta exceeds 6 bits, it is split into a remainder that fits into
//  6 bits and a part that does not. The latter is encoded as a long record
//  with PC_JUMP as pseudo reloc info mode. The former is encoded as part of
//  the following record in the usual way. The long pc jump record has variable
//  length:
//               pc-jump:        [PC_JUMP] 11
//                               1 [7 bits data]
//                                  ...
//                               0 [7 bits data]
//               (Bits 6..31 of pc delta, encoded with VLQ.)

constexpr int kTagBits = 2;
constexpr int kTagMask = (1 << kTagBits) - 1;
constexpr int kLongTagBits = 6;

constexpr int kEmbeddedObjectTag = 0;
constexpr int kCodeTargetTag = 1;
constexpr int kWasmStubCallTag = 2;
constexpr int kDefaultTag = 3;

constexpr int kSmallPCDeltaBits = kBitsPerByte - kTagBits;
constexpr int kSmallPCDeltaMask = (1 << kSmallPCDeltaBits) - 1;
}  // namespace detail

// -----------------------------------------------------------------------------
// Relocation information

// Relocation information consists of the address (pc) of the datum
// to which the relocation information applies, the relocation mode
// (rmode), and an optional data field. The relocation mode may be
// "descriptive" and not indicate a need for relocation, but simply
// describe a property of the datum. Such rmodes are useful for GC
// and nice disassembly output.

class RelocInfo {
 public:
  // The minimum size of a comment is equal to two bytes for the extra tagged
  // pc and kSystemPointerSize for the actual pointer to the comment.
  static constexpr int kMinRelocCommentSize = 2 + kSystemPointerSize;

  // The maximum size for a call instruction including pc-jump.
  static constexpr int kMaxCallSize = 6;

  // The maximum pc delta that will use the short encoding.
  static constexpr int kMaxSmallPCDelta = detail::kSmallPCDeltaMask;

  enum Mode : int8_t {
    // Please note the order is important (see IsRealRelocMode, IsGCRelocMode,
    // and IsShareableRelocMode predicates below).

    NO_INFO,  // Never recorded value. Most common one, hence value 0.

    CODE_TARGET,
    // TODO(ishell): rename to NEAR_CODE_TARGET.
    RELATIVE_CODE_TARGET,  // LAST_CODE_TARGET_MODE
    COMPRESSED_EMBEDDED_OBJECT,
    FULL_EMBEDDED_OBJECT,  // LAST_GCED_ENUM

    WASM_CALL,  // FIRST_SHAREABLE_RELOC_MODE
    WASM_STUB_CALL,
    WASM_CANONICAL_SIG_ID,

    EXTERNAL_REFERENCE,  // The address of an external C++ function.
    INTERNAL_REFERENCE,  // An address inside the same function.

    // The relative address (target-table) in the switch table.
    RELATIVE_SWITCH_TABLE_ENTRY,

    // Encoded internal reference, used only on RISCV64, RISCV32, MIPS64
    // and PPC.
    INTERNAL_REFERENCE_ENCODED,

    // An off-heap instruction stream target. See http://goo.gl/Z2HUiM.
    // TODO(ishell): rename to BUILTIN_ENTRY.
    OFF_HEAP_TARGET,  // FIRST_BUILTIN_ENTRY_MODE
    // An un-embedded off-heap instruction stream target.
    // See http://crbug.com/v8/11527 for details.
    NEAR_BUILTIN_ENTRY,  // LAST_BUILTIN_ENTRY_MODE

    // Marks constant and veneer pools. Only used on ARM and ARM64.
    // They use a custom noncompact encoding.
    CONST_POOL,
    VENEER_POOL,

    DEOPT_SCRIPT_OFFSET,
    DEOPT_INLINING_ID,  // Deoptimization source position.
    DEOPT_REASON,       // Deoptimization reason index.
    DEOPT_ID,           // Deoptimization inlining id.
    DEOPT_NODE_ID,      // Id of the node that caused deoptimization. This
                        // information is only recorded in debug builds.

    // This is not an actual reloc mode, but used to encode a long pc jump that
    // cannot be encoded as part of another record.
    PC_JUMP,

    // Pseudo-types
    NUMBER_OF_MODES,

    LAST_CODE_TARGET_MODE = RELATIVE_CODE_TARGET,
    FIRST_REAL_RELOC_MODE = CODE_TARGET,
    LAST_REAL_RELOC_MODE = VENEER_POOL,
    FIRST_EMBEDDED_OBJECT_RELOC_MODE = COMPRESSED_EMBEDDED_OBJECT,
    LAST_EMBEDDED_OBJECT_RELOC_MODE = FULL_EMBEDDED_OBJECT,
    LAST_GCED_ENUM = LAST_EMBEDDED_OBJECT_RELOC_MODE,
    FIRST_BUILTIN_ENTRY_MODE = OFF_HEAP_TARGET,
    LAST_BUILTIN_ENTRY_MODE = NEAR_BUILTIN_ENTRY,
    FIRST_SHAREABLE_RELOC_MODE = WASM_CALL,
  };

  static_assert(NUMBER_OF_MODES <= kBitsPerInt);

  RelocInfo() = default;

  RelocInfo(Address pc, Mode rmode, intptr_t data,
            Address constant_pool = kNullAddress)
      : pc_(pc), rmode_(rmode), data_(data), constant_pool_(constant_pool) {
    DCHECK_IMPLIES(!COMPRESS_POINTERS_BOOL,
                   rmode != COMPRESSED_EMBEDDED_OBJECT);
  }

  // Convenience ctor.
  RelocInfo(Address pc, Mode rmode) : RelocInfo(pc, rmode, 0) {}

  static constexpr bool IsRealRelocMode(Mode mode) {
    return mode >= FIRST_REAL_RELOC_MODE && mode <= LAST_REAL_RELOC_MODE;
  }
  // Is the relocation mode affected by GC?
  static constexpr bool IsGCRelocMode(Mode mode) {
    return mode <= LAST_GCED_ENUM;
  }
  static constexpr bool IsShareableRelocMode(Mode mode) {
    return mode == RelocInfo::NO_INFO ||
           mode >= RelocInfo::FIRST_SHAREABLE_RELOC_MODE;
  }
  static constexpr bool IsCodeTarget(Mode mode) { return mode == CODE_TARGET; }
  static constexpr bool IsCodeTargetMode(Mode mode) {
    return mode <= LAST_CODE_TARGET_MODE;
  }
  static constexpr bool IsRelativeCodeTarget(Mode mode) {
    return mode == RELATIVE_CODE_TARGET;
  }
  static constexpr bool IsFullEmbeddedObject(Mode mode) {
    return mode == FULL_EMBEDDED_OBJECT;
  }
  static constexpr bool IsCompressedEmbeddedObject(Mode mode) {
    return COMPRESS_POINTERS_BOOL && mode == COMPRESSED_EMBEDDED_OBJECT;
  }
  static constexpr bool IsEmbeddedObjectMode(Mode mode) {
    return base::IsInRange(mode, FIRST_EMBEDDED_OBJECT_RELOC_MODE,
                           LAST_EMBEDDED_OBJECT_RELOC_MODE);
  }
  static constexpr bool IsWasmCall(Mode mode) { return mode == WASM_CALL; }
  static constexpr bool IsWasmStubCall(Mode mode) {
    return mode == WASM_STUB_CALL;
  }
  static constexpr bool IsWasmCanonicalSigId(Mode mode) {
    return mode == WASM_CANONICAL_SIG_ID;
  }
  static constexpr bool IsConstPool(Mode mode) { return mode == CONST_POOL; }
  static constexpr bool IsVeneerPool(Mode mode) { return mode == VENEER_POOL; }
  static constexpr bool IsDeoptPosition(Mode mode) {
    return mode == DEOPT_SCRIPT_OFFSET || mode == DEOPT_INLINING_ID;
  }
  static constexpr bool IsDeoptReason(Mode mode) {
    return mode == DEOPT_REASON;
  }
  static constexpr bool IsDeoptId(Mode mode) { return mode == DEOPT_ID; }
  static constexpr bool IsDeoptNodeId(Mode mode) {
    return mode == DEOPT_NODE_ID;
  }
  static constexpr bool IsExternalReference(Mode mode) {
    return mode == EXTERNAL_REFERENCE;
  }
  static constexpr bool IsInternalReference(Mode mode) {
    return mode == INTERNAL_REFERENCE;
  }
  static constexpr bool IsRelativeSwitchTableEntry(Mode mode) {
    return mode == RELATIVE_SWITCH_TABLE_ENTRY;
  }
  static constexpr bool IsInternalReferenceEncoded(Mode mode) {
    return mode == INTERNAL_REFERENCE_ENCODED;
  }
  static constexpr bool IsOffHeapTarget(Mode mode) {
    return mode == OFF_HEAP_TARGET;
  }
  static constexpr bool IsNearBuiltinEntry(Mode mode) {
    return mode == NEAR_BUILTIN_ENTRY;
  }
  static constexpr bool IsBuiltinEntryMode(Mode mode) {
    return base::IsInRange(mode, FIRST_BUILTIN_ENTRY_MODE,
                           LAST_BUILTIN_ENTRY_MODE);
  }
  static constexpr bool IsNoInfo(Mode mode) { return mode == NO_INFO; }

  static bool IsOnlyForSerializer(Mode mode) {
#ifdef V8_TARGET_ARCH_IA32
    // On ia32, inlined off-heap trampolines must be relocated.
    DCHECK_NE((kApplyMask & ModeMask(OFF_HEAP_TARGET)), 0);
    DCHECK_EQ((kApplyMask & ModeMask(EXTERNAL_REFERENCE)), 0);
    return mode == EXTERNAL_REFERENCE;
#else
    DCHECK_EQ((kApplyMask & ModeMask(OFF_HEAP_TARGET)), 0);
    DCHECK_EQ((kApplyMask & ModeMask(EXTERNAL_REFERENCE)), 0);
    return mode == EXTERNAL_REFERENCE || mode == OFF_HEAP_TARGET;
#endif
  }

  static bool IsOnlyForDisassembler(Mode mode) {
    return mode == RELATIVE_SWITCH_TABLE_ENTRY;
  }

  static constexpr int ModeMask(Mode mode) { return 1 << mode; }

  // Accessors
  Address pc() const { return pc_; }
  Mode rmode() const { return rmode_; }
  intptr_t data() const { return data_; }

  // Is the pointer this relocation info refers to coded like a plain pointer
  // or is it strange in some way (e.g. relative or patched into a series of
  // instructions).
  bool IsCodedSpecially();

  // The static pendant to IsCodedSpecially, just for off-heap targets. Used
  // during deserialization, when we don't actually have a RelocInfo handy.
  static bool OffHeapTargetIsCodedSpecially();

  // If true, the pointer this relocation info refers to is an entry in the
  // constant pool, otherwise the pointer is embedded in the instruction stream.
  bool IsInConstantPool();

  Address wasm_call_address() const;
  Address wasm_stub_call_address() const;
  V8_EXPORT_PRIVATE uint32_t wasm_canonical_sig_id() const;

  uint32_t wasm_call_tag() const;

  void set_off_heap_target_address(
      Address target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);

  // this relocation applies to;
  // can only be called if IsCodeTarget(rmode_)
  V8_INLINE Address target_address();
  // Cage base value is used for decompressing compressed embedded references.
  V8_INLINE Tagged<HeapObject> target_object(PtrComprCageBase cage_base);

  V8_INLINE Handle<HeapObject> target_object_handle(Assembler* origin);

  // Decodes builtin ID encoded as a PC-relative offset. This encoding is used
  // during code generation of call/jump with NEAR_BUILTIN_ENTRY.
  V8_INLINE Builtin target_builtin_at(Assembler* origin);
  V8_INLINE Address target_off_heap_target();

  // Returns the address of the constant pool entry where the target address
  // is held.  This should only be called if IsInConstantPool returns true.
  V8_INLINE Address constant_pool_entry_address();

  // Read the address of the word containing the target_address in an
  // instruction stream.  What this means exactly is architecture-independent.
  // The only architecture-independent user of this function is the serializer.
  // The serializer uses it to find out how many raw bytes of instruction to
  // output before the next target.  Architecture-independent code shouldn't
  // dereference the pointer it gets back from this.
  V8_INLINE Address target_address_address();
  bool HasTargetAddressAddress() const;

  // This indicates how much space a target takes up when deserializing a code
  // stream.  For most architectures this is just the size of a pointer.  For
  // an instruction like movw/movt where the target bits are mixed into the
  // instruction bits the size of the target will be zero, indicating that the
  // serializer should not step forwards in memory after a target is resolved
  // and written.  In this case the target_address_address function above
  // should return the end of the instructions to be patched, allowing the
  // deserializer to deserialize the instructions as raw bytes and put them in
  // place, ready to be patched with the target.
  V8_INLINE int target_address_size();

  // Read the reference in the instruction this relocation
  // applies to; can only be called if rmode_ is EXTERNAL_REFERENCE.
  V8_INLINE Address target_external_reference();

  // Read the reference in the instruction this relocation
  // applies to; can only be called if rmode_ is INTERNAL_REFERENCE.
  V8_INLINE Address target_internal_reference();

  // Return the reference address this relocation applies to;
  // can only be called if rmode_ is INTERNAL_REFERENCE.
  V8_INLINE Address target_internal_reference_address();

  template <typename ObjectVisitor>
  void Visit(Tagged<InstructionStream> host, ObjectVisitor* visitor) {
    Mode mode = rmode();
    if (IsEmbeddedObjectMode(mode)) {
      visitor->VisitEmbeddedPointer(host, this);
    } else if (IsCodeTargetMode(mode)) {
      visitor->VisitCodeTarget(host, this);
    } else if (IsExternalReference(mode)) {
      visitor->VisitExternalReference(host, this);
    } else if (IsInternalReference(mode) || IsInternalReferenceEncoded(mode)) {
      visitor->VisitInternalReference(host, this);
    } else if (IsBuiltinEntryMode(mode)) {
      visitor->VisitOffHeapTarget(host, this);
    }
  }

#ifdef ENABLE_DISASSEMBLER
  // Printing
  static const char* RelocModeName(Mode rmode);
  void Print(Isolate* isolate, std::ostream& os);
#endif  // ENABLE_DISASSEMBLER
#ifdef VERIFY_HEAP
  void Verify(Isolate* isolate);
#endif

  static const int kApplyMask;  // Modes affected by apply.  Depends on arch.

  static constexpr int AllRealModesMask() {
    constexpr Mode kFirstUnrealRelocMode =
        static_cast<Mode>(RelocInfo::LAST_REAL_RELOC_MODE + 1);
    return (ModeMask(kFirstUnrealRelocMode) - 1) &
           ~(ModeMask(RelocInfo::FIRST_REAL_RELOC_MODE) - 1);
  }

  static int EmbeddedObjectModeMask() {
    return ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT) |
           ModeMask(RelocInfo::COMPRESSED_EMBEDDED_OBJECT);
  }

  // In addition to modes covered by the apply mask (which is applied at GC
  // time, among others), this covers all modes that are relocated by
  // InstructionStream::CopyFromNoFlush after code generation.
  static int PostCodegenRelocationMask() {
    return ModeMask(RelocInfo::CODE_TARGET) |
           ModeMask(RelocInfo::COMPRESSED_EMBEDDED_OBJECT) |
           ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT) |
           ModeMask(RelocInfo::NEAR_BUILTIN_ENTRY) |
           ModeMask(RelocInfo::WASM_STUB_CALL) |
           ModeMask(RelocInfo::RELATIVE_CODE_TARGET) | kApplyMask;
  }

 protected:
  // On ARM/ARM64, note that pc_ is the address of the instruction referencing
  // the constant pool and not the address of the constant pool entry.
  Address pc_;
  Mode rmode_;
  intptr_t data_ = 0;
  Address constant_pool_ = kNullAddress;

  template <typename RelocIteratorType>
  friend class RelocIteratorBase;
};

class WritableRelocInfo : public RelocInfo {
 public:
  WritableRelocInfo(WritableJitAllocation& jit_allocation, Address pc,
                    Mode rmode)
      : RelocInfo(pc, rmode) {}
  WritableRelocInfo(WritableJitAllocation& jit_allocation, Address pc,
                    Mode rmode, intptr_t data, Address constant_pool)
      : RelocInfo(pc, rmode, data, constant_pool) {}

  // Apply a relocation by delta bytes. When the code object is moved, PC
  // relative addresses have to be updated as well as absolute addresses
  // inside the code (internal references).
  // Do not forget to flush the icache afterwards!
  V8_INLINE void apply(intptr_t delta);

  void set_wasm_call_address(Address);
  void set_wasm_stub_call_address(Address);
  void set_wasm_canonical_sig_id(uint32_t);

  void set_target_address(
      Tagged<InstructionStream> host, Address target,
      WriteBarrierMode write_barrier_mode = UPDATE_WRITE_BARRIER,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
  // Use this overload only when an InstructionStream host is not available.
  void set_target_address(Address target, ICacheFlushMode icache_flush_mode =
                                              FLUSH_ICACHE_IF_NEEDED);

  V8_INLINE void set_target_object(
      Tagged<InstructionStream> host, Tagged<HeapObject> target,
      WriteBarrierMode write_barrier_mode = UPDATE_WRITE_BARRIER,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
  // Use this overload only when an InstructionStream host is not available.
  V8_INLINE void set_target_object(
      Tagged<HeapObject> target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);

  V8_INLINE void set_target_external_reference(
      Address, ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
};

// RelocInfoWriter serializes a stream of relocation info. It writes towards
// lower addresses.
class RelocInfoWriter {
 public:
  RelocInfoWriter() : pos_(nullptr), last_pc_(nullptr) {}

  RelocInfoWriter(const RelocInfoWriter&) = delete;
  RelocInfoWriter& operator=(const RelocInfoWriter&) = delete;

  uint8_t* pos() const { return pos_; }
  uint8_t* last_pc() const { return last_pc_; }

  void Write(const RelocInfo* rinfo);

  // Update the state of the stream after reloc info buffer
  // and/or code is moved while the stream is active.
  void Reposition(uint8_t* pos, uint8_t* pc) {
    pos_ = pos;
    last_pc_ = pc;
  }

  // Max size (bytes) of a written RelocInfo. Longest encoding is
  // ExtraTag, VariableLengthPCJump, ExtraTag, pc_delta, data_delta.
  static constexpr int kMaxSize = 1 + 4 + 1 + 1 + kSystemPointerSize;

 private:
  inline uint32_t WriteLongPCJump(uint32_t pc_delta);

  inline void WriteShortTaggedPC(uint32_t pc_delta, int tag);
  inline void WriteShortData(intptr_t data_delta);

  inline void WriteMode(RelocInfo::Mode rmode);
  inline void WriteModeAndPC(uint32_t pc_delta, RelocInfo::Mode rmode);
  inline void WriteIntData(int data_delta);

  uint8_t* pos_;
  uint8_t* last_pc_;
};

// A RelocIterator iterates over relocation information.
// Typical use:
//
//   for (RelocIterator it(code); !it.done(); it.next()) {
//     // do something with it.rinfo() here
//   }
//
// A mask can be specified to skip unwanted modes.
template <typename RelocInfoT>
class RelocIteratorBase {
 public:
  static constexpr int kAllModesMask = -1;

  RelocIteratorBase(RelocIteratorBase&&) V8_NOEXCEPT = default;
  RelocIteratorBase(const RelocIteratorBase&) = delete;
  RelocIteratorBase& operator=(const RelocIteratorBase&) = delete;

  bool done() const { return done_; }
  void next();

  // The returned pointer is valid until the next call to next().
  RelocInfoT* rinfo() {
    DCHECK(!done());
    return &rinfo_;
  }

 protected:
  V8_INLINE RelocIteratorBase(RelocInfoT reloc_info, const uint8_t* pos,
                              const uint8_t* end, int mode_mask);

  // Used for efficiently skipping unwanted modes.
  bool SetMode(RelocInfo::Mode mode) {
    if ((mode_mask_ & (1 << mode)) == 0) return false;
    rinfo_.rmode_ = mode;
    return true;
  }

  RelocInfo::Mode GetMode() const {
    return static_cast<RelocInfo::Mode>((*pos_ >> detail::kTagBits) &
                                        ((1 << detail::kLongTagBits) - 1));
  }

  void Advance(int bytes = 1) { pos_ -= bytes; }
  int AdvanceGetTag() { return *--pos_ & detail::kTagMask; }
  void AdvanceReadLongPCJump();
  void AdvanceReadPC() { rinfo_.pc_ += *--pos_; }
  void AdvanceReadInt();

  void ReadShortTaggedPC() { rinfo_.pc_ += *pos_ >> detail::kTagBits; }
  void ReadShortData();

  const uint8_t* pos_;
  const uint8_t* const end_;
  RelocInfoT rinfo_;
  bool done_ = false;
  const int mode_mask_;
};

extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
    RelocIteratorBase<RelocInfo>;
extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
    RelocIteratorBase<WritableRelocInfo>;

class V8_EXPORT_PRIVATE RelocIterator : public RelocIteratorBase<RelocInfo> {
 public:
  // Prefer using this ctor when possible:
  explicit RelocIterator(Tagged<InstructionStream> istream, int mode_mask);
  // Convenience wrapper.
  explicit RelocIterator(Tagged<Code> code, int mode_mask = kAllModesMask);

  // For Wasm.
  explicit RelocIterator(base::Vector<uint8_t> instructions,
                         base::Vector<const uint8_t> reloc_info,
                         Address const_pool, int mode_mask = kAllModesMask);
  // For the disassembler.
  explicit RelocIterator(const CodeReference code_reference);
  // For FinalizeEmbeddedCodeTargets when creating embedded builtins.
  explicit RelocIterator(EmbeddedData* embedded_data, Tagged<Code> code,
                         int mode_mask);

  RelocIterator(RelocIterator&&) V8_NOEXCEPT = default;
  RelocIterator(const RelocIterator&) = delete;
  RelocIterator& operator=(const RelocIterator&) = delete;

 private:
  RelocIterator(Address pc, Address constant_pool, const uint8_t* pos,
                const uint8_t* end, int mode_mask);
};

class V8_EXPORT_PRIVATE WritableRelocIterator
    : public RelocIteratorBase<WritableRelocInfo> {
 public:
  // Constructor for iterating InstructionStreams.
  WritableRelocIterator(WritableJitAllocation& jit_allocation,
                        Tagged<InstructionStream> istream,
                        Address constant_pool, int mode_mask);
  // Constructor for iterating Wasm code.
  WritableRelocIterator(WritableJitAllocation& jit_allocation,
                        base::Vector<uint8_t> instructions,
                        base::Vector<const uint8_t> reloc_info,
                        Address constant_pool, int mode_mask = kAllModesMask);
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RELOC_INFO_H_
            node-23.7.0/deps/v8/src/codegen/riscv/                                                              0000775 0000000 0000000 00000000000 14746647661 0017366 5                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        node-23.7.0/deps/v8/src/codegen/riscv/assembler-riscv-inl.h                                         0000664 0000000 0000000 00000031446 14746647661 0023430 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright (c) 1994-2006 Sun Microsystems Inc.
// All Rights Reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// - Redistributions of source code must retain the above copyright notice,
// this list of conditions and the following disclaimer.
//
// - Redistribution in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// - Neither the name of Sun Microsystems or the names of contributors may
// be used to endorse or promote products derived from this software without
// specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// The original source code covered by the above license above has been
// modified significantly by Google Inc.
// Copyright 2021 the V8 project authors. All rights reserved.

#ifndef V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_
#define V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_

#include "src/codegen/assembler-arch.h"
#include "src/codegen/assembler.h"
#include "src/debug/debug.h"
#include "src/objects/objects-inl.h"

namespace v8 {
namespace internal {

bool CpuFeatures::SupportsOptimizer() { return IsSupported(FPU); }

void Assembler::CheckBuffer() {
  if (buffer_space() <= kGap) {
    GrowBuffer();
  }
}

// -----------------------------------------------------------------------------
// WritableRelocInfo.

void WritableRelocInfo::apply(intptr_t delta) {
  if (IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_)) {
    // Absolute code pointer inside code object moves with the code object.
    Assembler::RelocateInternalReference(rmode_, pc_, delta);
  } else {
    DCHECK(IsRelativeCodeTarget(rmode_));
    Assembler::RelocateRelativeReference(rmode_, pc_, delta);
  }
}

Address RelocInfo::target_address() {
  DCHECK(IsCodeTargetMode(rmode_) || IsWasmCall(rmode_) ||
         IsNearBuiltinEntry(rmode_) || IsWasmStubCall(rmode_));
  return Assembler::target_address_at(pc_, constant_pool_);
}

Address RelocInfo::target_address_address() {
  DCHECK(HasTargetAddressAddress());
  // Read the address of the word containing the target_address in an
  // instruction stream.
  // The only architecture-independent user of this function is the serializer.
  // The serializer uses it to find out how many raw bytes of instruction to
  // output before the next target.
  // For an instruction like LUI/ORI where the target bits are mixed into the
  // instruction bits, the size of the target will be zero, indicating that the
  // serializer should not step forward in memory after a target is resolved
  // and written. In this case the target_address_address function should
  // return the end of the instructions to be patched, allowing the
  // deserializer to deserialize the instructions as raw bytes and put them in
  // place, ready to be patched with the target. After jump optimization,
  // that is the address of the instruction that follows J/JAL/JR/JALR
  // instruction.
#ifdef V8_TARGET_ARCH_RISCV64
  return pc_ + Assembler::kInstructionsFor64BitConstant * kInstrSize;
#elif defined(V8_TARGET_ARCH_RISCV32)
  return pc_ + Assembler::kInstructionsFor32BitConstant * kInstrSize;
#endif
}

Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }

int RelocInfo::target_address_size() {
  if (IsCodedSpecially()) {
    return Assembler::kSpecialTargetSize;
  } else {
    return kSystemPointerSize;
  }
}

void Assembler::set_target_compressed_address_at(
    Address pc, Address constant_pool, Tagged_t target,
    ICacheFlushMode icache_flush_mode) {
  Assembler::set_target_address_at(
      pc, constant_pool, static_cast<Address>(target), icache_flush_mode);
}

Tagged_t Assembler::target_compressed_address_at(Address pc,
                                                 Address constant_pool) {
  return static_cast<Tagged_t>(target_address_at(pc, constant_pool));
}

Handle<Object> Assembler::code_target_object_handle_at(Address pc,
                                                       Address constant_pool) {
  int index =
      static_cast<int>(target_address_at(pc, constant_pool)) & 0xFFFFFFFF;
  return GetCodeTarget(index);
}

Handle<HeapObject> Assembler::compressed_embedded_object_handle_at(
    Address pc, Address const_pool) {
  return GetEmbeddedObject(target_compressed_address_at(pc, const_pool));
}

void Assembler::deserialization_set_special_target_at(
    Address instruction_payload, Tagged<Code> code, Address target) {
  set_target_address_at(instruction_payload,
                        !code.is_null() ? code->constant_pool() : kNullAddress,
                        target);
}

int Assembler::deserialization_special_target_size(
    Address instruction_payload) {
  return kSpecialTargetSize;
}

void Assembler::set_target_internal_reference_encoded_at(Address pc,
                                                         Address target) {
#ifdef V8_TARGET_ARCH_RISCV64
  set_target_value_at(pc, static_cast<uint64_t>(target));
#elif defined(V8_TARGET_ARCH_RISCV32)
  set_target_value_at(pc, static_cast<uint32_t>(target));
#endif
}

void Assembler::deserialization_set_target_internal_reference_at(
    Address pc, Address target, RelocInfo::Mode mode) {
  if (RelocInfo::IsInternalReferenceEncoded(mode)) {
    DCHECK(IsLui(instr_at(pc)));
    set_target_internal_reference_encoded_at(pc, target);
  } else {
    DCHECK(RelocInfo::IsInternalReference(mode));
    Memory<Address>(pc) = target;
  }
}

Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {
  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
  if (IsCompressedEmbeddedObject(rmode_)) {
    return Cast<HeapObject>(
        Tagged<Object>(V8HeapCompressionScheme::DecompressTagged(
            cage_base,
            Assembler::target_compressed_address_at(pc_, constant_pool_))));
  } else {
    return Cast<HeapObject>(
        Tagged<Object>(Assembler::target_address_at(pc_, constant_pool_)));
  }
}

Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
  if (IsCodeTarget(rmode_)) {
    return Cast<HeapObject>(
        origin->code_target_object_handle_at(pc_, constant_pool_));
  } else if (IsCompressedEmbeddedObject(rmode_)) {
    return origin->compressed_embedded_object_handle_at(pc_, constant_pool_);
  } else if (IsFullEmbeddedObject(rmode_)) {
    return Handle<HeapObject>(reinterpret_cast<Address*>(
        Assembler::target_address_at(pc_, constant_pool_)));
  } else {
    DCHECK(IsRelativeCodeTarget(rmode_));
    return origin->relative_code_target_object_handle_at(pc_);
  }
}

void WritableRelocInfo::set_target_object(Tagged<HeapObject> target,
                                          ICacheFlushMode icache_flush_mode) {
  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
  if (IsCompressedEmbeddedObject(rmode_)) {
    DCHECK(COMPRESS_POINTERS_BOOL);
    // We must not compress pointers to objects outside of the main pointer
    // compression cage as we wouldn't be able to decompress them with the
    // correct cage base.
    DCHECK_IMPLIES(V8_ENABLE_SANDBOX_BOOL, !IsTrustedSpaceObject(target));
    DCHECK_IMPLIES(V8_EXTERNAL_CODE_SPACE_BOOL, !IsCodeSpaceObject(target));
    Assembler::set_target_compressed_address_at(
        pc_, constant_pool_,
        V8HeapCompressionScheme::CompressObject(target.ptr()),
        icache_flush_mode);
  } else {
    DCHECK(IsFullEmbeddedObject(rmode_));
    Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                     icache_flush_mode);
  }
}

Address RelocInfo::target_external_reference() {
  DCHECK(rmode_ == EXTERNAL_REFERENCE);
  return Assembler::target_address_at(pc_, constant_pool_);
}

void WritableRelocInfo::set_target_external_reference(
    Address target, ICacheFlushMode icache_flush_mode) {
  DCHECK(rmode_ == RelocInfo::EXTERNAL_REFERENCE);
  Assembler::set_target_address_at(pc_, constant_pool_, target,
                                   icache_flush_mode);
}

Address RelocInfo::target_internal_reference() {
  if (IsInternalReference(rmode_)) {
    return Memory<Address>(pc_);
  } else {
    // Encoded internal references are j/jal instructions.
    DCHECK(IsInternalReferenceEncoded(rmode_));
    DCHECK(Assembler::IsLui(Assembler::instr_at(pc_ + 0 * kInstrSize)));
    Address address = Assembler::target_address_at(pc_);
    return address;
  }
}

Address RelocInfo::target_internal_reference_address() {
  DCHECK(IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_));
  return pc_;
}

Handle<Code> Assembler::relative_code_target_object_handle_at(
    Address pc) const {
  Instr instr1 = Assembler::instr_at(pc);
  Instr instr2 = Assembler::instr_at(pc + kInstrSize);
  DCHECK(IsAuipc(instr1));
  DCHECK(IsJalr(instr2));
  int32_t code_target_index = BrachlongOffset(instr1, instr2);
  return Cast<Code>(GetEmbeddedObject(code_target_index));
}

Builtin Assembler::target_builtin_at(Address pc) {
  Instr instr1 = Assembler::instr_at(pc);
  Instr instr2 = Assembler::instr_at(pc + kInstrSize);
  DCHECK(IsAuipc(instr1));
  DCHECK(IsJalr(instr2));
  int32_t builtin_id = BrachlongOffset(instr1, instr2);
  DCHECK(Builtins::IsBuiltinId(builtin_id));
  return static_cast<Builtin>(builtin_id);
}

Builtin RelocInfo::target_builtin_at(Assembler* origin) {
  DCHECK(IsNearBuiltinEntry(rmode_));
  return Assembler::target_builtin_at(pc_);
}

Address RelocInfo::target_off_heap_target() {
  DCHECK(IsOffHeapTarget(rmode_));
  return Assembler::target_address_at(pc_, constant_pool_);
}

EnsureSpace::EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }

int32_t Assembler::target_constant32_at(Address pc) {
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));

  // Interpret instructions for address generated by li: See listing in
  // Assembler::set_target_address_at() just below.
  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
      IsAddi(*reinterpret_cast<Instr*>(instr1))) {
    // Assemble the 32bit value.
    int32_t constant32 = (int32_t)(instr0->Imm20UValue() << kImm20Shift) +
                         (int32_t)instr1->Imm12Value();
    return constant32;
  }
  // We should never get here, force a bad address if we do.
  UNREACHABLE();
}

void Assembler::set_target_constant32_at(Address pc, uint32_t target,
                                         ICacheFlushMode icache_flush_mode) {
  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
#ifdef DEBUG
  // Check we have the result from a li macro-instruction.
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
         IsAddi(*reinterpret_cast<Instr*>(instr1)));
#endif
  int32_t high_20 = ((target + 0x800) >> 12);  // 20 bits
  int32_t low_12 = target & 0xfff;             // 12 bits
  *p = *p & 0xfff;
  *p = *p | ((int32_t)high_20 << 12);
  *(p + 1) = *(p + 1) & 0xfffff;
  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
    FlushInstructionCache(pc, 2 * kInstrSize);
  }
  DCHECK_EQ(target_constant32_at(pc), target);
}

uint32_t Assembler::uint32_constant_at(Address pc, Address constant_pool) {
  Instruction* instr0 = reinterpret_cast<Instruction*>(pc);
  Instruction* instr1 = reinterpret_cast<Instruction*>(pc + 1 * kInstrSize);
  CHECK(IsLui(*reinterpret_cast<Instr*>(instr0)));
  CHECK(IsAddi(*reinterpret_cast<Instr*>(instr1)));
  return target_constant32_at(pc);
}
void Assembler::set_uint32_constant_at(Address pc, Address constant_pool,
                                       uint32_t new_constant,
                                       ICacheFlushMode icache_flush_mode) {
  Instruction* instr1 = reinterpret_cast<Instruction*>(pc);
  Instruction* instr2 = reinterpret_cast<Instruction*>(pc + 1 * kInstrSize);
  CHECK(IsLui(*reinterpret_cast<Instr*>(instr1)));
  CHECK(IsAddi(*reinterpret_cast<Instr*>(instr2)));
  set_target_constant32_at(pc, new_constant, icache_flush_mode);
}

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_
                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/codegen/riscv/assembler-riscv.cc                                            0000664 0000000 0000000 00000230413 14746647661 0023001 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright (c) 1994-2006 Sun Microsystems Inc.
// All Rights Reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// - Redistributions of source code must retain the above copyright notice,
// this list of conditions and the following disclaimer.
//
// - Redistribution in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// - Neither the name of Sun Microsystems or the names of contributors may
// be used to endorse or promote products derived from this software without
// specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// The original source code covered by the above license above has been
// modified significantly by Google Inc.
// Copyright 2021 the V8 project authors. All rights reserved.

#include "src/codegen/riscv/assembler-riscv.h"

#include "src/base/bits.h"
#include "src/base/cpu.h"
#include "src/codegen/assembler-inl.h"
#include "src/codegen/safepoint-table.h"
#include "src/deoptimizer/deoptimizer.h"
#include "src/diagnostics/disasm.h"
#include "src/diagnostics/disassembler.h"
#include "src/objects/heap-number-inl.h"

namespace v8 {
namespace internal {
// Get the CPU features enabled by the build. For cross compilation the
// preprocessor symbols __riscv_f and __riscv_d
// can be defined to enable FPU instructions when building the
// snapshot.
static unsigned CpuFeaturesImpliedByCompiler() {
  unsigned answer = 0;
#if defined(__riscv_f) && defined(__riscv_d)
  answer |= 1u << FPU;
#endif  // def __riscv_f

#if (defined __riscv_vector) && (__riscv_v >= 1000000)
  answer |= 1u << RISCV_SIMD;
#endif  // def CAN_USE_RVV_INSTRUCTIONS

#if (defined __riscv_zba)
  answer |= 1u << ZBA;
#endif  // def __riscv_zba

#if (defined __riscv_zbb)
  answer |= 1u << ZBB;
#endif  // def __riscv_zbb

#if (defined __riscv_zbs)
  answer |= 1u << ZBS;
#endif  // def __riscv_zbs

#if (defined _riscv_zicond)
  answer |= 1u << ZICOND;
#endif  // def _riscv_zicond
  return answer;
}

static unsigned SimulatorFeatures() {
  unsigned answer = 0;
  answer |= 1u << RISCV_SIMD;
  answer |= 1u << ZBA;
  answer |= 1u << ZBB;
  answer |= 1u << ZBS;
  answer |= 1u << ZICOND;
  answer |= 1u << FPU;
  return answer;
}

bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(RISCV_SIMD); }

void CpuFeatures::ProbeImpl(bool cross_compile) {
  supported_ |= CpuFeaturesImpliedByCompiler();
  // Only use statically determined features for cross compile (snapshot).
  if (cross_compile) return;
  // Probe for additional features at runtime.

#ifdef USE_SIMULATOR
  supported_ |= SimulatorFeatures();
#else
  base::CPU cpu;
  if (cpu.has_fpu()) supported_ |= 1u << FPU;
  if (cpu.has_rvv()) supported_ |= 1u << RISCV_SIMD;
#ifdef V8_COMPRESS_POINTERS
  if (cpu.riscv_mmu() == base::CPU::RV_MMU_MODE::kRiscvSV57) {
    FATAL("SV57 is not supported");
    UNIMPLEMENTED();
  }
#endif  // V8_COMPRESS_POINTERS
#endif  // USE_SIMULATOR
  // Set a static value on whether SIMD is supported.
  // This variable is only used for certain archs to query SupportWasmSimd128()
  // at runtime in builtins using an extern ref. Other callers should use
  // CpuFeatures::SupportWasmSimd128().
  CpuFeatures::supports_wasm_simd_128_ = CpuFeatures::SupportsWasmSimd128();
}

void CpuFeatures::PrintTarget() {}
void CpuFeatures::PrintFeatures() {
  printf("supports_wasm_simd_128=%d\n", CpuFeatures::SupportsWasmSimd128());
  printf("zba=%d,zbb=%d,zbs=%d\n", CpuFeatures::IsSupported(ZBA),
         CpuFeatures::IsSupported(ZBB), CpuFeatures::IsSupported(ZBS));
}
int ToNumber(Register reg) {
  DCHECK(reg.is_valid());
  const int kNumbers[] = {
      0,   // zero_reg
      1,   // ra
      2,   // sp
      3,   // gp
      4,   // tp
      5,   // t0
      6,   // t1
      7,   // t2
      8,   // s0/fp
      9,   // s1
      10,  // a0
      11,  // a1
      12,  // a2
      13,  // a3
      14,  // a4
      15,  // a5
      16,  // a6
      17,  // a7
      18,  // s2
      19,  // s3
      20,  // s4
      21,  // s5
      22,  // s6
      23,  // s7
      24,  // s8
      25,  // s9
      26,  // s10
      27,  // s11
      28,  // t3
      29,  // t4
      30,  // t5
      31,  // t6
  };
  return kNumbers[reg.code()];
}

Register ToRegister(int num) {
  DCHECK(num >= 0 && num < kNumRegisters);
  const Register kRegisters[] = {
      zero_reg, ra, sp, gp, tp, t0, t1, t2, fp, s1, a0,  a1,  a2, a3, a4, a5,
      a6,       a7, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, t3, t4, t5, t6};
  return kRegisters[num];
}

// -----------------------------------------------------------------------------
// Implementation of RelocInfo.

const int RelocInfo::kApplyMask =
    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE) |
    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE_ENCODED) |
    RelocInfo::ModeMask(RelocInfo::NEAR_BUILTIN_ENTRY) |
    RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);

bool RelocInfo::IsCodedSpecially() {
  // The deserializer needs to know whether a pointer is specially coded.  Being
  // specially coded on RISC-V means that it is a lui/addi instruction, and that
  // is always the case inside code objects.
  return true;
}

bool RelocInfo::IsInConstantPool() { return false; }

uint32_t RelocInfo::wasm_call_tag() const {
  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);
  return static_cast<uint32_t>(
      Assembler::target_address_at(pc_, constant_pool_));
}

// -----------------------------------------------------------------------------
// Implementation of Operand and MemOperand.
// See assembler-riscv-inl.h for inlined constructors.

Operand::Operand(Handle<HeapObject> handle)
    : rm_(no_reg), rmode_(RelocInfo::FULL_EMBEDDED_OBJECT) {
  value_.immediate = static_cast<intptr_t>(handle.address());
}

Operand Operand::EmbeddedNumber(double value) {
  int32_t smi;
  if (DoubleToSmiInteger(value, &smi)) return Operand(Smi::FromInt(smi));
  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
  result.is_heap_number_request_ = true;
  result.value_.heap_number_request = HeapNumberRequest(value);
  return result;
}

MemOperand::MemOperand(Register rm, int32_t offset) : Operand(rm) {
  offset_ = offset;
}

MemOperand::MemOperand(Register rm, int32_t unit, int32_t multiplier,
                       OffsetAddend offset_addend)
    : Operand(rm) {
  offset_ = unit * multiplier + offset_addend;
}

void Assembler::AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate) {
  DCHECK_IMPLIES(isolate == nullptr, heap_number_requests_.empty());
  for (auto& request : heap_number_requests_) {
    Handle<HeapObject> object =
        isolate->factory()->NewHeapNumber<AllocationType::kOld>(
            request.heap_number());
    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();
    set_target_value_at(pc, reinterpret_cast<uintptr_t>(object.location()));
  }
}

// -----------------------------------------------------------------------------
// Specific instructions, constants, and masks.

Assembler::Assembler(const AssemblerOptions& options,
                     std::unique_ptr<AssemblerBuffer> buffer)
    : AssemblerBase(options, std::move(buffer)),
      VU(this),
      scratch_register_list_({t3, t5}),
      constpool_(this) {
  reloc_info_writer.Reposition(buffer_start_ + buffer_->size(), pc_);

  last_trampoline_pool_end_ = 0;
  no_trampoline_pool_before_ = 0;
  trampoline_pool_blocked_nesting_ = 0;
  // We leave space (16 * kTrampolineSlotsSize)
  // for BlockTrampolinePoolScope buffer.
  next_buffer_check_ = v8_flags.force_long_branches
                           ? kMaxInt
                           : kMaxBranchOffset - kTrampolineSlotsSize * 16;
  internal_trampoline_exception_ = false;
  last_bound_pos_ = 0;

  trampoline_emitted_ = v8_flags.force_long_branches;
  unbound_labels_count_ = 0;
  block_buffer_growth_ = false;
}

void Assembler::AbortedCodeGeneration() { constpool_.Clear(); }
Assembler::~Assembler() { CHECK(constpool_.IsEmpty()); }

void Assembler::GetCode(Isolate* isolate, CodeDesc* desc) {
  GetCode(isolate->main_thread_local_isolate(), desc);
}
void Assembler::GetCode(LocalIsolate* isolate, CodeDesc* desc,
                        SafepointTableBuilder* safepoint_table_builder,
                        int handler_table_offset) {
  // As a crutch to avoid having to add manual Align calls wherever we use a
  // raw workflow to create InstructionStream objects (mostly in tests), add
  // another Align call here. It does no harm - the end of the InstructionStream
  // object is aligned to the (larger) kCodeAlignment anyways.
  // TODO(jgruber): Consider moving responsibility for proper alignment to
  // metadata table builders (safepoint, handler, constant pool, code
  // comments).
  DataAlign(InstructionStream::kMetadataAlignment);

  ForceConstantPoolEmissionWithoutJump();

  int code_comments_size = WriteCodeComments();

  DCHECK(pc_ <= reloc_info_writer.pos());  // No overlap.

  AllocateAndInstallRequestedHeapNumbers(isolate);

  // Set up code descriptor.
  // TODO(jgruber): Reconsider how these offsets and sizes are maintained up to
  // this point to make CodeDesc initialization less fiddly.

  static constexpr int kConstantPoolSize = 0;
  const int instruction_size = pc_offset();
  const int code_comments_offset = instruction_size - code_comments_size;
  const int constant_pool_offset = code_comments_offset - kConstantPoolSize;
  const int handler_table_offset2 = (handler_table_offset == kNoHandlerTable)
                                        ? constant_pool_offset
                                        : handler_table_offset;
  const int safepoint_table_offset =
      (safepoint_table_builder == kNoSafepointTable)
          ? handler_table_offset2
          : safepoint_table_builder->safepoint_table_offset();
  const int reloc_info_offset =
      static_cast<int>(reloc_info_writer.pos() - buffer_->start());
  CodeDesc::Initialize(desc, this, safepoint_table_offset,
                       handler_table_offset2, constant_pool_offset,
                       code_comments_offset, reloc_info_offset);
}

void Assembler::Align(int m) {
  DCHECK(m >= 4 && base::bits::IsPowerOfTwo(m));
  while ((pc_offset() & (m - 1)) != 0) {
    NOP();
  }
}

void Assembler::CodeTargetAlign() {
  // No advantage to aligning branch/call targets to more than
  // single instruction, that I am aware of.
  Align(4);
}

// Labels refer to positions in the (to be) generated code.
// There are bound, linked, and unused labels.
//
// Bound labels refer to known positions in the already
// generated code. pos() is the position the label refers to.
//
// Linked labels refer to unknown positions in the code
// to be generated; pos() is the position of the last
// instruction using the label.

// The link chain is terminated by a value in the instruction of 0,
// which is an otherwise illegal value (branch 0 is inf loop). When this case
// is detected, return an position of -1, an otherwise illegal position.
const int kEndOfChain = -1;
const int kEndOfJumpChain = 0;

int Assembler::target_at(int pos, bool is_internal) {
  if (is_internal) {
    uintptr_t* p = reinterpret_cast<uintptr_t*>(buffer_start_ + pos);
    uintptr_t address = *p;
    if (address == kEndOfJumpChain) {
      return kEndOfChain;
    } else {
      uintptr_t instr_address = reinterpret_cast<uintptr_t>(p);
      DCHECK(instr_address - address < INT_MAX);
      int delta = static_cast<int>(instr_address - address);
      DCHECK(pos > delta);
      return pos - delta;
    }
  }
  Instruction* instruction = Instruction::At(buffer_start_ + pos);
  DEBUG_PRINTF("target_at: %p (%d)\n\t",
               reinterpret_cast<Instr*>(buffer_start_ + pos), pos);
  Instr instr = instruction->InstructionBits();
  disassembleInstr(buffer_start_ + pos);

  switch (instruction->InstructionOpcodeType()) {
    case BRANCH: {
      int32_t imm13 = BranchOffset(instr);
      if (imm13 == kEndOfJumpChain) {
        // EndOfChain sentinel is returned directly, not relative to pc or pos.
        return kEndOfChain;
      } else {
        return pos + imm13;
      }
    }
    case JAL: {
      int32_t imm21 = JumpOffset(instr);
      if (imm21 == kEndOfJumpChain) {
        // EndOfChain sentinel is returned directly, not relative to pc or pos.
        return kEndOfChain;
      } else {
        return pos + imm21;
      }
    }
    case JALR: {
      int32_t imm12 = instr >> 20;
      if (imm12 == kEndOfJumpChain) {
        // EndOfChain sentinel is returned directly, not relative to pc or pos.
        return kEndOfChain;
      } else {
        return pos + imm12;
      }
    }
    case LUI: {
      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
      pc = target_address_at(pc);
      uintptr_t instr_address =
          reinterpret_cast<uintptr_t>(buffer_start_ + pos);
      uintptr_t imm = reinterpret_cast<uintptr_t>(pc);
      if (imm == kEndOfJumpChain) {
        return kEndOfChain;
      } else {
        DCHECK(instr_address - imm < INT_MAX);
        int32_t delta = static_cast<int32_t>(instr_address - imm);
        DCHECK(pos > delta);
        return pos - delta;
      }
    }
    case AUIPC: {
      Instr instr_auipc = instr;
      Instr instr_I = instr_at(pos + 4);
      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));
      int32_t offset = BrachlongOffset(instr_auipc, instr_I);
      if (offset == kEndOfJumpChain) return kEndOfChain;
      return offset + pos;
    }
    case RO_C_J: {
      int32_t offset = instruction->RvcImm11CJValue();
      if (offset == kEndOfJumpChain) return kEndOfChain;
      return offset + pos;
    }
    case RO_C_BNEZ:
    case RO_C_BEQZ: {
      int32_t offset = instruction->RvcImm8BValue();
      if (offset == kEndOfJumpChain) return kEndOfChain;
      return pos + offset;
    }
    default: {
      if (instr == kEndOfJumpChain) {
        return kEndOfChain;
      } else {
        int32_t imm18 =
            ((instr & static_cast<int32_t>(kImm16Mask)) << 16) >> 14;
        return (imm18 + pos);
      }
    }
  }
}

static inline Instr SetBranchOffset(int32_t pos, int32_t target_pos,
                                    Instr instr) {
  int32_t imm = target_pos - pos;
  DCHECK_EQ(imm & 1, 0);
  DCHECK(is_intn(imm, Assembler::kBranchOffsetBits));

  instr &= ~kBImm12Mask;
  int32_t imm12 = ((imm & 0x800) >> 4) |   // bit  11
                  ((imm & 0x1e) << 7) |    // bits 4-1
                  ((imm & 0x7e0) << 20) |  // bits 10-5
                  ((imm & 0x1000) << 19);  // bit 12

  return instr | (imm12 & kBImm12Mask);
}

static inline Instr SetLoadOffset(int32_t offset, Instr instr) {
#if V8_TARGET_ARCH_RISCV64
  DCHECK(Assembler::IsLd(instr));
#elif V8_TARGET_ARCH_RISCV32
  DCHECK(Assembler::IsLw(instr));
#endif
  DCHECK(is_int12(offset));
  instr &= ~kImm12Mask;
  int32_t imm12 = offset << kImm12Shift;
  return instr | (imm12 & kImm12Mask);
}

static inline Instr SetAuipcOffset(int32_t offset, Instr instr) {
  DCHECK(Assembler::IsAuipc(instr));
  DCHECK(is_int20(offset));
  instr = (instr & ~kImm31_12Mask) | ((offset & kImm19_0Mask) << 12);
  return instr;
}

static inline Instr SetJalrOffset(int32_t offset, Instr instr) {
  DCHECK(Assembler::IsJalr(instr));
  DCHECK(is_int12(offset));
  instr &= ~kImm12Mask;
  int32_t imm12 = offset << kImm12Shift;
  DCHECK(Assembler::IsJalr(instr | (imm12 & kImm12Mask)));
  DCHECK_EQ(Assembler::JalrOffset(instr | (imm12 & kImm12Mask)), offset);
  return instr | (imm12 & kImm12Mask);
}

static inline Instr SetJalOffset(int32_t pos, int32_t target_pos, Instr instr) {
  DCHECK(Assembler::IsJal(instr));
  int32_t imm = target_pos - pos;
  DCHECK_EQ(imm & 1, 0);
  DCHECK(is_intn(imm, Assembler::kJumpOffsetBits));

  instr &= ~kImm20Mask;
  int32_t imm20 = (imm & 0xff000) |          // bits 19-12
                  ((imm & 0x800) << 9) |     // bit  11
                  ((imm & 0x7fe) << 20) |    // bits 10-1
                  ((imm & 0x100000) << 11);  // bit  20

  return instr | (imm20 & kImm20Mask);
}

static inline ShortInstr SetCJalOffset(int32_t pos, int32_t target_pos,
                                       Instr instr) {
  DCHECK(Assembler::IsCJal(instr));
  int32_t imm = target_pos - pos;
  DCHECK_EQ(imm & 1, 0);
  DCHECK(is_intn(imm, Assembler::kCJalOffsetBits));
  instr &= ~kImm11Mask;
  int16_t imm11 = ((imm & 0x800) >> 1) | ((imm & 0x400) >> 4) |
                  ((imm & 0x300) >> 1) | ((imm & 0x80) >> 3) |
                  ((imm & 0x40) >> 1) | ((imm & 0x20) >> 5) |
                  ((imm & 0x10) << 5) | (imm & 0xe);
  imm11 = imm11 << kImm11Shift;
  DCHECK(Assembler::IsCJal(instr | (imm11 & kImm11Mask)));
  return instr | (imm11 & kImm11Mask);
}
static inline Instr SetCBranchOffset(int32_t pos, int32_t target_pos,
                                     Instr instr) {
  DCHECK(Assembler::IsCBranch(instr));
  int32_t imm = target_pos - pos;
  DCHECK_EQ(imm & 1, 0);
  DCHECK(is_intn(imm, Assembler::kCBranchOffsetBits));

  instr &= ~kRvcBImm8Mask;
  int32_t imm8 = ((imm & 0x20) >> 5) | ((imm & 0x6)) | ((imm & 0xc0) >> 3) |
                 ((imm & 0x18) << 2) | ((imm & 0x100) >> 1);
  imm8 = ((imm8 & 0x1f) << 2) | ((imm8 & 0xe0) << 5);
  DCHECK(Assembler::IsCBranch(instr | imm8 & kRvcBImm8Mask));

  return instr | (imm8 & kRvcBImm8Mask);
}

// We have to use a temporary register for things that can be relocated even
// if they can be encoded in RISC-V's 12 bits of immediate-offset instruction
// space.  There is no guarantee that the relocated location can be similarly
// encoded.
bool Assembler::MustUseReg(RelocInfo::Mode rmode) {
  return !RelocInfo::IsNoInfo(rmode);
}

void Assembler::disassembleInstr(uint8_t* pc) {
  if (!v8_flags.riscv_debug) return;
  disasm::NameConverter converter;
  disasm::Disassembler disasm(converter);
  base::EmbeddedVector<char, 128> disasm_buffer;

  disasm.InstructionDecode(disasm_buffer, pc);
  DEBUG_PRINTF("%s\n", disasm_buffer.begin());
}

void Assembler::target_at_put(int pos, int target_pos, bool is_internal,
                              bool trampoline) {
  if (is_internal) {
    uintptr_t imm = reinterpret_cast<uintptr_t>(buffer_start_) + target_pos;
    *reinterpret_cast<uintptr_t*>(buffer_start_ + pos) = imm;
    return;
  }
  DEBUG_PRINTF("\ttarget_at_put: %p (%d) to %p (%d)\n",
               reinterpret_cast<Instr*>(buffer_start_ + pos), pos,
               reinterpret_cast<Instr*>(buffer_start_ + target_pos),
               target_pos);
  Instruction* instruction = Instruction::At(buffer_start_ + pos);
  Instr instr = instruction->InstructionBits();

  switch (instruction->InstructionOpcodeType()) {
    case BRANCH: {
      instr = SetBranchOffset(pos, target_pos, instr);
      instr_at_put(pos, instr);
    } break;
    case JAL: {
      DCHECK(IsJal(instr));
      instr = SetJalOffset(pos, target_pos, instr);
      instr_at_put(pos, instr);
    } break;
    case LUI: {
      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
      set_target_value_at(
          pc, reinterpret_cast<uintptr_t>(buffer_start_ + target_pos));
    } break;
    case AUIPC: {
      Instr instr_auipc = instr;
      Instr instr_I = instr_at(pos + 4);
      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));

      intptr_t offset = target_pos - pos;
      if (is_int21(offset) && IsJalr(instr_I) && trampoline) {
        DCHECK(is_int21(offset) && ((offset & 1) == 0));
        Instr instr = JAL;
        instr = SetJalOffset(pos, target_pos, instr);
        DCHECK(IsJal(instr));
        DCHECK(JumpOffset(instr) == offset);
        instr_at_put(pos, instr);
        instr_at_put(pos + 4, kNopByte);
      } else {
        CHECK(is_int32(offset + 0x800));

        int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
        int32_t Lo12 = (int32_t)offset << 20 >> 20;

        instr_auipc =
            (instr_auipc & ~kImm31_12Mask) | ((Hi20 & kImm19_0Mask) << 12);
        instr_at_put(pos, instr_auipc);

        const int kImm31_20Mask = ((1 << 12) - 1) << 20;
        const int kImm11_0Mask = ((1 << 12) - 1);
        instr_I = (instr_I & ~kImm31_20Mask) | ((Lo12 & kImm11_0Mask) << 20);
        instr_at_put(pos + 4, instr_I);
      }
    } break;
    case RO_C_J: {
      ShortInstr short_instr = SetCJalOffset(pos, target_pos, instr);
      instr_at_put(pos, short_instr);
    } break;
    case RO_C_BNEZ:
    case RO_C_BEQZ: {
      instr = SetCBranchOffset(pos, target_pos, instr);
      instr_at_put(pos, instr);
    } break;
    default: {
      // Emitted label constant, not part of a branch.
      // Make label relative to Code pointer of generated InstructionStream
      // object.
      instr_at_put(
          pos, target_pos + (InstructionStream::kHeaderSize - kHeapObjectTag));
    } break;
  }

  disassembleInstr(buffer_start_ + pos);
  if (instruction->InstructionOpcodeType() == AUIPC) {
    disassembleInstr(buffer_start_ + pos + 4);
  }
}

void Assembler::print(const Label* L) {
  if (L->is_unused()) {
    PrintF("unused label\n");
  } else if (L->is_bound()) {
    PrintF("bound label to %d\n", L->pos());
  } else if (L->is_linked()) {
    Label l;
    l.link_to(L->pos());
    PrintF("unbound label");
    while (l.is_linked()) {
      PrintF("@ %d ", l.pos());
      Instr instr = instr_at(l.pos());
      if ((instr & ~kImm16Mask) == 0) {
        PrintF("value\n");
      } else {
        PrintF("%d\n", instr);
      }
      next(&l, is_internal_reference(&l));
    }
  } else {
    PrintF("label in inconsistent state (pos = %d)\n", L->pos_);
  }
}

void Assembler::bind_to(Label* L, int pos) {
  DCHECK(0 <= pos && pos <= pc_offset());  // Must have valid binding position.
  DEBUG_PRINTF("\tbinding %d to label %p\n", pos, L);
  int trampoline_pos = kInvalidSlotPos;
  bool is_internal = false;
  if (L->is_linked() && !trampoline_emitted_) {
    unbound_labels_count_--;
    if (!is_internal_reference(L)) {
      next_buffer_check_ += kTrampolineSlotsSize;
    }
  }

  while (L->is_linked()) {
    int fixup_pos = L->pos();
    int dist = pos - fixup_pos;
    is_internal = is_internal_reference(L);
    next(L, is_internal);  // Call next before overwriting link with target
                           // at fixup_pos.
    Instr instr = instr_at(fixup_pos);
    DEBUG_PRINTF("\tfixup: %d to %d\n", fixup_pos, dist);
    if (is_internal) {
      target_at_put(fixup_pos, pos, is_internal);
    } else {
      if (IsBranch(instr)) {
        if (dist > kMaxBranchOffset) {
          if (trampoline_pos == kInvalidSlotPos) {
            trampoline_pos = get_trampoline_entry(fixup_pos);
            CHECK_NE(trampoline_pos, kInvalidSlotPos);
          }
          CHECK((trampoline_pos - fixup_pos) <= kMaxBranchOffset);
          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
          target_at_put(fixup_pos, trampoline_pos, false, true);
          fixup_pos = trampoline_pos;
        }
        target_at_put(fixup_pos, pos, false);
      } else if (IsJal(instr)) {
        if (dist > kMaxJumpOffset) {
          if (trampoline_pos == kInvalidSlotPos) {
            trampoline_pos = get_trampoline_entry(fixup_pos);
            CHECK_NE(trampoline_pos, kInvalidSlotPos);
          }
          CHECK((trampoline_pos - fixup_pos) <= kMaxJumpOffset);
          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
          target_at_put(fixup_pos, trampoline_pos, false, true);
          fixup_pos = trampoline_pos;
        }
        target_at_put(fixup_pos, pos, false);
      } else {
        target_at_put(fixup_pos, pos, false);
      }
    }
  }
  L->bind_to(pos);

  // Keep track of the last bound label so we don't eliminate any instructions
  // before a bound label.
  if (pos > last_bound_pos_) last_bound_pos_ = pos;
}

void Assembler::bind(Label* L) {
  DCHECK(!L->is_bound());  // Label can only be bound once.
  bind_to(L, pc_offset());
}

void Assembler::next(Label* L, bool is_internal) {
  DCHECK(L->is_linked());
  int link = target_at(L->pos(), is_internal);
  if (link == kEndOfChain) {
    L->Unuse();
  } else {
    DCHECK_GE(link, 0);
    DEBUG_PRINTF("\tnext: %p to %p (%d)\n", L,
                 reinterpret_cast<Instr*>(buffer_start_ + link), link);
    L->link_to(link);
  }
}

bool Assembler::is_near(Label* L) {
  DCHECK(L->is_bound());
  return is_intn((pc_offset() - L->pos()), kJumpOffsetBits);
}

bool Assembler::is_near(Label* L, OffsetSize bits) {
  if (L == nullptr || !L->is_bound()) return true;
  return is_intn((pc_offset() - L->pos()), bits);
}

bool Assembler::is_near_branch(Label* L) {
  DCHECK(L->is_bound());
  return is_intn((pc_offset() - L->pos()), kBranchOffsetBits);
}

int Assembler::BranchOffset(Instr instr) {
  // | imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1|11] | opcode |
  //  31          25                      11          7
  int32_t imm13 = ((instr & 0xf00) >> 7) | ((instr & 0x7e000000) >> 20) |
                  ((instr & 0x80) << 4) | ((instr & 0x80000000) >> 19);
  imm13 = imm13 << 19 >> 19;
  return imm13;
}

int Assembler::BrachlongOffset(Instr auipc, Instr instr_I) {
  DCHECK(reinterpret_cast<Instruction*>(&instr_I)->InstructionType() ==
         InstructionBase::kIType);
  DCHECK(IsAuipc(auipc));
  DCHECK_EQ((auipc & kRdFieldMask) >> kRdShift,
            (instr_I & kRs1FieldMask) >> kRs1Shift);
  int32_t imm_auipc = AuipcOffset(auipc);
  int32_t imm12 = static_cast<int32_t>(instr_I & kImm12Mask) >> 20;
  int32_t offset = imm12 + imm_auipc;
  return offset;
}

int Assembler::PatchBranchlongOffset(Address pc, Instr instr_auipc,
                                     Instr instr_jalr, int32_t offset) {
  DCHECK(IsAuipc(instr_auipc));
  DCHECK(IsJalr(instr_jalr));
  CHECK(is_int32(offset + 0x800));
  int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
  int32_t Lo12 = (int32_t)offset << 20 >> 20;
  instr_at_put(pc, SetAuipcOffset(Hi20, instr_auipc));
  instr_at_put(pc + 4, SetJalrOffset(Lo12, instr_jalr));
  DCHECK(offset ==
         BrachlongOffset(Assembler::instr_at(pc), Assembler::instr_at(pc + 4)));
  return 2;
}

// Returns the next free trampoline entry.
int32_t Assembler::get_trampoline_entry(int32_t pos) {
  int32_t trampoline_entry = kInvalidSlotPos;
  if (!internal_trampoline_exception_) {
    DEBUG_PRINTF("\tstart: %d,pos: %d\n", trampoline_.start(), pos);
    if (trampoline_.start() > pos) {
      trampoline_entry = trampoline_.take_slot();
    }

    if (kInvalidSlotPos == trampoline_entry) {
      internal_trampoline_exception_ = true;
    }
  }
  return trampoline_entry;
}

uintptr_t Assembler::jump_address(Label* L) {
  intptr_t target_pos;
  DEBUG_PRINTF("\tjump_address: %p to %p (%d)\n", L,
               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
               pc_offset());
  if (L->is_bound()) {
    target_pos = L->pos();
  } else {
    if (L->is_linked()) {
      target_pos = L->pos();  // L's link.
      L->link_to(pc_offset());
    } else {
      L->link_to(pc_offset());
      if (!trampoline_emitted_) {
        unbound_labels_count_++;
        next_buffer_check_ -= kTrampolineSlotsSize;
      }
      DEBUG_PRINTF("\tstarted link\n");
      return kEndOfJumpChain;
    }
  }
  uintptr_t imm = reinterpret_cast<uintptr_t>(buffer_start_) + target_pos;
  if (v8_flags.riscv_c_extension)
    DCHECK_EQ(imm & 1, 0);
  else
    DCHECK_EQ(imm & 3, 0);

  return imm;
}

int32_t Assembler::branch_long_offset(Label* L) {
  intptr_t target_pos;

  DEBUG_PRINTF("\tbranch_long_offset: %p to %p (%d)\n", L,
               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
               pc_offset());
  if (L->is_bound()) {
    target_pos = L->pos();
  } else {
    if (L->is_linked()) {
      target_pos = L->pos();  // L's link.
      L->link_to(pc_offset());
    } else {
      L->link_to(pc_offset());
      if (!trampoline_emitted_) {
        unbound_labels_count_++;
        next_buffer_check_ -= kTrampolineSlotsSize;
      }
      DEBUG_PRINTF("\tstarted link\n");
      return kEndOfJumpChain;
    }
  }
  intptr_t offset = target_pos - pc_offset();
  if (v8_flags.riscv_c_extension)
    DCHECK_EQ(offset & 1, 0);
  else
    DCHECK_EQ(offset & 3, 0);
  DCHECK(is_int32(offset));
  VU.clear();
  return static_cast<int32_t>(offset);
}

int32_t Assembler::branch_offset_helper(Label* L, OffsetSize bits) {
  int32_t target_pos;

  DEBUG_PRINTF("\tbranch_offset_helper: %p to %p (%d)\n", L,
               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
               pc_offset());
  if (L->is_bound()) {
    target_pos = L->pos();
    DEBUG_PRINTF("\tbound: %d", target_pos);
  } else {
    if (L->is_linked()) {
      target_pos = L->pos();
      L->link_to(pc_offset());
      DEBUG_PRINTF("\tadded to link: %d\n", target_pos);
    } else {
      L->link_to(pc_offset());
      if (!trampoline_emitted_) {
        unbound_labels_count_++;
        next_buffer_check_ -= kTrampolineSlotsSize;
      }
      DEBUG_PRINTF("\tstarted link\n");
      return kEndOfJumpChain;
    }
  }

  int32_t offset = target_pos - pc_offset();
  DCHECK(is_intn(offset, bits));
  DCHECK_EQ(offset & 1, 0);
  DEBUG_PRINTF("\toffset = %d\n", offset);
  VU.clear();
  return offset;
}

void Assembler::label_at_put(Label* L, int at_offset) {
  int target_pos;
  DEBUG_PRINTF("\tlabel_at_put: %p @ %p (%d)\n", L,
               reinterpret_cast<Instr*>(buffer_start_ + at_offset), at_offset);
  if (L->is_bound()) {
    target_pos = L->pos();
    instr_at_put(at_offset, target_pos + (InstructionStream::kHeaderSize -
                                          kHeapObjectTag));
  } else {
    if (L->is_linked()) {
      target_pos = L->pos();  // L's link.
      int32_t imm18 = target_pos - at_offset;
      DCHECK_EQ(imm18 & 3, 0);
      int32_t imm16 = imm18 >> 2;
      DCHECK(is_int16(imm16));
      instr_at_put(at_offset, (int32_t)(imm16 & kImm16Mask));
    } else {
      target_pos = kEndOfJumpChain;
      instr_at_put(at_offset, target_pos);
      if (!trampoline_emitted_) {
        unbound_labels_count_++;
        next_buffer_check_ -= kTrampolineSlotsSize;
      }
    }
    L->link_to(at_offset);
  }
}

//===----------------------------------------------------------------------===//
// Instructions
//===----------------------------------------------------------------------===//

// Definitions for using compressed vs non compressed

void Assembler::NOP() {
  if (v8_flags.riscv_c_extension)
    c_nop();
  else
    nop();
}

void Assembler::EBREAK() {
  if (v8_flags.riscv_c_extension)
    c_ebreak();
  else
    ebreak();
}

// Assembler Pseudo Instructions (Tables 25.2 and 25.3, RISC-V Unprivileged ISA)

void Assembler::nop() { addi(ToRegister(0), ToRegister(0), 0); }

inline int64_t signExtend(uint64_t V, int N) {
  return int64_t(V << (64 - N)) >> (64 - N);
}

#if V8_TARGET_ARCH_RISCV64
void Assembler::RV_li(Register rd, int64_t imm) {
  UseScratchRegisterScope temps(this);
  if (RecursiveLiCount(imm) > GeneralLiCount(imm, temps.hasAvailable())) {
    GeneralLi(rd, imm);
  } else {
    RecursiveLi(rd, imm);
  }
}

int Assembler::RV_li_count(int64_t imm, bool is_get_temp_reg) {
  if (RecursiveLiCount(imm) > GeneralLiCount(imm, is_get_temp_reg)) {
    return GeneralLiCount(imm, is_get_temp_reg);
  } else {
    return RecursiveLiCount(imm);
  }
}

void Assembler::GeneralLi(Register rd, int64_t imm) {
  // 64-bit imm is put in the register rd.
  // In most cases the imm is 32 bit and 2 instructions are generated. If a
  // temporary register is available, in the worst case, 6 instructions are
  // generated for a full 64-bit immediate. If temporay register is not
  // available the maximum will be 8 instructions. If imm is more than 32 bits
  // and a temp register is available, imm is divided into two 32-bit parts,
  // low_32 and up_32. Each part is built in a separate register. low_32 is
  // built before up_32. If low_32 is negative (upper 32 bits are 1), 0xffffffff
  // is subtracted from up_32 before up_32 is built. This compensates for 32
  // bits of 1's in the lower when the two registers are added. If no temp is
  // available, the upper 32 bit is built in rd, and the lower 32 bits are
  // devided to 3 parts (11, 11, and 10 bits). The parts are shifted and added
  // to the upper part built in rd.
  if (is_int32(imm + 0x800)) {
    // 32-bit case. Maximum of 2 instructions generated
    int64_t high_20 = ((imm + 0x800) >> 12);
    int64_t low_12 = imm << 52 >> 52;
    if (high_20) {
      lui(rd, (int32_t)high_20);
      if (low_12) {
        addi(rd, rd, low_12);
      }
    } else {
      addi(rd, zero_reg, low_12);
    }
    return;
  } else {
    UseScratchRegisterScope temps(this);
    // 64-bit case: divide imm into two 32-bit parts, upper and lower
    int64_t up_32 = imm >> 32;
    int64_t low_32 = imm & 0xffffffffull;
    Register temp_reg = rd;
    // Check if a temporary register is available
    if (up_32 == 0 || low_32 == 0) {
      // No temp register is needed
    } else {
      BlockTrampolinePoolScope block_trampoline_pool(this);
      temp_reg = temps.hasAvailable() ? temps.Acquire() : no_reg;
    }
    if (temp_reg != no_reg) {
      // keep track of hardware behavior for lower part in sim_low
      int64_t sim_low = 0;
      // Build lower part
      if (low_32 != 0) {
        int64_t high_20 = ((low_32 + 0x800) >> 12);
        int64_t low_12 = low_32 & 0xfff;
        if (high_20) {
          // Adjust to 20 bits for the case of overflow
          high_20 &= 0xfffff;
          sim_low = ((high_20 << 12) << 32) >> 32;
          lui(rd, (int32_t)high_20);
          if (low_12) {
            sim_low += (low_12 << 52 >> 52) | low_12;
            addi(rd, rd, low_12);
          }
        } else {
          sim_low = low_12;
          ori(rd, zero_reg, low_12);
        }
      }
      if (sim_low & 0x100000000) {
        // Bit 31 is 1. Either an overflow or a negative 64 bit
        if (up_32 == 0) {
          // Positive number, but overflow because of the add 0x800
          slli(rd, rd, 32);
          srli(rd, rd, 32);
          return;
        }
        // low_32 is a negative 64 bit after the build
        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
      }
      if (up_32 == 0) {
        return;
      }
      // Build upper part in a temporary register
      if (low_32 == 0) {
        // Build upper part in rd
        temp_reg = rd;
      }
      int64_t high_20 = (up_32 + 0x800) >> 12;
      int64_t low_12 = up_32 & 0xfff;
      if (high_20) {
        // Adjust to 20 bits for the case of overflow
        high_20 &= 0xfffff;
        lui(temp_reg, (int32_t)high_20);
        if (low_12) {
          addi(temp_reg, temp_reg, low_12);
        }
      } else {
        ori(temp_reg, zero_reg, low_12);
      }
      // Put it at the bgining of register
      slli(temp_reg, temp_reg, 32);
      if (low_32 != 0) {
        add(rd, rd, temp_reg);
      }
      return;
    }
    // No temp register. Build imm in rd.
    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
    // parts to the upper part by doing shift and add.
    // First build upper part in rd.
    int64_t high_20 = (up_32 + 0x800) >> 12;
    int64_t low_12 = up_32 & 0xfff;
    if (high_20) {
      // Adjust to 20 bits for the case of overflow
      high_20 &= 0xfffff;
      lui(rd, (int32_t)high_20);
      if (low_12) {
        addi(rd, rd, low_12);
      }
    } else {
      ori(rd, zero_reg, low_12);
    }
    // upper part already in rd. Each part to be added to rd, has maximum of 11
    // bits, and always starts with a 1. rd is shifted by the size of the part
    // plus the number of zeros between the parts. Each part is added after the
    // left shift.
    uint32_t mask = 0x80000000;
    int32_t shift_val = 0;
    int32_t i;
    for (i = 0; i < 32; i++) {
      if ((low_32 & mask) == 0) {
        mask >>= 1;
        shift_val++;
        if (i == 31) {
          // rest is zero
          slli(rd, rd, shift_val);
        }
        continue;
      }
      // The first 1 seen
      int32_t part;
      if ((i + 11) < 32) {
        // Pick 11 bits
        part = ((uint32_t)(low_32 << i) >> i) >> (32 - (i + 11));
        slli(rd, rd, shift_val + 11);
        ori(rd, rd, part);
        i += 10;
        mask >>= 11;
      } else {
        part = (uint32_t)(low_32 << i) >> i;
        slli(rd, rd, shift_val + (32 - i));
        ori(rd, rd, part);
        break;
      }
      shift_val = 0;
    }
  }
}

void Assembler::li_ptr(Register rd, int64_t imm) {
#ifdef RISCV_USE_SV39
  // Initialize rd with an address
  // Pointers are 39 bits
  // 4 fixed instructions are generated
  DCHECK_EQ((imm & 0xffffff8000000000ll), 0);
  int64_t a8 = imm & 0xff;                      // bits 0:7. 8 bits
  int64_t high_31 = (imm >> 8) & 0x7fffffff;    // 31 bits
  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
  int64_t low_12 = high_31 & 0xfff;             // 12 bits
  lui(rd, (int32_t)high_20);
  addi(rd, rd, low_12);  // 31 bits in rd.
  slli(rd, rd, 8);       // Space for next 8 bis
  ori(rd, rd, a8);       // 8 bits are put in.
#else
  // Initialize rd with an address
  // Pointers are 48 bits
  // 6 fixed instructions are generated
  DCHECK_EQ((imm & 0xfff0000000000000ll), 0);
  int64_t a6 = imm & 0x3f;                      // bits 0:5. 6 bits
  int64_t b11 = (imm >> 6) & 0x7ff;             // bits 6:11. 11 bits
  int64_t high_31 = (imm >> 17) & 0x7fffffff;   // 31 bits
  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
  int64_t low_12 = high_31 & 0xfff;             // 12 bits
  lui(rd, (int32_t)high_20);
  addi(rd, rd, low_12);  // 31 bits in rd.
  slli(rd, rd, 11);      // Space for next 11 bis
  ori(rd, rd, b11);      // 11 bits are put in. 42 bit in rd
  slli(rd, rd, 6);       // Space for next 6 bits
  ori(rd, rd, a6);       // 6 bits are put in. 48 bis in rd
#endif
}

void Assembler::li_constant(Register rd, int64_t imm) {
  DEBUG_PRINTF("\tli_constant(%d, %" PRIx64 " <%" PRId64 ">)\n", ToNumber(rd),
               imm, imm);
  lui(rd, (imm + (1LL << 47) + (1LL << 35) + (1LL << 23) + (1LL << 11)) >>
              48);  // Bits 63:48
  addiw(rd, rd,
        (imm + (1LL << 35) + (1LL << 23) + (1LL << 11)) << 16 >>
            52);  // Bits 47:36
  slli(rd, rd, 12);
  addi(rd, rd, (imm + (1LL << 23) + (1LL << 11)) << 28 >> 52);  // Bits 35:24
  slli(rd, rd, 12);
  addi(rd, rd, (imm + (1LL << 11)) << 40 >> 52);  // Bits 23:12
  slli(rd, rd, 12);
  addi(rd, rd, imm << 52 >> 52);  // Bits 11:0
}

void Assembler::li_constant32(Register rd, int32_t imm) {
  ASM_CODE_COMMENT(this);
  DEBUG_PRINTF("\tli_constant(%d, %x <%d>)\n", ToNumber(rd), imm, imm);
  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
  int32_t low_12 = imm & 0xfff;             // bits11:0
  lui(rd, high_20);
  addi(rd, rd, low_12);
}

#elif V8_TARGET_ARCH_RISCV32
void Assembler::RV_li(Register rd, int32_t imm) {
  int32_t high_20 = ((imm + 0x800) >> 12);
  int32_t low_12 = imm & 0xfff;
  if (high_20) {
    lui(rd, high_20);
    if (low_12) {
      addi(rd, rd, low_12);
    }
  } else {
    addi(rd, zero_reg, low_12);
  }
}

int Assembler::RV_li_count(int32_t imm, bool is_get_temp_reg) {
  int count = 0;
  // imitate Assembler::RV_li
  int32_t high_20 = ((imm + 0x800) >> 12);
  int32_t low_12 = imm & 0xfff;
  if (high_20) {
    count++;
    if (low_12) {
      count++;
    }
  } else {
    // if high_20 is 0, always need one instruction to load the low_12 bit
    count++;
  }

  return count;
}

void Assembler::li_ptr(Register rd, int32_t imm) {
  // Initialize rd with an address
  // Pointers are 32 bits
  // 2 fixed instructions are generated
  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
  int32_t low_12 = imm & 0xfff;             // bits11:0
  lui(rd, high_20);
  addi(rd, rd, low_12);
}

void Assembler::li_constant(Register rd, int32_t imm) {
  ASM_CODE_COMMENT(this);
  DEBUG_PRINTF("\tli_constant(%d, %x <%d>)\n", ToNumber(rd), imm, imm);
  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
  int32_t low_12 = imm & 0xfff;             // bits11:0
  lui(rd, high_20);
  addi(rd, rd, low_12);
}
#endif

// Break / Trap instructions.
void Assembler::break_(uint32_t code, bool break_as_stop) {
  // We need to invalidate breaks that could be stops as well because the
  // simulator expects a char pointer after the stop instruction.
  // See constants-mips.h for explanation.
  DCHECK(
      (break_as_stop && code <= kMaxStopCode && code > kMaxTracepointCode) ||
      (!break_as_stop && (code > kMaxStopCode || code <= kMaxTracepointCode)));

  // since ebreak does not allow additional immediate field, we use the
  // immediate field of lui instruction immediately following the ebreak to
  // encode the "code" info
  ebreak();
  DCHECK(is_uint20(code));
  lui(zero_reg, code);
}

void Assembler::stop(uint32_t code) {
  DCHECK_GT(code, kMaxWatchpointCode);
  DCHECK_LE(code, kMaxStopCode);
#if defined(V8_HOST_ARCH_RISCV64) || defined(V8_HOST_ARCH_RISCV32)
  break_(0x54321);
#else  // V8_HOST_ARCH_RISCV64 || V8_HOST_ARCH_RISCV32
  break_(code, true);
#endif
}

// Original MIPS Instructions

// ------------Memory-instructions-------------

bool Assembler::NeedAdjustBaseAndOffset(const MemOperand& src,
                                        OffsetAccessType access_type,
                                        int second_access_add_to_offset) {
  bool two_accesses = static_cast<bool>(access_type);
  DCHECK_LE(second_access_add_to_offset, 7);  // Must be <= 7.

  // is_int12 must be passed a signed value, hence the static cast below.
  if (is_int12(src.offset()) &&
      (!two_accesses || is_int12(static_cast<int32_t>(
                            src.offset() + second_access_add_to_offset)))) {
    // Nothing to do: 'offset' (and, if needed, 'offset + 4', or other specified
    // value) fits into int12.
    return false;
  }
  return true;
}

void Assembler::AdjustBaseAndOffset(MemOperand* src, Register scratch,
                                    OffsetAccessType access_type,
                                    int second_Access_add_to_offset) {
  // This method is used to adjust the base register and offset pair
  // for a load/store when the offset doesn't fit into int12.

  // Must not overwrite the register 'base' while loading 'offset'.
  constexpr int32_t kMinOffsetForSimpleAdjustment = 0x7F8;
  constexpr int32_t kMaxOffsetForSimpleAdjustment =
      2 * kMinOffsetForSimpleAdjustment;
  if (0 <= src->offset() && src->offset() <= kMaxOffsetForSimpleAdjustment) {
    addi(scratch, src->rm(), kMinOffsetForSimpleAdjustment);
    src->offset_ -= kMinOffsetForSimpleAdjustment;
  } else if (-kMaxOffsetForSimpleAdjustment <= src->offset() &&
             src->offset() < 0) {
    addi(scratch, src->rm(), -kMinOffsetForSimpleAdjustment);
    src->offset_ += kMinOffsetForSimpleAdjustment;
  } else if (access_type == OffsetAccessType::SINGLE_ACCESS) {
    RV_li(scratch, (static_cast<intptr_t>(src->offset()) + 0x800) >> 12 << 12);
    add(scratch, scratch, src->rm());
    src->offset_ = src->offset() << 20 >> 20;
  } else {
    RV_li(scratch, src->offset());
    add(scratch, scratch, src->rm());
    src->offset_ = 0;
  }
  src->rm_ = scratch;
}

int Assembler::RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
                                         intptr_t pc_delta) {
  if (RelocInfo::IsInternalReference(rmode)) {
    intptr_t* p = reinterpret_cast<intptr_t*>(pc);
    if (*p == kEndOfJumpChain) {
      return 0;  // Number of instructions patched.
    }
    *p += pc_delta;
    return 2;  // Number of instructions patched.
  }
  Instr instr = instr_at(pc);
  DCHECK(RelocInfo::IsInternalReferenceEncoded(rmode));
  if (IsLui(instr)) {
    uintptr_t target_address = target_address_at(pc) + pc_delta;
    DEBUG_PRINTF("\ttarget_address 0x%" PRIxPTR "\n", target_address);
    set_target_value_at(pc, target_address);
#if V8_TARGET_ARCH_RISCV64
#ifdef RISCV_USE_SV39
    return 6;  // Number of instructions patched.
#else
    return 8;  // Number of instructions patched.
#endif
#elif V8_TARGET_ARCH_RISCV32
    return 2;  // Number of instructions patched.
#endif
  } else {
    UNIMPLEMENTED();
  }
}

void Assembler::RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
                                          intptr_t pc_delta) {
  Instr instr = instr_at(pc);
  Instr instr1 = instr_at(pc + 1 * kInstrSize);
  DCHECK(RelocInfo::IsRelativeCodeTarget(rmode));
  if (IsAuipc(instr) && IsJalr(instr1)) {
    int32_t imm;
    imm = BrachlongOffset(instr, instr1);
    imm -= pc_delta;
    PatchBranchlongOffset(pc, instr, instr1, imm);
    return;
  } else {
    UNREACHABLE();
  }
}

void Assembler::GrowBuffer() {
  DEBUG_PRINTF("GrowBuffer: %p -> ", buffer_start_);
  // Compute new buffer size.
  int old_size = buffer_->size();
  int new_size = std::min(2 * old_size, old_size + 1 * MB);

  // Some internal data structures overflow for very large buffers,
  // they must ensure that kMaximalBufferSize is not too large.
  if (new_size > kMaximalBufferSize) {
    V8::FatalProcessOutOfMemory(nullptr, "Assembler::GrowBuffer");
  }

  // Set up new buffer.
  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);
  DCHECK_EQ(new_size, new_buffer->size());
  uint8_t* new_start = new_buffer->start();

  // Copy the data.
  intptr_t pc_delta = new_start - buffer_start_;
  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);
  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();
  MemMove(new_start, buffer_start_, pc_offset());
  MemMove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),
          reloc_size);

  // Switch buffers.
  buffer_ = std::move(new_buffer);
  buffer_start_ = new_start;
  DEBUG_PRINTF("%p\n", buffer_start_);
  pc_ += pc_delta;
  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,
                               reloc_info_writer.last_pc() + pc_delta);

  // Relocate runtime entries.
  base::Vector<uint8_t> instructions{buffer_start_,
                                     static_cast<size_t>(pc_offset())};
  base::Vector<const uint8_t> reloc_info{reloc_info_writer.pos(), reloc_size};
  for (RelocIterator it(instructions, reloc_info, 0); !it.done(); it.next()) {
    RelocInfo::Mode rmode = it.rinfo()->rmode();
    if (rmode == RelocInfo::INTERNAL_REFERENCE) {
      RelocateInternalReference(rmode, it.rinfo()->pc(), pc_delta);
    }
  }

  DCHECK(!overflow());
}

void Assembler::db(uint8_t data) {
  if (!is_buffer_growth_blocked()) CheckBuffer();
  DEBUG_PRINTF("%p(%d): constant 0x%x\n", pc_, pc_offset(), data);
  EmitHelper(data);
}

void Assembler::dd(uint32_t data) {
  if (!is_buffer_growth_blocked()) CheckBuffer();
  DEBUG_PRINTF("%p(%d): constant 0x%x\n", pc_, pc_offset(), data);
  EmitHelper(data);
}

void Assembler::dq(uint64_t data) {
  if (!is_buffer_growth_blocked()) CheckBuffer();
  DEBUG_PRINTF("%p(%d): constant 0x%" PRIx64 "\n", pc_, pc_offset(), data);
  EmitHelper(data);
}

void Assembler::dd(Label* label) {
  uintptr_t data;
  if (!is_buffer_growth_blocked()) CheckBuffer();
  if (label->is_bound()) {
    data = reinterpret_cast<uintptr_t>(buffer_start_ + label->pos());
  } else {
    data = jump_address(label);
    internal_reference_positions_.insert(label->pos());
  }
  RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE);
  EmitHelper(data);
}

void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
  if (!ShouldRecordRelocInfo(rmode)) return;
  // We do not try to reuse pool constants.
  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data);
  DCHECK_GE(buffer_space(), kMaxRelocSize);  // Too late to grow buffer here.
  reloc_info_writer.Write(&rinfo);
}

void Assembler::BlockTrampolinePoolFor(int instructions) {
  DEBUG_PRINTF("\tBlockTrampolinePoolFor %d", instructions);
  CheckTrampolinePoolQuick(instructions);
  DEBUG_PRINTF("\tpc_offset %d,BlockTrampolinePoolBefore %d\n", pc_offset(),
               pc_offset() + instructions * kInstrSize);
  BlockTrampolinePoolBefore(pc_offset() + instructions * kInstrSize);
}

void Assembler::CheckTrampolinePool() {
  // Some small sequences of instructions must not be broken up by the
  // insertion of a trampoline pool; such sequences are protected by setting
  // either trampoline_pool_blocked_nesting_ or no_trampoline_pool_before_,
  // which are both checked here. Also, recursive calls to CheckTrampolinePool
  // are blocked by trampoline_pool_blocked_nesting_.
  DEBUG_PRINTF("\tpc_offset %d no_trampoline_pool_before:%d\n", pc_offset(),
               no_trampoline_pool_before_);
  DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
               trampoline_pool_blocked_nesting_);
  if ((trampoline_pool_blocked_nesting_ > 0) ||
      (pc_offset() < no_trampoline_pool_before_)) {
    // Emission is currently blocked; make sure we try again as soon as
    // possible.
    if (trampoline_pool_blocked_nesting_ > 0) {
      next_buffer_check_ = pc_offset() + kInstrSize;
    } else {
      next_buffer_check_ = no_trampoline_pool_before_;
    }
    return;
  }

  DCHECK(!trampoline_emitted_);
  DCHECK_GE(unbound_labels_count_, 0);
  if (unbound_labels_count_ > 0) {
    // First we emit jump, then we emit trampoline pool.
    {
      DEBUG_PRINTF("inserting trampoline pool at %p (%d)\n",
                   reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
                   pc_offset());
      BlockTrampolinePoolScope block_trampoline_pool(this);
      Label after_pool;
      j(&after_pool);

      int pool_start = pc_offset();
      for (int i = 0; i < unbound_labels_count_; i++) {
        int32_t imm;
        imm = branch_long_offset(&after_pool);
        CHECK(is_int32(imm + 0x800));
        int32_t Hi20 = (((int32_t)imm + 0x800) >> 12);
        int32_t Lo12 = (int32_t)imm << 20 >> 20;
        auipc(t6, Hi20);  // Read PC + Hi20 into t6
        jr(t6, Lo12);     // jump PC + Hi20 + Lo12
      }
      // If unbound_labels_count_ is big enough, label after_pool will
      // need a trampoline too, so we must create the trampoline before
      // the bind operation to make sure function 'bind' can get this
      // information.
      trampoline_ = Trampoline(pool_start, unbound_labels_count_);
      bind(&after_pool);

      trampoline_emitted_ = true;
      // As we are only going to emit trampoline once, we need to prevent any
      // further emission.
      next_buffer_check_ = kMaxInt;
    }
  } else {
    // Number of branches to unbound label at this point is zero, so we can
    // move next buffer check to maximum.
    next_buffer_check_ =
        pc_offset() + kMaxBranchOffset - kTrampolineSlotsSize * 16;
  }
  return;
}

void Assembler::set_target_address_at(Address pc, Address constant_pool,
                                      Address target,
                                      ICacheFlushMode icache_flush_mode) {
  Instr* instr = reinterpret_cast<Instr*>(pc);
  if (IsAuipc(*instr)) {
#if V8_TARGET_ARCH_RISCV64
    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
#elif V8_TARGET_ARCH_RISCV32
    if (IsLw(*reinterpret_cast<Instr*>(pc + 4))) {
#endif
      int32_t Hi20 = AuipcOffset(*instr);
      int32_t Lo12 = LoadOffset(*reinterpret_cast<Instr*>(pc + 4));
      Memory<Address>(pc + Hi20 + Lo12) = target;
      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
        FlushInstructionCache(pc + Hi20 + Lo12, 2 * kInstrSize);
      }
    } else {
      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
      intptr_t imm = (intptr_t)target - (intptr_t)pc;
      Instr instr = instr_at(pc);
      Instr instr1 = instr_at(pc + 1 * kInstrSize);
      DCHECK(is_int32(imm + 0x800));
      int num = PatchBranchlongOffset(pc, instr, instr1, (int32_t)imm);
      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
        FlushInstructionCache(pc, num * kInstrSize);
      }
    }
  } else {
    set_target_address_at(pc, target, icache_flush_mode);
  }
}

Address Assembler::target_address_at(Address pc, Address constant_pool) {
  Instr* instr = reinterpret_cast<Instr*>(pc);
  if (IsAuipc(*instr)) {
#if V8_TARGET_ARCH_RISCV64
    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
#elif V8_TARGET_ARCH_RISCV32
    if (IsLw(*reinterpret_cast<Instr*>(pc + 4))) {
#endif
      int32_t Hi20 = AuipcOffset(*instr);
      int32_t Lo12 = LoadOffset(*reinterpret_cast<Instr*>(pc + 4));
      return Memory<Address>(pc + Hi20 + Lo12);
    } else {
      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
      int32_t Hi20 = AuipcOffset(*instr);
      int32_t Lo12 = JalrOffset(*reinterpret_cast<Instr*>(pc + 4));
      return pc + Hi20 + Lo12;
    }

  } else {
    return target_address_at(pc);
  }
}

#if V8_TARGET_ARCH_RISCV64
Address Assembler::target_address_at(Address pc) {
  DEBUG_PRINTF("target_address_at: pc: %lx\t", pc);
#ifdef RISCV_USE_SV39
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
  Instruction* instr2 = Instruction::At((unsigned char*)(pc + 2 * kInstrSize));
  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));

  // Interpret instructions for address generated by li: See listing in
  // Assembler::set_target_address_at() just below.
  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
      IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
      IsSlli(*reinterpret_cast<Instr*>(instr2)) &&
      IsOri(*reinterpret_cast<Instr*>(instr3))) {
    // Assemble the 64 bit value.
    int64_t addr = (int64_t)(instr0->Imm20UValue() << kImm20Shift) +
                   (int64_t)instr1->Imm12Value();
    addr <<= 8;
    addr |= (int64_t)instr3->Imm12Value();
#else
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
  Instruction* instr2 = Instruction::At((unsigned char*)(pc + 2 * kInstrSize));
  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
  Instruction* instr4 = Instruction::At((unsigned char*)(pc + 4 * kInstrSize));
  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));

  // Interpret instructions for address generated by li: See listing in
  // Assembler::set_target_address_at() just below.
  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
      IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
      IsSlli(*reinterpret_cast<Instr*>(instr2)) &&
      IsOri(*reinterpret_cast<Instr*>(instr3)) &&
      IsSlli(*reinterpret_cast<Instr*>(instr4)) &&
      IsOri(*reinterpret_cast<Instr*>(instr5))) {
    // Assemble the 64 bit value.
    int64_t addr = (int64_t)(instr0->Imm20UValue() << kImm20Shift) +
                   (int64_t)instr1->Imm12Value();
    addr <<= 11;
    addr |= (int64_t)instr3->Imm12Value();
    addr <<= 6;
    addr |= (int64_t)instr5->Imm12Value();
#endif
    DEBUG_PRINTF("addr: %" PRIx64 "\n", addr);
    return static_cast<Address>(addr);
  }
  // We should never get here, force a bad address if we do.
  UNREACHABLE();
}
// On RISC-V, a 48-bit target address is stored in an 6-instruction sequence:
//  lui(reg, (int32_t)high_20); // 19 high bits
//  addi(reg, reg, low_12); // 12 following bits. total is 31 high bits in reg.
//  slli(reg, reg, 11); // Space for next 11 bits
//  ori(reg, reg, b11); // 11 bits are put in. 42 bit in reg
//  slli(reg, reg, 6); // Space for next 6 bits
//  ori(reg, reg, a6); // 6 bits are put in. all 48 bis in reg
//
// If define RISCV_USE_SV39, a 39-bit target address is stored in an
// 4-instruction sequence:
//  lui(reg, (int32_t)high_20); // 20 high bits
//  addi(reg, reg, low_12); // 12 following bits. total is 32 high bits in reg.
//  slli(reg, reg, 7); // Space for next 7 bits
//  ori(reg, reg, a7); // 7 bits are put in.
//
// Patching the address must replace all instructions, and flush the i-cache.
// Note that this assumes the use of SV48, the 48-bit virtual memory system.
void Assembler::set_target_value_at(Address pc, uint64_t target,
                                    ICacheFlushMode icache_flush_mode) {
  DEBUG_PRINTF("set_target_value_at: pc: %" PRIxPTR "\ttarget: %" PRIx64 "\n",
               pc, target);
  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
#ifdef RISCV_USE_SV39
  DCHECK_EQ((target & 0xffffff8000000000ll), 0);
#ifdef DEBUG
  // Check we have the result from a li macro-instruction.
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
         IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
         IsOri(*reinterpret_cast<Instr*>(instr3)));
#endif
  int64_t a8 = target & 0xff;                    // bits 0:7. 8 bits
  int64_t high_31 = (target >> 8) & 0x7fffffff;  // 31 bits
  int64_t high_20 = ((high_31 + 0x800) >> 12);   // 19 bits
  int64_t low_12 = high_31 & 0xfff;              // 12 bits
  *p = *p & 0xfff;
  *p = *p | ((int32_t)high_20 << 12);
  *(p + 1) = *(p + 1) & 0xfffff;
  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
  *(p + 2) = *(p + 2) & 0xfffff;
  *(p + 2) = *(p + 2) | (8 << 20);
  *(p + 3) = *(p + 3) & 0xfffff;
  *(p + 3) = *(p + 3) | ((int32_t)a8 << 20);
  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
    FlushInstructionCache(pc, 6 * kInstrSize);
  }
#else
  DCHECK_EQ((target & 0xffff000000000000ll), 0);
#ifdef DEBUG
  // Check we have the result from a li macro-instruction.
  Instruction* instr0 = Instruction::At((unsigned char*)pc);
  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));
  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
         IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
         IsOri(*reinterpret_cast<Instr*>(instr3)) &&
         IsOri(*reinterpret_cast<Instr*>(instr5)));
#endif
  int64_t a6 = target & 0x3f;                     // bits 0:6. 6 bits
  int64_t b11 = (target >> 6) & 0x7ff;            // bits 6:11. 11 bits
  int64_t high_31 = (target >> 17) & 0x7fffffff;  // 31 bits
  int64_t high_20 = ((high_31 + 0x800) >> 12);    // 19 bits
  int64_t low_12 = high_31 & 0xfff;               // 12 bits
  *p = *p & 0xfff;
  *p = *p | ((int32_t)high_20 << 12);
  *(p + 1) = *(p + 1) & 0xfffff;
  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
  *(p + 2) = *(p + 2) & 0xfffff;
  *(p + 2) = *(p + 2) | (11 << 20);
  *(p + 3) = *(p + 3) & 0xfffff;
  *(p + 3) = *(p + 3) | ((int32_t)b11 << 20);
  *(p + 4) = *(p + 4) & 0xfffff;
  *(p + 4) = *(p + 4) | (6 << 20);
  *(p + 5) = *(p + 5) & 0xfffff;
  *(p + 5) = *(p + 5) | ((int32_t)a6 << 20);
  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
    FlushInstructionCache(pc, 8 * kInstrSize);
  }
#endif
  DCHECK_EQ(target_address_at(pc), target);
}

#elif V8_TARGET_ARCH_RISCV32
Address Assembler::target_address_at(Address pc) {
  DEBUG_PRINTF("target_address_at: pc: %x\t", pc);
  int32_t addr = target_constant32_at(pc);
  DEBUG_PRINTF("addr: %x\n", addr);
  return static_cast<Address>(addr);
}
// On RISC-V, a 32-bit target address is stored in an 2-instruction sequence:
//  lui(reg, high_20); // 20 high bits
//  addi(reg, reg, low_12); // 12 following bits. total is 31 high bits in reg.
//
// Patching the address must replace all instructions, and flush the i-cache.
void Assembler::set_target_value_at(Address pc, uint32_t target,
                                    ICacheFlushMode icache_flush_mode) {
  DEBUG_PRINTF("set_target_value_at: pc: %x\ttarget: %x\n", pc, target);
  set_target_constant32_at(pc, target, icache_flush_mode);
}
#endif

bool UseScratchRegisterScope::hasAvailable() const {
  return !available_->is_empty();
}

bool Assembler::IsConstantPoolAt(Instruction* instr) {
  // The constant pool marker is made of two instructions. These instructions
  // will never be emitted by the JIT, so checking for the first one is enough:
  // 0: ld x0, x0, #offset
  Instr instr_value = *reinterpret_cast<Instr*>(instr);
#if V8_TARGET_ARCH_RISCV64
  bool result = IsLd(instr_value) && (instr->Rs1Value() == kRegCode_zero_reg) &&
                (instr->RdValue() == kRegCode_zero_reg);
#elif V8_TARGET_ARCH_RISCV32
  bool result = IsLw(instr_value) && (instr->Rs1Value() == kRegCode_zero_reg) &&
                (instr->RdValue() == kRegCode_zero_reg);
#endif
#ifdef DEBUG
  // It is still worth asserting the marker is complete.
  // 1: j 0x0
  Instruction* instr_following = instr + kInstrSize;
  DCHECK(!result || (IsJal(*reinterpret_cast<Instr*>(instr_following)) &&
                     instr_following->Imm20JValue() == 0 &&
                     instr_following->RdValue() == kRegCode_zero_reg));
#endif
  return result;
}

int Assembler::ConstantPoolSizeAt(Instruction* instr) {
  if (IsConstantPoolAt(instr)) {
    return instr->Imm12Value();
  } else {
    return -1;
  }
}

void Assembler::RecordConstPool(int size) {
  // We only need this for debugger support, to correctly compute offsets in the
  // code.
  Assembler::BlockPoolsScope block_pools(this);
  RecordRelocInfo(RelocInfo::CONST_POOL, static_cast<intptr_t>(size));
}

void Assembler::EmitPoolGuard() {
  // We must generate only one instruction as this is used in scopes that
  // control the size of the code generated.
  j(0);
}

// -----------------------------------------------------------------------------
// Assembler.
template <typename T>
void Assembler::EmitHelper(T x) {
  *reinterpret_cast<T*>(pc_) = x;
  pc_ += sizeof(x);
}

void Assembler::emit(Instr x) {
  if (!is_buffer_growth_blocked()) {
    CheckBuffer();
  }
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  EmitHelper(x);
  disassembleInstr(pc_ - sizeof(x));
  CheckTrampolinePoolQuick();
}

void Assembler::emit(ShortInstr x) {
  if (!is_buffer_growth_blocked()) {
    CheckBuffer();
  }
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  EmitHelper(x);
  disassembleInstr(pc_ - sizeof(x));
  CheckTrampolinePoolQuick();
}

void Assembler::emit(uint64_t data) {
  DEBUG_PRINTF("%p(%d): ", pc_, pc_offset());
  if (!is_buffer_growth_blocked()) CheckBuffer();
  EmitHelper(data);
}

// Constant Pool

void ConstantPool::EmitPrologue(Alignment require_alignment) {
  // Recorded constant pool size is expressed in number of 32-bits words,
  // and includes prologue and alignment, but not the jump around the pool
  // and the size of the marker itself.
  const int marker_size = 1;
  int word_count =
      ComputeSize(Jump::kOmitted, require_alignment) / kInt32Size - marker_size;
#if V8_TARGET_ARCH_RISCV64
  assm_->ld(zero_reg, zero_reg, word_count);
#elif V8_TARGET_ARCH_RISCV32
  assm_->lw(zero_reg, zero_reg, word_count);
#endif
  assm_->EmitPoolGuard();
}

int ConstantPool::PrologueSize(Jump require_jump) const {
  // Prologue is:
  //   j over  ;; if require_jump
  //   ld x0, x0, #pool_size
  //   j 0x0
  int prologue_size = require_jump == Jump::kRequired ? kInstrSize : 0;
  prologue_size += 2 * kInstrSize;
  return prologue_size;
}

void ConstantPool::SetLoadOffsetToConstPoolEntry(int load_offset,
                                                 Instruction* entry_offset,
                                                 const ConstantPoolKey& key) {
  Instr instr_auipc = assm_->instr_at(load_offset);
  Instr instr_load = assm_->instr_at(load_offset + 4);
  // Instruction to patch must be 'ld/lw rd, offset(rd)' with 'offset == 0'.
  DCHECK(assm_->IsAuipc(instr_auipc));
#if V8_TARGET_ARCH_RISCV64
  DCHECK(assm_->IsLd(instr_load));
#elif V8_TARGET_ARCH_RISCV32
  DCHECK(assm_->IsLw(instr_load));
#endif
  DCHECK_EQ(assm_->LoadOffset(instr_load), 0);
  DCHECK_EQ(assm_->AuipcOffset(instr_auipc), 0);
  int32_t distance = static_cast<int32_t>(
      reinterpret_cast<Address>(entry_offset) -
      reinterpret_cast<Address>(assm_->toAddress(load_offset)));
  CHECK(is_int32(distance + 0x800));
  int32_t Hi20 = (((int32_t)distance + 0x800) >> 12);
  int32_t Lo12 = (int32_t)distance << 20 >> 20;
  assm_->instr_at_put(load_offset, SetAuipcOffset(Hi20, instr_auipc));
  assm_->instr_at_put(load_offset + 4, SetLoadOffset(Lo12, instr_load));
}

void ConstantPool::Check(Emission force_emit, Jump require_jump,
                         size_t margin) {
  // Some short sequence of instruction must not be broken up by constant pool
  // emission, such sequences are protected by a ConstPool::BlockScope.
  if (IsBlocked()) {
    // Something is wrong if emission is forced and blocked at the same time.
    DCHECK_EQ(force_emit, Emission::kIfNeeded);
    return;
  }

  // We emit a constant pool only if :
  //  * it is not empty
  //  * emission is forced by parameter force_emit (e.g. at function end).
  //  * emission is mandatory or opportune according to {ShouldEmitNow}.
  if (!IsEmpty() && (force_emit == Emission::kForced ||
                     ShouldEmitNow(require_jump, margin))) {
    // Emit veneers for branches that would go out of range during emission of
    // the constant pool.
    int worst_case_size = ComputeSize(Jump::kRequired, Alignment::kRequired);

    // Check that the code buffer is large enough before emitting the constant
    // pool (this includes the gap to the relocation information).
    int needed_space = worst_case_size + assm_->kGap;
    while (assm_->buffer_space() <= needed_space) {
      assm_->GrowBuffer();
    }

    EmitAndClear(require_jump);
  }
  // Since a constant pool is (now) empty, move the check offset forward by
  // the standard interval.
  SetNextCheckIn(ConstantPool::kCheckInterval);
}

// Pool entries are accessed with pc relative load therefore this cannot be more
// than 1 * MB. Since constant pool emission checks are interval based, and we
// want to keep entries close to the code, we try to emit every 64KB.
const size_t ConstantPool::kMaxDistToPool32 = 1 * MB;
const size_t ConstantPool::kMaxDistToPool64 = 1 * MB;
const size_t ConstantPool::kCheckInterval = 128 * kInstrSize;
const size_t ConstantPool::kApproxDistToPool32 = 64 * KB;
const size_t ConstantPool::kApproxDistToPool64 = kApproxDistToPool32;

const size_t ConstantPool::kOpportunityDistToPool32 = 64 * KB;
const size_t ConstantPool::kOpportunityDistToPool64 = 64 * KB;
const size_t ConstantPool::kApproxMaxEntryCount = 512;

#if defined(V8_TARGET_ARCH_RISCV64)
// LLVM Code
//===- RISCVMatInt.cpp - Immediate materialisation -------------*- C++
//-*--===//
//
//  Part of the LLVM Project, under the Apache License v2.0 with LLVM
//  Exceptions. See https://llvm.org/LICENSE.txt for license information.
//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
void Assembler::RecursiveLi(Register rd, int64_t val) {
  if (val > 0 && RecursiveLiImplCount(val) > 2) {
    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
    if (countFillZero < RecursiveLiImplCount(val)) {
      RecursiveLiImpl(rd, ShiftedVal);
      srli(rd, rd, LeadingZeros);
      return;
    }
  }
  RecursiveLiImpl(rd, val);
}

int Assembler::RecursiveLiCount(int64_t val) {
  if (val > 0 && RecursiveLiImplCount(val) > 2) {
    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
    // Fill in the bits that will be shifted out with 1s. An example where
    // this helps is trailing one masks with 32 or more ones. This will
    // generate ADDI -1 and an SRLI.
    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
    if (countFillZero < RecursiveLiImplCount(val)) {
      return countFillZero;
    }
  }
  return RecursiveLiImplCount(val);
}

void Assembler::RecursiveLiImpl(Register rd, int64_t Val) {
  if (is_int32(Val)) {
    // Depending on the active bits in the immediate Value v, the following
    // instruction sequences are emitted:
    //
    // v == 0                        : ADDI
    // v[0,12) != 0 && v[12,32) == 0 : ADDI
    // v[0,12) == 0 && v[12,32) != 0 : LUI
    // v[0,32) != 0                  : LUI+ADDI(W)
    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
    int64_t Lo12 = Val << 52 >> 52;

    if (Hi20) {
      lui(rd, (int32_t)Hi20);
    }

    if (Lo12 || Hi20 == 0) {
      if (Hi20) {
        addiw(rd, rd, Lo12);
      } else {
        addi(rd, zero_reg, Lo12);
      }
    }
    return;
  }

  // In the worst case, for a full 64-bit constant, a sequence of 8
  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
  // up to 32 bits while the following ADDI instructions contribute up to 12
  // bits each.
  //
  // On the first glance, implementing this seems to be possible by simply
  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
  // to the fact that ADDI performs a sign extended addition, doing it like
  // that would only be possible when at most 11 bits of the ADDI instructions
  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
  // actually requires that the constant is processed starting with the least
  // significant bit.
  //
  // In the following, constants are processed from LSB to MSB but instruction
  // emission is performed from MSB to LSB by recursively calling
  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
  // from the constant and the optimal shift amount, which can be greater than
  // 12 bits if the constant is sparse, is determined. Then, the shifted
  // remaining constant is processed recursively and gets emitted as soon as
  // it fits into 32 bits. The emission of the shifts and additions is
  // subsequently performed when the recursion returns.

  int64_t Lo12 = Val << 52 >> 52;
  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);

  // If the remaining bits don't fit in 12 bits, we might be able to reduce
  // the shift amount in order to use LUI which will zero the lower 12 bits.
  bool Unsigned = false;
  if (ShiftAmount > 12 && !is_int12(Hi52)) {
    if (is_int32((uint64_t)Hi52 << 12)) {
      // Reduce the shift amount and add zeros to the LSBs so it will match
      // LUI.
      ShiftAmount -= 12;
      Hi52 = (uint64_t)Hi52 << 12;
    }
  }
  RecursiveLi(rd, Hi52);

  if (Unsigned) {
  } else {
    slli(rd, rd, ShiftAmount);
  }
  if (Lo12) {
    addi(rd, rd, Lo12);
  }
}

int Assembler::RecursiveLiImplCount(int64_t Val) {
  int count = 0;
  if (is_int32(Val)) {
    // Depending on the active bits in the immediate Value v, the following
    // instruction sequences are emitted:
    //
    // v == 0                        : ADDI
    // v[0,12) != 0 && v[12,32) == 0 : ADDI
    // v[0,12) == 0 && v[12,32) != 0 : LUI
    // v[0,32) != 0                  : LUI+ADDI(W)
    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
    int64_t Lo12 = Val << 52 >> 52;

    if (Hi20) {
      // lui(rd, (int32_t)Hi20);
      count++;
    }

    if (Lo12 || Hi20 == 0) {
      //   unsigned AddiOpc = (IsRV64 && Hi20) ? RISCV::ADDIW : RISCV::ADDI;
      //   Res.push_back(RISCVMatInt::Inst(AddiOpc, Lo12));
      count++;
    }
    return count;
  }

  // In the worst case, for a full 64-bit constant, a sequence of 8
  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
  // up to 32 bits while the following ADDI instructions contribute up to 12
  // bits each.
  //
  // On the first glance, implementing this seems to be possible by simply
  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
  // to the fact that ADDI performs a sign extended addition, doing it like
  // that would only be possible when at most 11 bits of the ADDI instructions
  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
  // actually requires that the constant is processed starting with the least
  // significant bit.
  //
  // In the following, constants are processed from LSB to MSB but instruction
  // emission is performed from MSB to LSB by recursively calling
  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
  // from the constant and the optimal shift amount, which can be greater than
  // 12 bits if the constant is sparse, is determined. Then, the shifted
  // remaining constant is processed recursively and gets emitted as soon as
  // it fits into 32 bits. The emission of the shifts and additions is
  // subsequently performed when the recursion returns.

  int64_t Lo12 = Val << 52 >> 52;
  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);

  // If the remaining bits don't fit in 12 bits, we might be able to reduce
  // the shift amount in order to use LUI which will zero the lower 12 bits.
  bool Unsigned = false;
  if (ShiftAmount > 12 && !is_int12(Hi52)) {
    if (is_int32((uint64_t)Hi52 << 12)) {
      // Reduce the shift amount and add zeros to the LSBs so it will match
      // LUI.
      ShiftAmount -= 12;
      Hi52 = (uint64_t)Hi52 << 12;
    }
  }

  count += RecursiveLiImplCount(Hi52);

  if (Unsigned) {
  } else {
    // slli(rd, rd, ShiftAmount);
    count++;
  }
  if (Lo12) {
    // addi(rd, rd, Lo12);
    count++;
  }
  return count;
}

int Assembler::GeneralLiCount(int64_t imm, bool is_get_temp_reg) {
  int count = 0;
  // imitate Assembler::RV_li
  if (is_int32(imm + 0x800)) {
    // 32-bit case. Maximum of 2 instructions generated
    int64_t high_20 = ((imm + 0x800) >> 12);
    int64_t low_12 = imm << 52 >> 52;
    if (high_20) {
      count++;
      if (low_12) {
        count++;
      }
    } else {
      count++;
    }
    return count;
  } else {
    // 64-bit case: divide imm into two 32-bit parts, upper and lower
    int64_t up_32 = imm >> 32;
    int64_t low_32 = imm & 0xffffffffull;
    // Check if a temporary register is available
    if (is_get_temp_reg) {
      // keep track of hardware behavior for lower part in sim_low
      int64_t sim_low = 0;
      // Build lower part
      if (low_32 != 0) {
        int64_t high_20 = ((low_32 + 0x800) >> 12);
        int64_t low_12 = low_32 & 0xfff;
        if (high_20) {
          // Adjust to 20 bits for the case of overflow
          high_20 &= 0xfffff;
          sim_low = ((high_20 << 12) << 32) >> 32;
          count++;
          if (low_12) {
            sim_low += (low_12 << 52 >> 52) | low_12;
            count++;
          }
        } else {
          sim_low = low_12;
          count++;
        }
      }
      if (sim_low & 0x100000000) {
        // Bit 31 is 1. Either an overflow or a negative 64 bit
        if (up_32 == 0) {
          // Positive number, but overflow because of the add 0x800
          count++;
          count++;
          return count;
        }
        // low_32 is a negative 64 bit after the build
        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
      }
      if (up_32 == 0) {
        return count;
      }
      int64_t high_20 = (up_32 + 0x800) >> 12;
      int64_t low_12 = up_32 & 0xfff;
      if (high_20) {
        // Adjust to 20 bits for the case of overflow
        high_20 &= 0xfffff;
        count++;
        if (low_12) {
          count++;
        }
      } else {
        count++;
      }
      // Put it at the bgining of register
      count++;
      if (low_32 != 0) {
        count++;
      }
      return count;
    }
    // No temp register. Build imm in rd.
    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
    // parts to the upper part by doing shift and add.
    // First build upper part in rd.
    int64_t high_20 = (up_32 + 0x800) >> 12;
    int64_t low_12 = up_32 & 0xfff;
    if (high_20) {
      // Adjust to 20 bits for the case of overflow
      high_20 &= 0xfffff;
      count++;
      if (low_12) {
        count++;
      }
    } else {
      count++;
    }
    // upper part already in rd. Each part to be added to rd, has maximum of
    // 11 bits, and always starts with a 1. rd is shifted by the size of the
    // part plus the number of zeros between the parts. Each part is added
    // after the left shift.
    uint32_t mask = 0x80000000;
    int32_t i;
    for (i = 0; i < 32; i++) {
      if ((low_32 & mask) == 0) {
        mask >>= 1;
        if (i == 31) {
          // rest is zero
          count++;
        }
        continue;
      }
      // The first 1 seen
      if ((i + 11) < 32) {
        // Pick 11 bits
        count++;
        count++;
        i += 10;
        mask >>= 11;
      } else {
        count++;
        count++;
        break;
      }
    }
  }
  return count;
}
#endif
}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                     node-23.7.0/deps/v8/src/codegen/riscv/assembler-riscv.h                                             0000664 0000000 0000000 00000102405 14746647661 0022642 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright (c) 1994-2006 Sun Microsystems Inc.
// All Rights Reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// - Redistributions of source code must retain the above copyright notice,
// this list of conditions and the following disclaimer.
//
// - Redistribution in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// - Neither the name of Sun Microsystems or the names of contributors may
// be used to endorse or promote products derived from this software without
// specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// The original source code covered by the above license above has been
// modified significantly by Google Inc.
// Copyright 2021 the V8 project authors. All rights reserved.

#ifndef V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_
#define V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_

#include <stdio.h>

#include <memory>
#include <set>

#include "src/codegen/assembler.h"
#include "src/codegen/constant-pool.h"
#include "src/codegen/constants-arch.h"
#include "src/codegen/external-reference.h"
#include "src/codegen/flush-instruction-cache.h"
#include "src/codegen/label.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/base-riscv-i.h"
#include "src/codegen/riscv/extension-riscv-a.h"
#include "src/codegen/riscv/extension-riscv-b.h"
#include "src/codegen/riscv/extension-riscv-c.h"
#include "src/codegen/riscv/extension-riscv-d.h"
#include "src/codegen/riscv/extension-riscv-f.h"
#include "src/codegen/riscv/extension-riscv-m.h"
#include "src/codegen/riscv/extension-riscv-v.h"
#include "src/codegen/riscv/extension-riscv-zicond.h"
#include "src/codegen/riscv/extension-riscv-zicsr.h"
#include "src/codegen/riscv/extension-riscv-zifencei.h"
#include "src/codegen/riscv/register-riscv.h"
#include "src/objects/contexts.h"
#include "src/objects/smi.h"

namespace v8 {
namespace internal {

#define DEBUG_PRINTF(...)     \
  if (v8_flags.riscv_debug) { \
    printf(__VA_ARGS__);      \
  }

class SafepointTableBuilder;

// -----------------------------------------------------------------------------
// Machine instruction Operands.
constexpr int kSmiShift = kSmiTagSize + kSmiShiftSize;
constexpr uintptr_t kSmiShiftMask = (1UL << kSmiShift) - 1;
// Class Operand represents a shifter operand in data processing instructions.
class Operand {
 public:
  // Immediate.
  V8_INLINE explicit Operand(intptr_t immediate,
                             RelocInfo::Mode rmode = RelocInfo::NO_INFO)
      : rm_(no_reg), rmode_(rmode) {
    value_.immediate = immediate;
  }

  V8_INLINE explicit Operand(Tagged<Smi> value)
      : Operand(static_cast<intptr_t>(value.ptr())) {}

  V8_INLINE explicit Operand(const ExternalReference& f)
      : rm_(no_reg), rmode_(RelocInfo::EXTERNAL_REFERENCE) {
    value_.immediate = static_cast<intptr_t>(f.address());
  }

  explicit Operand(Handle<HeapObject> handle);

  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.

  // Register.
  V8_INLINE explicit Operand(Register rm) : rm_(rm) {}

  // Return true if this is a register operand.
  V8_INLINE bool is_reg() const { return rm_.is_valid(); }
  inline intptr_t immediate() const {
    DCHECK(!is_reg());
    DCHECK(!IsHeapNumberRequest());
    return value_.immediate;
  }

  bool IsImmediate() const { return !rm_.is_valid(); }

  HeapNumberRequest heap_number_request() const {
    DCHECK(IsHeapNumberRequest());
    return value_.heap_number_request;
  }

  bool IsHeapNumberRequest() const {
    DCHECK_IMPLIES(is_heap_number_request_, IsImmediate());
    DCHECK_IMPLIES(is_heap_number_request_,
                   rmode_ == RelocInfo::FULL_EMBEDDED_OBJECT ||
                       rmode_ == RelocInfo::CODE_TARGET);
    return is_heap_number_request_;
  }

  Register rm() const { return rm_; }

  RelocInfo::Mode rmode() const { return rmode_; }

 private:
  Register rm_;
  union Value {
    Value() {}
    HeapNumberRequest heap_number_request;  // if is_heap_number_request_
    intptr_t immediate;                     // otherwise
  } value_;                                 // valid if rm_ == no_reg
  bool is_heap_number_request_ = false;
  RelocInfo::Mode rmode_;

  friend class Assembler;
  friend class MacroAssembler;
};

// On RISC-V we have only one addressing mode with base_reg + offset.
// Class MemOperand represents a memory operand in load and store instructions.
class V8_EXPORT_PRIVATE MemOperand : public Operand {
 public:
  // Immediate value attached to offset.
  enum OffsetAddend { offset_minus_one = -1, offset_zero = 0 };

  explicit MemOperand(Register rn, int32_t offset = 0);
  explicit MemOperand(Register rn, int32_t unit, int32_t multiplier,
                      OffsetAddend offset_addend = offset_zero);
  int32_t offset() const { return offset_; }

  void set_offset(int32_t offset) { offset_ = offset; }

  bool OffsetIsInt12Encodable() const { return is_int12(offset_); }

 private:
  int32_t offset_;

  friend class Assembler;
};

class V8_EXPORT_PRIVATE Assembler : public AssemblerBase,
                                    public AssemblerRISCVI,
                                    public AssemblerRISCVA,
                                    public AssemblerRISCVB,
                                    public AssemblerRISCVF,
                                    public AssemblerRISCVD,
                                    public AssemblerRISCVM,
                                    public AssemblerRISCVC,
                                    public AssemblerRISCVZifencei,
                                    public AssemblerRISCVZicsr,
                                    public AssemblerRISCVZicond,
                                    public AssemblerRISCVV {
 public:
  // Create an assembler. Instructions and relocation information are emitted
  // into a buffer, with the instructions starting from the beginning and the
  // relocation information starting from the end of the buffer. See CodeDesc
  // for a detailed comment on the layout (globals.h).
  //
  // If the provided buffer is nullptr, the assembler allocates and grows its
  // own buffer. Otherwise it takes ownership of the provided buffer.
  explicit Assembler(const AssemblerOptions&,
                     std::unique_ptr<AssemblerBuffer> = {});

  virtual ~Assembler();
  void AbortedCodeGeneration();
  // GetCode emits any pending (non-emitted) code and fills the descriptor desc.
  static constexpr int kNoHandlerTable = 0;
  static constexpr SafepointTableBuilder* kNoSafepointTable = nullptr;
  void GetCode(LocalIsolate* isolate, CodeDesc* desc,
               SafepointTableBuilder* safepoint_table_builder,
               int handler_table_offset);

  // Convenience wrapper for allocating with an Isolate.
  void GetCode(Isolate* isolate, CodeDesc* desc);
  // Convenience wrapper for code without safepoint or handler tables.
  void GetCode(LocalIsolate* isolate, CodeDesc* desc) {
    GetCode(isolate, desc, kNoSafepointTable, kNoHandlerTable);
  }

  // Unused on this architecture.
  void MaybeEmitOutOfLineConstantPool() {}

  // Label operations & relative jumps (PPUM Appendix D).
  //
  // Takes a branch opcode (cc) and a label (L) and generates
  // either a backward branch or a forward branch and links it
  // to the label fixup chain. Usage:
  //
  // Label L;    // unbound label
  // j(cc, &L);  // forward branch to unbound label
  // bind(&L);   // bind label to the current pc
  // j(cc, &L);  // backward branch to bound label
  // bind(&L);   // illegal: a label may be bound only once
  //
  // Note: The same Label can be used for forward and backward branches
  // but it may be bound only once.
  void bind(Label* L);  // Binds an unbound label L to current code position.

  // Determines if Label is bound and near enough so that branch instruction
  // can be used to reach it, instead of jump instruction.
  bool is_near(Label* L);
  bool is_near(Label* L, OffsetSize bits);
  bool is_near_branch(Label* L);

  // Get offset from instr.
  int BranchOffset(Instr instr);
  static int BrachlongOffset(Instr auipc, Instr jalr);
  static int PatchBranchlongOffset(Address pc, Instr auipc, Instr instr_I,
                                   int32_t offset);

  // Returns the branch offset to the given label from the current code
  // position. Links the label to the current position if it is still unbound.
  // Manages the jump elimination optimization if the second parameter is true.
  virtual int32_t branch_offset_helper(Label* L, OffsetSize bits);
  uintptr_t jump_address(Label* L);
  int32_t branch_long_offset(Label* L);

  // Puts a labels target address at the given position.
  // The high 8 bits are set to zero.
  void label_at_put(Label* L, int at_offset);

  // During code generation builtin targets in PC-relative call/jump
  // instructions are temporarily encoded as builtin ID until the generated
  // code is moved into the code space.
  static inline Builtin target_builtin_at(Address pc);

  // Read/Modify the code target address in the branch/call instruction at pc.
  // The isolate argument is unused (and may be nullptr) when skipping flushing.
  static Address target_address_at(Address pc);
  V8_INLINE static void set_target_address_at(
      Address pc, Address target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED) {
    set_target_value_at(pc, target, icache_flush_mode);
  }

  static Address target_address_at(Address pc, Address constant_pool);

  static void set_target_address_at(
      Address pc, Address constant_pool, Address target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);

  // Read/Modify the code target address in the branch/call instruction at pc.
  inline static Tagged_t target_compressed_address_at(Address pc,
                                                      Address constant_pool);
  inline static void set_target_compressed_address_at(
      Address pc, Address constant_pool, Tagged_t target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);

  inline Handle<Object> code_target_object_handle_at(Address pc,
                                                     Address constant_pool);
  inline Handle<HeapObject> compressed_embedded_object_handle_at(
      Address pc, Address constant_pool);

  static bool IsConstantPoolAt(Instruction* instr);
  static int ConstantPoolSizeAt(Instruction* instr);
  // See Assembler::CheckConstPool for more info.
  void EmitPoolGuard();

#if defined(V8_TARGET_ARCH_RISCV64)
  static void set_target_value_at(
      Address pc, uint64_t target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
#elif defined(V8_TARGET_ARCH_RISCV32)
  static void set_target_value_at(
      Address pc, uint32_t target,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
#endif

  static inline int32_t target_constant32_at(Address pc);
  static inline void set_target_constant32_at(
      Address pc, uint32_t target, ICacheFlushMode icache_flush_mode);

  static void JumpLabelToJumpRegister(Address pc);

  // This sets the branch destination (which gets loaded at the call address).
  // This is for calls and branches within generated code.  The serializer
  // has already deserialized the lui/ori instructions etc.
  inline static void deserialization_set_special_target_at(Address location,
                                                           Tagged<Code> code,
                                                           Address target);

  // Get the size of the special target encoded at 'instruction_payload'.
  inline static int deserialization_special_target_size(
      Address instruction_payload);

  // This sets the internal reference at the pc.
  inline static void deserialization_set_target_internal_reference_at(
      Address pc, Address target,
      RelocInfo::Mode mode = RelocInfo::INTERNAL_REFERENCE);

  // Read/modify the uint32 constant used at pc.
  static inline uint32_t uint32_constant_at(Address pc, Address constant_pool);
  static inline void set_uint32_constant_at(
      Address pc, Address constant_pool, uint32_t new_constant,
      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);

  // Here we are patching the address in the LUI/ADDI instruction pair.
  // These values are used in the serialization process and must be zero for
  // RISC-V platform, as InstructionStream, Embedded Object or
  // External-reference pointers are split across two consecutive instructions
  // and don't exist separately in the code, so the serializer should not step
  // forwards in memory after a target is resolved and written.
  static constexpr int kSpecialTargetSize = 0;

  // Number of consecutive instructions used to store 32bit/64bit constant.
  // This constant was used in RelocInfo::target_address_address() function
  // to tell serializer address of the instruction that follows
  // LUI/ADDI instruction pair.
  static constexpr int kInstructionsFor32BitConstant = 2;
  static constexpr int kInstructionsFor64BitConstant = 8;

  // Difference between address of current opcode and value read from pc
  // register.
  static constexpr int kPcLoadDelta = 4;

  // Bits available for offset field in branches
  static constexpr int kBranchOffsetBits = 13;

  // Bits available for offset field in jump
  static constexpr int kJumpOffsetBits = 21;

  // Bits available for offset field in compresed jump
  static constexpr int kCJalOffsetBits = 12;

  // Bits available for offset field in compressed branch
  static constexpr int kCBranchOffsetBits = 9;

  // Max offset for b instructions with 12-bit offset field (multiple of 2)
  static constexpr int kMaxBranchOffset = (1 << (13 - 1)) - 1;

  // Max offset for jal instruction with 20-bit offset field (multiple of 2)
  static constexpr int kMaxJumpOffset = (1 << (21 - 1)) - 1;

  static constexpr int kTrampolineSlotsSize = 2 * kInstrSize;

  RegList* GetScratchRegisterList() { return &scratch_register_list_; }

  // ---------------------------------------------------------------------------
  // InstructionStream generation.

  // Insert the smallest number of nop instructions
  // possible to align the pc offset to a multiple
  // of m. m must be a power of 2 (>= 4).
  void Align(int m);
  // Insert the smallest number of zero bytes possible to align the pc offset
  // to a mulitple of m. m must be a power of 2 (>= 2).
  void DataAlign(int m);
  // Aligns code to something that's optimal for a jump target for the platform.
  void CodeTargetAlign();
  void LoopHeaderAlign() { CodeTargetAlign(); }

  // Different nop operations are used by the code generator to detect certain
  // states of the generated code.
  enum NopMarkerTypes {
    NON_MARKING_NOP = 0,
    DEBUG_BREAK_NOP,
    // IC markers.
    PROPERTY_ACCESS_INLINED,
    PROPERTY_ACCESS_INLINED_CONTEXT,
    PROPERTY_ACCESS_INLINED_CONTEXT_DONT_DELETE,
    // Helper values.
    LAST_CODE_MARKER,
    FIRST_IC_MARKER = PROPERTY_ACCESS_INLINED,
  };

  void NOP();
  void EBREAK();

  // Assembler Pseudo Instructions (Tables 25.2, 25.3, RISC-V Unprivileged ISA)
  void nop();
#if defined(V8_TARGET_ARCH_RISCV64)
  void RecursiveLiImpl(Register rd, int64_t imm);
  void RecursiveLi(Register rd, int64_t imm);
  static int RecursiveLiCount(int64_t imm);
  static int RecursiveLiImplCount(int64_t imm);
  void RV_li(Register rd, int64_t imm);
  static int RV_li_count(int64_t imm, bool is_get_temp_reg = false);
  // Returns the number of instructions required to load the immediate
  void GeneralLi(Register rd, int64_t imm);
  static int GeneralLiCount(int64_t imm, bool is_get_temp_reg = false);
  // Loads an immediate, always using 8 instructions, regardless of the value,
  // so that it can be modified later.
  void li_constant(Register rd, int64_t imm);
  void li_constant32(Register rd, int32_t imm);
  void li_ptr(Register rd, int64_t imm);
#endif
#if defined(V8_TARGET_ARCH_RISCV32)
  void RV_li(Register rd, int32_t imm);
  static int RV_li_count(int32_t imm, bool is_get_temp_reg = false);

  void li_constant(Register rd, int32_t imm);
  void li_ptr(Register rd, int32_t imm);
#endif

  void break_(uint32_t code, bool break_as_stop = false);
  void stop(uint32_t code = kMaxStopCode);

  // Check the code size generated from label to here.
  int SizeOfCodeGeneratedSince(Label* label) {
    return pc_offset() - label->pos();
  }

  // Check the number of instructions generated from label to here.
  int InstructionsGeneratedSince(Label* label) {
    return SizeOfCodeGeneratedSince(label) / kInstrSize;
  }

  using BlockConstPoolScope = ConstantPool::BlockScope;
  // Class for scoping postponing the trampoline pool generation.
  class BlockTrampolinePoolScope {
   public:
    explicit BlockTrampolinePoolScope(Assembler* assem, int margin = 0)
        : assem_(assem) {
      assem_->StartBlockTrampolinePool();
    }

    explicit BlockTrampolinePoolScope(Assembler* assem, PoolEmissionCheck check)
        : assem_(assem) {
      assem_->StartBlockTrampolinePool();
    }
    ~BlockTrampolinePoolScope() { assem_->EndBlockTrampolinePool(); }

   private:
    Assembler* assem_;
    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockTrampolinePoolScope);
  };

  class V8_NODISCARD BlockPoolsScope {
   public:
    // Block Trampoline Pool and Constant Pool. Emits pools if necessary to
    // ensure that {margin} more bytes can be emitted without triggering pool
    // emission.
    explicit BlockPoolsScope(Assembler* assem, size_t margin = 0)
        : block_const_pool_(assem, margin), block_trampoline_pool_(assem) {}

    BlockPoolsScope(Assembler* assem, PoolEmissionCheck check)
        : block_const_pool_(assem, check), block_trampoline_pool_(assem) {}
    ~BlockPoolsScope() {}

   private:
    BlockConstPoolScope block_const_pool_;
    BlockTrampolinePoolScope block_trampoline_pool_;
    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockPoolsScope);
  };

  // Class for postponing the assembly buffer growth. Typically used for
  // sequences of instructions that must be emitted as a unit, before
  // buffer growth (and relocation) can occur.
  // This blocking scope is not nestable.
  class BlockGrowBufferScope {
   public:
    explicit BlockGrowBufferScope(Assembler* assem) : assem_(assem) {
      assem_->StartBlockGrowBuffer();
    }
    ~BlockGrowBufferScope() { assem_->EndBlockGrowBuffer(); }

   private:
    Assembler* assem_;

    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockGrowBufferScope);
  };

  // Record a deoptimization reason that can be used by a log or cpu profiler.
  // Use --trace-deopt to enable.
  void RecordDeoptReason(DeoptimizeReason reason, uint32_t node_id,
                         SourcePosition position, int id);

  static int RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
                                       intptr_t pc_delta);
  static void RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
                                        intptr_t pc_delta);

  // Writes a single byte or word of data in the code stream.  Used for
  // inline tables, e.g., jump-tables.
  void db(uint8_t data);
  void dd(uint32_t data);
  void dq(uint64_t data);
  void dp(uintptr_t data) { dq(data); }
  void dd(Label* label);

  Instruction* pc() const { return reinterpret_cast<Instruction*>(pc_); }

  Instruction* InstructionAt(ptrdiff_t offset) const {
    return reinterpret_cast<Instruction*>(buffer_start_ + offset);
  }

  // Postpone the generation of the trampoline pool for the specified number of
  // instructions.
  void BlockTrampolinePoolFor(int instructions);

  // Check if there is less than kGap bytes available in the buffer.
  // If this is the case, we need to grow the buffer before emitting
  // an instruction or relocation information.
  inline bool overflow() const { return pc_ >= reloc_info_writer.pos() - kGap; }

  // Get the number of bytes available in the buffer.
  inline intptr_t available_space() const {
    return reloc_info_writer.pos() - pc_;
  }

  // Read/patch instructions.
  static Instr instr_at(Address pc) { return *reinterpret_cast<Instr*>(pc); }
  static void instr_at_put(Address pc, Instr instr) {
    *reinterpret_cast<Instr*>(pc) = instr;
  }
  Instr instr_at(int pos) {
    return *reinterpret_cast<Instr*>(buffer_start_ + pos);
  }
  void instr_at_put(int pos, Instr instr) {
    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
  }

  void instr_at_put(int pos, ShortInstr instr) {
    *reinterpret_cast<ShortInstr*>(buffer_start_ + pos) = instr;
  }

  Address toAddress(int pos) {
    return reinterpret_cast<Address>(buffer_start_ + pos);
  }

  void CheckTrampolinePool();

  // Get the code target object for a pc-relative call or jump.
  V8_INLINE Handle<Code> relative_code_target_object_handle_at(
      Address pc_) const;

  inline int UnboundLabelsCount() { return unbound_labels_count_; }

  void RecordConstPool(int size);

  void ForceConstantPoolEmissionWithoutJump() {
    constpool_.Check(Emission::kForced, Jump::kOmitted);
  }
  void ForceConstantPoolEmissionWithJump() {
    constpool_.Check(Emission::kForced, Jump::kRequired);
  }
  // Check if the const pool needs to be emitted while pretending that {margin}
  // more bytes of instructions have already been emitted.
  void EmitConstPoolWithJumpIfNeeded(size_t margin = 0) {
    constpool_.Check(Emission::kIfNeeded, Jump::kRequired, margin);
  }

  void EmitConstPoolWithoutJumpIfNeeded(size_t margin = 0) {
    constpool_.Check(Emission::kIfNeeded, Jump::kOmitted, margin);
  }

  void RecordEntry(uint32_t data, RelocInfo::Mode rmode) {
    constpool_.RecordEntry(data, rmode);
  }

  void RecordEntry(uint64_t data, RelocInfo::Mode rmode) {
    constpool_.RecordEntry(data, rmode);
  }

  void CheckTrampolinePoolQuick(int extra_instructions = 0) {
    DEBUG_PRINTF("\tpc_offset:%d %d\n", pc_offset(),
                 next_buffer_check_ - extra_instructions * kInstrSize);
    if (pc_offset() >= next_buffer_check_ - extra_instructions * kInstrSize) {
      CheckTrampolinePool();
    }
  }

  friend class VectorUnit;
  class VectorUnit {
   public:
    inline int32_t sew() const { return 2 ^ (sew_ + 3); }

    inline int32_t vlmax() const {
      if ((lmul_ & 0b100) != 0) {
        return (kRvvVLEN / sew()) >> (lmul_ & 0b11);
      } else {
        return ((kRvvVLEN << lmul_) / sew());
      }
    }

    explicit VectorUnit(Assembler* assm) : assm_(assm) {}

    void set(Register rd, VSew sew, Vlmul lmul) {
      if (sew != sew_ || lmul != lmul_ || vl != vlmax()) {
        sew_ = sew;
        lmul_ = lmul;
        vl = vlmax();
        assm_->vsetvlmax(rd, sew_, lmul_);
      }
    }

    void set(Register rd, int8_t sew, int8_t lmul) {
      DCHECK_GE(sew, E8);
      DCHECK_LE(sew, E64);
      DCHECK_GE(lmul, m1);
      DCHECK_LE(lmul, mf2);
      set(rd, VSew(sew), Vlmul(lmul));
    }

    void set(FPURoundingMode mode) {
      if (mode_ != mode) {
        assm_->addi(kScratchReg, zero_reg, mode << kFcsrFrmShift);
        assm_->fscsr(kScratchReg);
        mode_ = mode;
      }
    }
    void set(Register rd, Register rs1, VSew sew, Vlmul lmul) {
      if (sew != sew_ || lmul != lmul_) {
        sew_ = sew;
        lmul_ = lmul;
        vl = 0;
        assm_->vsetvli(rd, rs1, sew_, lmul_);
      }
    }

    void set(VSew sew, Vlmul lmul) {
      if (sew != sew_ || lmul != lmul_) {
        sew_ = sew;
        lmul_ = lmul;
        assm_->vsetvl(sew_, lmul_);
      }
    }

    void clear() {
      sew_ = kVsInvalid;
      lmul_ = kVlInvalid;
    }

   private:
    VSew sew_ = kVsInvalid;
    Vlmul lmul_ = kVlInvalid;
    int32_t vl = 0;
    Assembler* assm_;
    FPURoundingMode mode_ = RNE;
  };

  VectorUnit VU;

  void ClearVectorunit() { VU.clear(); }

 protected:
  // Readable constants for base and offset adjustment helper, these indicate if
  // aside from offset, another value like offset + 4 should fit into int16.
  enum class OffsetAccessType : bool {
    SINGLE_ACCESS = false,
    TWO_ACCESSES = true
  };

  // Determine whether need to adjust base and offset of memroy load/store
  bool NeedAdjustBaseAndOffset(
      const MemOperand& src, OffsetAccessType = OffsetAccessType::SINGLE_ACCESS,
      int second_Access_add_to_offset = 4);

  // Helper function for memory load/store using base register and offset.
  void AdjustBaseAndOffset(
      MemOperand* src, Register scratch,
      OffsetAccessType access_type = OffsetAccessType::SINGLE_ACCESS,
      int second_access_add_to_offset = 4);

  inline static void set_target_internal_reference_encoded_at(Address pc,
                                                              Address target);

  intptr_t buffer_space() const { return reloc_info_writer.pos() - pc_; }

  // Decode branch instruction at pos and return branch target pos.
  int target_at(int pos, bool is_internal);

  // Patch branch instruction at pos to branch to given branch target pos.
  void target_at_put(int pos, int target_pos, bool is_internal,
                     bool trampoline = false);

  // Say if we need to relocate with this mode.
  bool MustUseReg(RelocInfo::Mode rmode);

  // Record reloc info for current pc_.
  void RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data = 0);

  // Block the emission of the trampoline pool before pc_offset.
  void BlockTrampolinePoolBefore(int pc_offset) {
    if (no_trampoline_pool_before_ < pc_offset)
      no_trampoline_pool_before_ = pc_offset;
  }

  void StartBlockTrampolinePool() {
    DEBUG_PRINTF("\tStartBlockTrampolinePool\n");
    trampoline_pool_blocked_nesting_++;
  }

  void EndBlockTrampolinePool() {
    trampoline_pool_blocked_nesting_--;
    DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
                 trampoline_pool_blocked_nesting_);
    if (trampoline_pool_blocked_nesting_ == 0) {
      CheckTrampolinePoolQuick(1);
    }
  }

  bool is_trampoline_pool_blocked() const {
    return trampoline_pool_blocked_nesting_ > 0;
  }

  bool has_exception() const { return internal_trampoline_exception_; }

  bool is_trampoline_emitted() const { return trampoline_emitted_; }

  // Temporarily block automatic assembly buffer growth.
  void StartBlockGrowBuffer() {
    DCHECK(!block_buffer_growth_);
    block_buffer_growth_ = true;
  }

  void EndBlockGrowBuffer() {
    DCHECK(block_buffer_growth_);
    block_buffer_growth_ = false;
  }

  bool is_buffer_growth_blocked() const { return block_buffer_growth_; }

 private:
  // Avoid overflows for displacements etc.
  static const int kMaximalBufferSize = 512 * MB;

  // Buffer size and constant pool distance are checked together at regular
  // intervals of kBufferCheckInterval emitted bytes.
  static constexpr int kBufferCheckInterval = 1 * KB / 2;

  // InstructionStream generation.
  // The relocation writer's position is at least kGap bytes below the end of
  // the generated instructions. This is so that multi-instruction sequences do
  // not have to check for overflow. The same is true for writes of large
  // relocation info entries.
  static constexpr int kGap = 64;
  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);

  // Repeated checking whether the trampoline pool should be emitted is rather
  // expensive. By default we only check again once a number of instructions
  // has been generated.
  static constexpr int kCheckConstIntervalInst = 32;
  static constexpr int kCheckConstInterval =
      kCheckConstIntervalInst * kInstrSize;

  int next_buffer_check_;  // pc offset of next buffer check.

  // Emission of the trampoline pool may be blocked in some code sequences.
  int trampoline_pool_blocked_nesting_;  // Block emission if this is not zero.
  int no_trampoline_pool_before_;  // Block emission before this pc offset.

  // Keep track of the last emitted pool to guarantee a maximal distance.
  int last_trampoline_pool_end_;  // pc offset of the end of the last pool.

  // Automatic growth of the assembly buffer may be blocked for some sequences.
  bool block_buffer_growth_;  // Block growth when true.

  // Relocation information generation.
  // Each relocation is encoded as a variable size value.
  static constexpr int kMaxRelocSize = RelocInfoWriter::kMaxSize;
  RelocInfoWriter reloc_info_writer;

  // The bound position, before this we cannot do instruction elimination.
  int last_bound_pos_;

  // InstructionStream emission.
  inline void CheckBuffer();
  void GrowBuffer();
  void emit(Instr x);
  void emit(ShortInstr x);
  void emit(uint64_t x);
  template <typename T>
  inline void EmitHelper(T x);

  static void disassembleInstr(uint8_t* pc);

  // Labels.
  void print(const Label* L);
  void bind_to(Label* L, int pos);
  void next(Label* L, bool is_internal);

  // One trampoline consists of:
  // - space for trampoline slots,
  // - space for labels.
  //
  // Space for trampoline slots is equal to slot_count * 2 * kInstrSize.
  // Space for trampoline slots precedes space for labels. Each label is of one
  // instruction size, so total amount for labels is equal to
  // label_count *  kInstrSize.
  class Trampoline {
   public:
    Trampoline() {
      start_ = 0;
      next_slot_ = 0;
      free_slot_count_ = 0;
      end_ = 0;
    }
    Trampoline(int start, int slot_count) {
      start_ = start;
      next_slot_ = start;
      free_slot_count_ = slot_count;
      end_ = start + slot_count * kTrampolineSlotsSize;
    }
    int start() { return start_; }
    int end() { return end_; }
    int take_slot() {
      int trampoline_slot = kInvalidSlotPos;
      if (free_slot_count_ <= 0) {
        // We have run out of space on trampolines.
        // Make sure we fail in debug mode, so we become aware of each case
        // when this happens.
        DCHECK(0);
        // Internal exception will be caught.
      } else {
        trampoline_slot = next_slot_;
        free_slot_count_--;
        next_slot_ += kTrampolineSlotsSize;
      }
      return trampoline_slot;
    }

   private:
    int start_;
    int end_;
    int next_slot_;
    int free_slot_count_;
  };

  int32_t get_trampoline_entry(int32_t pos);
  int unbound_labels_count_;
  // After trampoline is emitted, long branches are used in generated code for
  // the forward branches whose target offsets could be beyond reach of branch
  // instruction. We use this information to trigger different mode of
  // branch instruction generation, where we use jump instructions rather
  // than regular branch instructions.
  bool trampoline_emitted_ = false;
  static constexpr int kInvalidSlotPos = -1;

  // Internal reference positions, required for unbounded internal reference
  // labels.
  std::set<intptr_t> internal_reference_positions_;
  bool is_internal_reference(Label* L) {
    return internal_reference_positions_.find(L->pos()) !=
           internal_reference_positions_.end();
  }

  Trampoline trampoline_;
  bool internal_trampoline_exception_;

  RegList scratch_register_list_;

 private:
  ConstantPool constpool_;

  void AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate);

  int WriteCodeComments();

  friend class RegExpMacroAssemblerRISCV;
  friend class RelocInfo;
  friend class BlockTrampolinePoolScope;
  friend class EnsureSpace;
  friend class ConstantPool;
};

class EnsureSpace {
 public:
  explicit inline EnsureSpace(Assembler* assembler);
};

// This scope utility allows scratch registers to be managed safely. The
// Assembler's GetScratchRegisterList() is used as a pool of scratch
// registers. These registers can be allocated on demand, and will be returned
// at the end of the scope.
//
// When the scope ends, the Assembler's list will be restored to its original
// state, even if the list is modified by some other means. Note that this scope
// can be nested but the destructors need to run in the opposite order as the
// constructors. We do not have assertions for this.
class V8_EXPORT_PRIVATE UseScratchRegisterScope {
 public:
  explicit UseScratchRegisterScope(Assembler* assembler)
      : available_(assembler->GetScratchRegisterList()),
        old_available_(*available_) {}

  ~UseScratchRegisterScope() { *available_ = old_available_; }

  // Take a register from the list and return it.
  Register Acquire() {
    DCHECK_NOT_NULL(available_);
    DCHECK(!available_->is_empty());
    int index =
        static_cast<int>(base::bits::CountTrailingZeros32(available_->bits()));
    *available_ &= RegList::FromBits(~(1U << index));

    return Register::from_code(index);
  }
  bool hasAvailable() const;
  void Include(const RegList& list) { *available_ |= list; }
  void Exclude(const RegList& list) {
    *available_ &= RegList::FromBits(~list.bits());
  }
  void Include(const Register& reg1, const Register& reg2 = no_reg) {
    RegList list({reg1, reg2});
    Include(list);
  }
  void Exclude(const Register& reg1, const Register& reg2 = no_reg) {
    RegList list({reg1, reg2});
    Exclude(list);
  }

 private:
  RegList* available_;
  RegList old_available_;
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_
                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/codegen/riscv/base-assembler-riscv.cc                                       0000664 0000000 0000000 00000053633 14746647661 0023720 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright (c) 1994-2006 Sun Microsystems Inc.
// All Rights Reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// - Redistributions of source code must retain the above copyright notice,
// this list of conditions and the following disclaimer.
//
// - Redistribution in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// - Neither the name of Sun Microsystems or the names of contributors may
// be used to endorse or promote products derived from this software without
// specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// The original source code covered by the above license above has been
// modified significantly by Google Inc.
// Copyright 2021 the V8 project authors. All rights reserved.

#include "src/codegen/riscv/base-assembler-riscv.h"

#include "src/base/cpu.h"

namespace v8 {
namespace internal {

// ----- Top-level instruction formats match those in the ISA manual
// (R, I, S, B, U, J). These match the formats defined in the compiler
void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, Register rd, Register rs1,
                                   Register rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, FPURegister rd,
                                   FPURegister rs1, FPURegister rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, Register rd,
                                   FPURegister rs1, Register rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, FPURegister rd,
                                   Register rs1, Register rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, FPURegister rd,
                                   FPURegister rs1, Register rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
                                   BaseOpcode opcode, Register rd,
                                   FPURegister rs1, FPURegister rs2) {
  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR4(uint8_t funct2, BaseOpcode opcode,
                                    Register rd, Register rs1, Register rs2,
                                    Register rs3, FPURoundingMode frm) {
  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrR4(uint8_t funct2, BaseOpcode opcode,
                                    FPURegister rd, FPURegister rs1,
                                    FPURegister rs2, FPURegister rs3,
                                    FPURoundingMode frm) {
  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrRAtomic(uint8_t funct5, bool aq, bool rl,
                                         uint8_t funct3, Register rd,
                                         Register rs1, Register rs2) {
  DCHECK(is_uint5(funct5) && is_uint3(funct3) && rd.is_valid() &&
         rs1.is_valid() && rs2.is_valid());
  Instr instr = AMO | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (rl << kRlShift) | (aq << kAqShift) | (funct5 << kFunct5Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrRFrm(uint8_t funct7, BaseOpcode opcode,
                                      Register rd, Register rs1, Register rs2,
                                      FPURoundingMode frm) {
  DCHECK(rd.is_valid() && rs1.is_valid() && rs2.is_valid() && is_uint3(frm));
  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrI(uint8_t funct3, BaseOpcode opcode,
                                   Register rd, Register rs1, int16_t imm12) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         (is_uint12(imm12) || is_int12(imm12)));
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrI(uint8_t funct3, BaseOpcode opcode,
                                   FPURegister rd, Register rs1,
                                   int16_t imm12) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         (is_uint12(imm12) || is_int12(imm12)));
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrIShift(uint8_t funct6, uint8_t funct3,
                                        BaseOpcode opcode, Register rd,
                                        Register rs1, uint8_t shamt) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         is_uint6(shamt));
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (shamt << kShamtShift) |
                (funct6 << kFunct6Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrIShiftW(uint8_t funct7, uint8_t funct3,
                                         BaseOpcode opcode, Register rd,
                                         Register rs1, uint8_t shamt) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         is_uint5(shamt));
  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
                (rs1.code() << kRs1Shift) | (shamt << kShamtWShift) |
                (funct7 << kFunct7Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrS(uint8_t funct3, BaseOpcode opcode,
                                   Register rs1, Register rs2, int16_t imm12) {
  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
         is_int12(imm12));
  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
                (rs2.code() << kRs2Shift) |
                ((imm12 & 0xfe0) << 20);  // bits 11-5
  emit(instr);
}

void AssemblerRiscvBase::GenInstrS(uint8_t funct3, BaseOpcode opcode,
                                   Register rs1, FPURegister rs2,
                                   int16_t imm12) {
  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
         is_int12(imm12));
  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
                (rs2.code() << kRs2Shift) |
                ((imm12 & 0xfe0) << 20);  // bits 11-5
  emit(instr);
}

void AssemblerRiscvBase::GenInstrB(uint8_t funct3, BaseOpcode opcode,
                                   Register rs1, Register rs2, int16_t imm13) {
  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
         is_int13(imm13) && ((imm13 & 1) == 0));
  Instr instr = opcode | ((imm13 & 0x800) >> 4) |  // bit  11
                ((imm13 & 0x1e) << 7) |            // bits 4-1
                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
                (rs2.code() << kRs2Shift) |
                ((imm13 & 0x7e0) << 20) |  // bits 10-5
                ((imm13 & 0x1000) << 19);  // bit 12
  emit(instr);
}

void AssemblerRiscvBase::GenInstrU(BaseOpcode opcode, Register rd,
                                   int32_t imm20) {
  DCHECK(rd.is_valid() && (is_int20(imm20) || is_uint20(imm20)));
  Instr instr = opcode | (rd.code() << kRdShift) | (imm20 << kImm20Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrJ(BaseOpcode opcode, Register rd,
                                   int32_t imm21) {
  DCHECK(rd.is_valid() && is_int21(imm21) && ((imm21 & 1) == 0));
  Instr instr = opcode | (rd.code() << kRdShift) |
                (imm21 & 0xff000) |          // bits 19-12
                ((imm21 & 0x800) << 9) |     // bit  11
                ((imm21 & 0x7fe) << 20) |    // bits 10-1
                ((imm21 & 0x100000) << 11);  // bit  20
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCR(uint8_t funct4, BaseOpcode opcode,
                                    Register rd, Register rs2) {
  DCHECK(is_uint4(funct4) && rd.is_valid() && rs2.is_valid());
  ShortInstr instr = opcode | (rs2.code() << kRvcRs2Shift) |
                     (rd.code() << kRvcRdShift) | (funct4 << kRvcFunct4Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCA(uint8_t funct6, BaseOpcode opcode,
                                    Register rd, uint8_t funct, Register rs2) {
  DCHECK(is_uint6(funct6) && rd.is_valid() && rs2.is_valid() &&
         is_uint2(funct));
  ShortInstr instr = opcode | ((rs2.code() & 0x7) << kRvcRs2sShift) |
                     ((rd.code() & 0x7) << kRvcRs1sShift) |
                     (funct6 << kRvcFunct6Shift) | (funct << kRvcFunct2Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCI(uint8_t funct3, BaseOpcode opcode,
                                    Register rd, int8_t imm6) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && is_int6(imm6));
  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) |
                     (rd.code() << kRvcRdShift) | ((imm6 & 0x20) << 7) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCIU(uint8_t funct3, BaseOpcode opcode,
                                     Register rd, uint8_t uimm6) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCIU(uint8_t funct3, BaseOpcode opcode,
                                     FPURegister rd, uint8_t uimm6) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCIW(uint8_t funct3, BaseOpcode opcode,
                                     Register rd, uint8_t uimm8) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint8(uimm8));
  ShortInstr instr = opcode | ((uimm8) << 5) |
                     ((rd.code() & 0x7) << kRvcRs2sShift) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCSS(uint8_t funct3, BaseOpcode opcode,
                                     Register rs2, uint8_t uimm6) {
  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCSS(uint8_t funct3, BaseOpcode opcode,
                                     FPURegister rs2, uint8_t uimm6) {
  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCL(uint8_t funct3, BaseOpcode opcode,
                                    Register rd, Register rs1, uint8_t uimm5) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         is_uint5(uimm5));
  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
                     ((rd.code() & 0x7) << kRvcRs2sShift) |
                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCL(uint8_t funct3, BaseOpcode opcode,
                                    FPURegister rd, Register rs1,
                                    uint8_t uimm5) {
  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
         is_uint5(uimm5));
  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
                     ((rd.code() & 0x7) << kRvcRs2sShift) |
                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift);
  emit(instr);
}
void AssemblerRiscvBase::GenInstrCJ(uint8_t funct3, BaseOpcode opcode,
                                    uint16_t uint11) {
  DCHECK(is_uint11(uint11));
  ShortInstr instr = opcode | (funct3 << kRvcFunct3Shift) | (uint11 << 2);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCS(uint8_t funct3, BaseOpcode opcode,
                                    Register rs2, Register rs1, uint8_t uimm5) {
  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
         is_uint5(uimm5));
  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCS(uint8_t funct3, BaseOpcode opcode,
                                    FPURegister rs2, Register rs1,
                                    uint8_t uimm5) {
  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
         is_uint5(uimm5));
  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCB(uint8_t funct3, BaseOpcode opcode,
                                    Register rs1, uint8_t uimm8) {
  DCHECK(is_uint3(funct3) && is_uint8(uimm8));
  ShortInstr instr = opcode | ((uimm8 & 0x1f) << 2) | ((uimm8 & 0xe0) << 5) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
                     (funct3 << kRvcFunct3Shift);
  emit(instr);
}

void AssemblerRiscvBase::GenInstrCBA(uint8_t funct3, uint8_t funct2,
                                     BaseOpcode opcode, Register rs1,
                                     int8_t imm6) {
  DCHECK(is_uint3(funct3) && is_uint2(funct2) && is_int6(imm6));
  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) | ((imm6 & 0x20) << 7) |
                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
                     (funct3 << kRvcFunct3Shift) | (funct2 << 10);
  emit(instr);
}
// ----- Instruction class templates match those in the compiler

void AssemblerRiscvBase::GenInstrBranchCC_rri(uint8_t funct3, Register rs1,
                                              Register rs2, int16_t imm13) {
  GenInstrB(funct3, BRANCH, rs1, rs2, imm13);
}

void AssemblerRiscvBase::GenInstrLoad_ri(uint8_t funct3, Register rd,
                                         Register rs1, int16_t imm12) {
  GenInstrI(funct3, LOAD, rd, rs1, imm12);
}

void AssemblerRiscvBase::GenInstrStore_rri(uint8_t funct3, Register rs1,
                                           Register rs2, int16_t imm12) {
  GenInstrS(funct3, STORE, rs1, rs2, imm12);
}

void AssemblerRiscvBase::GenInstrALU_ri(uint8_t funct3, Register rd,
                                        Register rs1, int16_t imm12) {
  GenInstrI(funct3, OP_IMM, rd, rs1, imm12);
}

void AssemblerRiscvBase::GenInstrShift_ri(bool arithshift, uint8_t funct3,
                                          Register rd, Register rs1,
                                          uint8_t shamt) {
  DCHECK(is_uint6(shamt));
  GenInstrIShift(arithshift << (kArithShiftShift - kFunct6Shift), funct3,
                 OP_IMM, rd, rs1, shamt);
}

void AssemblerRiscvBase::GenInstrALU_rr(uint8_t funct7, uint8_t funct3,
                                        Register rd, Register rs1,
                                        Register rs2) {
  GenInstrR(funct7, funct3, OP, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrCSR_ir(uint8_t funct3, Register rd,
                                        ControlStatusReg csr, Register rs1) {
  GenInstrI(funct3, SYSTEM, rd, rs1, csr);
}

void AssemblerRiscvBase::GenInstrCSR_ii(uint8_t funct3, Register rd,
                                        ControlStatusReg csr, uint8_t imm5) {
  GenInstrI(funct3, SYSTEM, rd, ToRegister(imm5), csr);
}

void AssemblerRiscvBase::GenInstrShiftW_ri(bool arithshift, uint8_t funct3,
                                           Register rd, Register rs1,
                                           uint8_t shamt) {
  GenInstrIShiftW(arithshift << (kArithShiftShift - kFunct7Shift), funct3,
                  OP_IMM_32, rd, rs1, shamt);
}

void AssemblerRiscvBase::GenInstrALUW_rr(uint8_t funct7, uint8_t funct3,
                                         Register rd, Register rs1,
                                         Register rs2) {
  GenInstrR(funct7, funct3, OP_32, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrPriv(uint8_t funct7, Register rs1,
                                      Register rs2) {
  GenInstrR(funct7, 0b000, SYSTEM, ToRegister(0), rs1, rs2);
}

void AssemblerRiscvBase::GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd,
                                           Register rs1, int16_t imm12) {
  GenInstrI(funct3, LOAD_FP, rd, rs1, imm12);
}

void AssemblerRiscvBase::GenInstrStoreFP_rri(uint8_t funct3, Register rs1,
                                             FPURegister rs2, int16_t imm12) {
  GenInstrS(funct3, STORE_FP, rs1, rs2, imm12);
}

void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
                                          FPURegister rd, FPURegister rs1,
                                          FPURegister rs2) {
  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
                                          FPURegister rd, Register rs1,
                                          Register rs2) {
  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
                                          FPURegister rd, FPURegister rs1,
                                          Register rs2) {
  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
                                          Register rd, FPURegister rs1,
                                          Register rs2) {
  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
}

void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
                                          Register rd, FPURegister rs1,
                                          FPURegister rs2) {
  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
}

}  // namespace internal
}  // namespace v8
                                                                                                     node-23.7.0/deps/v8/src/codegen/riscv/base-assembler-riscv.h                                        0000664 0000000 0000000 00000022601 14746647661 0023551 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright (c) 1994-2006 Sun Microsystems Inc.
// All Rights Reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met:
//
// - Redistributions of source code must retain the above copyright notice,
// this list of conditions and the following disclaimer.
//
// - Redistribution in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution.
//
// - Neither the name of Sun Microsystems or the names of contributors may
// be used to endorse or promote products derived from this software without
// specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// The original source code covered by the above license above has been
// modified significantly by Google Inc.
// Copyright 2021 the V8 project authors. All rights reserved.

#ifndef V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_
#define V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_

#include <stdio.h>

#include <memory>
#include <set>

#include "src/codegen/assembler.h"
#include "src/codegen/constant-pool.h"
#include "src/codegen/external-reference.h"
#include "src/codegen/label.h"
#include "src/codegen/machine-type.h"
#include "src/codegen/riscv/constants-riscv.h"
#include "src/codegen/riscv/register-riscv.h"
#include "src/objects/contexts.h"
#include "src/objects/smi.h"

namespace v8 {
namespace internal {

#define DEBUG_PRINTF(...)     \
  if (v8_flags.riscv_debug) { \
    printf(__VA_ARGS__);      \
  }

class SafepointTableBuilder;

class AssemblerRiscvBase {
 protected:
  // Returns the branch offset to the given label from the current code
  // position. Links the label to the current position if it is still unbound.
  // Manages the jump elimination optimization if the second parameter is true.
  enum OffsetSize : int {
    kOffset21 = 21,  // RISCV jal
    kOffset12 = 12,  // RISCV imm12
    kOffset20 = 20,  // RISCV imm20
    kOffset13 = 13,  // RISCV branch
    kOffset32 = 32,  // RISCV auipc + instr_I
    kOffset11 = 11,  // RISCV C_J
    kOffset9 = 9     // RISCV compressed branch
  };
  virtual int32_t branch_offset_helper(Label* L, OffsetSize bits) = 0;

  virtual void emit(Instr x) = 0;
  virtual void emit(ShortInstr x) = 0;
  virtual void emit(uint64_t x) = 0;

  virtual void ClearVectorunit() = 0;
  // Instruction generation.

  // ----- Top-level instruction formats match those in the ISA manual
  // (R, I, S, B, U, J). These match the formats defined in LLVM's
  // RISCVInstrFormats.td.
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
                 Register rs1, Register rs2);
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
                 FPURegister rd, FPURegister rs1, FPURegister rs2);
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
                 FPURegister rs1, Register rs2);
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
                 FPURegister rd, Register rs1, Register rs2);
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
                 FPURegister rd, FPURegister rs1, Register rs2);
  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
                 FPURegister rs1, FPURegister rs2);
  void GenInstrR4(uint8_t funct2, BaseOpcode opcode, Register rd, Register rs1,
                  Register rs2, Register rs3, FPURoundingMode frm);
  void GenInstrR4(uint8_t funct2, BaseOpcode opcode, FPURegister rd,
                  FPURegister rs1, FPURegister rs2, FPURegister rs3,
                  FPURoundingMode frm);
  void GenInstrRAtomic(uint8_t funct5, bool aq, bool rl, uint8_t funct3,
                       Register rd, Register rs1, Register rs2);
  void GenInstrRFrm(uint8_t funct7, BaseOpcode opcode, Register rd,
                    Register rs1, Register rs2, FPURoundingMode frm);
  void GenInstrI(uint8_t funct3, BaseOpcode opcode, Register rd, Register rs1,
                 int16_t imm12);
  void GenInstrI(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
                 Register rs1, int16_t imm12);
  void GenInstrIShift(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
                      Register rd, Register rs1, uint8_t shamt);
  void GenInstrIShiftW(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
                       Register rd, Register rs1, uint8_t shamt);
  void GenInstrS(uint8_t funct3, BaseOpcode opcode, Register rs1, Register rs2,
                 int16_t imm12);
  void GenInstrS(uint8_t funct3, BaseOpcode opcode, Register rs1,
                 FPURegister rs2, int16_t imm12);
  void GenInstrB(uint8_t funct3, BaseOpcode opcode, Register rs1, Register rs2,
                 int16_t imm12);
  void GenInstrU(BaseOpcode opcode, Register rd, int32_t imm20);
  void GenInstrJ(BaseOpcode opcode, Register rd, int32_t imm20);
  void GenInstrCR(uint8_t funct4, BaseOpcode opcode, Register rd, Register rs2);
  void GenInstrCA(uint8_t funct6, BaseOpcode opcode, Register rd, uint8_t funct,
                  Register rs2);
  void GenInstrCI(uint8_t funct3, BaseOpcode opcode, Register rd, int8_t imm6);
  void GenInstrCIU(uint8_t funct3, BaseOpcode opcode, Register rd,
                   uint8_t uimm6);
  void GenInstrCIU(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
                   uint8_t uimm6);
  void GenInstrCIW(uint8_t funct3, BaseOpcode opcode, Register rd,
                   uint8_t uimm8);
  void GenInstrCSS(uint8_t funct3, BaseOpcode opcode, FPURegister rs2,
                   uint8_t uimm6);
  void GenInstrCSS(uint8_t funct3, BaseOpcode opcode, Register rs2,
                   uint8_t uimm6);
  void GenInstrCL(uint8_t funct3, BaseOpcode opcode, Register rd, Register rs1,
                  uint8_t uimm5);
  void GenInstrCL(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
                  Register rs1, uint8_t uimm5);
  void GenInstrCS(uint8_t funct3, BaseOpcode opcode, Register rs2, Register rs1,
                  uint8_t uimm5);
  void GenInstrCS(uint8_t funct3, BaseOpcode opcode, FPURegister rs2,
                  Register rs1, uint8_t uimm5);
  void GenInstrCJ(uint8_t funct3, BaseOpcode opcode, uint16_t uint11);
  void GenInstrCB(uint8_t funct3, BaseOpcode opcode, Register rs1,
                  uint8_t uimm8);
  void GenInstrCBA(uint8_t funct3, uint8_t funct2, BaseOpcode opcode,
                   Register rs1, int8_t imm6);

  // ----- Instruction class templates match those in LLVM's RISCVInstrInfo.td
  void GenInstrBranchCC_rri(uint8_t funct3, Register rs1, Register rs2,
                            int16_t imm12);
  void GenInstrLoad_ri(uint8_t funct3, Register rd, Register rs1,
                       int16_t imm12);
  void GenInstrStore_rri(uint8_t funct3, Register rs1, Register rs2,
                         int16_t imm12);
  void GenInstrALU_ri(uint8_t funct3, Register rd, Register rs1, int16_t imm12);
  void GenInstrShift_ri(bool arithshift, uint8_t funct3, Register rd,
                        Register rs1, uint8_t shamt);
  void GenInstrALU_rr(uint8_t funct7, uint8_t funct3, Register rd, Register rs1,
                      Register rs2);
  void GenInstrCSR_ir(uint8_t funct3, Register rd, ControlStatusReg csr,
                      Register rs1);
  void GenInstrCSR_ii(uint8_t funct3, Register rd, ControlStatusReg csr,
                      uint8_t rs1);
  void GenInstrShiftW_ri(bool arithshift, uint8_t funct3, Register rd,
                         Register rs1, uint8_t shamt);
  void GenInstrALUW_rr(uint8_t funct7, uint8_t funct3, Register rd,
                       Register rs1, Register rs2);
  void GenInstrPriv(uint8_t funct7, Register rs1, Register rs2);
  void GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd, Register rs1,
                         int16_t imm12);
  void GenInstrStoreFP_rri(uint8_t funct3, Register rs1, FPURegister rs2,
                           int16_t imm12);
  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
                        FPURegister rs1, FPURegister rs2);
  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
                        Register rs1, Register rs2);
  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
                        FPURegister rs1, Register rs2);
  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
                        FPURegister rs1, Register rs2);
  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
                        FPURegister rs1, FPURegister rs2);
  virtual void BlockTrampolinePoolFor(int instructions) = 0;
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_
                                                                                                                               node-23.7.0/deps/v8/src/codegen/riscv/base-constants-riscv.cc                                       0000664 0000000 0000000 00000023541 14746647661 0023752 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2021 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/base-constants-riscv.h"

#include "src/codegen/riscv/constants-riscv.h"
#include "src/execution/simulator.h"

namespace v8 {
namespace internal {

// -----------------------------------------------------------------------------
// Registers.

// These register names are defined in a way to match the native disassembler
// formatting. See for example the command "objdump -d <binary file>".
const char* Registers::names_[kNumSimuRegisters] = {
    "zero_reg", "ra", "sp", "gp", "tp",  "t0",  "t1", "t2", "fp", "s1", "a0",
    "a1",       "a2", "a3", "a4", "a5",  "a6",  "a7", "s2", "s3", "s4", "s5",
    "s6",       "s7", "s8", "s9", "s10", "s11", "t3", "t4", "t5", "t6", "pc"};

// List of alias names which can be used when referring to RISC-V registers.
const Registers::RegisterAlias Registers::aliases_[] = {
    {0, "zero"},
    {33, "pc"},
    {8, "s0"},
    {8, "s0_fp"},
    {kInvalidRegister, nullptr}};

const char* Registers::Name(int reg) {
  const char* result;
  if ((0 <= reg) && (reg < kNumSimuRegisters)) {
    result = names_[reg];
  } else {
    result = "noreg";
  }
  return result;
}

int Registers::Number(const char* name) {
  // Look through the canonical names.
  for (int i = 0; i < kNumSimuRegisters; i++) {
    if (strcmp(names_[i], name) == 0) {
      return i;
    }
  }

  // Look through the alias names.
  int i = 0;
  while (aliases_[i].reg != kInvalidRegister) {
    if (strcmp(aliases_[i].name, name) == 0) {
      return aliases_[i].reg;
    }
    i++;
  }

  // No register with the reguested name found.
  return kInvalidRegister;
}

/*
const char* FPURegisters::names_[kNumFPURegisters] = {
    "f0",  "f1",  "f2",  "f3",  "f4",  "f5",  "f6",  "f7",  "f8",  "f9",  "f10",
    "f11", "f12", "f13", "f14", "f15", "f16", "f17", "f18", "f19", "f20", "f21",
    "f22", "f23", "f24", "f25", "f26", "f27", "f28", "f29", "f30", "f31"};
*/
const char* FPURegisters::names_[kNumFPURegisters] = {
    "ft0", "ft1", "ft2",  "ft3",  "ft4", "ft5", "ft6",  "ft7",
    "fs0", "fs1", "fa0",  "fa1",  "fa2", "fa3", "fa4",  "fa5",
    "fa6", "fa7", "fs2",  "fs3",  "fs4", "fs5", "fs6",  "fs7",
    "fs8", "fs9", "fs10", "fs11", "ft8", "ft9", "ft10", "ft11"};

// List of alias names which can be used when referring to RISC-V FP registers.
const FPURegisters::RegisterAlias FPURegisters::aliases_[] = {
    {kInvalidRegister, nullptr}};

const char* FPURegisters::Name(int creg) {
  const char* result;
  if ((0 <= creg) && (creg < kNumFPURegisters)) {
    result = names_[creg];
  } else {
    result = "nocreg";
  }
  return result;
}

int FPURegisters::Number(const char* name) {
  // Look through the canonical names.
  for (int i = 0; i < kNumFPURegisters; i++) {
    if (strcmp(names_[i], name) == 0) {
      return i;
    }
  }

  // Look through the alias names.
  int i = 0;
  while (aliases_[i].creg != kInvalidRegister) {
    if (strcmp(aliases_[i].name, name) == 0) {
      return aliases_[i].creg;
    }
    i++;
  }

  // No Cregister with the reguested name found.
  return kInvalidFPURegister;
}

const char* VRegisters::names_[kNumVRegisters] = {
    "v0",  "v1",  "v2",  "v3",  "v4",  "v5",  "v6",  "v7",  "v8",  "v9",  "v10",
    "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v20", "v21",
    "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31"};

const VRegisters::RegisterAlias VRegisters::aliases_[] = {
    {kInvalidRegister, nullptr}};

const char* VRegisters::Name(int creg) {
  const char* result;
  if ((0 <= creg) && (creg < kNumVRegisters)) {
    result = names_[creg];
  } else {
    result = "nocreg";
  }
  return result;
}

int VRegisters::Number(const char* name) {
  // Look through the canonical names.
  for (int i = 0; i < kNumVRegisters; i++) {
    if (strcmp(names_[i], name) == 0) {
      return i;
    }
  }

  // Look through the alias names.
  int i = 0;
  while (aliases_[i].creg != kInvalidRegister) {
    if (strcmp(aliases_[i].name, name) == 0) {
      return aliases_[i].creg;
    }
    i++;
  }

  // No Cregister with the reguested name found.
  return kInvalidVRegister;
}

bool InstructionBase::IsShortInstruction() const {
  uint8_t FirstByte = *reinterpret_cast<const uint8_t*>(this);
  return (FirstByte & 0x03) <= C2;
}

template <class T>
int InstructionGetters<T>::RvcRdValue() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcRdShift + kRvcRdBits - 1, kRvcRdShift);
}

template <class T>
int InstructionGetters<T>::RvcRs2Value() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcRs2Shift + kRvcRs2Bits - 1, kRvcRs2Shift);
}

template <class T>
int InstructionGetters<T>::RvcRs1sValue() const {
  DCHECK(this->IsShortInstruction());
  return 0b1000 + this->Bits(kRvcRs1sShift + kRvcRs1sBits - 1, kRvcRs1sShift);
}

template <class T>
int InstructionGetters<T>::RvcRs2sValue() const {
  DCHECK(this->IsShortInstruction());
  return 0b1000 + this->Bits(kRvcRs2sShift + kRvcRs2sBits - 1, kRvcRs2sShift);
}

template <class T>
inline int InstructionGetters<T>::RvcFunct6Value() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcFunct6Shift + kRvcFunct6Bits - 1, kRvcFunct6Shift);
}

template <class T>
inline int InstructionGetters<T>::RvcFunct4Value() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcFunct4Shift + kRvcFunct4Bits - 1, kRvcFunct4Shift);
}

template <class T>
inline int InstructionGetters<T>::RvcFunct3Value() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcFunct3Shift + kRvcFunct3Bits - 1, kRvcFunct3Shift);
}

template <class T>
inline int InstructionGetters<T>::RvcFunct2Value() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcFunct2Shift + kRvcFunct2Bits - 1, kRvcFunct2Shift);
}

template <class T>
inline int InstructionGetters<T>::RvcFunct2BValue() const {
  DCHECK(this->IsShortInstruction());
  return this->Bits(kRvcFunct2BShift + kRvcFunct2Bits - 1, kRvcFunct2BShift);
}

template <class T>
uint32_t InstructionGetters<T>::Rvvzimm() const {
  if ((this->InstructionBits() &
       (kBaseOpcodeMask | kFunct3Mask | 0x80000000)) == RO_V_VSETVLI) {
    uint32_t Bits = this->InstructionBits();
    uint32_t zimm = Bits & kRvvZimmMask;
    return zimm >> kRvvZimmShift;
  } else {
    DCHECK_EQ(
        this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
        RO_V_VSETIVLI);
    uint32_t Bits = this->InstructionBits();
    uint32_t zimm = Bits & kRvvZimmMask;
    return (zimm >> kRvvZimmShift) & 0x3FF;
  }
}

template <class T>
uint32_t InstructionGetters<T>::Rvvuimm() const {
  DCHECK_EQ(
      this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
      RO_V_VSETIVLI);
  uint32_t Bits = this->InstructionBits();
  uint32_t uimm = Bits & kRvvUimmMask;
  return uimm >> kRvvUimmShift;
}

template <class T>
bool InstructionGetters<T>::IsLoad() {
  return OperandFunct3() == RO_LB || OperandFunct3() == RO_LBU ||
         OperandFunct3() == RO_LH || OperandFunct3() == RO_LHU ||
         OperandFunct3() == RO_LW ||
#ifdef V8_TARGET_ARCH_RISCV64
         OperandFunct3() == RO_LD || OperandFunct3() == RO_LWU ||
#endif
         BaseOpcode() == LOAD_FP;
}

template <class T>
bool InstructionGetters<T>::IsStore() {
  return OperandFunct3() == RO_SB || OperandFunct3() == RO_SH ||
         OperandFunct3() == RO_SW ||
#ifdef V8_TARGET_ARCH_RISCV64
         OperandFunct3() == RO_SD ||
#endif
         BaseOpcode() == STORE_FP;
}

template class InstructionGetters<InstructionBase>;
#ifdef USE_SIMULATOR
template class InstructionGetters<SimInstructionBase>;
#endif

InstructionBase::Type InstructionBase::InstructionType() const {
  if (IsIllegalInstruction()) {
    return kUnsupported;
  }
  // RV64C Instruction
  if (v8_flags.riscv_c_extension && IsShortInstruction()) {
    switch (InstructionBits() & kRvcOpcodeMask) {
      case RO_C_ADDI4SPN:
        return kCIWType;
      case RO_C_FLD:
      case RO_C_LW:
#ifdef V8_TARGET_ARCH_RISCV64
      case RO_C_LD:
#endif
        return kCLType;
      case RO_C_FSD:
      case RO_C_SW:
#ifdef V8_TARGET_ARCH_RISCV64
      case RO_C_SD:
#endif
        return kCSType;
      case RO_C_NOP_ADDI:
      case RO_C_LI:
#ifdef V8_TARGET_ARCH_RISCV64
      case RO_C_ADDIW:
#endif
      case RO_C_LUI_ADD:
        return kCIType;
      case RO_C_MISC_ALU:
        if (Bits(11, 10) != 0b11)
          return kCBType;
        else
          return kCAType;
      case RO_C_J:
        return kCJType;
      case RO_C_BEQZ:
      case RO_C_BNEZ:
        return kCBType;
      case RO_C_SLLI:
      case RO_C_FLDSP:
      case RO_C_LWSP:
#ifdef V8_TARGET_ARCH_RISCV64
      case RO_C_LDSP:
#endif
        return kCIType;
      case RO_C_JR_MV_ADD:
        return kCRType;
      case RO_C_FSDSP:
      case RO_C_SWSP:
#ifdef V8_TARGET_ARCH_RISCV64
      case RO_C_SDSP:
#endif
        return kCSSType;
      default:
        break;
    }
  } else {
    // RISCV routine
    switch (InstructionBits() & kBaseOpcodeMask) {
      case LOAD:
        return kIType;
      case LOAD_FP:
        return kIType;
      case MISC_MEM:
        return kIType;
      case OP_IMM:
        return kIType;
      case AUIPC:
        return kUType;
      case OP_IMM_32:
        return kIType;
      case STORE:
        return kSType;
      case STORE_FP:
        return kSType;
      case AMO:
        return kRType;
      case OP:
        return kRType;
      case LUI:
        return kUType;
      case OP_32:
        return kRType;
      case MADD:
      case MSUB:
      case NMSUB:
      case NMADD:
        return kR4Type;
      case OP_FP:
        return kRType;
      case BRANCH:
        return kBType;
      case JALR:
        return kIType;
      case JAL:
        return kJType;
      case SYSTEM:
        return kIType;
      case OP_V:
        return kVType;
    }
  }
  return kUnsupported;
}

}  // namespace internal
}  // namespace v8
                                                                                                                                                               node-23.7.0/deps/v8/src/codegen/riscv/base-constants-riscv.h                                        0000664 0000000 0000000 00000120302 14746647661 0023605 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_
#define V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_

#include "src/base/logging.h"
#include "src/base/macros.h"
#include "src/common/globals.h"
#include "src/flags/flags.h"

#ifdef DEBUG
#define UNIMPLEMENTED_RISCV()                                               \
  v8::internal::PrintF("%s, \tline %d: \tfunction %s  not implemented. \n", \
                       __FILE__, __LINE__, __func__);
#else
#define UNIMPLEMENTED_RISCV()
#endif

#define UNSUPPORTED_RISCV()                                        \
  v8::internal::PrintF("Unsupported instruction %d.\n", __LINE__); \
  UNIMPLEMENTED();

enum Endianness { kLittle, kBig };

#if defined(V8_TARGET_LITTLE_ENDIAN)
static const Endianness kArchEndian = kLittle;
#elif defined(V8_TARGET_BIG_ENDIAN)
static const Endianness kArchEndian = kBig;
#else
#error Unknown endianness
#endif

#if defined(V8_TARGET_LITTLE_ENDIAN)
const uint32_t kLeastSignificantByteInInt32Offset = 0;
const uint32_t kLessSignificantWordInDoublewordOffset = 0;
#elif defined(V8_TARGET_BIG_ENDIAN)
const uint32_t kLeastSignificantByteInInt32Offset = 3;
const uint32_t kLessSignificantWordInDoublewordOffset = 4;
#else
#error Unknown endianness
#endif

#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif
#include <inttypes.h>

// Defines constants and accessor classes to assemble, disassemble and
// simulate RISC-V instructions.
//
// See: The RISC-V Instruction Set Manual
//      Volume I: User-Level ISA
// Try https://content.riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf.
namespace v8 {
namespace internal {
using Opcode = uint32_t;

// Actual value of root register is offset from the root array's start
// to take advantage of negative displacement values.
constexpr int kRootRegisterBias = 256;

#define RVV_LMUL(V) \
  V(m1)             \
  V(m2)             \
  V(m4)             \
  V(m8)             \
  V(RESERVERD)      \
  V(mf8)            \
  V(mf4)            \
  V(mf2)

enum Vlmul {
#define DEFINE_FLAG(name) name,
  RVV_LMUL(DEFINE_FLAG)
#undef DEFINE_FLAG
      kVlInvalid
};

#define RVV_SEW(V) \
  V(E8)            \
  V(E16)           \
  V(E32)           \
  V(E64)

#define DEFINE_FLAG(name) name,
enum VSew {
  RVV_SEW(DEFINE_FLAG)
#undef DEFINE_FLAG
      kVsInvalid
};

// RISC-V can perform PC-relative jumps within a 32-bit range using the
// following two instructions:
//   auipc   t6, imm20    ; t0 = PC + imm20 * 2^12
//   jalr    ra, t6, imm12; ra = PC + 4, PC = t0 + imm12,
// Both imm20 and imm12 are treated as two's-complement signed values, usually
// calculated as:
//   imm20 = (offset + 0x800) >> 12
//   imm12 = offset & 0xfff
// offset is the signed offset from the auipc instruction. Adding 0x800 handles
// the offset, but if the offset is >= 2^31 - 2^11, it will overflow. Therefore,
// the true 32-bit range is:
//   [-2^31 - 2^11, 2^31 - 2^11)
constexpr size_t kMaxPCRelativeCodeRangeInMB = 2047;

// -----------------------------------------------------------------------------
// Registers and FPURegisters.

// Number of general purpose registers.
const int kNumRegisters = 32;
const int kInvalidRegister = -1;

// Number of registers with pc.
const int kNumSimuRegisters = 33;

// In the simulator, the PC register is simulated as the 34th register.
const int kPCRegister = 34;

// Number coprocessor registers.
const int kNumFPURegisters = 32;
const int kInvalidFPURegister = -1;

// Number vectotr registers
const int kNumVRegisters = 32;
const int kInvalidVRegister = -1;
// 'pref' instruction hints
const int32_t kPrefHintLoad = 0;
const int32_t kPrefHintStore = 1;
const int32_t kPrefHintLoadStreamed = 4;
const int32_t kPrefHintStoreStreamed = 5;
const int32_t kPrefHintLoadRetained = 6;
const int32_t kPrefHintStoreRetained = 7;
const int32_t kPrefHintWritebackInvalidate = 25;
const int32_t kPrefHintPrepareForStore = 30;

// Helper functions for converting between register numbers and names.
class Registers {
 public:
  // Return the name of the register.
  static const char* Name(int reg);

  // Lookup the register number for the name provided.
  static int Number(const char* name);

  struct RegisterAlias {
    int reg;
    const char* name;
  };

 private:
  static const char* names_[kNumSimuRegisters];
  static const RegisterAlias aliases_[];
};

// Helper functions for converting between register numbers and names.
class FPURegisters {
 public:
  // Return the name of the register.
  static const char* Name(int reg);

  // Lookup the register number for the name provided.
  static int Number(const char* name);

  struct RegisterAlias {
    int creg;
    const char* name;
  };

 private:
  static const char* names_[kNumFPURegisters];
  static const RegisterAlias aliases_[];
};

class VRegisters {
 public:
  // Return the name of the register.
  static const char* Name(int reg);

  // Lookup the register number for the name provided.
  static int Number(const char* name);

  struct RegisterAlias {
    int creg;
    const char* name;
  };

 private:
  static const char* names_[kNumVRegisters];
  static const RegisterAlias aliases_[];
};

// -----------------------------------------------------------------------------
// Instructions encoding constants.

// On RISCV all instructions are 32 bits, except for RVC.
using Instr = int32_t;
using ShortInstr = int16_t;

// Special Software Interrupt codes when used in the presence of the RISC-V
// simulator.
enum SoftwareInterruptCodes {
  // Transition to C code.
  call_rt_redirected = 0xfffff
};

// On RISC-V Simulator breakpoints can have different codes:
// - Breaks between 0 and kMaxWatchpointCode are treated as simple watchpoints,
//   the simulator will run through them and print the registers.
// - Breaks between kMaxWatchpointCode and kMaxStopCode are treated as stop()
//   instructions (see Assembler::stop()).
// - Breaks larger than kMaxStopCode are simple breaks, dropping you into the
//   debugger.
const uint32_t kMaxTracepointCode = 63;
const uint32_t kMaxWatchpointCode = 31;
const uint32_t kMaxStopCode = 127;
static_assert(kMaxWatchpointCode < kMaxStopCode);
static_assert(kMaxTracepointCode < kMaxStopCode);

// Debug parameters.
//
// For example:
//
// __ Debug(TRACE_ENABLE | LOG_TRACE);
// starts tracing: set v8_flags.trace-sim is true.
// __ Debug(TRACE_ENABLE | LOG_REGS);
// PrintAllregs.
// __ Debug(TRACE_DISABLE | LOG_TRACE);
// stops tracing: set v8_flags.trace-sim is false.
const unsigned kDebuggerTracingDirectivesMask = 0b111 << 3;
enum DebugParameters : uint32_t {
  NO_PARAM = 1 << 5,
  BREAK = 1 << 0,
  LOG_TRACE = 1 << 1,
  LOG_REGS = 1 << 2,
  LOG_ALL = LOG_TRACE,
  // Trace control.
  TRACE_ENABLE = 1 << 3 | NO_PARAM,
  TRACE_DISABLE = 1 << 4 | NO_PARAM,
};

// ----- Fields offset and length.
// RISCV constants
const int kBaseOpcodeShift = 0;
const int kBaseOpcodeBits = 7;
const int kFunct6Shift = 26;
const int kFunct6Bits = 6;
const int kFunct7Shift = 25;
const int kFunct7Bits = 7;
const int kFunct5Shift = 27;
const int kFunct5Bits = 5;
const int kFunct3Shift = 12;
const int kFunct3Bits = 3;
const int kFunct2Shift = 25;
const int kFunct2Bits = 2;
const int kRs1Shift = 15;
const int kRs1Bits = 5;
const int kVs1Shift = 15;
const int kVs1Bits = 5;
const int kVs2Shift = 20;
const int kVs2Bits = 5;
const int kVdShift = 7;
const int kVdBits = 5;
const int kRs2Shift = 20;
const int kRs2Bits = 5;
const int kRs3Shift = 27;
const int kRs3Bits = 5;
const int kRdShift = 7;
const int kRdBits = 5;
const int kRlShift = 25;
const int kAqShift = 26;
const int kImm12Shift = 20;
const int kImm12Bits = 12;
const int kImm11Shift = 2;
const int kImm11Bits = 11;
const int kShamtShift = 20;
const int kShamtBits = 5;
const uint32_t kShamtMask = (((1 << kShamtBits) - 1) << kShamtShift);
const int kShamtWShift = 20;
// FIXME: remove this once we have a proper way to handle the wide shift amount
const int kShamtWBits = 6;
const int kArithShiftShift = 30;
const int kImm20Shift = 12;
const int kImm20Bits = 20;
const int kCsrShift = 20;
const int kCsrBits = 12;
const int kMemOrderBits = 4;
const int kPredOrderShift = 24;
const int kSuccOrderShift = 20;

// for C extension
const int kRvcFunct4Shift = 12;
const int kRvcFunct4Bits = 4;
const int kRvcFunct3Shift = 13;
const int kRvcFunct3Bits = 3;
const int kRvcRs1Shift = 7;
const int kRvcRs1Bits = 5;
const int kRvcRs2Shift = 2;
const int kRvcRs2Bits = 5;
const int kRvcRdShift = 7;
const int kRvcRdBits = 5;
const int kRvcRs1sShift = 7;
const int kRvcRs1sBits = 3;
const int kRvcRs2sShift = 2;
const int kRvcRs2sBits = 3;
const int kRvcFunct2Shift = 5;
const int kRvcFunct2BShift = 10;
const int kRvcFunct2Bits = 2;
const int kRvcFunct6Shift = 10;
const int kRvcFunct6Bits = 6;

const uint32_t kRvcOpcodeMask =
    0b11 | (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
const uint32_t kRvcFunct3Mask =
    (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
const uint32_t kRvcFunct4Mask =
    (((1 << kRvcFunct4Bits) - 1) << kRvcFunct4Shift);
const uint32_t kRvcFunct6Mask =
    (((1 << kRvcFunct6Bits) - 1) << kRvcFunct6Shift);
const uint32_t kRvcFunct2Mask =
    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2Shift);
const uint32_t kRvcFunct2BMask =
    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2BShift);
const uint32_t kCRTypeMask = kRvcOpcodeMask | kRvcFunct4Mask;
const uint32_t kCSTypeMask = kRvcOpcodeMask | kRvcFunct6Mask;
const uint32_t kCATypeMask = kRvcOpcodeMask | kRvcFunct6Mask | kRvcFunct2Mask;
const uint32_t kRvcBImm8Mask = (((1 << 5) - 1) << 2) | (((1 << 3) - 1) << 10);

// for RVV extension
constexpr int kRvvELEN = 64;
#ifdef RVV_VLEN
constexpr int kRvvVLEN = RVV_VLEN;
// TODO(riscv): support rvv 256/512/1024
static_assert(
    kRvvVLEN == 128,
    "RVV extension only supports 128bit wide VLEN at current RISC-V backend.");
#else
constexpr int kRvvVLEN = 128;
#endif
constexpr int kRvvSLEN = kRvvVLEN;

const int kRvvFunct6Shift = 26;
const int kRvvFunct6Bits = 6;
const uint32_t kRvvFunct6Mask =
    (((1 << kRvvFunct6Bits) - 1) << kRvvFunct6Shift);

const int kRvvVmBits = 1;
const int kRvvVmShift = 25;
const uint32_t kRvvVmMask = (((1 << kRvvVmBits) - 1) << kRvvVmShift);

const int kRvvVs2Bits = 5;
const int kRvvVs2Shift = 20;
const uint32_t kRvvVs2Mask = (((1 << kRvvVs2Bits) - 1) << kRvvVs2Shift);

const int kRvvVs1Bits = 5;
const int kRvvVs1Shift = 15;
const uint32_t kRvvVs1Mask = (((1 << kRvvVs1Bits) - 1) << kRvvVs1Shift);

const int kRvvRs1Bits = kRvvVs1Bits;
const int kRvvRs1Shift = kRvvVs1Shift;
const uint32_t kRvvRs1Mask = (((1 << kRvvRs1Bits) - 1) << kRvvRs1Shift);

const int kRvvRs2Bits = 5;
const int kRvvRs2Shift = 20;
const uint32_t kRvvRs2Mask = (((1 << kRvvRs2Bits) - 1) << kRvvRs2Shift);

const int kRvvImm5Bits = kRvvVs1Bits;
const int kRvvImm5Shift = kRvvVs1Shift;
const uint32_t kRvvImm5Mask = (((1 << kRvvImm5Bits) - 1) << kRvvImm5Shift);

const int kRvvVdBits = 5;
const int kRvvVdShift = 7;
const uint32_t kRvvVdMask = (((1 << kRvvVdBits) - 1) << kRvvVdShift);

const int kRvvRdBits = kRvvVdBits;
const int kRvvRdShift = kRvvVdShift;
const uint32_t kRvvRdMask = (((1 << kRvvRdBits) - 1) << kRvvRdShift);

const int kRvvZimmBits = 11;
const int kRvvZimmShift = 20;
const uint32_t kRvvZimmMask = (((1 << kRvvZimmBits) - 1) << kRvvZimmShift);

const int kRvvUimmShift = kRvvRs1Shift;
const int kRvvUimmBits = kRvvRs1Bits;
const uint32_t kRvvUimmMask = (((1 << kRvvUimmBits) - 1) << kRvvUimmShift);

const int kRvvWidthBits = 3;
const int kRvvWidthShift = 12;
const uint32_t kRvvWidthMask = (((1 << kRvvWidthBits) - 1) << kRvvWidthShift);

const int kRvvMopBits = 2;
const int kRvvMopShift = 26;
const uint32_t kRvvMopMask = (((1 << kRvvMopBits) - 1) << kRvvMopShift);

const int kRvvMewBits = 1;
const int kRvvMewShift = 28;
const uint32_t kRvvMewMask = (((1 << kRvvMewBits) - 1) << kRvvMewShift);

const int kRvvNfBits = 3;
const int kRvvNfShift = 29;
const uint32_t kRvvNfMask = (((1 << kRvvNfBits) - 1) << kRvvNfShift);

// RISCV Instruction bit masks
const uint32_t kBaseOpcodeMask = ((1 << kBaseOpcodeBits) - 1)
                                 << kBaseOpcodeShift;
const uint32_t kFunct3Mask = ((1 << kFunct3Bits) - 1) << kFunct3Shift;
const uint32_t kFunct5Mask = ((1 << kFunct5Bits) - 1) << kFunct5Shift;
const uint32_t kFunct6Mask = ((1 << kFunct6Bits) - 1) << kFunct6Shift;
const uint32_t kFunct7Mask = ((1 << kFunct7Bits) - 1) << kFunct7Shift;
const uint32_t kFunct2Mask = 0b11 << kFunct7Shift;
const uint32_t kRTypeMask = kBaseOpcodeMask | kFunct3Mask | kFunct7Mask;
const uint32_t kRATypeMask = kBaseOpcodeMask | kFunct3Mask | kFunct5Mask;
const uint32_t kRFPTypeMask = kBaseOpcodeMask | kFunct7Mask;
const uint32_t kR4TypeMask = kBaseOpcodeMask | kFunct3Mask | kFunct2Mask;
const uint32_t kITypeMask = kBaseOpcodeMask | kFunct3Mask;
const uint32_t kSTypeMask = kBaseOpcodeMask | kFunct3Mask;
const uint32_t kBTypeMask = kBaseOpcodeMask | kFunct3Mask;
const uint32_t kUTypeMask = kBaseOpcodeMask;
const uint32_t kJTypeMask = kBaseOpcodeMask;
const uint32_t kVTypeMask = kRvvFunct6Mask | kFunct3Mask | kBaseOpcodeMask;
const uint32_t kRs1FieldMask = ((1 << kRs1Bits) - 1) << kRs1Shift;
const uint32_t kRs2FieldMask = ((1 << kRs2Bits) - 1) << kRs2Shift;
const uint32_t kRs3FieldMask = ((1 << kRs3Bits) - 1) << kRs3Shift;
const uint32_t kRdFieldMask = ((1 << kRdBits) - 1) << kRdShift;
const uint32_t kBImm12Mask = kFunct7Mask | kRdFieldMask;
const uint32_t kImm20Mask = ((1 << kImm20Bits) - 1) << kImm20Shift;
const uint32_t kImm12Mask = ((1 << kImm12Bits) - 1) << kImm12Shift;
const uint32_t kImm11Mask = ((1 << kImm11Bits) - 1) << kImm11Shift;
const uint32_t kImm31_12Mask = ((1 << 20) - 1) << 12;
const uint32_t kImm19_0Mask = ((1 << 20) - 1);

const int kNopByte = 0x00000013;
// Original MIPS constants
const int kImm16Shift = 0;
const int kImm16Bits = 16;
const uint32_t kImm16Mask = ((1 << kImm16Bits) - 1) << kImm16Shift;

// ----- Emulated conditions.
// On RISC-V we use this enum to abstract from conditional branch instructions.
// The 'U' prefix is used to specify unsigned comparisons.
// Opposite conditions must be paired as odd/even numbers
// because 'NegateCondition' function flips LSB to negate condition.
enum Condition : int {  // Any value < 0 is considered no_condition.
  overflow = 0,
  no_overflow = 1,
  Uless = 2,
  Ugreater_equal = 3,
  Uless_equal = 4,
  Ugreater = 5,
  equal = 6,
  not_equal = 7,  // Unordered or Not Equal.
  less = 8,
  greater_equal = 9,
  less_equal = 10,
  greater = 11,
  cc_always = 12,

  // Aliases.
  eq = equal,
  ne = not_equal,
  ge = greater_equal,
  lt = less,
  gt = greater,
  le = less_equal,
  al = cc_always,
  ult = Uless,
  uge = Ugreater_equal,
  ule = Uless_equal,
  ugt = Ugreater,

  // Unified cross-platform condition names/aliases.
  kEqual = equal,
  kNotEqual = not_equal,
  kLessThan = less,
  kGreaterThan = greater,
  kLessThanEqual = less_equal,
  kGreaterThanEqual = greater_equal,
  kUnsignedLessThan = Uless,
  kUnsignedGreaterThan = Ugreater,
  kUnsignedLessThanEqual = Uless_equal,
  kUnsignedGreaterThanEqual = Ugreater_equal,
  kOverflow = overflow,
  kNoOverflow = no_overflow,
  kZero = equal,
  kNotZero = not_equal,
};

// Returns the equivalent of !cc.
inline Condition NegateCondition(Condition cc) {
  DCHECK(cc != cc_always);
  return static_cast<Condition>(cc ^ 1);
}

inline Condition NegateFpuCondition(Condition cc) {
  DCHECK(cc != cc_always);
  switch (cc) {
    case ult:
      return ge;
    case ugt:
      return le;
    case uge:
      return lt;
    case ule:
      return gt;
    case lt:
      return uge;
    case gt:
      return ule;
    case ge:
      return ult;
    case le:
      return ugt;
    case eq:
      return ne;
    case ne:
      return eq;
    default:
      return cc;
  }
}

// ----- Coprocessor conditions.
enum FPUCondition {
  kNoFPUCondition = -1,
  EQ = 0x02,  // Ordered and Equal
  NE = 0x03,  // Unordered or Not Equal
  LT = 0x04,  // Ordered and Less Than
  GE = 0x05,  // Ordered and Greater Than or Equal
  LE = 0x06,  // Ordered and Less Than or Equal
  GT = 0x07,  // Ordered and Greater Than
};

enum CheckForInexactConversion {
  kCheckForInexactConversion,
  kDontCheckForInexactConversion
};

enum class MaxMinKind : int { kMin = 0, kMax = 1 };

// ----------------------------------------------------------------------------
// RISCV flags

enum ControlStatusReg {
  csr_fflags = 0x001,   // Floating-Point Accrued Exceptions (RW)
  csr_frm = 0x002,      // Floating-Point Dynamic Rounding Mode (RW)
  csr_fcsr = 0x003,     // Floating-Point Control and Status Register (RW)
  csr_cycle = 0xc00,    // Cycle counter for RDCYCLE instruction (RO)
  csr_time = 0xc01,     // Timer for RDTIME instruction (RO)
  csr_instret = 0xc02,  // Insns-retired counter for RDINSTRET instruction (RO)
  csr_cycleh = 0xc80,   // Upper 32 bits of cycle, RV32I only (RO)
  csr_timeh = 0xc81,    // Upper 32 bits of time, RV32I only (RO)
  csr_instreth = 0xc82  // Upper 32 bits of instret, RV32I only (RO)
};

enum FFlagsMask {
  kInvalidOperation = 0b10000,  // NV: Invalid
  kDivideByZero = 0b1000,       // DZ:  Divide by Zero
  kFPUOverflow = 0b100,         // OF: Overflow
  kUnderflow = 0b10,            // UF: Underflow
  kInexact = 0b1                // NX:  Inexact
};

enum FPURoundingMode {
  RNE = 0b000,  // Round to Nearest, ties to Even
  RTZ = 0b001,  // Round towards Zero
  RDN = 0b010,  // Round Down (towards -infinity)
  RUP = 0b011,  // Round Up (towards +infinity)
  RMM = 0b100,  // Round to Nearest, tiest to Max Magnitude
  DYN = 0b111   // In instruction's rm field, selects dynamic rounding mode;
                // In Rounding Mode register, Invalid
};

enum MemoryOdering {
  PSI = 0b1000,  // PI or SI
  PSO = 0b0100,  // PO or SO
  PSR = 0b0010,  // PR or SR
  PSW = 0b0001,  // PW or SW
  PSIORW = PSI | PSO | PSR | PSW
};

const int kFloat32ExponentBias = 127;
const int kFloat32MantissaBits = 23;
const int kFloat32ExponentBits = 8;
const int kFloat64ExponentBias = 1023;
const int kFloat64MantissaBits = 52;
const int kFloat64ExponentBits = 11;

enum FClassFlag {
  kNegativeInfinity = 1,
  kNegativeNormalNumber = 1 << 1,
  kNegativeSubnormalNumber = 1 << 2,
  kNegativeZero = 1 << 3,
  kPositiveZero = 1 << 4,
  kPositiveSubnormalNumber = 1 << 5,
  kPositiveNormalNumber = 1 << 6,
  kPositiveInfinity = 1 << 7,
  kSignalingNaN = 1 << 8,
  kQuietNaN = 1 << 9
};

enum TailAgnosticType {
  ta = 0x1,  // Tail agnostic
  tu = 0x0,  // Tail undisturbed
};

enum MaskAgnosticType {
  ma = 0x1,  // Mask agnostic
  mu = 0x0,  // Mask undisturbed
};
enum MaskType {
  Mask = 0x0,  // use the mask
  NoMask = 0x1,
};

// -----------------------------------------------------------------------------
// Hints.

// Branch hints are not used on RISC-V.  They are defined so that they can
// appear in shared function signatures, but will be ignored in RISC-V
// implementations.
enum Hint { no_hint = 0 };

inline Hint NegateHint(Hint hint) { return no_hint; }

enum BaseOpcode : uint32_t {
  LOAD = 0b0000011,      // I form: LB LH LW LBU LHU
  LOAD_FP = 0b0000111,   // I form: FLW FLD FLQ
  MISC_MEM = 0b0001111,  // I special form: FENCE FENCE.I
  OP_IMM = 0b0010011,    // I form: ADDI SLTI SLTIU XORI ORI ANDI SLLI SRLI SRAI
  // Note: SLLI/SRLI/SRAI I form first, then func3 001/101 => R type
  AUIPC = 0b0010111,      // U form: AUIPC
  OP_IMM_32 = 0b0011011,  // I form: ADDIW SLLIW SRLIW SRAIW
  // Note:  SRLIW SRAIW I form first, then func3 101 special shift encoding
  STORE = 0b0100011,     // S form: SB SH SW SD
  STORE_FP = 0b0100111,  // S form: FSW FSD FSQ
  AMO = 0b0101111,       // R form: All A instructions
  OP = 0b0110011,      // R: ADD SUB SLL SLT SLTU XOR SRL SRA OR AND and 32M set
  LUI = 0b0110111,     // U form: LUI
  OP_32 = 0b0111011,   // R: ADDW SUBW SLLW SRLW SRAW MULW DIVW DIVUW REMW REMUW
  MADD = 0b1000011,    // R4 type: FMADD.S FMADD.D FMADD.Q
  MSUB = 0b1000111,    // R4 type: FMSUB.S FMSUB.D FMSUB.Q
  NMSUB = 0b1001011,   // R4 type: FNMSUB.S FNMSUB.D FNMSUB.Q
  NMADD = 0b1001111,   // R4 type: FNMADD.S FNMADD.D FNMADD.Q
  OP_FP = 0b1010011,   // R type: Q ext
  BRANCH = 0b1100011,  // B form: BEQ BNE, BLT, BGE, BLTU BGEU
  JALR = 0b1100111,    // I form: JALR
  JAL = 0b1101111,     // J form: JAL
  SYSTEM = 0b1110011,  // I form: ECALL EBREAK Zicsr ext
  OP_V = 0b1010111,    // V form: RVV

  // C extension
  C0 = 0b00,
  C1 = 0b01,
  C2 = 0b10,
  FUNCT2_0 = 0b00,
  FUNCT2_1 = 0b01,
  FUNCT2_2 = 0b10,
  FUNCT2_3 = 0b11,
};

// -----------------------------------------------------------------------------
// Specific instructions, constants, and masks.
// These constants are declared in assembler-riscv64.cc, as they use named
// registers and other constants.

// An Illegal instruction
const Instr kIllegalInstr = 0;  // All other bits are 0s (i.e., ecall)
// An ECALL instruction, used for redirected real time call
const Instr rtCallRedirInstr = SYSTEM;  // All other bits are 0s (i.e., ecall)
// An EBreak instruction, used for debugging and semi-hosting
const Instr kBreakInstr = SYSTEM | 1 << kImm12Shift;  // ebreak

constexpr uint8_t kInstrSize = 4;
constexpr uint8_t kShortInstrSize = 2;
constexpr uint8_t kInstrSizeLog2 = 2;

class InstructionBase {
 public:
  enum {
    // On RISC-V, PC cannot actually be directly accessed. We behave as if PC
    // was always the value of the current instruction being executed.
    kPCReadOffset = 0
  };

  // Instruction type.
  enum Type {
    kRType,
    kR4Type,  // Special R4 for Q extension
    kIType,
    kSType,
    kBType,
    kUType,
    kJType,
    // C extension
    kCRType,
    kCIType,
    kCSSType,
    kCIWType,
    kCLType,
    kCSType,
    kCAType,
    kCBType,
    kCJType,
    // V extension
    kVType,
    kVLType,
    kVSType,
    kVAMOType,
    kVIVVType,
    kVFVVType,
    kVMVVType,
    kVIVIType,
    kVIVXType,
    kVFVFType,
    kVMVXType,
    kVSETType,
    kUnsupported = -1
  };

  inline bool IsIllegalInstruction() const {
    uint16_t FirstHalfWord = *reinterpret_cast<const uint16_t*>(this);
    return FirstHalfWord == 0;
  }

  bool IsShortInstruction() const;

  inline uint8_t InstructionSize() const {
    return (v8_flags.riscv_c_extension && this->IsShortInstruction())
               ? kShortInstrSize
               : kInstrSize;
  }

  // Get the raw instruction bits.
  inline Instr InstructionBits() const {
    if (v8_flags.riscv_c_extension && this->IsShortInstruction()) {
      return 0x0000FFFF & (*reinterpret_cast<const ShortInstr*>(this));
    }
    return *reinterpret_cast<const Instr*>(this);
  }

  // Set the raw instruction bits to value.
  inline void SetInstructionBits(Instr value) {
    *reinterpret_cast<Instr*>(this) = value;
  }

  // Read one particular bit out of the instruction bits.
  inline int Bit(int nr) const { return (InstructionBits() >> nr) & 1; }

  // Read a bit field out of the instruction bits.
  inline int Bits(int hi, int lo) const {
    return (InstructionBits() >> lo) & ((2U << (hi - lo)) - 1);
  }

  // Accessors for the different named fields used in the RISC-V encoding.
  inline BaseOpcode BaseOpcodeValue() const {
    return static_cast<BaseOpcode>(
        Bits(kBaseOpcodeShift + kBaseOpcodeBits - 1, kBaseOpcodeShift));
  }

  // Return the fields at their original place in the instruction encoding.
  inline BaseOpcode BaseOpcodeFieldRaw() const {
    return static_cast<BaseOpcode>(InstructionBits() & kBaseOpcodeMask);
  }

  // Safe to call within R-type instructions
  inline int Funct7FieldRaw() const { return InstructionBits() & kFunct7Mask; }

  // Safe to call within R-type instructions
  inline int Funct6FieldRaw() const { return InstructionBits() & kFunct6Mask; }

  // Safe to call within R-, I-, S-, or B-type instructions
  inline int Funct3FieldRaw() const { return InstructionBits() & kFunct3Mask; }

  // Safe to call within R-, I-, S-, or B-type instructions
  inline int Rs1FieldRawNoAssert() const {
    return InstructionBits() & kRs1FieldMask;
  }

  // Safe to call within R-, S-, or B-type instructions
  inline int Rs2FieldRawNoAssert() const {
    return InstructionBits() & kRs2FieldMask;
  }

  // Safe to call within R4-type instructions
  inline int Rs3FieldRawNoAssert() const {
    return InstructionBits() & kRs3FieldMask;
  }

  inline int32_t ITypeBits() const { return InstructionBits() & kITypeMask; }

  inline int32_t InstructionOpcodeType() const {
    if (IsShortInstruction()) {
      return InstructionBits() & kRvcOpcodeMask;
    } else {
      return InstructionBits() & kBaseOpcodeMask;
    }
  }

  // Get the encoding type of the instruction.
  Type InstructionType() const;

 protected:
  InstructionBase() {}
};

template <class T>
class InstructionGetters : public T {
 public:
  uint32_t OperandFunct3() const {
    return this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask);
  }
  bool IsLoad();
  bool IsStore();
  inline int BaseOpcode() const {
    return this->InstructionBits() & kBaseOpcodeMask;
  }

  inline int RvcOpcode() const {
    DCHECK(this->IsShortInstruction());
    return this->InstructionBits() & kRvcOpcodeMask;
  }

  inline int Rs1Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kRType ||
           this->InstructionType() == InstructionBase::kR4Type ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType ||
           this->InstructionType() == InstructionBase::kBType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kVType);
    return this->Bits(kRs1Shift + kRs1Bits - 1, kRs1Shift);
  }

  inline int Rs2Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kRType ||
           this->InstructionType() == InstructionBase::kR4Type ||
           this->InstructionType() == InstructionBase::kSType ||
           this->InstructionType() == InstructionBase::kBType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kVType);
    return this->Bits(kRs2Shift + kRs2Bits - 1, kRs2Shift);
  }

  inline int Rs3Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kR4Type);
    return this->Bits(kRs3Shift + kRs3Bits - 1, kRs3Shift);
  }

  inline int Vs1Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType);
    return this->Bits(kVs1Shift + kVs1Bits - 1, kVs1Shift);
  }

  inline int Vs2Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType);
    return this->Bits(kVs2Shift + kVs2Bits - 1, kVs2Shift);
  }

  inline int VdValue() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType);
    return this->Bits(kVdShift + kVdBits - 1, kVdShift);
  }

  inline int RdValue() const {
    DCHECK(this->InstructionType() == InstructionBase::kRType ||
           this->InstructionType() == InstructionBase::kR4Type ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType ||
           this->InstructionType() == InstructionBase::kUType ||
           this->InstructionType() == InstructionBase::kJType ||
           this->InstructionType() == InstructionBase::kVType);
    return this->Bits(kRdShift + kRdBits - 1, kRdShift);
  }

  inline int RvcRs1Value() const { return this->RvcRdValue(); }

  int RvcRdValue() const;

  int RvcRs2Value() const;

  int RvcRs1sValue() const;

  int RvcRs2sValue() const;

  int Funct7Value() const;

  inline int Funct3Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kRType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType ||
           this->InstructionType() == InstructionBase::kBType);
    return this->Bits(kFunct3Shift + kFunct3Bits - 1, kFunct3Shift);
  }

  inline int Funct5Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kRType &&
           this->BaseOpcode() == OP_FP);
    return this->Bits(kFunct5Shift + kFunct5Bits - 1, kFunct5Shift);
  }

  int RvcFunct6Value() const;

  int RvcFunct4Value() const;

  int RvcFunct3Value() const;

  int RvcFunct2Value() const;

  int RvcFunct2BValue() const;

  inline int CsrValue() const {
    DCHECK(this->InstructionType() == InstructionBase::kIType &&
           this->BaseOpcode() == SYSTEM);
    return (this->Bits(kCsrShift + kCsrBits - 1, kCsrShift));
  }

  inline int RoundMode() const {
    DCHECK((this->InstructionType() == InstructionBase::kRType ||
            this->InstructionType() == InstructionBase::kR4Type) &&
           this->BaseOpcode() == OP_FP);
    return this->Bits(kFunct3Shift + kFunct3Bits - 1, kFunct3Shift);
  }

  inline int MemoryOrder(bool is_pred) const {
    DCHECK((this->InstructionType() == InstructionBase::kIType &&
            this->BaseOpcode() == MISC_MEM));
    if (is_pred) {
      return this->Bits(kPredOrderShift + kMemOrderBits - 1, kPredOrderShift);
    } else {
      return this->Bits(kSuccOrderShift + kMemOrderBits - 1, kSuccOrderShift);
    }
  }

  inline int Imm12Value() const {
    DCHECK(this->InstructionType() == InstructionBase::kIType);
    int Value = this->Bits(kImm12Shift + kImm12Bits - 1, kImm12Shift);
    return Value << 20 >> 20;
  }

  inline int32_t Imm12SExtValue() const {
    int32_t Value = this->Imm12Value() << 20 >> 20;
    return Value;
  }

  inline int BranchOffset() const {
    DCHECK(this->InstructionType() == InstructionBase::kBType);
    // | imm[12|10:5] | rs2 | rs1 | funct3 | imm[4:1|11] | opcode |
    //  31          25                      11          7
    uint32_t Bits = this->InstructionBits();
    int16_t imm13 = ((Bits & 0xf00) >> 7) | ((Bits & 0x7e000000) >> 20) |
                    ((Bits & 0x80) << 4) | ((Bits & 0x80000000) >> 19);
    return imm13 << 19 >> 19;
  }

  inline int StoreOffset() const {
    DCHECK(this->InstructionType() == InstructionBase::kSType);
    // | imm[11:5] | rs2 | rs1 | funct3 | imm[4:0] | opcode |
    //  31       25                      11       7
    uint32_t Bits = this->InstructionBits();
    int16_t imm12 = ((Bits & 0xf80) >> 7) | ((Bits & 0xfe000000) >> 20);
    return imm12 << 20 >> 20;
  }

  inline int Imm20UValue() const {
    DCHECK(this->InstructionType() == InstructionBase::kUType);
    // | imm[31:12] | rd | opcode |
    //  31        12
    int32_t Bits = this->InstructionBits();
    return Bits >> 12;
  }

  inline int Imm20JValue() const {
    DCHECK(this->InstructionType() == InstructionBase::kJType);
    // | imm[20|10:1|11|19:12] | rd | opcode |
    //  31                   12
    uint32_t Bits = this->InstructionBits();
    int32_t imm20 = ((Bits & 0x7fe00000) >> 20) | ((Bits & 0x100000) >> 9) |
                    (Bits & 0xff000) | ((Bits & 0x80000000) >> 11);
    return imm20 << 11 >> 11;
  }

  inline bool IsArithShift() const {
    // Valid only for right shift operations
    DCHECK((this->BaseOpcode() == OP || this->BaseOpcode() == OP_32 ||
            this->BaseOpcode() == OP_IMM || this->BaseOpcode() == OP_IMM_32) &&
           this->Funct3Value() == 0b101);
    return this->InstructionBits() & 0x40000000;
  }

  inline int Shamt() const {
    // Valid only for shift instructions (SLLI, SRLI, SRAI)
    DCHECK(((this->InstructionBits() & kBaseOpcodeMask) == OP_IMM ||
            (this->InstructionBits() & kBaseOpcodeMask) == OP_IMM_32) &&
           (this->Funct3Value() == 0b001 || this->Funct3Value() == 0b101));
    // | 0A0000 | shamt | rs1 | funct3 | rd | opcode |
    //  31       25    20
    return this->Bits(kImm12Shift + 5, kImm12Shift);
  }

  inline int Shamt32() const {
    // Valid only for shift instructions (SLLIW, SRLIW, SRAIW)
    DCHECK((this->InstructionBits() & kBaseOpcodeMask) == OP_IMM_32 &&
           (this->Funct3Value() == 0b001 || this->Funct3Value() == 0b101));
    // | 0A00000 | shamt | rs1 | funct3 | rd | opcode |
    //  31        24   20
    return this->Bits(kImm12Shift + 4, kImm12Shift);
  }

  inline int RvcImm6Value() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | imm[5] | rs1/rd | imm[4:0] | opcode |
    //  15         12              6        2
    uint32_t Bits = this->InstructionBits();
    int32_t imm6 = ((Bits & 0x1000) >> 7) | ((Bits & 0x7c) >> 2);
    return imm6 << 26 >> 26;
  }

  inline int RvcImm6Addi16spValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | nzimm[9] | 2 | nzimm[4|6|8:7|5] | opcode |
    //  15         12           6                2
    uint32_t Bits = this->InstructionBits();
    int32_t imm10 = ((Bits & 0x1000) >> 3) | ((Bits & 0x40) >> 2) |
                    ((Bits & 0x20) << 1) | ((Bits & 0x18) << 4) |
                    ((Bits & 0x4) << 3);
    DCHECK_NE(imm10, 0);
    return imm10 << 22 >> 22;
  }

  inline int RvcImm8Addi4spnValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | nzimm[11]  | rd' | opcode |
    //  15      13           5     2
    uint32_t Bits = this->InstructionBits();
    int32_t uimm10 = ((Bits & 0x20) >> 2) | ((Bits & 0x40) >> 4) |
                     ((Bits & 0x780) >> 1) | ((Bits & 0x1800) >> 7);
    DCHECK_NE(uimm10, 0);
    return uimm10;
  }

  inline int RvcShamt6() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | nzuimm[5] | rs1/rd | nzuimm[4:0] | opcode |
    //  15         12                 6           2
    int32_t imm6 = this->RvcImm6Value();
    return imm6 & 0x3f;
  }

  inline int RvcImm6LwspValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | uimm[5] | rs1 | uimm[4:2|7:6] | opcode |
    //  15         12            6             2
    uint32_t Bits = this->InstructionBits();
    int32_t imm8 =
        ((Bits & 0x1000) >> 7) | ((Bits & 0x70) >> 2) | ((Bits & 0xc) << 4);
    return imm8;
  }

  inline int RvcImm6LdspValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | uimm[5] | rs1 | uimm[4:3|8:6] | opcode |
    //  15         12            6             2
    uint32_t Bits = this->InstructionBits();
    int32_t imm9 =
        ((Bits & 0x1000) >> 7) | ((Bits & 0x60) >> 2) | ((Bits & 0x1c) << 4);
    return imm9;
  }

  inline int RvcImm6SwspValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | uimm[5:2|7:6] | rs2 | opcode |
    //  15       12            7
    uint32_t Bits = this->InstructionBits();
    int32_t imm8 = ((Bits & 0x1e00) >> 7) | ((Bits & 0x180) >> 1);
    return imm8;
  }

  inline int RvcImm6SdspValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | uimm[5:3|8:6] | rs2 | opcode |
    //  15       12            7
    uint32_t Bits = this->InstructionBits();
    int32_t imm9 = ((Bits & 0x1c00) >> 7) | ((Bits & 0x380) >> 1);
    return imm9;
  }

  inline int RvcImm5WValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | imm[5:3] | rs1 | imm[2|6] | rd | opcode |
    //  15       12       10     6          4     2
    uint32_t Bits = this->InstructionBits();
    int32_t imm7 =
        ((Bits & 0x1c00) >> 7) | ((Bits & 0x40) >> 4) | ((Bits & 0x20) << 1);
    return imm7;
  }

  inline int RvcImm5DValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | imm[5:3] | rs1 | imm[7:6] | rd | opcode |
    //  15       12        10    6          4     2
    uint32_t Bits = this->InstructionBits();
    int32_t imm8 = ((Bits & 0x1c00) >> 7) | ((Bits & 0x60) << 1);
    return imm8;
  }

  inline int RvcImm11CJValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | [11|4|9:8|10|6|7|3:1|5] | opcode |
    //  15      12                        2
    uint32_t Bits = this->InstructionBits();
    int32_t imm12 = ((Bits & 0x4) << 3) | ((Bits & 0x38) >> 2) |
                    ((Bits & 0x40) << 1) | ((Bits & 0x80) >> 1) |
                    ((Bits & 0x100) << 2) | ((Bits & 0x600) >> 1) |
                    ((Bits & 0x800) >> 7) | ((Bits & 0x1000) >> 1);
    return imm12 << 20 >> 20;
  }

  inline int RvcImm8BValue() const {
    DCHECK(this->IsShortInstruction());
    // | funct3 | imm[8|4:3] | rs1` | imm[7:6|2:1|5]  | opcode |
    //  15       12        10       7                 2
    uint32_t Bits = this->InstructionBits();
    int32_t imm9 = ((Bits & 0x4) << 3) | ((Bits & 0x18) >> 2) |
                   ((Bits & 0x60) << 1) | ((Bits & 0xc00) >> 7) |
                   ((Bits & 0x1000) >> 4);
    return imm9 << 23 >> 23;
  }

  inline int vl_vs_width() {
    int width = 0;
    if ((this->InstructionBits() & kBaseOpcodeMask) != LOAD_FP &&
        (this->InstructionBits() & kBaseOpcodeMask) != STORE_FP)
      return -1;
    switch (this->InstructionBits() & (kRvvWidthMask | kRvvMewMask)) {
      case 0x0:
        width = 8;
        break;
      case 0x00005000:
        width = 16;
        break;
      case 0x00006000:
        width = 32;
        break;
      case 0x00007000:
        width = 64;
        break;
      case 0x10000000:
        width = 128;
        break;
      case 0x10005000:
        width = 256;
        break;
      case 0x10006000:
        width = 512;
        break;
      case 0x10007000:
        width = 1024;
        break;
      default:
        width = -1;
        break;
    }
    return width;
  }

  uint32_t Rvvzimm() const;

  uint32_t Rvvuimm() const;

  inline uint32_t RvvVsew() const {
    uint32_t zimm = this->Rvvzimm();
    uint32_t vsew = (zimm >> 3) & 0x7;
    return vsew;
  }

  inline uint32_t RvvVlmul() const {
    uint32_t zimm = this->Rvvzimm();
    uint32_t vlmul = zimm & 0x7;
    return vlmul;
  }

  inline uint8_t RvvVM() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType ||
           this->InstructionType() == InstructionBase::kIType ||
           this->InstructionType() == InstructionBase::kSType);
    return this->Bits(kRvvVmShift + kRvvVmBits - 1, kRvvVmShift);
  }

  inline const char* RvvSEW() const {
    uint32_t vsew = this->RvvVsew();
    switch (vsew) {
#define CAST_VSEW(name) \
  case name:            \
    return #name;
      RVV_SEW(CAST_VSEW)
      default:
        return "unknown";
#undef CAST_VSEW
    }
  }

  inline const char* RvvLMUL() const {
    uint32_t vlmul = this->RvvVlmul();
    switch (vlmul) {
#define CAST_VLMUL(name) \
  case name:             \
    return #name;
      RVV_LMUL(CAST_VLMUL)
      default:
        return "unknown";
#undef CAST_VLMUL
    }
  }

#define sext(x, len) (((int32_t)(x) << (32 - len)) >> (32 - len))
#define zext(x, len) (((uint32_t)(x) << (32 - len)) >> (32 - len))

  inline int32_t RvvSimm5() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType);
    return sext(this->Bits(kRvvImm5Shift + kRvvImm5Bits - 1, kRvvImm5Shift),
                kRvvImm5Bits);
  }

  inline uint32_t RvvUimm5() const {
    DCHECK(this->InstructionType() == InstructionBase::kVType);
    uint32_t imm = this->Bits(kRvvImm5Shift + kRvvImm5Bits - 1, kRvvImm5Shift);
    return zext(imm, kRvvImm5Bits);
  }
#undef sext
#undef zext
  inline bool AqValue() const { return this->Bits(kAqShift, kAqShift); }

  inline bool RlValue() const { return this->Bits(kRlShift, kRlShift); }

  // Say if the instruction is a break or a trap.
  bool IsTrap() const;
};

class Instruction : public InstructionGetters<InstructionBase> {
 public:
  // Instructions are read of out a code stream. The only way to get a
  // reference to an instruction is to convert a pointer. There is no way
  // to allocate or create instances of class Instruction.
  // Use the At(pc) function to create references to Instruction.
  static Instruction* At(uint8_t* pc) {
    return reinterpret_cast<Instruction*>(pc);
  }

 private:
  // We need to prevent the creation of instances of class Instruction.
  DISALLOW_IMPLICIT_CONSTRUCTORS(Instruction);
};

// -----------------------------------------------------------------------------
// RISC-V assembly various constants.

// C/C++ argument slots size.
const int kCArgSlotCount = 0;

// TODO(plind): below should be based on kSystemPointerSize
// TODO(plind): find all usages and remove the needless instructions for n64.
const int kCArgsSlotsSize = kCArgSlotCount * kInstrSize * 2;

const int kInvalidStackOffset = -1;
const int kBranchReturnOffset = 2 * kInstrSize;

static const int kNegOffset = 0x00008000;

// -----------------------------------------------------------------------------
// Instructions.

template <class P>
bool InstructionGetters<P>::IsTrap() const {
  return (this->InstructionBits() == kBreakInstr);
}

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_
                                                                                                                                                                                                                                                                                                                              node-23.7.0/deps/v8/src/codegen/riscv/base-riscv-i.cc                                               0000664 0000000 0000000 00000022435 14746647661 0022167 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/base-riscv-i.h"

namespace v8 {
namespace internal {

void AssemblerRISCVI::lui(Register rd, int32_t imm20) {
  GenInstrU(LUI, rd, imm20);
}

void AssemblerRISCVI::auipc(Register rd, int32_t imm20) {
  GenInstrU(AUIPC, rd, imm20);
}

// Jumps

void AssemblerRISCVI::jal(Register rd, int32_t imm21) {
  GenInstrJ(JAL, rd, imm21);
  ClearVectorunit();
  BlockTrampolinePoolFor(1);
}

void AssemblerRISCVI::jalr(Register rd, Register rs1, int16_t imm12) {
  GenInstrI(0b000, JALR, rd, rs1, imm12);
  ClearVectorunit();
  BlockTrampolinePoolFor(1);
}

// Branches

void AssemblerRISCVI::beq(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b000, rs1, rs2, imm13);
  ClearVectorunit();
}

void AssemblerRISCVI::bne(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b001, rs1, rs2, imm13);
  ClearVectorunit();
}

void AssemblerRISCVI::blt(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b100, rs1, rs2, imm13);
  ClearVectorunit();
}

void AssemblerRISCVI::bge(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b101, rs1, rs2, imm13);
  ClearVectorunit();
}

void AssemblerRISCVI::bltu(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b110, rs1, rs2, imm13);
  ClearVectorunit();
}

void AssemblerRISCVI::bgeu(Register rs1, Register rs2, int16_t imm13) {
  GenInstrBranchCC_rri(0b111, rs1, rs2, imm13);
  ClearVectorunit();
}

// Loads

void AssemblerRISCVI::lb(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b000, rd, rs1, imm12);
}

void AssemblerRISCVI::lh(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b001, rd, rs1, imm12);
}

void AssemblerRISCVI::lw(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b010, rd, rs1, imm12);
}

void AssemblerRISCVI::lbu(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b100, rd, rs1, imm12);
}

void AssemblerRISCVI::lhu(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b101, rd, rs1, imm12);
}

// Stores

void AssemblerRISCVI::sb(Register source, Register base, int16_t imm12) {
  GenInstrStore_rri(0b000, base, source, imm12);
}

void AssemblerRISCVI::sh(Register source, Register base, int16_t imm12) {
  GenInstrStore_rri(0b001, base, source, imm12);
}

void AssemblerRISCVI::sw(Register source, Register base, int16_t imm12) {
  GenInstrStore_rri(0b010, base, source, imm12);
}

// Arithmetic with immediate

void AssemblerRISCVI::addi(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b000, rd, rs1, imm12);
}

void AssemblerRISCVI::slti(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b010, rd, rs1, imm12);
}

void AssemblerRISCVI::sltiu(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b011, rd, rs1, imm12);
}

void AssemblerRISCVI::xori(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b100, rd, rs1, imm12);
}

void AssemblerRISCVI::ori(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b110, rd, rs1, imm12);
}

void AssemblerRISCVI::andi(Register rd, Register rs1, int16_t imm12) {
  GenInstrALU_ri(0b111, rd, rs1, imm12);
}

void AssemblerRISCVI::slli(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShift_ri(0, 0b001, rd, rs1, shamt & 0x3f);
}

void AssemblerRISCVI::srli(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShift_ri(0, 0b101, rd, rs1, shamt & 0x3f);
}

void AssemblerRISCVI::srai(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShift_ri(1, 0b101, rd, rs1, shamt & 0x3f);
}

// Arithmetic

void AssemblerRISCVI::add(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVI::sub(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVI::sll(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVI::slt(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVI::sltu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVI::xor_(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b100, rd, rs1, rs2);
}

void AssemblerRISCVI::srl(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVI::sra(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100000, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVI::or_(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b110, rd, rs1, rs2);
}

void AssemblerRISCVI::and_(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000000, 0b111, rd, rs1, rs2);
}

// Memory fences

void AssemblerRISCVI::fence(uint8_t pred, uint8_t succ) {
  DCHECK(is_uint4(pred) && is_uint4(succ));
  uint16_t imm12 = succ | (pred << 4) | (0b0000 << 8);
  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
}

void AssemblerRISCVI::fence_tso() {
  uint16_t imm12 = (0b0011) | (0b0011 << 4) | (0b1000 << 8);
  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
}

// Environment call / break

void AssemblerRISCVI::ecall() {
  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 0);
}

void AssemblerRISCVI::ebreak() {
  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 1);
}

// This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
// instruction (i.e., it should always trap, if your implementation has invalid
// instruction traps).
void AssemblerRISCVI::unimp() {
  GenInstrI(0b001, SYSTEM, ToRegister(0), ToRegister(0), 0b110000000000);
}

bool AssemblerRISCVI::IsBranch(Instr instr) {
  return (instr & kBaseOpcodeMask) == BRANCH;
}

bool AssemblerRISCVI::IsJump(Instr instr) {
  int Op = instr & kBaseOpcodeMask;
  return Op == JAL || Op == JALR;
}

bool AssemblerRISCVI::IsNop(Instr instr) { return instr == kNopByte; }

bool AssemblerRISCVI::IsJal(Instr instr) {
  return (instr & kBaseOpcodeMask) == JAL;
}

bool AssemblerRISCVI::IsJalr(Instr instr) {
  return (instr & kBaseOpcodeMask) == JALR;
}

bool AssemblerRISCVI::IsLui(Instr instr) {
  return (instr & kBaseOpcodeMask) == LUI;
}
bool AssemblerRISCVI::IsAuipc(Instr instr) {
  return (instr & kBaseOpcodeMask) == AUIPC;
}
bool AssemblerRISCVI::IsAddi(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDI;
}
bool AssemblerRISCVI::IsOri(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ORI;
}
bool AssemblerRISCVI::IsSlli(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_SLLI;
}

int AssemblerRISCVI::JumpOffset(Instr instr) {
  int32_t imm21 = ((instr & 0x7fe00000) >> 20) | ((instr & 0x100000) >> 9) |
                  (instr & 0xff000) | ((instr & 0x80000000) >> 11);
  imm21 = imm21 << 11 >> 11;
  return imm21;
}

int AssemblerRISCVI::JalrOffset(Instr instr) {
  DCHECK(IsJalr(instr));
  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
  return imm12;
}

int AssemblerRISCVI::AuipcOffset(Instr instr) {
  DCHECK(IsAuipc(instr));
  int32_t imm20 = static_cast<int32_t>(instr & kImm20Mask);
  return imm20;
}

bool AssemblerRISCVI::IsLw(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_LW;
}

int AssemblerRISCVI::LoadOffset(Instr instr) {
#if V8_TARGET_ARCH_RISCV64
  DCHECK(IsLd(instr));
#elif V8_TARGET_ARCH_RISCV32
  DCHECK(IsLw(instr));
#endif
  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
  return imm12;
}

#ifdef V8_TARGET_ARCH_RISCV64

bool AssemblerRISCVI::IsAddiw(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDIW;
}

bool AssemblerRISCVI::IsLd(Instr instr) {
  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_LD;
}

void AssemblerRISCVI::lwu(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b110, rd, rs1, imm12);
}

void AssemblerRISCVI::ld(Register rd, Register rs1, int16_t imm12) {
  GenInstrLoad_ri(0b011, rd, rs1, imm12);
}

void AssemblerRISCVI::sd(Register source, Register base, int16_t imm12) {
  GenInstrStore_rri(0b011, base, source, imm12);
}

void AssemblerRISCVI::addiw(Register rd, Register rs1, int16_t imm12) {
  GenInstrI(0b000, OP_IMM_32, rd, rs1, imm12);
}

void AssemblerRISCVI::slliw(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShiftW_ri(0, 0b001, rd, rs1, shamt & 0x1f);
}

void AssemblerRISCVI::srliw(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShiftW_ri(0, 0b101, rd, rs1, shamt & 0x1f);
}

void AssemblerRISCVI::sraiw(Register rd, Register rs1, uint8_t shamt) {
  GenInstrShiftW_ri(1, 0b101, rd, rs1, shamt & 0x1f);
}

void AssemblerRISCVI::addw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVI::subw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0100000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVI::sllw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000000, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVI::srlw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000000, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVI::sraw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0100000, 0b101, rd, rs1, rs2);
}

#endif

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/codegen/riscv/base-riscv-i.h                                                0000664 0000000 0000000 00000017672 14746647661 0022040 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-i.h"
#include "src/codegen/riscv/register-riscv.h"

#ifndef V8_CODEGEN_RISCV_BASE_RISCV_I_H_
#define V8_CODEGEN_RISCV_BASE_RISCV_I_H_

namespace v8 {
namespace internal {
class AssemblerRISCVI : public AssemblerRiscvBase {
 public:
  void lui(Register rd, int32_t imm20);
  void auipc(Register rd, int32_t imm20);

  // Jumps
  void jal(Register rd, int32_t imm20);
  void jalr(Register rd, Register rs1, int16_t imm12);

  // Branches
  void beq(Register rs1, Register rs2, int16_t imm12);
  void bne(Register rs1, Register rs2, int16_t imm12);
  void blt(Register rs1, Register rs2, int16_t imm12);
  void bge(Register rs1, Register rs2, int16_t imm12);
  void bltu(Register rs1, Register rs2, int16_t imm12);
  void bgeu(Register rs1, Register rs2, int16_t imm12);
  // Loads
  void lb(Register rd, Register rs1, int16_t imm12);
  void lh(Register rd, Register rs1, int16_t imm12);
  void lw(Register rd, Register rs1, int16_t imm12);
  void lbu(Register rd, Register rs1, int16_t imm12);
  void lhu(Register rd, Register rs1, int16_t imm12);

  // Stores
  void sb(Register source, Register base, int16_t imm12);
  void sh(Register source, Register base, int16_t imm12);
  void sw(Register source, Register base, int16_t imm12);

  // Arithmetic with immediate
  void addi(Register rd, Register rs1, int16_t imm12);
  void slti(Register rd, Register rs1, int16_t imm12);
  void sltiu(Register rd, Register rs1, int16_t imm12);
  void xori(Register rd, Register rs1, int16_t imm12);
  void ori(Register rd, Register rs1, int16_t imm12);
  void andi(Register rd, Register rs1, int16_t imm12);
  void slli(Register rd, Register rs1, uint8_t shamt);
  void srli(Register rd, Register rs1, uint8_t shamt);
  void srai(Register rd, Register rs1, uint8_t shamt);

  // Arithmetic
  void add(Register rd, Register rs1, Register rs2);
  void sub(Register rd, Register rs1, Register rs2);
  void sll(Register rd, Register rs1, Register rs2);
  void slt(Register rd, Register rs1, Register rs2);
  void sltu(Register rd, Register rs1, Register rs2);
  void xor_(Register rd, Register rs1, Register rs2);
  void srl(Register rd, Register rs1, Register rs2);
  void sra(Register rd, Register rs1, Register rs2);
  void or_(Register rd, Register rs1, Register rs2);
  void and_(Register rd, Register rs1, Register rs2);

  // Other pseudo instructions that are not part of RISCV pseudo assemly
  void nor(Register rd, Register rs, Register rt) {
    or_(rd, rs, rt);
    not_(rd, rd);
  }

  // Memory fences
  void fence(uint8_t pred, uint8_t succ);
  void fence_tso();

  // Environment call / break
  void ecall();
  void ebreak();

  void sync() { fence(0b1111, 0b1111); }

  // This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
  // instruction (i.e., it should always trap, if your implementation has
  // invalid instruction traps).
  void unimp();

  static int JumpOffset(Instr instr);
  static int AuipcOffset(Instr instr);
  static int JalrOffset(Instr instr);
  static int LoadOffset(Instr instr);

  // Check if an instruction is a branch of some kind.
  static bool IsBranch(Instr instr);
  static bool IsNop(Instr instr);
  static bool IsJump(Instr instr);
  static bool IsJal(Instr instr);
  static bool IsJalr(Instr instr);
  static bool IsLui(Instr instr);
  static bool IsAuipc(Instr instr);
  static bool IsAddi(Instr instr);
  static bool IsOri(Instr instr);
  static bool IsSlli(Instr instr);
  static bool IsLw(Instr instr);

  inline int32_t branch_offset(Label* L) {
    return branch_offset_helper(L, OffsetSize::kOffset13);
  }
  inline int32_t jump_offset(Label* L) {
    return branch_offset_helper(L, OffsetSize::kOffset21);
  }

  // Branches
  void beq(Register rs1, Register rs2, Label* L) {
    beq(rs1, rs2, branch_offset(L));
  }
  void bne(Register rs1, Register rs2, Label* L) {
    bne(rs1, rs2, branch_offset(L));
  }
  void blt(Register rs1, Register rs2, Label* L) {
    blt(rs1, rs2, branch_offset(L));
  }
  void bge(Register rs1, Register rs2, Label* L) {
    bge(rs1, rs2, branch_offset(L));
  }
  void bltu(Register rs1, Register rs2, Label* L) {
    bltu(rs1, rs2, branch_offset(L));
  }
  void bgeu(Register rs1, Register rs2, Label* L) {
    bgeu(rs1, rs2, branch_offset(L));
  }

  void beqz(Register rs, int16_t imm13) { beq(rs, zero_reg, imm13); }
  void beqz(Register rs1, Label* L) { beqz(rs1, branch_offset(L)); }
  void bnez(Register rs, int16_t imm13) { bne(rs, zero_reg, imm13); }
  void bnez(Register rs1, Label* L) { bnez(rs1, branch_offset(L)); }
  void blez(Register rs, int16_t imm13) { bge(zero_reg, rs, imm13); }
  void blez(Register rs1, Label* L) { blez(rs1, branch_offset(L)); }
  void bgez(Register rs, int16_t imm13) { bge(rs, zero_reg, imm13); }
  void bgez(Register rs1, Label* L) { bgez(rs1, branch_offset(L)); }
  void bltz(Register rs, int16_t imm13) { blt(rs, zero_reg, imm13); }
  void bltz(Register rs1, Label* L) { bltz(rs1, branch_offset(L)); }
  void bgtz(Register rs, int16_t imm13) { blt(zero_reg, rs, imm13); }

  void bgtz(Register rs1, Label* L) { bgtz(rs1, branch_offset(L)); }
  void bgt(Register rs1, Register rs2, int16_t imm13) { blt(rs2, rs1, imm13); }
  void bgt(Register rs1, Register rs2, Label* L) {
    bgt(rs1, rs2, branch_offset(L));
  }
  void ble(Register rs1, Register rs2, int16_t imm13) { bge(rs2, rs1, imm13); }
  void ble(Register rs1, Register rs2, Label* L) {
    ble(rs1, rs2, branch_offset(L));
  }
  void bgtu(Register rs1, Register rs2, int16_t imm13) {
    bltu(rs2, rs1, imm13);
  }
  void bgtu(Register rs1, Register rs2, Label* L) {
    bgtu(rs1, rs2, branch_offset(L));
  }
  void bleu(Register rs1, Register rs2, int16_t imm13) {
    bgeu(rs2, rs1, imm13);
  }
  void bleu(Register rs1, Register rs2, Label* L) {
    bleu(rs1, rs2, branch_offset(L));
  }

  void j(int32_t imm21) { jal(zero_reg, imm21); }
  void j(Label* L) { j(jump_offset(L)); }
  void b(Label* L) { j(L); }
  void jal(int32_t imm21) { jal(ra, imm21); }
  void jal(Label* L) { jal(jump_offset(L)); }
  void jr(Register rs) { jalr(zero_reg, rs, 0); }
  void jr(Register rs, int32_t imm12) { jalr(zero_reg, rs, imm12); }
  void jalr(Register rs, int32_t imm12) { jalr(ra, rs, imm12); }
  void jalr(Register rs) { jalr(ra, rs, 0); }
  void ret() { jalr(zero_reg, ra, 0); }
  void call(int32_t offset) {
    auipc(ra, (offset >> 12) + ((offset & 0x800) >> 11));
    jalr(ra, ra, offset << 20 >> 20);
  }

  void mv(Register rd, Register rs) { addi(rd, rs, 0); }
  void not_(Register rd, Register rs) { xori(rd, rs, -1); }
  void neg(Register rd, Register rs) { sub(rd, zero_reg, rs); }
  void seqz(Register rd, Register rs) { sltiu(rd, rs, 1); }
  void snez(Register rd, Register rs) { sltu(rd, zero_reg, rs); }
  void sltz(Register rd, Register rs) { slt(rd, rs, zero_reg); }
  void sgtz(Register rd, Register rs) { slt(rd, zero_reg, rs); }

#if V8_TARGET_ARCH_RISCV64
  void lwu(Register rd, Register rs1, int16_t imm12);
  void ld(Register rd, Register rs1, int16_t imm12);
  void sd(Register source, Register base, int16_t imm12);
  void addiw(Register rd, Register rs1, int16_t imm12);
  void slliw(Register rd, Register rs1, uint8_t shamt);
  void srliw(Register rd, Register rs1, uint8_t shamt);
  void sraiw(Register rd, Register rs1, uint8_t shamt);
  void addw(Register rd, Register rs1, Register rs2);
  void subw(Register rd, Register rs1, Register rs2);
  void sllw(Register rd, Register rs1, Register rs2);
  void srlw(Register rd, Register rs1, Register rs2);
  void sraw(Register rd, Register rs1, Register rs2);
  void negw(Register rd, Register rs) { subw(rd, zero_reg, rs); }
  void sext_w(Register rd, Register rs) { addiw(rd, rs, 0); }

  static bool IsAddiw(Instr instr);
  static bool IsLd(Instr instr);
#endif
};

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_BASE_RISCV_I_H_
                                                                      node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-a.h                                            0000664 0000000 0000000 00000005145 14746647661 0022737 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// RV32A Standard Extension
constexpr Opcode RO_LR_W =
    AMO | (0b010 << kFunct3Shift) | (0b00010 << kFunct5Shift);
constexpr Opcode RO_SC_W =
    AMO | (0b010 << kFunct3Shift) | (0b00011 << kFunct5Shift);
constexpr Opcode RO_AMOSWAP_W =
    AMO | (0b010 << kFunct3Shift) | (0b00001 << kFunct5Shift);
constexpr Opcode RO_AMOADD_W =
    AMO | (0b010 << kFunct3Shift) | (0b00000 << kFunct5Shift);
constexpr Opcode RO_AMOXOR_W =
    AMO | (0b010 << kFunct3Shift) | (0b00100 << kFunct5Shift);
constexpr Opcode RO_AMOAND_W =
    AMO | (0b010 << kFunct3Shift) | (0b01100 << kFunct5Shift);
constexpr Opcode RO_AMOOR_W =
    AMO | (0b010 << kFunct3Shift) | (0b01000 << kFunct5Shift);
constexpr Opcode RO_AMOMIN_W =
    AMO | (0b010 << kFunct3Shift) | (0b10000 << kFunct5Shift);
constexpr Opcode RO_AMOMAX_W =
    AMO | (0b010 << kFunct3Shift) | (0b10100 << kFunct5Shift);
constexpr Opcode RO_AMOMINU_W =
    AMO | (0b010 << kFunct3Shift) | (0b11000 << kFunct5Shift);
constexpr Opcode RO_AMOMAXU_W =
    AMO | (0b010 << kFunct3Shift) | (0b11100 << kFunct5Shift);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64A Standard Extension (in addition to RV32A)
constexpr Opcode RO_LR_D =
    AMO | (0b011 << kFunct3Shift) | (0b00010 << kFunct5Shift);
constexpr Opcode RO_SC_D =
    AMO | (0b011 << kFunct3Shift) | (0b00011 << kFunct5Shift);
constexpr Opcode RO_AMOSWAP_D =
    AMO | (0b011 << kFunct3Shift) | (0b00001 << kFunct5Shift);
constexpr Opcode RO_AMOADD_D =
    AMO | (0b011 << kFunct3Shift) | (0b00000 << kFunct5Shift);
constexpr Opcode RO_AMOXOR_D =
    AMO | (0b011 << kFunct3Shift) | (0b00100 << kFunct5Shift);
constexpr Opcode RO_AMOAND_D =
    AMO | (0b011 << kFunct3Shift) | (0b01100 << kFunct5Shift);
constexpr Opcode RO_AMOOR_D =
    AMO | (0b011 << kFunct3Shift) | (0b01000 << kFunct5Shift);
constexpr Opcode RO_AMOMIN_D =
    AMO | (0b011 << kFunct3Shift) | (0b10000 << kFunct5Shift);
constexpr Opcode RO_AMOMAX_D =
    AMO | (0b011 << kFunct3Shift) | (0b10100 << kFunct5Shift);
constexpr Opcode RO_AMOMINU_D =
    AMO | (0b011 << kFunct3Shift) | (0b11000 << kFunct5Shift);
constexpr Opcode RO_AMOMAXU_D =
    AMO | (0b011 << kFunct3Shift) | (0b11100 << kFunct5Shift);
#endif  // V8_TARGET_ARCH_RISCV64
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_
                                                                                                                                                                                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-b.h                                            0000664 0000000 0000000 00000012126 14746647661 0022735 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_B_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_B_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// Zba
#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_ADDUW =
    OP_32 | (0b000 << kFunct3Shift) | (0b0000100 << kFunct7Shift);
constexpr Opcode RO_SH1ADDUW =
    OP_32 | (0b010 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_SH2ADDUW =
    OP_32 | (0b100 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_SH3ADDUW =
    OP_32 | (0b110 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_SLLIUW =
    OP_IMM_32 | (0b001 << kFunct3Shift) | (0b000010 << kFunct6Shift);
#endif

constexpr Opcode RO_SH1ADD =
    OP | (0b010 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_SH2ADD =
    OP | (0b100 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_SH3ADD =
    OP | (0b110 << kFunct3Shift) | (0b0010000 << kFunct7Shift);

// Zbb
constexpr Opcode RO_ANDN =
    OP | (0b111 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
constexpr Opcode RO_ORN =
    OP | (0b110 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
constexpr Opcode RO_XNOR =
    OP | (0b100 << kFunct3Shift) | (0b0100000 << kFunct7Shift);

constexpr Opcode OP_COUNT =
    OP_IMM | (0b001 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_CLZ = OP_COUNT | (0b00000 << kShamtShift);
constexpr Opcode RO_CTZ = OP_COUNT | (0b00001 << kShamtShift);
constexpr Opcode RO_CPOP = OP_COUNT | (0b00010 << kShamtShift);
#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode OP_COUNTW =
    OP_IMM_32 | (0b001 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_CLZW = OP_COUNTW | (0b00000 << kShamtShift);
constexpr Opcode RO_CTZW = OP_COUNTW | (0b00001 << kShamtShift);
constexpr Opcode RO_CPOPW = OP_COUNTW | (0b00010 << kShamtShift);
#endif

constexpr Opcode RO_MAX =
    OP | (0b110 << kFunct3Shift) | (0b0000101 << kFunct7Shift);
constexpr Opcode RO_MAXU =
    OP | (0b111 << kFunct3Shift) | (0b0000101 << kFunct7Shift);

constexpr Opcode RO_MIN =
    OP | (0b100 << kFunct3Shift) | (0b0000101 << kFunct7Shift);
constexpr Opcode RO_MINU =
    OP | (0b101 << kFunct3Shift) | (0b0000101 << kFunct7Shift);

constexpr Opcode RO_SEXTB = OP_IMM | (0b001 << kFunct3Shift) |
                            (0b0110000 << kFunct7Shift) |
                            (0b00100 << kShamtShift);
constexpr Opcode RO_SEXTH = OP_IMM | (0b001 << kFunct3Shift) |
                            (0b0110000 << kFunct7Shift) |
                            (0b00101 << kShamtShift);
#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_ZEXTH = OP_32 | (0b100 << kFunct3Shift) |
                            (0b0000100 << kFunct7Shift) |
                            (0b00000 << kShamtShift);
#elif defined(V8_TARGET_ARCH_RISCV32)
constexpr Opcode RO_ZEXTH = OP | (0b100 << kFunct3Shift) |
                            (0b0000100 << kFunct7Shift) |
                            (0b00000 << kShamtShift);
#endif

// Zbb: bitwise rotation
constexpr Opcode RO_ROL =
    OP | (0b001 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_ROR =
    OP | (0b101 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_ORCB =
    OP_IMM | (0b101 << kFunct3Shift) | (0b001010000111 << kImm12Shift);

#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_RORI =
    OP_IMM | (0b101 << kFunct3Shift) | (0b011000 << kFunct6Shift);
#elif defined(V8_TARGET_ARCH_RISCV32)
constexpr Opcode RO_RORI =
    OP_IMM | (0b101 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
#endif

#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_ROLW =
    OP_32 | (0b001 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_RORIW =
    OP_IMM_32 | (0b101 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
constexpr Opcode RO_RORW =
    OP_32 | (0b101 << kFunct3Shift) | (0b0110000 << kFunct7Shift);
#endif

constexpr Opcode RO_REV8 =
    OP_IMM | (0b101 << kFunct3Shift) | (0b011010 << kFunct6Shift);
#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_REV8_IMM12 = 0b011010111000;
#elif defined(V8_TARGET_ARCH_RISCV32)
constexpr Opcode RO_REV8_IMM12 = 0b011010011000;
#endif
// Zbs
constexpr Opcode RO_BCLR =
    OP | (0b001 << kFunct3Shift) | (0b0100100 << kFunct7Shift);
constexpr Opcode RO_BCLRI =
    OP_IMM | (0b001 << kFunct3Shift) | (0b010010 << kFunct6Shift);

constexpr Opcode RO_BEXT =
    OP | (0b101 << kFunct3Shift) | (0b0100100 << kFunct7Shift);
constexpr Opcode RO_BEXTI =
    OP_IMM | (0b101 << kFunct3Shift) | (0b010010 << kFunct6Shift);

constexpr Opcode RO_BINV =
    OP | (0b001 << kFunct3Shift) | (0b0110100 << kFunct7Shift);
constexpr Opcode RO_BINVI =
    OP_IMM | (0b001 << kFunct3Shift) | (0b011010 << kFunct6Shift);

constexpr Opcode RO_BSET =
    OP | (0b001 << kFunct3Shift) | (0b0010100 << kFunct7Shift);
constexpr Opcode RO_BSETI =
    OP_IMM | (0b001 << kFunct3Shift) | (0b0010100 << kFunct7Shift);

}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_B_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-c.h                                            0000664 0000000 0000000 00000006172 14746647661 0022742 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

constexpr Opcode RO_C_ADDI4SPN = C0 | (0b000 << kRvcFunct3Shift);
constexpr Opcode RO_C_ADDI16SP = C1 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_LW = C0 | (0b010 << kRvcFunct3Shift);
constexpr Opcode RO_C_SW = C0 | (0b110 << kRvcFunct3Shift);
constexpr Opcode RO_C_NOP_ADDI = C1 | (0b000 << kRvcFunct3Shift);
constexpr Opcode RO_C_LI = C1 | (0b010 << kRvcFunct3Shift);
constexpr Opcode RO_C_SUB =
    C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift);
constexpr Opcode RO_C_XOR =
    C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift);
constexpr Opcode RO_C_OR =
    C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_2 << kRvcFunct2Shift);
constexpr Opcode RO_C_AND =
    C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_3 << kRvcFunct2Shift);
constexpr Opcode RO_C_LUI_ADD = C1 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_MISC_ALU = C1 | (0b100 << kRvcFunct3Shift);
constexpr Opcode RO_C_J = C1 | (0b101 << kRvcFunct3Shift);
constexpr Opcode RO_C_BEQZ = C1 | (0b110 << kRvcFunct3Shift);
constexpr Opcode RO_C_BNEZ = C1 | (0b111 << kRvcFunct3Shift);
constexpr Opcode RO_C_SLLI = C2 | (0b000 << kRvcFunct3Shift);
constexpr Opcode RO_C_LWSP = C2 | (0b010 << kRvcFunct3Shift);
constexpr Opcode RO_C_JR_MV_ADD = C2 | (0b100 << kRvcFunct3Shift);
constexpr Opcode RO_C_JR = C2 | (0b1000 << kRvcFunct4Shift);
constexpr Opcode RO_C_MV = C2 | (0b1000 << kRvcFunct4Shift);
constexpr Opcode RO_C_EBREAK = C2 | (0b1001 << kRvcFunct4Shift);
constexpr Opcode RO_C_JALR = C2 | (0b1001 << kRvcFunct4Shift);
constexpr Opcode RO_C_ADD = C2 | (0b1001 << kRvcFunct4Shift);
constexpr Opcode RO_C_SWSP = C2 | (0b110 << kRvcFunct3Shift);

constexpr Opcode RO_C_FSD = C0 | (0b101 << kRvcFunct3Shift);
constexpr Opcode RO_C_FLD = C0 | (0b001 << kRvcFunct3Shift);
constexpr Opcode RO_C_FLDSP = C2 | (0b001 << kRvcFunct3Shift);
constexpr Opcode RO_C_FSDSP = C2 | (0b101 << kRvcFunct3Shift);
#ifdef V8_TARGET_ARCH_RISCV64
constexpr Opcode RO_C_LD = C0 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_SD = C0 | (0b111 << kRvcFunct3Shift);
constexpr Opcode RO_C_LDSP = C2 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_SDSP = C2 | (0b111 << kRvcFunct3Shift);
constexpr Opcode RO_C_ADDIW = C1 | (0b001 << kRvcFunct3Shift);
constexpr Opcode RO_C_SUBW =
    C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift);
constexpr Opcode RO_C_ADDW =
    C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift);
#endif
#ifdef V8_TARGET_ARCH_RISCV32
constexpr Opcode RO_C_FLWSP = C2 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_FSWSP = C2 | (0b111 << kRvcFunct3Shift);
constexpr Opcode RO_C_FLW = C0 | (0b011 << kRvcFunct3Shift);
constexpr Opcode RO_C_FSW = C0 | (0b111 << kRvcFunct3Shift);
#endif
// clang-format on
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_
                                                                                                                                                                                                                                                                                                                                                                                                      node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-d.h                                            0000664 0000000 0000000 00000007005 14746647661 0022737 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// RV32D Standard Extension
constexpr Opcode RO_FLD = LOAD_FP | (0b011 << kFunct3Shift);
constexpr Opcode RO_FSD = STORE_FP | (0b011 << kFunct3Shift);
constexpr Opcode RO_FMADD_D = MADD | (0b01 << kFunct2Shift);
constexpr Opcode RO_FMSUB_D = MSUB | (0b01 << kFunct2Shift);
constexpr Opcode RO_FNMSUB_D = NMSUB | (0b01 << kFunct2Shift);
constexpr Opcode RO_FNMADD_D = NMADD | (0b01 << kFunct2Shift);
constexpr Opcode RO_FADD_D = OP_FP | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_FSUB_D = OP_FP | (0b0000101 << kFunct7Shift);
constexpr Opcode RO_FMUL_D = OP_FP | (0b0001001 << kFunct7Shift);
constexpr Opcode RO_FDIV_D = OP_FP | (0b0001101 << kFunct7Shift);
constexpr Opcode RO_FSQRT_D =
    OP_FP | (0b0101101 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FSGNJ_D =
    OP_FP | (0b000 << kFunct3Shift) | (0b0010001 << kFunct7Shift);
constexpr Opcode RO_FSGNJN_D =
    OP_FP | (0b001 << kFunct3Shift) | (0b0010001 << kFunct7Shift);
constexpr Opcode RO_FSQNJX_D =
    OP_FP | (0b010 << kFunct3Shift) | (0b0010001 << kFunct7Shift);
constexpr Opcode RO_FMIN_D =
    OP_FP | (0b000 << kFunct3Shift) | (0b0010101 << kFunct7Shift);
constexpr Opcode RO_FMAX_D =
    OP_FP | (0b001 << kFunct3Shift) | (0b0010101 << kFunct7Shift);
constexpr Opcode RO_FCVT_S_D =
    OP_FP | (0b0100000 << kFunct7Shift) | (0b00001 << kRs2Shift);
constexpr Opcode RO_FCVT_D_S =
    OP_FP | (0b0100001 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FEQ_D =
    OP_FP | (0b010 << kFunct3Shift) | (0b1010001 << kFunct7Shift);
constexpr Opcode RO_FLT_D =
    OP_FP | (0b001 << kFunct3Shift) | (0b1010001 << kFunct7Shift);
constexpr Opcode RO_FLE_D =
    OP_FP | (0b000 << kFunct3Shift) | (0b1010001 << kFunct7Shift);
constexpr Opcode RO_FCLASS_D = OP_FP | (0b001 << kFunct3Shift) |
                               (0b1110001 << kFunct7Shift) |
                               (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_W_D =
    OP_FP | (0b1100001 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_WU_D =
    OP_FP | (0b1100001 << kFunct7Shift) | (0b00001 << kRs2Shift);
constexpr Opcode RO_FCVT_D_W =
    OP_FP | (0b1101001 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_D_WU =
    OP_FP | (0b1101001 << kFunct7Shift) | (0b00001 << kRs2Shift);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64D Standard Extension (in addition to RV32D)
constexpr Opcode RO_FCVT_L_D =
    OP_FP | (0b1100001 << kFunct7Shift) | (0b00010 << kRs2Shift);
constexpr Opcode RO_FCVT_LU_D =
    OP_FP | (0b1100001 << kFunct7Shift) | (0b00011 << kRs2Shift);
constexpr Opcode RO_FMV_X_D = OP_FP | (0b000 << kFunct3Shift) |
                              (0b1110001 << kFunct7Shift) |
                              (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_D_L =
    OP_FP | (0b1101001 << kFunct7Shift) | (0b00010 << kRs2Shift);
constexpr Opcode RO_FCVT_D_LU =
    OP_FP | (0b1101001 << kFunct7Shift) | (0b00011 << kRs2Shift);
constexpr Opcode RO_FMV_D_X = OP_FP | (0b000 << kFunct3Shift) |
                              (0b1111001 << kFunct7Shift) |
                              (0b00000 << kRs2Shift);
#endif
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-f.h                                            0000664 0000000 0000000 00000006224 14746647661 0022743 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// RV32F Standard Extension
constexpr Opcode RO_FLW = LOAD_FP | (0b010 << kFunct3Shift);
constexpr Opcode RO_FSW = STORE_FP | (0b010 << kFunct3Shift);
constexpr Opcode RO_FMADD_S = MADD | (0b00 << kFunct2Shift);
constexpr Opcode RO_FMSUB_S = MSUB | (0b00 << kFunct2Shift);
constexpr Opcode RO_FNMSUB_S = NMSUB | (0b00 << kFunct2Shift);
constexpr Opcode RO_FNMADD_S = NMADD | (0b00 << kFunct2Shift);
constexpr Opcode RO_FADD_S = OP_FP | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_FSUB_S = OP_FP | (0b0000100 << kFunct7Shift);
constexpr Opcode RO_FMUL_S = OP_FP | (0b0001000 << kFunct7Shift);
constexpr Opcode RO_FDIV_S = OP_FP | (0b0001100 << kFunct7Shift);
constexpr Opcode RO_FSQRT_S =
    OP_FP | (0b0101100 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FSGNJ_S =
    OP_FP | (0b000 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_FSGNJN_S =
    OP_FP | (0b001 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_FSQNJX_S =
    OP_FP | (0b010 << kFunct3Shift) | (0b0010000 << kFunct7Shift);
constexpr Opcode RO_FMIN_S =
    OP_FP | (0b000 << kFunct3Shift) | (0b0010100 << kFunct7Shift);
constexpr Opcode RO_FMAX_S =
    OP_FP | (0b001 << kFunct3Shift) | (0b0010100 << kFunct7Shift);
constexpr Opcode RO_FCVT_W_S =
    OP_FP | (0b1100000 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_WU_S =
    OP_FP | (0b1100000 << kFunct7Shift) | (0b00001 << kRs2Shift);
constexpr Opcode RO_FMV = OP_FP | (0b1110000 << kFunct7Shift) |
                          (0b000 << kFunct3Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FEQ_S =
    OP_FP | (0b010 << kFunct3Shift) | (0b1010000 << kFunct7Shift);
constexpr Opcode RO_FLT_S =
    OP_FP | (0b001 << kFunct3Shift) | (0b1010000 << kFunct7Shift);
constexpr Opcode RO_FLE_S =
    OP_FP | (0b000 << kFunct3Shift) | (0b1010000 << kFunct7Shift);
constexpr Opcode RO_FCLASS_S =
    OP_FP | (0b001 << kFunct3Shift) | (0b1110000 << kFunct7Shift);
constexpr Opcode RO_FCVT_S_W =
    OP_FP | (0b1101000 << kFunct7Shift) | (0b00000 << kRs2Shift);
constexpr Opcode RO_FCVT_S_WU =
    OP_FP | (0b1101000 << kFunct7Shift) | (0b00001 << kRs2Shift);
constexpr Opcode RO_FMV_W_X =
    OP_FP | (0b000 << kFunct3Shift) | (0b1111000 << kFunct7Shift);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64F Standard Extension (in addition to RV32F)
constexpr Opcode RO_FCVT_L_S =
    OP_FP | (0b1100000 << kFunct7Shift) | (0b00010 << kRs2Shift);
constexpr Opcode RO_FCVT_LU_S =
    OP_FP | (0b1100000 << kFunct7Shift) | (0b00011 << kRs2Shift);
constexpr Opcode RO_FCVT_S_L =
    OP_FP | (0b1101000 << kFunct7Shift) | (0b00010 << kRs2Shift);
constexpr Opcode RO_FCVT_S_LU =
    OP_FP | (0b1101000 << kFunct7Shift) | (0b00011 << kRs2Shift);
#endif  // V8_TARGET_ARCH_RISCV64
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
                                                                                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-i.h                                            0000664 0000000 0000000 00000010503 14746647661 0022741 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// Note use RO (RiscV Opcode) prefix
// RV32I Base Instruction Set
constexpr Opcode RO_LUI = LUI;
constexpr Opcode RO_AUIPC = AUIPC;
constexpr Opcode RO_JAL = JAL;
constexpr Opcode RO_JALR = JALR | (0b000 << kFunct3Shift);
constexpr Opcode RO_BEQ = BRANCH | (0b000 << kFunct3Shift);
constexpr Opcode RO_BNE = BRANCH | (0b001 << kFunct3Shift);
constexpr Opcode RO_BLT = BRANCH | (0b100 << kFunct3Shift);
constexpr Opcode RO_BGE = BRANCH | (0b101 << kFunct3Shift);
constexpr Opcode RO_BLTU = BRANCH | (0b110 << kFunct3Shift);
constexpr Opcode RO_BGEU = BRANCH | (0b111 << kFunct3Shift);
constexpr Opcode RO_LB = LOAD | (0b000 << kFunct3Shift);
constexpr Opcode RO_LH = LOAD | (0b001 << kFunct3Shift);
constexpr Opcode RO_LW = LOAD | (0b010 << kFunct3Shift);
constexpr Opcode RO_LBU = LOAD | (0b100 << kFunct3Shift);
constexpr Opcode RO_LHU = LOAD | (0b101 << kFunct3Shift);
constexpr Opcode RO_SB = STORE | (0b000 << kFunct3Shift);
constexpr Opcode RO_SH = STORE | (0b001 << kFunct3Shift);
constexpr Opcode RO_SW = STORE | (0b010 << kFunct3Shift);

constexpr Opcode RO_ADDI = OP_IMM | (0b000 << kFunct3Shift);
constexpr Opcode RO_SLTI = OP_IMM | (0b010 << kFunct3Shift);
constexpr Opcode RO_SLTIU = OP_IMM | (0b011 << kFunct3Shift);
constexpr Opcode RO_XORI = OP_IMM | (0b100 << kFunct3Shift);
constexpr Opcode RO_ORI = OP_IMM | (0b110 << kFunct3Shift);
constexpr Opcode RO_ANDI = OP_IMM | (0b111 << kFunct3Shift);

constexpr Opcode OP_SHL = OP_IMM | (0b001 << kFunct3Shift);
constexpr Opcode RO_SLLI = OP_SHL | (0b000000 << kFunct6Shift);

constexpr Opcode OP_SHR = OP_IMM | (0b101 << kFunct3Shift);
constexpr Opcode RO_SRLI = OP_SHR | (0b000000 << kFunct6Shift);
constexpr Opcode RO_SRAI = OP_SHR | (0b010000 << kFunct6Shift);

constexpr Opcode RO_ADD =
    OP | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SUB =
    OP | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
constexpr Opcode RO_SLL =
    OP | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SLT =
    OP | (0b010 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SLTU =
    OP | (0b011 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_XOR =
    OP | (0b100 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SRL =
    OP | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SRA =
    OP | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
constexpr Opcode RO_OR =
    OP | (0b110 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_AND =
    OP | (0b111 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_FENCE = MISC_MEM | (0b000 << kFunct3Shift);
constexpr Opcode RO_ECALL = SYSTEM | (0b000 << kFunct3Shift);
// RO_EBREAK = SYSTEM | (0b000 << kFunct3Shift), // Same as ECALL, use imm12

#if V8_TARGET_ARCH_RISCV64
  // RV64I Base Instruction Set (in addition to RV32I)
constexpr Opcode RO_LWU = LOAD | (0b110 << kFunct3Shift);
constexpr Opcode RO_LD = LOAD | (0b011 << kFunct3Shift);
constexpr Opcode RO_SD = STORE | (0b011 << kFunct3Shift);
constexpr Opcode RO_ADDIW = OP_IMM_32 | (0b000 << kFunct3Shift);

constexpr Opcode OP_SHLW = OP_IMM_32 | (0b001 << kFunct3Shift);
constexpr Opcode RO_SLLIW = OP_SHLW | (0b0000000 << kFunct7Shift);

constexpr Opcode OP_SHRW = OP_IMM_32 | (0b101 << kFunct3Shift);
constexpr Opcode RO_SRLIW = OP_SHRW | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SRAIW = OP_SHRW | (0b0100000 << kFunct7Shift);

constexpr Opcode RO_ADDW =
    OP_32 | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SUBW =
    OP_32 | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
constexpr Opcode RO_SLLW =
    OP_32 | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SRLW =
    OP_32 | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift);
constexpr Opcode RO_SRAW =
    OP_32 | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift);
#endif
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
                                                                                                                                                                                             node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-m.h                                            0000664 0000000 0000000 00000003360 14746647661 0022750 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// RV32M Standard Extension
constexpr Opcode RO_MUL =
    OP | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_MULH =
    OP | (0b001 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_MULHSU =
    OP | (0b010 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_MULHU =
    OP | (0b011 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_DIV =
    OP | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_DIVU =
    OP | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_REM =
    OP | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_REMU =
    OP | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift);

#ifdef V8_TARGET_ARCH_RISCV64
// RV64M Standard Extension (in addition to RV32M)
constexpr Opcode RO_MULW =
    OP_32 | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_DIVW =
    OP_32 | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_DIVUW =
    OP_32 | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_REMW =
    OP_32 | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
constexpr Opcode RO_REMUW =
    OP_32 | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift);
#endif
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_
                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-v.h                                            0000664 0000000 0000000 00000063734 14746647661 0022774 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {

// RVV Extension
constexpr Opcode OP_IVV = OP_V | (0b000 << kFunct3Shift);
constexpr Opcode OP_FVV = OP_V | (0b001 << kFunct3Shift);
constexpr Opcode OP_MVV = OP_V | (0b010 << kFunct3Shift);
constexpr Opcode OP_IVI = OP_V | (0b011 << kFunct3Shift);
constexpr Opcode OP_IVX = OP_V | (0b100 << kFunct3Shift);
constexpr Opcode OP_FVF = OP_V | (0b101 << kFunct3Shift);
constexpr Opcode OP_MVX = OP_V | (0b110 << kFunct3Shift);

constexpr Opcode RO_V_VSETVLI = OP_V | (0b111 << kFunct3Shift) | 0b0 << 31;
constexpr Opcode RO_V_VSETIVLI = OP_V | (0b111 << kFunct3Shift) | 0b11 << 30;
constexpr Opcode RO_V_VSETVL = OP_V | (0b111 << kFunct3Shift) | 0b1 << 31;

// RVV LOAD/STORE
constexpr Opcode RO_V_VL =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift);
constexpr Opcode RO_V_VLS =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift);
constexpr Opcode RO_V_VLX =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift);

constexpr Opcode RO_V_VS =
    STORE_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift);
constexpr Opcode RO_V_VSS =
    STORE_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift);
constexpr Opcode RO_V_VSX =
    STORE_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift);
constexpr Opcode RO_V_VSU =
    STORE_FP | (0b01 << kRvvMopShift) | (0b000 << kRvvNfShift);
// THE kFunct6Shift is mop
constexpr Opcode RO_V_VLSEG2 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG3 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG4 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG5 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG6 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG7 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VLSEG8 =
    LOAD_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift);

constexpr Opcode RO_V_VSSEG2 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG3 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG4 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG5 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG6 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG7 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VSSEG8 =
    STORE_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift);

constexpr Opcode RO_V_VLSSEG2 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG3 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG4 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG5 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG6 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG7 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VLSSEG8 =
    LOAD_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift);

constexpr Opcode RO_V_VSSSEG2 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG3 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG4 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG5 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG6 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG7 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VSSSEG8 =
    STORE_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift);

constexpr Opcode RO_V_VLXSEG2 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG3 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG4 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG5 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG6 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG7 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VLXSEG8 =
    LOAD_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift);

constexpr Opcode RO_V_VSXSEG2 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG3 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG4 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG5 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG6 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG7 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift);
constexpr Opcode RO_V_VSXSEG8 =
    STORE_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift);

// RVV Vector Arithmetic Instruction
constexpr Opcode VADD_FUNCT6 = 0b000000;
constexpr Opcode RO_V_VADD_VI = OP_IVI | (VADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VADD_VV = OP_IVV | (VADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VADD_VX = OP_IVX | (VADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSUB_FUNCT6 = 0b000010;
constexpr Opcode RO_V_VSUB_VX = OP_IVX | (VSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSUB_VV = OP_IVV | (VSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VDIVU_FUNCT6 = 0b100000;
constexpr Opcode RO_V_VDIVU_VX = OP_MVX | (VDIVU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VDIVU_VV = OP_MVV | (VDIVU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VDIV_FUNCT6 = 0b100001;
constexpr Opcode RO_V_VDIV_VX = OP_MVX | (VDIV_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VDIV_VV = OP_MVV | (VDIV_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VREMU_FUNCT6 = 0b100010;
constexpr Opcode RO_V_VREMU_VX = OP_MVX | (VREMU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VREMU_VV = OP_MVV | (VREMU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VREM_FUNCT6 = 0b100011;
constexpr Opcode RO_V_VREM_VX = OP_MVX | (VREM_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VREM_VV = OP_MVV | (VREM_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMULHU_FUNCT6 = 0b100100;
constexpr Opcode RO_V_VMULHU_VX = OP_MVX | (VMULHU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMULHU_VV = OP_MVV | (VMULHU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMUL_FUNCT6 = 0b100101;
constexpr Opcode RO_V_VMUL_VX = OP_MVX | (VMUL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMUL_VV = OP_MVV | (VMUL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWMUL_FUNCT6 = 0b111011;
constexpr Opcode RO_V_VWMUL_VX = OP_MVX | (VWMUL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VWMUL_VV = OP_MVV | (VWMUL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWMULU_FUNCT6 = 0b111000;
constexpr Opcode RO_V_VWMULU_VX = OP_MVX | (VWMULU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VWMULU_VV = OP_MVV | (VWMULU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMULHSU_FUNCT6 = 0b100110;
constexpr Opcode RO_V_VMULHSU_VX = OP_MVX | (VMULHSU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMULHSU_VV = OP_MVV | (VMULHSU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMULH_FUNCT6 = 0b100111;
constexpr Opcode RO_V_VMULH_VX = OP_MVX | (VMULH_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMULH_VV = OP_MVV | (VMULH_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWADD_FUNCT6 = 0b110001;
constexpr Opcode RO_V_VWADD_VV = OP_MVV | (VWADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VWADD_VX = OP_MVX | (VWADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWADDU_FUNCT6 = 0b110000;
constexpr Opcode RO_V_VWADDU_VV = OP_MVV | (VWADDU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VWADDU_VX = OP_MVX | (VWADDU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWADDUW_FUNCT6 = 0b110101;
constexpr Opcode RO_V_VWADDUW_VX = OP_MVX | (VWADDUW_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VWADDUW_VV = OP_MVV | (VWADDUW_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VCOMPRESS_FUNCT6 = 0b010111;
constexpr Opcode RO_V_VCOMPRESS_VV =
    OP_MVV | (VCOMPRESS_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSADDU_FUNCT6 = 0b100000;
constexpr Opcode RO_V_VSADDU_VI = OP_IVI | (VSADDU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSADDU_VV = OP_IVV | (VSADDU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSADDU_VX = OP_IVX | (VSADDU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSADD_FUNCT6 = 0b100001;
constexpr Opcode RO_V_VSADD_VI = OP_IVI | (VSADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSADD_VV = OP_IVV | (VSADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSADD_VX = OP_IVX | (VSADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSSUB_FUNCT6 = 0b100011;
constexpr Opcode RO_V_VSSUB_VV = OP_IVV | (VSSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSSUB_VX = OP_IVX | (VSSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSSUBU_FUNCT6 = 0b100010;
constexpr Opcode RO_V_VSSUBU_VV = OP_IVV | (VSSUBU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSSUBU_VX = OP_IVX | (VSSUBU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VRSUB_FUNCT6 = 0b000011;
constexpr Opcode RO_V_VRSUB_VX = OP_IVX | (VRSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VRSUB_VI = OP_IVI | (VRSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMINU_FUNCT6 = 0b000100;
constexpr Opcode RO_V_VMINU_VX = OP_IVX | (VMINU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMINU_VV = OP_IVV | (VMINU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMIN_FUNCT6 = 0b000101;
constexpr Opcode RO_V_VMIN_VX = OP_IVX | (VMIN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMIN_VV = OP_IVV | (VMIN_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMAXU_FUNCT6 = 0b000110;
constexpr Opcode RO_V_VMAXU_VX = OP_IVX | (VMAXU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMAXU_VV = OP_IVV | (VMAXU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMAX_FUNCT6 = 0b000111;
constexpr Opcode RO_V_VMAX_VX = OP_IVX | (VMAX_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMAX_VV = OP_IVV | (VMAX_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VAND_FUNCT6 = 0b001001;
constexpr Opcode RO_V_VAND_VI = OP_IVI | (VAND_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VAND_VV = OP_IVV | (VAND_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VAND_VX = OP_IVX | (VAND_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VOR_FUNCT6 = 0b001010;
constexpr Opcode RO_V_VOR_VI = OP_IVI | (VOR_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VOR_VV = OP_IVV | (VOR_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VOR_VX = OP_IVX | (VOR_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VXOR_FUNCT6 = 0b001011;
constexpr Opcode RO_V_VXOR_VI = OP_IVI | (VXOR_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VXOR_VV = OP_IVV | (VXOR_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VXOR_VX = OP_IVX | (VXOR_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VRGATHER_FUNCT6 = 0b001100;
constexpr Opcode RO_V_VRGATHER_VI =
    OP_IVI | (VRGATHER_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VRGATHER_VV =
    OP_IVV | (VRGATHER_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VRGATHER_VX =
    OP_IVX | (VRGATHER_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMV_FUNCT6 = 0b010111;
constexpr Opcode RO_V_VMV_VI = OP_IVI | (VMV_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMV_VV = OP_IVV | (VMV_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMV_VX = OP_IVX | (VMV_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMV_VF = OP_FVF | (VMV_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode RO_V_VMERGE_VI = RO_V_VMV_VI;
constexpr Opcode RO_V_VMERGE_VV = RO_V_VMV_VV;
constexpr Opcode RO_V_VMERGE_VX = RO_V_VMV_VX;
constexpr Opcode RO_V_VFMERGE_VF = RO_V_VFMV_VF;

constexpr Opcode VMSEQ_FUNCT6 = 0b011000;
constexpr Opcode RO_V_VMSEQ_VI = OP_IVI | (VMSEQ_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSEQ_VV = OP_IVV | (VMSEQ_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSEQ_VX = OP_IVX | (VMSEQ_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSNE_FUNCT6 = 0b011001;
constexpr Opcode RO_V_VMSNE_VI = OP_IVI | (VMSNE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSNE_VV = OP_IVV | (VMSNE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSNE_VX = OP_IVX | (VMSNE_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSLTU_FUNCT6 = 0b011010;
constexpr Opcode RO_V_VMSLTU_VV = OP_IVV | (VMSLTU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLTU_VX = OP_IVX | (VMSLTU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSLT_FUNCT6 = 0b011011;
constexpr Opcode RO_V_VMSLT_VV = OP_IVV | (VMSLT_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLT_VX = OP_IVX | (VMSLT_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSLE_FUNCT6 = 0b011101;
constexpr Opcode RO_V_VMSLE_VI = OP_IVI | (VMSLE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLE_VV = OP_IVV | (VMSLE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLE_VX = OP_IVX | (VMSLE_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSLEU_FUNCT6 = 0b011100;
constexpr Opcode RO_V_VMSLEU_VI = OP_IVI | (VMSLEU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLEU_VV = OP_IVV | (VMSLEU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSLEU_VX = OP_IVX | (VMSLEU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSGTU_FUNCT6 = 0b011110;
constexpr Opcode RO_V_VMSGTU_VI = OP_IVI | (VMSGTU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSGTU_VX = OP_IVX | (VMSGTU_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMSGT_FUNCT6 = 0b011111;
constexpr Opcode RO_V_VMSGT_VI = OP_IVI | (VMSGT_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMSGT_VX = OP_IVX | (VMSGT_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSLIDEUP_FUNCT6 = 0b001110;
constexpr Opcode RO_V_VSLIDEUP_VI =
    OP_IVI | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLIDEUP_VX =
    OP_IVX | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLIDE1UP_VX =
    OP_MVX | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSLIDE1UP_VF =
    OP_FVF | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSLIDEDOWN_FUNCT6 = 0b001111;
constexpr Opcode RO_V_VSLIDEDOWN_VI =
    OP_IVI | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLIDEDOWN_VX =
    OP_IVX | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLIDE1DOWN_VX =
    OP_MVX | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSLIDE1DOWN_VF =
    OP_FVF | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSRL_FUNCT6 = 0b101000;
constexpr Opcode RO_V_VSRL_VI = OP_IVI | (VSRL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSRL_VV = OP_IVV | (VSRL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSRL_VX = OP_IVX | (VSRL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSRA_FUNCT6 = 0b101001;
constexpr Opcode RO_V_VSRA_VI = OP_IVI | (VSRA_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSRA_VV = OP_IVV | (VSRA_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSRA_VX = OP_IVX | (VSRA_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSLL_FUNCT6 = 0b100101;
constexpr Opcode RO_V_VSLL_VI = OP_IVI | (VSLL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLL_VV = OP_IVV | (VSLL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSLL_VX = OP_IVX | (VSLL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VSMUL_FUNCT6 = 0b100111;
constexpr Opcode RO_V_VSMUL_VV = OP_IVV | (VSMUL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VSMUL_VX = OP_IVX | (VSMUL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VADC_FUNCT6 = 0b010000;
constexpr Opcode RO_V_VADC_VI = OP_IVI | (VADC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VADC_VV = OP_IVV | (VADC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VADC_VX = OP_IVX | (VADC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMADC_FUNCT6 = 0b010001;
constexpr Opcode RO_V_VMADC_VI = OP_IVI | (VMADC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMADC_VV = OP_IVV | (VMADC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMADC_VX = OP_IVX | (VMADC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWXUNARY0_FUNCT6 = 0b010000;
constexpr Opcode VRXUNARY0_FUNCT6 = 0b010000;
constexpr Opcode VMUNARY0_FUNCT6 = 0b010100;

constexpr Opcode RO_V_VWXUNARY0 =
    OP_MVV | (VWXUNARY0_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VRXUNARY0 =
    OP_MVX | (VRXUNARY0_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMUNARY0 = OP_MVV | (VMUNARY0_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VID_V = 0b10001;

constexpr Opcode VXUNARY0_FUNCT6 = 0b010010;
constexpr Opcode RO_V_VXUNARY0 = OP_MVV | (VXUNARY0_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VWFUNARY0_FUNCT6 = 0b010000;
constexpr Opcode RO_V_VFMV_FS = OP_FVV | (VWFUNARY0_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VRFUNARY0_FUNCT6 = 0b010000;
constexpr Opcode RO_V_VFMV_SF = OP_FVF | (VRFUNARY0_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VREDMAXU_FUNCT6 = 0b000110;
constexpr Opcode RO_V_VREDMAXU = OP_MVV | (VREDMAXU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode VREDMAX_FUNCT6 = 0b000111;
constexpr Opcode RO_V_VREDMAX = OP_MVV | (VREDMAX_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VREDMINU_FUNCT6 = 0b000100;
constexpr Opcode RO_V_VREDMINU = OP_MVV | (VREDMINU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode VREDMIN_FUNCT6 = 0b000101;
constexpr Opcode RO_V_VREDMIN = OP_MVV | (VREDMIN_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFUNARY0_FUNCT6 = 0b010010;
constexpr Opcode RO_V_VFUNARY0 = OP_FVV | (VFUNARY0_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode VFUNARY1_FUNCT6 = 0b010011;
constexpr Opcode RO_V_VFUNARY1 = OP_FVV | (VFUNARY1_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFCVT_XU_F_V = 0b00000;
constexpr Opcode VFCVT_X_F_V = 0b00001;
constexpr Opcode VFCVT_F_XU_V = 0b00010;
constexpr Opcode VFCVT_F_X_V = 0b00011;
constexpr Opcode VFWCVT_XU_F_V = 0b01000;
constexpr Opcode VFWCVT_X_F_V = 0b01001;
constexpr Opcode VFWCVT_F_XU_V = 0b01010;
constexpr Opcode VFWCVT_F_X_V = 0b01011;
constexpr Opcode VFWCVT_F_F_V = 0b01100;
constexpr Opcode VFNCVT_F_F_W = 0b10100;
constexpr Opcode VFNCVT_X_F_W = 0b10001;
constexpr Opcode VFNCVT_XU_F_W = 0b10000;

constexpr Opcode VFCLASS_V = 0b10000;
constexpr Opcode VFSQRT_V = 0b00000;
constexpr Opcode VFRSQRT7_V = 0b00100;
constexpr Opcode VFREC7_V = 0b00101;

constexpr Opcode VFADD_FUNCT6 = 0b000000;
constexpr Opcode RO_V_VFADD_VV = OP_FVV | (VFADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFADD_VF = OP_FVF | (VFADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFSUB_FUNCT6 = 0b000010;
constexpr Opcode RO_V_VFSUB_VV = OP_FVV | (VFSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSUB_VF = OP_FVF | (VFSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFDIV_FUNCT6 = 0b100000;
constexpr Opcode RO_V_VFDIV_VV = OP_FVV | (VFDIV_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFDIV_VF = OP_FVF | (VFDIV_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMUL_FUNCT6 = 0b100100;
constexpr Opcode RO_V_VFMUL_VV = OP_FVV | (VFMUL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMUL_VF = OP_FVF | (VFMUL_FUNCT6 << kRvvFunct6Shift);

// Vector Widening Floating-Point Add/Subtract Instructions
constexpr Opcode VFWADD_FUNCT6 = 0b110000;
constexpr Opcode RO_V_VFWADD_VV = OP_FVV | (VFWADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWADD_VF = OP_FVF | (VFWADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWSUB_FUNCT6 = 0b110010;
constexpr Opcode RO_V_VFWSUB_VV = OP_FVV | (VFWSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWSUB_VF = OP_FVF | (VFWSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWADD_W_FUNCT6 = 0b110100;
constexpr Opcode RO_V_VFWADD_W_VV =
    OP_FVV | (VFWADD_W_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWADD_W_VF =
    OP_FVF | (VFWADD_W_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWSUB_W_FUNCT6 = 0b110110;
constexpr Opcode RO_V_VFWSUB_W_VV =
    OP_FVV | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWSUB_W_VF =
    OP_FVF | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift);

// Vector Widening Floating-Point Reduction Instructions
constexpr Opcode VFWREDUSUM_FUNCT6 = 0b110001;
constexpr Opcode RO_V_VFWREDUSUM_VS =
    OP_FVV | (VFWREDUSUM_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWREDOSUM_FUNCT6 = 0b110011;
constexpr Opcode RO_V_VFWREDOSUM_VS =
    OP_FVV | (VFWREDOSUM_FUNCT6 << kRvvFunct6Shift);

// Vector Widening Floating-Point Multiply
constexpr Opcode VFWMUL_FUNCT6 = 0b111000;
constexpr Opcode RO_V_VFWMUL_VV = OP_FVV | (VFWMUL_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWMUL_VF = OP_FVF | (VFWMUL_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFEQ_FUNCT6 = 0b011000;
constexpr Opcode RO_V_VMFEQ_VV = OP_FVV | (VMFEQ_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMFEQ_VF = OP_FVF | (VMFEQ_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFNE_FUNCT6 = 0b011100;
constexpr Opcode RO_V_VMFNE_VV = OP_FVV | (VMFNE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMFNE_VF = OP_FVF | (VMFNE_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFLT_FUNCT6 = 0b011011;
constexpr Opcode RO_V_VMFLT_VV = OP_FVV | (VMFLT_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMFLT_VF = OP_FVF | (VMFLT_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFLE_FUNCT6 = 0b011001;
constexpr Opcode RO_V_VMFLE_VV = OP_FVV | (VMFLE_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VMFLE_VF = OP_FVF | (VMFLE_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFGE_FUNCT6 = 0b011111;
constexpr Opcode RO_V_VMFGE_VF = OP_FVF | (VMFGE_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VMFGT_FUNCT6 = 0b011101;
constexpr Opcode RO_V_VMFGT_VF = OP_FVF | (VMFGT_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMAX_FUNCT6 = 0b000110;
constexpr Opcode RO_V_VFMAX_VV = OP_FVV | (VFMAX_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMAX_VF = OP_FVF | (VFMAX_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFREDMAX_FUNCT6 = 0b0001111;
constexpr Opcode RO_V_VFREDMAX_VV =
    OP_FVV | (VFREDMAX_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMIN_FUNCT6 = 0b000100;
constexpr Opcode RO_V_VFMIN_VV = OP_FVV | (VFMIN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMIN_VF = OP_FVF | (VFMIN_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFSGNJ_FUNCT6 = 0b001000;
constexpr Opcode RO_V_VFSGNJ_VV = OP_FVV | (VFSGNJ_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSGNJ_VF = OP_FVF | (VFSGNJ_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFSGNJN_FUNCT6 = 0b001001;
constexpr Opcode RO_V_VFSGNJN_VV = OP_FVV | (VFSGNJN_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSGNJN_VF = OP_FVF | (VFSGNJN_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFSGNJX_FUNCT6 = 0b001010;
constexpr Opcode RO_V_VFSGNJX_VV = OP_FVV | (VFSGNJX_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFSGNJX_VF = OP_FVF | (VFSGNJX_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMADD_FUNCT6 = 0b101000;
constexpr Opcode RO_V_VFMADD_VV = OP_FVV | (VFMADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMADD_VF = OP_FVF | (VFMADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFNMADD_FUNCT6 = 0b101001;
constexpr Opcode RO_V_VFNMADD_VV = OP_FVV | (VFNMADD_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFNMADD_VF = OP_FVF | (VFNMADD_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMSUB_FUNCT6 = 0b101010;
constexpr Opcode RO_V_VFMSUB_VV = OP_FVV | (VFMSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMSUB_VF = OP_FVF | (VFMSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFNMSUB_FUNCT6 = 0b101011;
constexpr Opcode RO_V_VFNMSUB_VV = OP_FVV | (VFNMSUB_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFNMSUB_VF = OP_FVF | (VFNMSUB_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMACC_FUNCT6 = 0b101100;
constexpr Opcode RO_V_VFMACC_VV = OP_FVV | (VFMACC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMACC_VF = OP_FVF | (VFMACC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFNMACC_FUNCT6 = 0b101101;
constexpr Opcode RO_V_VFNMACC_VV = OP_FVV | (VFNMACC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFNMACC_VF = OP_FVF | (VFNMACC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFMSAC_FUNCT6 = 0b101110;
constexpr Opcode RO_V_VFMSAC_VV = OP_FVV | (VFMSAC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFMSAC_VF = OP_FVF | (VFMSAC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFNMSAC_FUNCT6 = 0b101111;
constexpr Opcode RO_V_VFNMSAC_VV = OP_FVV | (VFNMSAC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFNMSAC_VF = OP_FVF | (VFNMSAC_FUNCT6 << kRvvFunct6Shift);

// Vector Widening Floating-Point Fused Multiply-Add Instructions
constexpr Opcode VFWMACC_FUNCT6 = 0b111100;
constexpr Opcode RO_V_VFWMACC_VV = OP_FVV | (VFWMACC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWMACC_VF = OP_FVF | (VFWMACC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWNMACC_FUNCT6 = 0b111101;
constexpr Opcode RO_V_VFWNMACC_VV =
    OP_FVV | (VFWNMACC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWNMACC_VF =
    OP_FVF | (VFWNMACC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWMSAC_FUNCT6 = 0b111110;
constexpr Opcode RO_V_VFWMSAC_VV = OP_FVV | (VFWMSAC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWMSAC_VF = OP_FVF | (VFWMSAC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VFWNMSAC_FUNCT6 = 0b111111;
constexpr Opcode RO_V_VFWNMSAC_VV =
    OP_FVV | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VFWNMSAC_VF =
    OP_FVF | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VNCLIP_FUNCT6 = 0b101111;
constexpr Opcode RO_V_VNCLIP_WV = OP_IVV | (VNCLIP_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VNCLIP_WX = OP_IVX | (VNCLIP_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VNCLIP_WI = OP_IVI | (VNCLIP_FUNCT6 << kRvvFunct6Shift);

constexpr Opcode VNCLIPU_FUNCT6 = 0b101110;
constexpr Opcode RO_V_VNCLIPU_WV = OP_IVV | (VNCLIPU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VNCLIPU_WX = OP_IVX | (VNCLIPU_FUNCT6 << kRvvFunct6Shift);
constexpr Opcode RO_V_VNCLIPU_WI = OP_IVI | (VNCLIPU_FUNCT6 << kRvvFunct6Shift);
// clang-format on
}  // namespace internal
}  // namespace v8

#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_
                                    node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-zicond.h                                       0000664 0000000 0000000 00000001254 14746647661 0024002 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2024 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICOND_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICOND_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {
// RV32/RV64 Zicond Standard Extension
constexpr Opcode RO_CZERO_EQZ =
    OP | (0b101 << kFunct3Shift) | (0b0000111 << kFunct7Shift);
constexpr Opcode RO_CZERO_NEZ =
    OP | (0b111 << kFunct3Shift) | (0b0000111 << kFunct7Shift);
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICOND_H_
                                                                                                                                                                                                                                                                                                                                                    node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-zicsr.h                                        0000664 0000000 0000000 00000002340 14746647661 0023643 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {
// RISCV CSR related bit mask and shift
const int kFcsrFlagsBits = 5;
const uint32_t kFcsrFlagsMask = (1 << kFcsrFlagsBits) - 1;
const int kFcsrFrmBits = 3;
const int kFcsrFrmShift = kFcsrFlagsBits;
const uint32_t kFcsrFrmMask = ((1 << kFcsrFrmBits) - 1) << kFcsrFrmShift;
const int kFcsrBits = kFcsrFlagsBits + kFcsrFrmBits;
const uint32_t kFcsrMask = kFcsrFlagsMask | kFcsrFrmMask;

// RV32/RV64 Zicsr Standard Extension
constexpr Opcode RO_CSRRW = SYSTEM | (0b001 << kFunct3Shift);
constexpr Opcode RO_CSRRS = SYSTEM | (0b010 << kFunct3Shift);
constexpr Opcode RO_CSRRC = SYSTEM | (0b011 << kFunct3Shift);
constexpr Opcode RO_CSRRWI = SYSTEM | (0b101 << kFunct3Shift);
constexpr Opcode RO_CSRRSI = SYSTEM | (0b110 << kFunct3Shift);
constexpr Opcode RO_CSRRCI = SYSTEM | (0b111 << kFunct3Shift);
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_
                                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/codegen/riscv/constant-riscv-zifencei.h                                     0000664 0000000 0000000 00000000766 14746647661 0024317 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_
#define V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_

#include "src/codegen/riscv/base-constants-riscv.h"
namespace v8 {
namespace internal {
constexpr Opcode RO_FENCE_I = MISC_MEM | (0b001 << kFunct3Shift);
}
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_
          node-23.7.0/deps/v8/src/codegen/riscv/constants-riscv.h                                             0000664 0000000 0000000 00000001635 14746647661 0022704 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2021 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
#define V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
#include "src/codegen/riscv/base-constants-riscv.h"
#include "src/codegen/riscv/constant-riscv-a.h"
#include "src/codegen/riscv/constant-riscv-b.h"
#include "src/codegen/riscv/constant-riscv-c.h"
#include "src/codegen/riscv/constant-riscv-d.h"
#include "src/codegen/riscv/constant-riscv-f.h"
#include "src/codegen/riscv/constant-riscv-i.h"
#include "src/codegen/riscv/constant-riscv-m.h"
#include "src/codegen/riscv/constant-riscv-v.h"
#include "src/codegen/riscv/constant-riscv-zicsr.h"
#include "src/codegen/riscv/constant-riscv-zifencei.h"
namespace v8 {
namespace internal {}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
                                                                                                   node-23.7.0/deps/v8/src/codegen/riscv/cpu-riscv.cc                                                  0000664 0000000 0000000 00000002157 14746647661 0021615 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2021 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

// CPU specific code for arm independent of OS goes here.

#include <sys/syscall.h>
#include <unistd.h>

#include "src/codegen/cpu-features.h"

namespace v8 {
namespace internal {

void CpuFeatures::FlushICache(void* start, size_t size) {
#if !defined(USE_SIMULATOR)
  char* end = reinterpret_cast<char*>(start) + size;
  // The definition of this syscall is equal to
  // SYSCALL_DEFINE3(riscv_flush_icache, uintptr_t, start,
  //                 uintptr_t, end, uintptr_t, flags)
  // The flag here is set to be SYS_RISCV_FLUSH_ICACHE_LOCAL, which is
  // defined as 1 in the Linux kernel.
  // SYS_riscv_flush_icache is a symbolic constant used in user-space code to
  // identify the flush_icache system call, while __NR_riscv_flush_icache is the
  // corresponding system call number used in the kernel to dispatch the system
  // call.
  syscall(__NR_riscv_flush_icache, start, end, 1);
#endif  // !USE_SIMULATOR.
}

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-a.cc                                          0000664 0000000 0000000 00000010367 14746647661 0023262 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-a.h"

namespace v8 {
namespace internal {

// RV32A Standard Extension
void AssemblerRISCVA::lr_w(bool aq, bool rl, Register rd, Register rs1) {
  GenInstrRAtomic(0b00010, aq, rl, 0b010, rd, rs1, zero_reg);
}

void AssemblerRISCVA::sc_w(bool aq, bool rl, Register rd, Register rs1,
                           Register rs2) {
  GenInstrRAtomic(0b00011, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amoswap_w(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b00001, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amoadd_w(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b00000, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amoxor_w(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b00100, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amoand_w(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b01100, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amoor_w(bool aq, bool rl, Register rd, Register rs1,
                              Register rs2) {
  GenInstrRAtomic(0b01000, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amomin_w(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b10000, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amomax_w(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b10100, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amominu_w(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b11000, aq, rl, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVA::amomaxu_w(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b11100, aq, rl, 0b010, rd, rs1, rs2);
}

// RV64A Standard Extension (in addition to RV32A)
#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVA::lr_d(bool aq, bool rl, Register rd, Register rs1) {
  GenInstrRAtomic(0b00010, aq, rl, 0b011, rd, rs1, zero_reg);
}

void AssemblerRISCVA::sc_d(bool aq, bool rl, Register rd, Register rs1,
                           Register rs2) {
  GenInstrRAtomic(0b00011, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amoswap_d(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b00001, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amoadd_d(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b00000, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amoxor_d(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b00100, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amoand_d(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b01100, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amoor_d(bool aq, bool rl, Register rd, Register rs1,
                              Register rs2) {
  GenInstrRAtomic(0b01000, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amomin_d(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b10000, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amomax_d(bool aq, bool rl, Register rd, Register rs1,
                               Register rs2) {
  GenInstrRAtomic(0b10100, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amominu_d(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b11000, aq, rl, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVA::amomaxu_d(bool aq, bool rl, Register rd, Register rs1,
                                Register rs2) {
  GenInstrRAtomic(0b11100, aq, rl, 0b011, rd, rs1, rs2);
}
#endif
}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                         node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-a.h                                           0000664 0000000 0000000 00000004523 14746647661 0023121 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-a.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_

namespace v8 {
namespace internal {
class AssemblerRISCVA : public AssemblerRiscvBase {
  // RV32A Standard Extension
 public:
  void lr_w(bool aq, bool rl, Register rd, Register rs1);
  void sc_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoswap_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoadd_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoxor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoand_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomin_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomax_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amominu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomaxu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64A Standard Extension (in addition to RV32A)
  void lr_d(bool aq, bool rl, Register rd, Register rs1);
  void sc_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoswap_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoadd_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoxor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoand_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amoor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomin_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomax_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amominu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
  void amomaxu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
#endif
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_
                                                                                                                                                                             node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-b.cc                                          0000664 0000000 0000000 00000014112 14746647661 0023253 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-b.h"

#include "src/codegen/riscv/base-assembler-riscv.h"
namespace v8 {
namespace internal {

// RV32B Standard Extension
void AssemblerRISCVB::sh1add(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0010000, 0b010, rd, rs1, rs2);
}
void AssemblerRISCVB::sh2add(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0010000, 0b100, rd, rs1, rs2);
}
void AssemblerRISCVB::sh3add(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0010000, 0b110, rd, rs1, rs2);
}
#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVB::adduw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000100, 0b000, rd, rs1, rs2);
}
void AssemblerRISCVB::sh1adduw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0010000, 0b010, rd, rs1, rs2);
}
void AssemblerRISCVB::sh2adduw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0010000, 0b100, rd, rs1, rs2);
}
void AssemblerRISCVB::sh3adduw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0010000, 0b110, rd, rs1, rs2);
}
void AssemblerRISCVB::slliuw(Register rd, Register rs1, uint8_t shamt) {
  GenInstrIShift(0b000010, 0b001, OP_IMM_32, rd, rs1, shamt);
}
#endif  // V8_TARGET_ARCH_RISCV64


void AssemblerRISCVB::andn(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100000, 0b111, rd, rs1, rs2);
}
void AssemblerRISCVB::orn(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100000, 0b110, rd, rs1, rs2);
}
void AssemblerRISCVB::xnor(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100000, 0b100, rd, rs1, rs2);
}

void AssemblerRISCVB::clz(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM, rd, rs, 0);
}
void AssemblerRISCVB::ctz(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM, rd, rs, 1);
}
void AssemblerRISCVB::cpop(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM, rd, rs, 2);
}
#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVB::clzw(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM_32, rd, rs, 0);
}
void AssemblerRISCVB::ctzw(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM_32, rd, rs, 1);
}
void AssemblerRISCVB::cpopw(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM_32, rd, rs, 2);
}
#endif

void AssemblerRISCVB::max(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000101, 0b110, rd, rs1, rs2);
}
void AssemblerRISCVB::maxu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000101, 0b111, rd, rs1, rs2);
}
void AssemblerRISCVB::min(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000101, 0b100, rd, rs1, rs2);
}
void AssemblerRISCVB::minu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000101, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVB::sextb(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM, rd, rs, 0b100);
}
void AssemblerRISCVB::sexth(Register rd, Register rs) {
  GenInstrIShiftW(0b0110000, 0b001, OP_IMM, rd, rs, 0b101);
}
void AssemblerRISCVB::zexth(Register rd, Register rs) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrALUW_rr(0b0000100, 0b100, rd, rs, zero_reg);
#else
  GenInstrALU_rr(0b0000100, 0b100, rd, rs, zero_reg);
#endif
}

void AssemblerRISCVB::rol(Register rd, Register rs1, Register rs2) {
  GenInstrR(0b0110000, 0b001, OP, rd, rs1, rs2);
}

void AssemblerRISCVB::ror(Register rd, Register rs1, Register rs2) {
  GenInstrR(0b0110000, 0b101, OP, rd, rs1, rs2);
}

void AssemblerRISCVB::orcb(Register rd, Register rs) {
  GenInstrI(0b101, OP_IMM, rd, rs, 0b001010000111);
}

void AssemblerRISCVB::rori(Register rd, Register rs1, uint8_t shamt) {
#ifdef V8_TARGET_ARCH_RISCV64
  DCHECK(is_uint6(shamt));
  GenInstrI(0b101, OP_IMM, rd, rs1, 0b011000000000 | shamt);
#else
  DCHECK(is_uint5(shamt));
  GenInstrI(0b101, OP_IMM, rd, rs1, 0b011000000000 | shamt);
#endif
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVB::rolw(Register rd, Register rs1, Register rs2) {
  GenInstrR(0b0110000, 0b001, OP_32, rd, rs1, rs2);
}
void AssemblerRISCVB::roriw(Register rd, Register rs1, uint8_t shamt) {
  DCHECK(is_uint5(shamt));
  GenInstrI(0b101, OP_IMM_32, rd, rs1, 0b011000000000 | shamt);
}
void AssemblerRISCVB::rorw(Register rd, Register rs1, Register rs2) {
  GenInstrR(0b0110000, 0b101, OP_32, rd, rs1, rs2);
}
#endif

void AssemblerRISCVB::rev8(Register rd, Register rs) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrI(0b101, OP_IMM, rd, rs, 0b011010111000);
#else
  GenInstrI(0b101, OP_IMM, rd, rs, 0b011010011000);
#endif
}


void AssemblerRISCVB::bclr(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100100, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVB::bclri(Register rd, Register rs, uint8_t shamt) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrIShift(0b010010, 0b001, OP_IMM, rd, rs, shamt);
#else
  GenInstrIShiftW(0b0100100, 0b001, OP_IMM, rd, rs, shamt);
#endif
}
void AssemblerRISCVB::bext(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0100100, 0b101, rd, rs1, rs2);
}
void AssemblerRISCVB::bexti(Register rd, Register rs1, uint8_t shamt) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrIShift(0b010010, 0b101, OP_IMM, rd, rs1, shamt);
#else
  GenInstrIShiftW(0b0100100, 0b101, OP_IMM, rd, rs1, shamt);
#endif
}
void AssemblerRISCVB::binv(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0110100, 0b001, rd, rs1, rs2);
}
void AssemblerRISCVB::binvi(Register rd, Register rs1, uint8_t shamt) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrIShift(0b011010, 0b001, OP_IMM, rd, rs1, shamt);
#else
  GenInstrIShiftW(0b0110100, 0b001, OP_IMM, rd, rs1, shamt);
#endif
}
void AssemblerRISCVB::bset(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0010100, 0b001, rd, rs1, rs2);
}
void AssemblerRISCVB::bseti(Register rd, Register rs1, uint8_t shamt) {
#ifdef V8_TARGET_ARCH_RISCV64
  GenInstrIShift(0b001010, 0b001, OP_IMM, rd, rs1, shamt);
#else
  GenInstrIShiftW(0b0010100, 0b001, OP_IMM, rd, rs1, shamt);
#endif
}
}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                                                                                                                                                                                      node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-b.h                                           0000664 0000000 0000000 00000005575 14746647661 0023132 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-b.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_B_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_B_H_

namespace v8 {
namespace internal {
class AssemblerRISCVB : public AssemblerRiscvBase {
  // RV32B Extension
 public:
  // Zba Extension
  void sh1add(Register rd, Register rs1, Register rs2);
  void sh2add(Register rd, Register rs1, Register rs2);
  void sh3add(Register rd, Register rs1, Register rs2);
#ifdef V8_TARGET_ARCH_RISCV64
  void adduw(Register rd, Register rs1, Register rs2);
  void zextw(Register rd, Register rs1) { adduw(rd, rs1, zero_reg); }
  void sh1adduw(Register rd, Register rs1, Register rs2);
  void sh2adduw(Register rd, Register rs1, Register rs2);
  void sh3adduw(Register rd, Register rs1, Register rs2);
  void slliuw(Register rd, Register rs1, uint8_t shamt);
#endif

  // Zbb Extension
  void andn(Register rd, Register rs1, Register rs2);
  void orn(Register rd, Register rs1, Register rs2);
  void xnor(Register rd, Register rs1, Register rs2);

  void clz(Register rd, Register rs);
  void ctz(Register rd, Register rs);
  void cpop(Register rd, Register rs);
#ifdef V8_TARGET_ARCH_RISCV64
  void clzw(Register rd, Register rs);
  void ctzw(Register rd, Register rs);
  void cpopw(Register rd, Register rs);
#endif

  void max(Register rd, Register rs1, Register rs2);
  void maxu(Register rd, Register rs1, Register rs2);
  void min(Register rd, Register rs1, Register rs2);
  void minu(Register rd, Register rs1, Register rs2);

  void sextb(Register rd, Register rs);
  void sexth(Register rd, Register rs);
  void zexth(Register rd, Register rs);

  // Zbb: bitwise rotation
  void rol(Register rd, Register rs1, Register rs2);
  void ror(Register rd, Register rs1, Register rs2);
  void rori(Register rd, Register rs1, uint8_t shamt);
  void orcb(Register rd, Register rs);
  void rev8(Register rd, Register rs);
#ifdef V8_TARGET_ARCH_RISCV64
  void rolw(Register rd, Register rs1, Register rs2);
  void roriw(Register rd, Register rs1, uint8_t shamt);
  void rorw(Register rd, Register rs1, Register rs2);
#endif

  // Zbs
  void bclr(Register rd, Register rs1, Register rs2);
  void bclri(Register rd, Register rs1, uint8_t shamt);
  void bext(Register rd, Register rs1, Register rs2);
  void bexti(Register rd, Register rs1, uint8_t shamt);
  void binv(Register rd, Register rs1, Register rs2);
  void binvi(Register rd, Register rs1, uint8_t shamt);
  void bset(Register rd, Register rs1, Register rs2);
  void bseti(Register rd, Register rs1, uint8_t shamt);
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_B_H_
                                                                                                                                   node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-c.cc                                          0000664 0000000 0000000 00000022041 14746647661 0023254 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-c.h"

namespace v8 {
namespace internal {
// RV64C Standard Extension
void AssemblerRISCVC::c_nop() { GenInstrCI(0b000, C1, zero_reg, 0); }

void AssemblerRISCVC::c_addi(Register rd, int8_t imm6) {
  DCHECK(rd != zero_reg && imm6 != 0);
  GenInstrCI(0b000, C1, rd, imm6);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_addiw(Register rd, int8_t imm6) {
  DCHECK(rd != zero_reg);
  GenInstrCI(0b001, C1, rd, imm6);
}
#endif

void AssemblerRISCVC::c_addi16sp(int16_t imm10) {
  DCHECK(is_int10(imm10) && (imm10 & 0xf) == 0);
  uint8_t uimm6 = ((imm10 & 0x200) >> 4) | (imm10 & 0x10) |
                  ((imm10 & 0x40) >> 3) | ((imm10 & 0x180) >> 6) |
                  ((imm10 & 0x20) >> 5);
  GenInstrCIU(0b011, C1, sp, uimm6);
}

void AssemblerRISCVC::c_addi4spn(Register rd, int16_t uimm10) {
  DCHECK(is_uint10(uimm10) && (uimm10 != 0));
  uint8_t uimm8 = ((uimm10 & 0x4) >> 1) | ((uimm10 & 0x8) >> 3) |
                  ((uimm10 & 0x30) << 2) | ((uimm10 & 0x3c0) >> 4);
  GenInstrCIW(0b000, C0, rd, uimm8);
}

void AssemblerRISCVC::c_li(Register rd, int8_t imm6) {
  DCHECK(rd != zero_reg);
  GenInstrCI(0b010, C1, rd, imm6);
}

void AssemblerRISCVC::c_lui(Register rd, int8_t imm6) {
  DCHECK(rd != zero_reg && rd != sp && imm6 != 0);
  GenInstrCI(0b011, C1, rd, imm6);
}

void AssemblerRISCVC::c_slli(Register rd, uint8_t shamt6) {
  DCHECK(rd != zero_reg && shamt6 != 0);
  GenInstrCIU(0b000, C2, rd, shamt6);
}

void AssemblerRISCVC::c_fldsp(FPURegister rd, uint16_t uimm9) {
  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
  GenInstrCIU(0b001, C2, rd, uimm6);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_ldsp(Register rd, uint16_t uimm9) {
  DCHECK(rd != zero_reg && is_uint9(uimm9) && (uimm9 & 0x7) == 0);
  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
  GenInstrCIU(0b011, C2, rd, uimm6);
}
#endif

void AssemblerRISCVC::c_lwsp(Register rd, uint16_t uimm8) {
  DCHECK(rd != zero_reg && is_uint8(uimm8) && (uimm8 & 0x3) == 0);
  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
  GenInstrCIU(0b010, C2, rd, uimm6);
}

void AssemblerRISCVC::c_jr(Register rs1) {
  DCHECK(rs1 != zero_reg);
  GenInstrCR(0b1000, C2, rs1, zero_reg);
  BlockTrampolinePoolFor(1);
}

void AssemblerRISCVC::c_mv(Register rd, Register rs2) {
  DCHECK(rd != zero_reg && rs2 != zero_reg);
  GenInstrCR(0b1000, C2, rd, rs2);
}

void AssemblerRISCVC::c_ebreak() { GenInstrCR(0b1001, C2, zero_reg, zero_reg); }

void AssemblerRISCVC::c_jalr(Register rs1) {
  DCHECK(rs1 != zero_reg);
  GenInstrCR(0b1001, C2, rs1, zero_reg);
  BlockTrampolinePoolFor(1);
}

void AssemblerRISCVC::c_add(Register rd, Register rs2) {
  DCHECK(rd != zero_reg && rs2 != zero_reg);
  GenInstrCR(0b1001, C2, rd, rs2);
}

// CA Instructions
void AssemblerRISCVC::c_sub(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100011, C1, rd, 0b00, rs2);
}

void AssemblerRISCVC::c_xor(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100011, C1, rd, 0b01, rs2);
}

void AssemblerRISCVC::c_or(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100011, C1, rd, 0b10, rs2);
}

void AssemblerRISCVC::c_and(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100011, C1, rd, 0b11, rs2);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_subw(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100111, C1, rd, 0b00, rs2);
}

void AssemblerRISCVC::c_addw(Register rd, Register rs2) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs2.code() & 0b11000) == 0b01000));
  GenInstrCA(0b100111, C1, rd, 0b01, rs2);
}
#endif

void AssemblerRISCVC::c_swsp(Register rs2, uint16_t uimm8) {
  DCHECK(is_uint8(uimm8) && (uimm8 & 0x3) == 0);
  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
  GenInstrCSS(0b110, C2, rs2, uimm6);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_sdsp(Register rs2, uint16_t uimm9) {
  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
  GenInstrCSS(0b111, C2, rs2, uimm6);
}
#endif

void AssemblerRISCVC::c_fsdsp(FPURegister rs2, uint16_t uimm9) {
  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
  GenInstrCSS(0b101, C2, rs2, uimm6);
}

// CL Instructions

void AssemblerRISCVC::c_lw(Register rd, Register rs1, uint16_t uimm7) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
         ((uimm7 & 0x3) == 0));
  uint8_t uimm5 =
      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
  GenInstrCL(0b010, C0, rd, rs1, uimm5);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_ld(Register rd, Register rs1, uint16_t uimm8) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
         ((uimm8 & 0x7) == 0));
  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
  GenInstrCL(0b011, C0, rd, rs1, uimm5);
}
#endif

void AssemblerRISCVC::c_fld(FPURegister rd, Register rs1, uint16_t uimm8) {
  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
         ((uimm8 & 0x7) == 0));
  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
  GenInstrCL(0b001, C0, rd, rs1, uimm5);
}

// CS Instructions

void AssemblerRISCVC::c_sw(Register rs2, Register rs1, uint16_t uimm7) {
  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
         ((uimm7 & 0x3) == 0));
  uint8_t uimm5 =
      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
  GenInstrCS(0b110, C0, rs2, rs1, uimm5);
}

#ifdef V8_TARGET_ARCH_RISCV64
void AssemblerRISCVC::c_sd(Register rs2, Register rs1, uint16_t uimm8) {
  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
         ((uimm8 & 0x7) == 0));
  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
  GenInstrCS(0b111, C0, rs2, rs1, uimm5);
}
#endif

void AssemblerRISCVC::c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8) {
  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
         ((uimm8 & 0x7) == 0));
  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
  GenInstrCS(0b101, C0, rs2, rs1, uimm5);
}

// CJ Instructions

void AssemblerRISCVC::c_j(int16_t imm12) {
  DCHECK(is_int12(imm12));
  int16_t uimm11 = ((imm12 & 0x800) >> 1) | ((imm12 & 0x400) >> 4) |
                   ((imm12 & 0x300) >> 1) | ((imm12 & 0x80) >> 3) |
                   ((imm12 & 0x40) >> 1) | ((imm12 & 0x20) >> 5) |
                   ((imm12 & 0x10) << 5) | (imm12 & 0xe);
  GenInstrCJ(0b101, C1, uimm11);
  BlockTrampolinePoolFor(1);
}

// CB Instructions

void AssemblerRISCVC::c_bnez(Register rs1, int16_t imm9) {
  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
  GenInstrCB(0b111, C1, rs1, uimm8);
}

void AssemblerRISCVC::c_beqz(Register rs1, int16_t imm9) {
  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
  GenInstrCB(0b110, C1, rs1, uimm8);
}

void AssemblerRISCVC::c_srli(Register rs1, int8_t shamt6) {
  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
  GenInstrCBA(0b100, 0b00, C1, rs1, shamt6);
}

void AssemblerRISCVC::c_srai(Register rs1, int8_t shamt6) {
  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
  GenInstrCBA(0b100, 0b01, C1, rs1, shamt6);
}

void AssemblerRISCVC::c_andi(Register rs1, int8_t imm6) {
  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(imm6));
  GenInstrCBA(0b100, 0b10, C1, rs1, imm6);
}

bool AssemblerRISCVC::IsCJal(Instr instr) {
  return (instr & kRvcOpcodeMask) == RO_C_J;
}

bool AssemblerRISCVC::IsCBranch(Instr instr) {
  int Op = instr & kRvcOpcodeMask;
  return Op == RO_C_BNEZ || Op == RO_C_BEQZ;
}

int AssemblerRISCVC::CJumpOffset(Instr instr) {
  int32_t imm12 = ((instr & 0x4) << 3) | ((instr & 0x38) >> 2) |
                  ((instr & 0x40) << 1) | ((instr & 0x80) >> 1) |
                  ((instr & 0x100) << 2) | ((instr & 0x600) >> 1) |
                  ((instr & 0x800) >> 7) | ((instr & 0x1000) >> 1);
  imm12 = imm12 << 20 >> 20;
  return imm12;
}

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-c.h                                           0000664 0000000 0000000 00000005357 14746647661 0023131 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-c.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_

namespace v8 {
namespace internal {
class AssemblerRISCVC : public AssemblerRiscvBase {
  // RV64C Standard Extension
 public:
  void c_nop();
  void c_addi(Register rd, int8_t imm6);

  void c_addi16sp(int16_t imm10);
  void c_addi4spn(Register rd, int16_t uimm10);
  void c_li(Register rd, int8_t imm6);
  void c_lui(Register rd, int8_t imm6);
  void c_slli(Register rd, uint8_t shamt6);
  void c_lwsp(Register rd, uint16_t uimm8);
  void c_jr(Register rs1);
  void c_mv(Register rd, Register rs2);
  void c_ebreak();
  void c_jalr(Register rs1);
  void c_j(int16_t imm12);
  void c_add(Register rd, Register rs2);
  void c_sub(Register rd, Register rs2);
  void c_and(Register rd, Register rs2);
  void c_xor(Register rd, Register rs2);
  void c_or(Register rd, Register rs2);
  void c_swsp(Register rs2, uint16_t uimm8);
  void c_lw(Register rd, Register rs1, uint16_t uimm7);
  void c_sw(Register rs2, Register rs1, uint16_t uimm7);
  void c_bnez(Register rs1, int16_t imm9);
  void c_beqz(Register rs1, int16_t imm9);
  void c_srli(Register rs1, int8_t shamt6);
  void c_srai(Register rs1, int8_t shamt6);
  void c_andi(Register rs1, int8_t imm6);

  void c_fld(FPURegister rd, Register rs1, uint16_t uimm8);
  void c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8);
  void c_fldsp(FPURegister rd, uint16_t uimm9);
  void c_fsdsp(FPURegister rs2, uint16_t uimm9);
#ifdef V8_TARGET_ARCH_RISCV64
  void c_ld(Register rd, Register rs1, uint16_t uimm8);
  void c_sd(Register rs2, Register rs1, uint16_t uimm8);
  void c_subw(Register rd, Register rs2);
  void c_addw(Register rd, Register rs2);
  void c_addiw(Register rd, int8_t imm6);
  void c_ldsp(Register rd, uint16_t uimm9);
  void c_sdsp(Register rs2, uint16_t uimm9);
#endif

  int CJumpOffset(Instr instr);

  static bool IsCBranch(Instr instr);
  static bool IsCJal(Instr instr);

  inline int16_t cjump_offset(Label* L) {
    return (int16_t)branch_offset_helper(L, OffsetSize::kOffset11);
  }
  inline int32_t cbranch_offset(Label* L) {
    return branch_offset_helper(L, OffsetSize::kOffset9);
  }

  void c_j(Label* L) { c_j(cjump_offset(L)); }
  void c_bnez(Register rs1, Label* L) { c_bnez(rs1, cbranch_offset(L)); }
  void c_beqz(Register rs1, Label* L) { c_beqz(rs1, cbranch_offset(L)); }
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_
                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-d.cc                                          0000664 0000000 0000000 00000013122 14746647661 0023255 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-d.h"

namespace v8 {
namespace internal {
// RV32D Standard Extension

void AssemblerRISCVD::fld(FPURegister rd, Register rs1, int16_t imm12) {
  GenInstrLoadFP_ri(0b011, rd, rs1, imm12);
}

void AssemblerRISCVD::fsd(FPURegister source, Register base, int16_t imm12) {
  GenInstrStoreFP_rri(0b011, base, source, imm12);
}

void AssemblerRISCVD::fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                              FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b01, MADD, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVD::fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                              FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b01, MSUB, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVD::fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                               FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b01, NMSUB, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVD::fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                               FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b01, NMADD, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVD::fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0000001, frm, rd, rs1, rs2);
}

void AssemblerRISCVD::fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0000101, frm, rd, rs1, rs2);
}

void AssemblerRISCVD::fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0001001, frm, rd, rs1, rs2);
}

void AssemblerRISCVD::fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0001101, frm, rd, rs1, rs2);
}

void AssemblerRISCVD::fsqrt_d(FPURegister rd, FPURegister rs1,
                              FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0101101, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVD::fsgnj_d(FPURegister rd, FPURegister rs1,
                              FPURegister rs2) {
  GenInstrALUFP_rr(0b0010001, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVD::fsgnjn_d(FPURegister rd, FPURegister rs1,
                               FPURegister rs2) {
  GenInstrALUFP_rr(0b0010001, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVD::fsgnjx_d(FPURegister rd, FPURegister rs1,
                               FPURegister rs2) {
  GenInstrALUFP_rr(0b0010001, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVD::fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b0010101, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVD::fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b0010101, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVD::fcvt_s_d(FPURegister rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0100000, frm, rd, rs1, ToRegister(1));
}

void AssemblerRISCVD::fcvt_d_s(FPURegister rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0100001, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVD::feq_d(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010001, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVD::flt_d(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010001, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVD::fle_d(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010001, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVD::fclass_d(Register rd, FPURegister rs1) {
  GenInstrALUFP_rr(0b1110001, 0b001, rd, rs1, zero_reg);
}

void AssemblerRISCVD::fcvt_w_d(Register rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVD::fcvt_wu_d(Register rd, FPURegister rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(1));
}

void AssemblerRISCVD::fcvt_d_w(FPURegister rd, Register rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVD::fcvt_d_wu(FPURegister rd, Register rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(1));
}

#ifdef V8_TARGET_ARCH_RISCV64
// RV64D Standard Extension (in addition to RV32D)

void AssemblerRISCVD::fcvt_l_d(Register rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(2));
}

void AssemblerRISCVD::fcvt_lu_d(Register rd, FPURegister rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(3));
}

void AssemblerRISCVD::fmv_x_d(Register rd, FPURegister rs1) {
  GenInstrALUFP_rr(0b1110001, 0b000, rd, rs1, zero_reg);
}

void AssemblerRISCVD::fcvt_d_l(FPURegister rd, Register rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(2));
}

void AssemblerRISCVD::fcvt_d_lu(FPURegister rd, Register rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(3));
}

void AssemblerRISCVD::fmv_d_x(FPURegister rd, Register rs1) {
  GenInstrALUFP_rr(0b1111001, 0b000, rd, rs1, zero_reg);
}
#endif

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                                                                                                                                                                              node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-d.h                                           0000664 0000000 0000000 00000006647 14746647661 0023135 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-d.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_

namespace v8 {
namespace internal {
class AssemblerRISCVD : public AssemblerRiscvBase {
  // RV32D Standard Extension
 public:
  void fld(FPURegister rd, Register rs1, int16_t imm12);
  void fsd(FPURegister source, Register base, int16_t imm12);
  void fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
               FPURegister rs3, FPURoundingMode frm = RNE);
  void fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
               FPURegister rs3, FPURoundingMode frm = RNE);
  void fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                FPURegister rs3, FPURoundingMode frm = RNE);
  void fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
                FPURegister rs3, FPURoundingMode frm = RNE);
  void fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fsqrt_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fsgnj_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fsgnjn_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fsgnjx_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fcvt_s_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_d_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void feq_d(Register rd, FPURegister rs1, FPURegister rs2);
  void flt_d(Register rd, FPURegister rs1, FPURegister rs2);
  void fle_d(Register rd, FPURegister rs1, FPURegister rs2);
  void fclass_d(Register rd, FPURegister rs1);
  void fcvt_w_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_wu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_d_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fcvt_d_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64D Standard Extension (in addition to RV32D)
  void fcvt_l_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_lu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fmv_x_d(Register rd, FPURegister rs1);
  void fcvt_d_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fcvt_d_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fmv_d_x(FPURegister rd, Register rs1);
#endif

  void fmv_d(FPURegister rd, FPURegister rs) { fsgnj_d(rd, rs, rs); }
  void fabs_d(FPURegister rd, FPURegister rs) { fsgnjx_d(rd, rs, rs); }
  void fneg_d(FPURegister rd, FPURegister rs) { fsgnjn_d(rd, rs, rs); }
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_
                                                                                         node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-f.cc                                          0000664 0000000 0000000 00000012356 14746647661 0023267 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-f.h"

namespace v8 {
namespace internal {

// RV32F Standard Extension

void AssemblerRISCVF::flw(FPURegister rd, Register rs1, int16_t imm12) {
  GenInstrLoadFP_ri(0b010, rd, rs1, imm12);
}

void AssemblerRISCVF::fsw(FPURegister source, Register base, int16_t imm12) {
  GenInstrStoreFP_rri(0b010, base, source, imm12);
}

void AssemblerRISCVF::fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                              FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b00, MADD, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVF::fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                              FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b00, MSUB, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVF::fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                               FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b00, NMSUB, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVF::fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                               FPURegister rs3, FPURoundingMode frm) {
  GenInstrR4(0b00, NMADD, rd, rs1, rs2, rs3, frm);
}

void AssemblerRISCVF::fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0000000, frm, rd, rs1, rs2);
}

void AssemblerRISCVF::fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0000100, frm, rd, rs1, rs2);
}

void AssemblerRISCVF::fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0001000, frm, rd, rs1, rs2);
}

void AssemblerRISCVF::fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                             FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0001100, frm, rd, rs1, rs2);
}

void AssemblerRISCVF::fsqrt_s(FPURegister rd, FPURegister rs1,
                              FPURoundingMode frm) {
  GenInstrALUFP_rr(0b0101100, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVF::fsgnj_s(FPURegister rd, FPURegister rs1,
                              FPURegister rs2) {
  GenInstrALUFP_rr(0b0010000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVF::fsgnjn_s(FPURegister rd, FPURegister rs1,
                               FPURegister rs2) {
  GenInstrALUFP_rr(0b0010000, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVF::fsgnjx_s(FPURegister rd, FPURegister rs1,
                               FPURegister rs2) {
  GenInstrALUFP_rr(0b0010000, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVF::fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b0010100, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVF::fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b0010100, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVF::fcvt_w_s(Register rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVF::fcvt_wu_s(Register rd, FPURegister rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(1));
}

void AssemblerRISCVF::fmv_x_w(Register rd, FPURegister rs1) {
  GenInstrALUFP_rr(0b1110000, 0b000, rd, rs1, zero_reg);
}

void AssemblerRISCVF::feq_s(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010000, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVF::flt_s(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010000, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVF::fle_s(Register rd, FPURegister rs1, FPURegister rs2) {
  GenInstrALUFP_rr(0b1010000, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVF::fclass_s(Register rd, FPURegister rs1) {
  GenInstrALUFP_rr(0b1110000, 0b001, rd, rs1, zero_reg);
}

void AssemblerRISCVF::fcvt_s_w(FPURegister rd, Register rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, zero_reg);
}

void AssemblerRISCVF::fcvt_s_wu(FPURegister rd, Register rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(1));
}

void AssemblerRISCVF::fmv_w_x(FPURegister rd, Register rs1) {
  GenInstrALUFP_rr(0b1111000, 0b000, rd, rs1, zero_reg);
}

#ifdef V8_TARGET_ARCH_RISCV64
// RV64F Standard Extension (in addition to RV32F)

void AssemblerRISCVF::fcvt_l_s(Register rd, FPURegister rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(2));
}

void AssemblerRISCVF::fcvt_lu_s(Register rd, FPURegister rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(3));
}

void AssemblerRISCVF::fcvt_s_l(FPURegister rd, Register rs1,
                               FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(2));
}

void AssemblerRISCVF::fcvt_s_lu(FPURegister rd, Register rs1,
                                FPURoundingMode frm) {
  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(3));
}
#endif

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                  node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-f.h                                           0000664 0000000 0000000 00000006415 14746647661 0023130 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-f.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_

namespace v8 {
namespace internal {
class AssemblerRISCVF : public AssemblerRiscvBase {
  // RV32F Standard Extension
 public:
  void flw(FPURegister rd, Register rs1, int16_t imm12);
  void fsw(FPURegister source, Register base, int16_t imm12);
  void fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
               FPURegister rs3, FPURoundingMode frm = RNE);
  void fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
               FPURegister rs3, FPURoundingMode frm = RNE);
  void fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                FPURegister rs3, FPURoundingMode frm = RNE);
  void fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
                FPURegister rs3, FPURoundingMode frm = RNE);
  void fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
              FPURoundingMode frm = RNE);
  void fsqrt_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fsgnj_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fsgnjn_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fsgnjx_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
  void fcvt_w_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_wu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fmv_x_w(Register rd, FPURegister rs1);
  void feq_s(Register rd, FPURegister rs1, FPURegister rs2);
  void flt_s(Register rd, FPURegister rs1, FPURegister rs2);
  void fle_s(Register rd, FPURegister rs1, FPURegister rs2);
  void fclass_s(Register rd, FPURegister rs1);
  void fcvt_s_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fcvt_s_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fmv_w_x(FPURegister rd, Register rs1);

#ifdef V8_TARGET_ARCH_RISCV64
  // RV64F Standard Extension (in addition to RV32F)
  void fcvt_l_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_lu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
  void fcvt_s_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
  void fcvt_s_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
#endif

  void fmv_s(FPURegister rd, FPURegister rs) { fsgnj_s(rd, rs, rs); }
  void fabs_s(FPURegister rd, FPURegister rs) { fsgnjx_s(rd, rs, rs); }
  void fneg_s(FPURegister rd, FPURegister rs) { fsgnjn_s(rd, rs, rs); }
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_
                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-m.cc                                          0000664 0000000 0000000 00000003744 14746647661 0023277 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.
#include "src/codegen/riscv/extension-riscv-m.h"

namespace v8 {
namespace internal {
// RV32M Standard Extension

void AssemblerRISCVM::mul(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVM::mulh(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b001, rd, rs1, rs2);
}

void AssemblerRISCVM::mulhsu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b010, rd, rs1, rs2);
}

void AssemblerRISCVM::mulhu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b011, rd, rs1, rs2);
}

void AssemblerRISCVM::div(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b100, rd, rs1, rs2);
}

void AssemblerRISCVM::divu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVM::rem(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b110, rd, rs1, rs2);
}

void AssemblerRISCVM::remu(Register rd, Register rs1, Register rs2) {
  GenInstrALU_rr(0b0000001, 0b111, rd, rs1, rs2);
}

#ifdef V8_TARGET_ARCH_RISCV64
// RV64M Standard Extension (in addition to RV32M)

void AssemblerRISCVM::mulw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000001, 0b000, rd, rs1, rs2);
}

void AssemblerRISCVM::divw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000001, 0b100, rd, rs1, rs2);
}

void AssemblerRISCVM::divuw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000001, 0b101, rd, rs1, rs2);
}

void AssemblerRISCVM::remw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000001, 0b110, rd, rs1, rs2);
}

void AssemblerRISCVM::remuw(Register rd, Register rs1, Register rs2) {
  GenInstrALUW_rr(0b0000001, 0b111, rd, rs1, rs2);
}
#endif
}  // namespace internal
}  // namespace v8
                            node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-m.h                                           0000664 0000000 0000000 00000002663 14746647661 0023140 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/codegen/assembler.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-m.h"
#include "src/codegen/riscv/register-riscv.h"
#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_

namespace v8 {
namespace internal {
class AssemblerRISCVM : public AssemblerRiscvBase {
  // RV32M Standard Extension
 public:
  void mul(Register rd, Register rs1, Register rs2);
  void mulh(Register rd, Register rs1, Register rs2);
  void mulhsu(Register rd, Register rs1, Register rs2);
  void mulhu(Register rd, Register rs1, Register rs2);
  void div(Register rd, Register rs1, Register rs2);
  void divu(Register rd, Register rs1, Register rs2);
  void rem(Register rd, Register rs1, Register rs2);
  void remu(Register rd, Register rs1, Register rs2);
#ifdef V8_TARGET_ARCH_RISCV64
  // RV64M Standard Extension (in addition to RV32M)
  void mulw(Register rd, Register rs1, Register rs2);
  void divw(Register rd, Register rs1, Register rs2);
  void divuw(Register rd, Register rs1, Register rs2);
  void remw(Register rd, Register rs1, Register rs2);
  void remuw(Register rd, Register rs1, Register rs2);
#endif
};
}  // namespace internal
}  // namespace v8
#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_
                                                                             node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-v.cc                                          0000664 0000000 0000000 00000105236 14746647661 0023307 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        
// Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/codegen/riscv/extension-riscv-v.h"

#include "src/codegen/assembler.h"
#include "src/codegen/riscv/constant-riscv-v.h"
#include "src/codegen/riscv/register-riscv.h"

namespace v8 {
namespace internal {

// RVV

void AssemblerRISCVV::vredmaxu_vs(VRegister vd, VRegister vs2, VRegister vs1,
                                  MaskType mask) {
  GenInstrV(VREDMAXU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
}

void AssemblerRISCVV::vredmax_vs(VRegister vd, VRegister vs2, VRegister vs1,
                                 MaskType mask) {
  GenInstrV(VREDMAX_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
}

void AssemblerRISCVV::vredmin_vs(VRegister vd, VRegister vs2, VRegister vs1,
                                 MaskType mask) {
  GenInstrV(VREDMIN_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
}

void AssemblerRISCVV::vredminu_vs(VRegister vd, VRegister vs2, VRegister vs1,
                                  MaskType mask) {
  GenInstrV(VREDMINU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
}

void AssemblerRISCVV::vmv_vv(VRegister vd, VRegister vs1) {
  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, v0, NoMask);
}

void AssemblerRISCVV::vmv_vx(VRegister vd, Register rs1) {
  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, v0, NoMask);
}

void AssemblerRISCVV::vmv_vi(VRegister vd, uint8_t simm5) {
  GenInstrV(VMV_FUNCT6, vd, simm5, v0, NoMask);
}

void AssemblerRISCVV::vmv_xs(Register rd, VRegister vs2) {
  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b00000, vs2, NoMask);
}

void AssemblerRISCVV::vmv_sx(VRegister vd, Register rs1) {
  GenInstrV(VRXUNARY0_FUNCT6, OP_MVX, vd, rs1, v0, NoMask);
}

void AssemblerRISCVV::vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2) {
  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
}

void AssemblerRISCVV::vmerge_vx(VRegister vd, Register rs1, VRegister vs2) {
  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
}

void AssemblerRISCVV::vmerge_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
  GenInstrV(VMV_FUNCT6, vd, imm5, vs2, Mask);
}

void AssemblerRISCVV::vadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
  GenInstrV(VADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
}

void AssemblerRISCVV::vadc_vx(VRegister vd, Register rs1, VRegister vs2) {
  GenInstrV(VADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
}

void AssemblerRISCVV::vadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
  GenInstrV(VADC_FUNCT6, vd, imm5, vs2, Mask);
}

void AssemblerRISCVV::vmadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
  GenInstrV(VMADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
}

void AssemblerRISCVV::vmadc_vx(VRegister vd, Register rs1, VRegister vs2) {
  GenInstrV(VMADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
}

void AssemblerRISCVV::vmadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
  GenInstrV(VMADC_FUNCT6, vd, imm5, vs2, Mask);
}

void AssemblerRISCVV::vrgather_vv(VRegister vd, VRegister vs2, VRegister vs1,
                                  MaskType mask) {
  DCHECK_NE(vd, vs1);
  DCHECK_NE(vd, vs2);
  GenInstrV(VRGATHER_FUNCT6, OP_IVV, vd, vs1, vs2, mask);
}

void AssemblerRISCVV::vrgather_vi(VRegister vd, VRegister vs2, int8_t imm5,
                                  MaskType mask) {
  DCHECK_NE(vd, vs2);
  GenInstrV(VRGATHER_FUNCT6, vd, imm5, vs2, mask);
}

void AssemblerRISCVV::vrgather_vx(VRegister vd, VRegister vs2, Register rs1,
                                  MaskType mask) {
  DCHECK_NE(vd, vs2);
  GenInstrV(VRGATHER_FUNCT6, OP_IVX, vd, rs1, vs2, mask);
}

void AssemblerRISCVV::vwaddu_wx(VRegister vd, VRegister vs2, Register rs1,
                                MaskType mask) {
  GenInstrV(VWADDUW_FUNCT6, OP_MVX, vd, rs1, vs2, mask);
}

void AssemblerRISCVV::vid_v(VRegister vd, MaskType mask) {
  GenInstrV(VMUNARY0_FUNCT6, OP_MVV, vd, VID_V, v0, mask);
}

#define DEFINE_OPIVV(name, funct6)                                            \
  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_IVV, vd, vs1, vs2, mask);                            \
  }

#define DEFINE_OPFVV(name, funct6)                                            \
  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
  }

#define DEFINE_OPFWV(name, funct6)                                            \
  void AssemblerRISCVV::name##_wv(VRegister vd, VRegister vs2, VRegister vs1, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
  }

#define DEFINE_OPFRED(name, funct6)                                           \
  void AssemblerRISCVV::name##_vs(VRegister vd, VRegister vs2, VRegister vs1, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
  }

#define DEFINE_OPIVX(name, funct6)                                           \
  void AssemblerRISCVV::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
                                  MaskType mask) {                           \
    GenInstrV(funct6, OP_IVX, vd, rs1, vs2, mask);                           \
  }

#define DEFINE_OPIVI(name, funct6)                                          \
  void AssemblerRISCVV::name##_vi(VRegister vd, VRegister vs2, int8_t imm5, \
                                  MaskType mask) {                          \
    GenInstrV(funct6, vd, imm5, vs2, mask);                                 \
  }

#define DEFINE_OPMVV(name, funct6)                                            \
  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_MVV, vd, vs1, vs2, mask);                            \
  }

// void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, Register
// rs1,
//                  VRegister vs2, MaskType mask = NoMask);
#define DEFINE_OPMVX(name, funct6)                                           \
  void AssemblerRISCVV::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
                                  MaskType mask) {                           \
    GenInstrV(funct6, OP_MVX, vd, rs1, vs2, mask);                           \
  }

#define DEFINE_OPFVF(name, funct6)                                  \
  void AssemblerRISCVV::name##_vf(VRegister vd, VRegister vs2,      \
                                  FPURegister fs1, MaskType mask) { \
    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                  \
  }

#define DEFINE_OPFWF(name, funct6)                                  \
  void AssemblerRISCVV::name##_wf(VRegister vd, VRegister vs2,      \
                                  FPURegister fs1, MaskType mask) { \
    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                  \
  }

#define DEFINE_OPFVV_FMA(name, funct6)                                        \
  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs1, VRegister vs2, \
                                  MaskType mask) {                            \
    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
  }

#define DEFINE_OPFVF_FMA(name, funct6)                            \
  void AssemblerRISCVV::name##_vf(VRegister vd, FPURegister fs1,  \
                                  VRegister vs2, MaskType mask) { \
    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                \
  }

// vector integer extension
#define DEFINE_OPMVV_VIE(name, vs1)                                        \
  void AssemblerRISCVV::name(VRegister vd, VRegister vs2, MaskType mask) { \
    GenInstrV(VXUNARY0_FUNCT6, OP_MVV, vd, vs1, vs2, mask);                \
  }

void AssemblerRISCVV::vfmv_vf(VRegister vd, FPURegister fs1) {
  GenInstrV(VMV_FUNCT6, OP_FVF, vd, fs1, v0, NoMask);
}

void AssemblerRISCVV::vfmv_fs(FPURegister fd, VRegister vs2) {
  GenInstrV(VWFUNARY0_FUNCT6, OP_FVV, fd, v0, vs2, NoMask);
}

void AssemblerRISCVV::vfmv_sf(VRegister vd, FPURegister fs) {
  GenInstrV(VRFUNARY0_FUNCT6, OP_FVF, vd, fs, v0, NoMask);
}

void AssemblerRISCVV::vfmerge_vf(VRegister vd, FPURegister fs1, VRegister vs2) {
  GenInstrV(VMV_FUNCT6, OP_FVF, vd, fs1, vs2, Mask);
}

DEFINE_OPIVV(vadd, VADD_FUNCT6)
DEFINE_OPIVX(vadd, VADD_FUNCT6)
DEFINE_OPIVI(vadd, VADD_FUNCT6)
DEFINE_OPIVV(vsub, VSUB_FUNCT6)
DEFINE_OPIVX(vsub, VSUB_FUNCT6)
DEFINE_OPMVX(vdiv, VDIV_FUNCT6)
DEFINE_OPMVX(vdivu, VDIVU_FUNCT6)
DEFINE_OPMVX(vmul, VMUL_FUNCT6)
DEFINE_OPMVX(vmulhu, VMULHU_FUNCT6)
DEFINE_OPMVX(vmulhsu, VMULHSU_FUNCT6)
DEFINE_OPMVX(vmulh, VMULH_FUNCT6)
DEFINE_OPMVV(vdiv, VDIV_FUNCT6)
DEFINE_OPMVV(vdivu, VDIVU_FUNCT6)
DEFINE_OPMVV(vmul, VMUL_FUNCT6)
DEFINE_OPMVV(vmulhu, VMULHU_FUNCT6)
DEFINE_OPMVV(vmulhsu, VMULHSU_FUNCT6)
DEFINE_OPMVV(vwmul, VWMUL_FUNCT6)
DEFINE_OPMVV(vwmulu, VWMULU_FUNCT6)
DEFINE_OPMVV(vmulh, VMULH_FUNCT6)
DEFINE_OPMVV(vwadd, VWADD_FUNCT6)
DEFINE_OPMVV(vwaddu, VWADDU_FUNCT6)
DEFINE_OPMVV(vcompress, VCOMPRESS_FUNCT6)
DEFINE_OPIVX(vsadd, VSADD_FUNCT6)
DEFINE_OPIVV(vsadd, VSADD_FUNCT6)
DEFINE_OPIVI(vsadd, VSADD_FUNCT6)
DEFINE_OPIVX(vsaddu, VSADDU_FUNCT6)
DEFINE_OPIVV(vsaddu, VSADDU_FUNCT6)
DEFINE_OPIVI(vsaddu, VSADDU_FUNCT6)
DEFINE_OPIVX(vssub, VSSUB_FUNCT6)
DEFINE_OPIVV(vssub, VSSUB_FUNCT6)
DEFINE_OPIVX(vssubu, VSSUBU_FUNCT6)
DEFINE_OPIVV(vssubu, VSSUBU_FUNCT6)
DEFINE_OPIVX(vrsub, VRSUB_FUNCT6)
DEFINE_OPIVI(vrsub, VRSUB_FUNCT6)
DEFINE_OPIVV(vminu, VMINU_FUNCT6)
DEFINE_OPIVX(vminu, VMINU_FUNCT6)
DEFINE_OPIVV(vmin, VMIN_FUNCT6)
DEFINE_OPIVX(vmin, VMIN_FUNCT6)
DEFINE_OPIVV(vmaxu, VMAXU_FUNCT6)
DEFINE_OPIVX(vmaxu, VMAXU_FUNCT6)
DEFINE_OPIVV(vmax, VMAX_FUNCT6)
DEFINE_OPIVX(vmax, VMAX_FUNCT6)
DEFINE_OPIVV(vand, VAND_FUNCT6)
DEFINE_OPIVX(vand, VAND_FUNCT6)
DEFINE_OPIVI(vand, VAND_FUNCT6)
DEFINE_OPIVV(vor, VOR_FUNCT6)
DEFINE_OPIVX(vor, VOR_FUNCT6)
DEFINE_OPIVI(vor, VOR_FUNCT6)
DEFINE_OPIVV(vxor, VXOR_FUNCT6)
DEFINE_OPIVX(vxor, VXOR_FUNCT6)
DEFINE_OPIVI(vxor, VXOR_FUNCT6)

DEFINE_OPIVX(vslidedown, VSLIDEDOWN_FUNCT6)
DEFINE_OPIVI(vslidedown, VSLIDEDOWN_FUNCT6)
DEFINE_OPMVX(vslide1down, VSLIDEDOWN_FUNCT6)
DEFINE_OPFVF(vfslide1down, VSLIDEDOWN_FUNCT6)
DEFINE_OPIVX(vslideup, VSLIDEUP_FUNCT6)
DEFINE_OPIVI(vslideup, VSLIDEUP_FUNCT6)
DEFINE_OPMVX(vslide1up, VSLIDEUP_FUNCT6)
DEFINE_OPFVF(vfslide1up, VSLIDEUP_FUNCT6)

DEFINE_OPIVV(vmseq, VMSEQ_FUNCT6)
DEFINE_OPIVX(vmseq, VMSEQ_FUNCT6)
DEFINE_OPIVI(vmseq, VMSEQ_FUNCT6)

DEFINE_OPIVV(vmsne, VMSNE_FUNCT6)
DEFINE_OPIVX(vmsne, VMSNE_FUNCT6)
DEFINE_OPIVI(vmsne, VMSNE_FUNCT6)

DEFINE_OPIVV(vmsltu, VMSLTU_FUNCT6)
DEFINE_OPIVX(vmsltu, VMSLTU_FUNCT6)

DEFINE_OPIVV(vmslt, VMSLT_FUNCT6)
DEFINE_OPIVX(vmslt, VMSLT_FUNCT6)

DEFINE_OPIVV(vmsle, VMSLE_FUNCT6)
DEFINE_OPIVX(vmsle, VMSLE_FUNCT6)
DEFINE_OPIVI(vmsle, VMSLE_FUNCT6)

DEFINE_OPIVV(vmsleu, VMSLEU_FUNCT6)
DEFINE_OPIVX(vmsleu, VMSLEU_FUNCT6)
DEFINE_OPIVI(vmsleu, VMSLEU_FUNCT6)

DEFINE_OPIVI(vmsgt, VMSGT_FUNCT6)
DEFINE_OPIVX(vmsgt, VMSGT_FUNCT6)

DEFINE_OPIVI(vmsgtu, VMSGTU_FUNCT6)
DEFINE_OPIVX(vmsgtu, VMSGTU_FUNCT6)

DEFINE_OPIVV(vsrl, VSRL_FUNCT6)
DEFINE_OPIVX(vsrl, VSRL_FUNCT6)
DEFINE_OPIVI(vsrl, VSRL_FUNCT6)

DEFINE_OPIVV(vsra, VSRA_FUNCT6)
DEFINE_OPIVX(vsra, VSRA_FUNCT6)
DEFINE_OPIVI(vsra, VSRA_FUNCT6)

DEFINE_OPIVV(vsll, VSLL_FUNCT6)
DEFINE_OPIVX(vsll, VSLL_FUNCT6)
DEFINE_OPIVI(vsll, VSLL_FUNCT6)

DEFINE_OPIVV(vsmul, VSMUL_FUNCT6)
DEFINE_OPIVX(vsmul, VSMUL_FUNCT6)

DEFINE_OPFVV(vfadd, VFADD_FUNCT6)
DEFINE_OPFVF(vfadd, VFADD_FUNCT6)
DEFINE_OPFVV(vfsub, VFSUB_FUNCT6)
DEFINE_OPFVF(vfsub, VFSUB_FUNCT6)
DEFINE_OPFVV(vfdiv, VFDIV_FUNCT6)
DEFINE_OPFVF(vfdiv, VFDIV_FUNCT6)
DEFINE_OPFVV(vfmul, VFMUL_FUNCT6)
DEFINE_OPFVF(vfmul, VFMUL_FUNCT6)
DEFINE_OPFVV(vmfeq, VMFEQ_FUNCT6)
DEFINE_OPFVV(vmfne, VMFNE_FUNCT6)
DEFINE_OPFVV(vmflt, VMFLT_FUNCT6)
DEFINE_OPFVV(vmfle, VMFLE_FUNCT6)
DEFINE_OPFVV(vfmax, VFMAX_FUNCT6)
DEFINE_OPFVV(vfmin, VFMIN_FUNCT6)

// Vector Widening Floating-Point Add/Subtract Instructions
DEFINE_OPFVV(vfwadd, VFWADD_FUNCT6)
DEFINE_OPFVF(vfwadd, VFWADD_FUNCT6)
DEFINE_OPFVV(vfwsub, VFWSUB_FUNCT6)
DEFINE_OPFVF(vfwsub, VFWSUB_FUNCT6)
DEFINE_OPFWV(vfwadd, VFWADD_W_FUNCT6)
DEFINE_OPFWF(vfwadd, VFWADD_W_FUNCT6)
DEFINE_OPFWV(vfwsub, VFWSUB_W_FUNCT6)
DEFINE_OPFWF(vfwsub, VFWSUB_W_FUNCT6)

// Vector Widening Floating-Point Reduction Instructions
DEFINE_OPFRED(vfwredusum, VFWREDUSUM_FUNCT6)
DEFINE_OPFRED(vfwredosum, VFWREDOSUM_FUNCT6)

// Vector Widening Floating-Point Multiply
DEFINE_OPFVV(vfwmul, VFWMUL_FUNCT6)
DEFINE_OPFVF(vfwmul, VFWMUL_FUNCT6)

DEFINE_OPFRED(vfredmax, VFREDMAX_FUNCT6)

DEFINE_OPFVV(vfsngj, VFSGNJ_FUNCT6)
DEFINE_OPFVF(vfsngj, VFSGNJ_FUNCT6)
DEFINE_OPFVV(vfsngjn, VFSGNJN_FUNCT6)
DEFINE_OPFVF(vfsngjn, VFSGNJN_FUNCT6)
DEFINE_OPFVV(vfsngjx, VFSGNJX_FUNCT6)
DEFINE_OPFVF(vfsngjx, VFSGNJX_FUNCT6)

// Vector Single-Width Floating-Point Fused Multiply-Add Instructions
DEFINE_OPFVV_FMA(vfmadd, VFMADD_FUNCT6)
DEFINE_OPFVF_FMA(vfmadd, VFMADD_FUNCT6)
DEFINE_OPFVV_FMA(vfmsub, VFMSUB_FUNCT6)
DEFINE_OPFVF_FMA(vfmsub, VFMSUB_FUNCT6)
DEFINE_OPFVV_FMA(vfmacc, VFMACC_FUNCT6)
DEFINE_OPFVF_FMA(vfmacc, VFMACC_FUNCT6)
DEFINE_OPFVV_FMA(vfmsac, VFMSAC_FUNCT6)
DEFINE_OPFVF_FMA(vfmsac, VFMSAC_FUNCT6)
DEFINE_OPFVV_FMA(vfnmadd, VFNMADD_FUNCT6)
DEFINE_OPFVF_FMA(vfnmadd, VFNMADD_FUNCT6)
DEFINE_OPFVV_FMA(vfnmsub, VFNMSUB_FUNCT6)
DEFINE_OPFVF_FMA(vfnmsub, VFNMSUB_FUNCT6)
DEFINE_OPFVV_FMA(vfnmacc, VFNMACC_FUNCT6)
DEFINE_OPFVF_FMA(vfnmacc, VFNMACC_FUNCT6)
DEFINE_OPFVV_FMA(vfnmsac, VFNMSAC_FUNCT6)
DEFINE_OPFVF_FMA(vfnmsac, VFNMSAC_FUNCT6)

// Vector Widening Floating-Point Fused Multiply-Add Instructions
DEFINE_OPFVV_FMA(vfwmacc, VFWMACC_FUNCT6)
DEFINE_OPFVF_FMA(vfwmacc, VFWMACC_FUNCT6)
DEFINE_OPFVV_FMA(vfwnmacc, VFWNMACC_FUNCT6)
DEFINE_OPFVF_FMA(vfwnmacc, VFWNMACC_FUNCT6)
DEFINE_OPFVV_FMA(vfwmsac, VFWMSAC_FUNCT6)
DEFINE_OPFVF_FMA(vfwmsac, VFWMSAC_FUNCT6)
DEFINE_OPFVV_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
DEFINE_OPFVF_FMA(vfwnmsac, VFWNMSAC_FUNCT6)

// Vector Narrowing Fixed-Point Clip Instructions
DEFINE_OPIVV(vnclip, VNCLIP_FUNCT6)
DEFINE_OPIVX(vnclip, VNCLIP_FUNCT6)
DEFINE_OPIVI(vnclip, VNCLIP_FUNCT6)
DEFINE_OPIVV(vnclipu, VNCLIPU_FUNCT6)
DEFINE_OPIVX(vnclipu, VNCLIPU_FUNCT6)
DEFINE_OPIVI(vnclipu, VNCLIPU_FUNCT6)

// Vector Integer Extension
DEFINE_OPMVV_VIE(vzext_vf8, 0b00010)
DEFINE_OPMVV_VIE(vsext_vf8, 0b00011)
DEFINE_OPMVV_VIE(vzext_vf4, 0b00100)
DEFINE_OPMVV_VIE(vsext_vf4, 0b00101)
DEFINE_OPMVV_VIE(vzext_vf2, 0b00110)
DEFINE_OPMVV_VIE(vsext_vf2, 0b00111)

#undef DEFINE_OPIVI
#undef DEFINE_OPIVV
#undef DEFINE_OPIVX
#undef DEFINE_OPFVV
#undef DEFINE_OPFWV
#undef DEFINE_OPFVF
#undef DEFINE_OPFWF
#undef DEFINE_OPFVV_FMA
#undef DEFINE_OPFVF_FMA
#undef DEFINE_OPMVV_VIE

void AssemblerRISCVV::vsetvli(Register rd, Register rs1, VSew vsew, Vlmul vlmul,
                              TailAgnosticType tail, MaskAgnosticType mask) {
  int32_t zimm = GenZimm(vsew, vlmul, tail, mask);
  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x0 << 31;
  emit(instr);
}

void AssemblerRISCVV::vsetivli(Register rd, uint8_t uimm, VSew vsew,
                               Vlmul vlmul, TailAgnosticType tail,
                               MaskAgnosticType mask) {
  DCHECK(is_uint5(uimm));
  int32_t zimm = GenZimm(vsew, vlmul, tail, mask) & 0x3FF;
  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
                ((uimm & 0x1F) << kRvvUimmShift) |
                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x3 << 30;
  emit(instr);
}

void AssemblerRISCVV::vsetvl(Register rd, Register rs1, Register rs2) {
  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
                ((rs2.code() & 0x1F) << kRvvRs2Shift) | 0x40 << 25;
  emit(instr);
}

uint8_t vsew_switch(VSew vsew) {
  uint8_t width;
  switch (vsew) {
    case E8:
      width = 0b000;
      break;
    case E16:
      width = 0b101;
      break;
    case E32:
      width = 0b110;
      break;
    default:
      width = 0b111;
      break;
  }
  return width;
}

// OPIVV OPFVV OPMVV
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
                                VRegister vs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((vd.code() & 0x1F) << kRvvVdShift) |
                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
                                int8_t vs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((vd.code() & 0x1F) << kRvvVdShift) |
                ((vs1 & 0x1F) << kRvvVs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}
// OPMVV OPFVV
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, Register rd,
                                VRegister vs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_MVV || opcode == OP_FVV);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((rd.code() & 0x1F) << kRvvVdShift) |
                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

// OPFVV
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, FPURegister fd,
                                VRegister vs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_FVV);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((fd.code() & 0x1F) << kRvvVdShift) |
                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

// OPIVX OPMVX
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
                                Register rs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_IVX || opcode == OP_MVX);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((vd.code() & 0x1F) << kRvvVdShift) |
                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

// OPFVF
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
                                FPURegister fs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_FVF);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((vd.code() & 0x1F) << kRvvVdShift) |
                ((fs1.code() & 0x1F) << kRvvRs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

// OPMVX
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Register rd, Register rs1,
                                VRegister vs2, MaskType mask) {
  Instr instr = (funct6 << kRvvFunct6Shift) | OP_MVX | (mask << kRvvVmShift) |
                ((rd.code() & 0x1F) << kRvvVdShift) |
                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}
// OPIVI
void AssemblerRISCVV::GenInstrV(uint8_t funct6, VRegister vd, int8_t imm5,
                                VRegister vs2, MaskType mask) {
  DCHECK(is_uint5(imm5) || is_int5(imm5));
  Instr instr = (funct6 << kRvvFunct6Shift) | OP_IVI | (mask << kRvvVmShift) |
                ((vd.code() & 0x1F) << kRvvVdShift) |
                (((uint32_t)imm5 << kRvvImm5Shift) & kRvvImm5Mask) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

// VL VS
void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
                                Register rs1, uint8_t umop, MaskType mask,
                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
                ((width << kRvvWidthShift) & kRvvWidthMask) |
                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
                ((umop << kRvvRs2Shift) & kRvvRs2Mask) |
                ((mask << kRvvVmShift) & kRvvVmMask) |
                ((IsMop << kRvvMopShift) & kRvvMopMask) |
                ((IsMew << kRvvMewShift) & kRvvMewMask) |
                ((Nf << kRvvNfShift) & kRvvNfMask);
  emit(instr);
}
void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
                                Register rs1, Register rs2, MaskType mask,
                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
                ((width << kRvvWidthShift) & kRvvWidthMask) |
                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
                ((rs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
                ((mask << kRvvVmShift) & kRvvVmMask) |
                ((IsMop << kRvvMopShift) & kRvvMopMask) |
                ((IsMew << kRvvMewShift) & kRvvMewMask) |
                ((Nf << kRvvNfShift) & kRvvNfMask);
  emit(instr);
}
// VL VS AMO
void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
                                Register rs1, VRegister vs2, MaskType mask,
                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
  DCHECK(opcode == LOAD_FP || opcode == STORE_FP || opcode == AMO);
  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
                ((width << kRvvWidthShift) & kRvvWidthMask) |
                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
                ((vs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
                ((mask << kRvvVmShift) & kRvvVmMask) |
                ((IsMop << kRvvMopShift) & kRvvMopMask) |
                ((IsMew << kRvvMewShift) & kRvvMewMask) |
                ((Nf << kRvvNfShift) & kRvvNfMask);
  emit(instr);
}
// vmv_xs vcpop_m vfirst_m
void AssemblerRISCVV::GenInstrV(uint8_t funct6, Opcode opcode, Register rd,
                                uint8_t vs1, VRegister vs2, MaskType mask) {
  DCHECK(opcode == OP_MVV);
  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
                ((rd.code() & 0x1F) << kRvvVdShift) |
                ((vs1 & 0x1F) << kRvvVs1Shift) |
                ((vs2.code() & 0x1F) << kRvvVs2Shift);
  emit(instr);
}

void AssemblerRISCVV::vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
                         MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b000);
}
void AssemblerRISCVV::vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
                          MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b000);
}
void AssemblerRISCVV::vlx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
                          MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0);
}

void AssemblerRISCVV::vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
                         MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b000);
}
void AssemblerRISCVV::vss(VRegister vs3, Register rs1, Register rs2, VSew vsew,
                          MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vs3, rs1, rs2, mask, 0b10, 0, 0b000);
}

void AssemblerRISCVV::vsx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
                          MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0b000);
}
void AssemblerRISCVV::vsu(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
                          MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b01, 0, 0b000);
}

void AssemblerRISCVV::vlseg2(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b001);
}

void AssemblerRISCVV::vlseg3(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b010);
}

void AssemblerRISCVV::vlseg4(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b011);
}

void AssemblerRISCVV::vlseg5(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b100);
}

void AssemblerRISCVV::vlseg6(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b101);
}

void AssemblerRISCVV::vlseg7(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b110);
}

void AssemblerRISCVV::vlseg8(VRegister vd, Register rs1, uint8_t lumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b111);
}
void AssemblerRISCVV::vsseg2(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b001);
}
void AssemblerRISCVV::vsseg3(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b010);
}
void AssemblerRISCVV::vsseg4(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b011);
}
void AssemblerRISCVV::vsseg5(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b100);
}
void AssemblerRISCVV::vsseg6(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b101);
}
void AssemblerRISCVV::vsseg7(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b110);
}
void AssemblerRISCVV::vsseg8(VRegister vd, Register rs1, uint8_t sumop,
                             VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b111);
}

void AssemblerRISCVV::vlsseg2(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
}
void AssemblerRISCVV::vlsseg3(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
}
void AssemblerRISCVV::vlsseg4(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
}
void AssemblerRISCVV::vlsseg5(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
}
void AssemblerRISCVV::vlsseg6(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
}
void AssemblerRISCVV::vlsseg7(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
}
void AssemblerRISCVV::vlsseg8(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
}
void AssemblerRISCVV::vssseg2(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
}
void AssemblerRISCVV::vssseg3(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
}
void AssemblerRISCVV::vssseg4(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
}
void AssemblerRISCVV::vssseg5(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
}
void AssemblerRISCVV::vssseg6(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
}
void AssemblerRISCVV::vssseg7(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
}
void AssemblerRISCVV::vssseg8(VRegister vd, Register rs1, Register rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
}

void AssemblerRISCVV::vlxseg2(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
}
void AssemblerRISCVV::vlxseg3(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
}
void AssemblerRISCVV::vlxseg4(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
}
void AssemblerRISCVV::vlxseg5(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
}
void AssemblerRISCVV::vlxseg6(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
}
void AssemblerRISCVV::vlxseg7(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
}
void AssemblerRISCVV::vlxseg8(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
}
void AssemblerRISCVV::vsxseg2(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
}
void AssemblerRISCVV::vsxseg3(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
}
void AssemblerRISCVV::vsxseg4(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
}
void AssemblerRISCVV::vsxseg5(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
}
void AssemblerRISCVV::vsxseg6(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
}
void AssemblerRISCVV::vsxseg7(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
}
void AssemblerRISCVV::vsxseg8(VRegister vd, Register rs1, VRegister rs2,
                              VSew vsew, MaskType mask) {
  uint8_t width = vsew_switch(vsew);
  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
}

void AssemblerRISCVV::vfirst_m(Register rd, VRegister vs2, MaskType mask) {
  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10001, vs2, mask);
}

void AssemblerRISCVV::vcpop_m(Register rd, VRegister vs2, MaskType mask) {
  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10000, vs2, mask);
}

LoadStoreLaneParams::LoadStoreLaneParams(MachineRepresentation rep,
                                         uint8_t laneidx) {
#ifdef CAN_USE_RVV_INSTRUCTIONS
  switch (rep) {
    case MachineRepresentation::kWord8:
      *this = LoadStoreLaneParams(laneidx, 8, kRvvVLEN / 16);
      break;
    case MachineRepresentation::kWord16:
      *this = LoadStoreLaneParams(laneidx, 16, kRvvVLEN / 8);
      break;
    case MachineRepresentation::kWord32:
      *this = LoadStoreLaneParams(laneidx, 32, kRvvVLEN / 4);
      break;
    case MachineRepresentation::kWord64:
      *this = LoadStoreLaneParams(laneidx, 64, kRvvVLEN / 2);
      break;
    default:
      UNREACHABLE();
  }
#else
  UNREACHABLE();
#endif
}

}  // namespace internal
}  // namespace v8
                                                                                                                                                                                                                                                                                                                                                                  node-23.7.0/deps/v8/src/codegen/riscv/extension-riscv-v.h                                           0000664 0000000 0000000 00000043355 14746647661 0023154 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_V_H_
#define V8_CODEGEN_RISCV_EXTENSION_RISCV_V_H_

#include "src/codegen/assembler.h"
#include "src/codegen/machine-type.h"
#include "src/codegen/riscv/base-assembler-riscv.h"
#include "src/codegen/riscv/constant-riscv-v.h"
#include "src/codegen/riscv/register-riscv.h"

namespace v8 {
namespace internal {

class AssemblerRISCVV : public AssemblerRiscvBase {
 public:
  // RVV
  static int32_t GenZimm(VSew vsew, Vlmul vlmul, TailAgnosticType tail = tu,
                         MaskAgnosticType mask = mu) {
    return (mask << 7) | (tail << 6) | ((vsew & 0x7) << 3) | (vlmul & 0x7);
  }

  void vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
          MaskType mask = NoMask);
  void vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
           MaskType mask = NoMask);
  void vlx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
           MaskType mask = NoMask);

  void vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
          MaskType mask = NoMask);
  void vss(VRegister vd, Register rs1, Register rs2, VSew vsew,
           MaskType mask = NoMask);
  void vsx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
           MaskType mask = NoMask);

  void vsu(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
           MaskType mask = NoMask);

#define SegInstr(OP)  \
  void OP##seg2(ARG); \
  void OP##seg3(ARG); \
  void OP##seg4(ARG); \
  void OP##seg5(ARG); \
  void OP##seg6(ARG); \
  void OP##seg7(ARG); \
  void OP##seg8(ARG);

#define ARG \
  VRegister vd, Register rs1, uint8_t lumop, VSew vsew, MaskType mask = NoMask

  SegInstr(vl) SegInstr(vs)
#undef ARG

#define ARG \
  VRegister vd, Register rs1, Register rs2, VSew vsew, MaskType mask = NoMask

      SegInstr(vls) SegInstr(vss)
#undef ARG

#define ARG \
  VRegister vd, Register rs1, VRegister rs2, VSew vsew, MaskType mask = NoMask

          SegInstr(vsx) SegInstr(vlx)
#undef ARG
#undef SegInstr

      // RVV Vector Arithmetic Instruction

      void vmv_vv(VRegister vd, VRegister vs1);
  void vmv_vx(VRegister vd, Register rs1);
  void vmv_vi(VRegister vd, uint8_t simm5);
  void vmv_xs(Register rd, VRegister vs2);
  void vmv_sx(VRegister vd, Register rs1);
  void vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2);
  void vmerge_vx(VRegister vd, Register rs1, VRegister vs2);
  void vmerge_vi(VRegister vd, uint8_t imm5