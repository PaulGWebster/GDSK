kind)              \
  case Simd128BinopOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_BINARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128UnaryOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd128UnaryOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_UNARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128ReduceOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)               \
  case Simd128ReduceOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_REDUCE_OPTIONAL_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128ShiftOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd128ShiftOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_SHIFT_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128TestOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)             \
  case Simd128TestOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_TEST_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128SplatOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd128SplatOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_SPLAT_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd128TernaryOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)                \
  case Simd128TernaryOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_128_TERNARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

void Simd128ExtractLaneOp::PrintOptions(std::ostream& os) const {
  os << '[';
  switch (kind) {
    case Kind::kI8x16S:
      os << "I8x16S";
      break;
    case Kind::kI8x16U:
      os << "I8x16U";
      break;
    case Kind::kI16x8S:
      os << "I16x8S";
      break;
    case Kind::kI16x8U:
      os << "I16x8U";
      break;
    case Kind::kI32x4:
      os << "I32x4";
      break;
    case Kind::kI64x2:
      os << "I64x2";
      break;
    case Kind::kF16x8:
      os << "F16x8";
      break;
    case Kind::kF32x4:
      os << "F32x4";
      break;
    case Kind::kF64x2:
      os << "F64x2";
      break;
  }
  os << ", " << static_cast<int32_t>(lane) << ']';
}

void Simd128ReplaceLaneOp::PrintOptions(std::ostream& os) const {
  os << '[';
  switch (kind) {
    case Kind::kI8x16:
      os << "I8x16";
      break;
    case Kind::kI16x8:
      os << "I16x8";
      break;
    case Kind::kI32x4:
      os << "I32x4";
      break;
    case Kind::kI64x2:
      os << "I64x2";
      break;
    case Kind::kF16x8:
      os << "F16x8";
      break;
    case Kind::kF32x4:
      os << "F32x4";
      break;
    case Kind::kF64x2:
      os << "F64x2";
      break;
  }
  os << ", " << static_cast<int32_t>(lane) << ']';
}

void Simd128LaneMemoryOp::PrintOptions(std::ostream& os) const {
  os << '[' << (mode == Mode::kLoad ? "Load" : "Store") << ", ";
  if (kind.maybe_unaligned) os << "unaligned, ";
  if (kind.with_trap_handler) os << "protected, ";
  switch (lane_kind) {
    case LaneKind::k8:
      os << '8';
      break;
    case LaneKind::k16:
      os << "16";
      break;
    case LaneKind::k32:
      os << "32";
      break;
    case LaneKind::k64:
      os << "64";
      break;
  }
  os << "bit, lane: " << static_cast<int>(lane);
  if (offset != 0) os << ", offset: " << offset;
  os << ']';
}

void Simd128LoadTransformOp::PrintOptions(std::ostream& os) const {
  os << '[';
  if (load_kind.maybe_unaligned) os << "unaligned, ";
  if (load_kind.with_trap_handler) os << "protected, ";

  switch (transform_kind) {
#define PRINT_KIND(kind)       \
  case TransformKind::k##kind: \
    os << #kind;               \
    break;
    FOREACH_SIMD_128_LOAD_TRANSFORM_OPCODE(PRINT_KIND)
#undef PRINT_KIND
  }

  os << ", offset: " << offset << ']';
}

void Simd128ShuffleOp::PrintOptions(std::ostream& os) const {
  PrintSimdValue(os, shuffle);
}

#if V8_ENABLE_WASM_SIMD256_REVEC
void Simd256ConstantOp::PrintOptions(std::ostream& os) const {
  PrintSimdValue(os, value);
}

void Simd256Extract128LaneOp::PrintOptions(std::ostream& os) const {
  os << '[' << static_cast<int>(lane) << ']';
}

void Simd256LoadTransformOp::PrintOptions(std::ostream& os) const {
  os << '[';
  if (load_kind.maybe_unaligned) os << "unaligned, ";
  if (load_kind.with_trap_handler) os << "protected, ";

  switch (transform_kind) {
#define PRINT_KIND(kind)       \
  case TransformKind::k##kind: \
    os << #kind;               \
    break;
    FOREACH_SIMD_256_LOAD_TRANSFORM_OPCODE(PRINT_KIND)
#undef PRINT_KIND
  }

  os << ", offset: " << offset << ']';
}

std::ostream& operator<<(std::ostream& os, Simd256UnaryOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd256UnaryOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_UNARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd256TernaryOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)                \
  case Simd256TernaryOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_TERNARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd256BinopOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd256BinopOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_BINARY_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd256ShiftOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd256ShiftOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_SHIFT_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

std::ostream& operator<<(std::ostream& os, Simd256SplatOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)              \
  case Simd256SplatOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_SPLAT_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}

#ifdef V8_TARGET_ARCH_X64
void Simd256ShufdOp::PrintOptions(std::ostream& os) const {
  os << '[' << std::bitset<8>(control) << ']';
}

void Simd256ShufpsOp::PrintOptions(std::ostream& os) const {
  os << '[' << std::bitset<8>(control) << ']';
}

std::ostream& operator<<(std::ostream& os, Simd256UnpackOp::Kind kind) {
  switch (kind) {
#define PRINT_KIND(kind)               \
  case Simd256UnpackOp::Kind::k##kind: \
    return os << #kind;
    FOREACH_SIMD_256_UNPACK_OPCODE(PRINT_KIND)
  }
#undef PRINT_KIND
}
#endif  // V8_TARGET_ARCH_X64
#endif  // V8_ENABLE_WASM_SIMD256_REVEC

void WasmAllocateArrayOp::PrintOptions(std::ostream& os) const {
  os << '[' << array_type->element_type() << ']';
}

void ArrayGetOp::PrintOptions(std::ostream& os) const {
  os << '[' << (is_signed ? "signed " : "")
     << (array_type->mutability() ? "" : "immutable ")
     << array_type->element_type() << ']';
}

#endif  // V8_ENABLE_WEBASSEBMLY

std::string Operation::ToString() const {
  std::stringstream ss;
  ss << *this;
  return ss.str();
}

base::LazyMutex SupportedOperations::mutex_ = LAZY_MUTEX_INITIALIZER;
SupportedOperations SupportedOperations::instance_;
bool SupportedOperations::initialized_;

void SupportedOperations::Initialize() {
  base::MutexGuard lock(mutex_.Pointer());
  if (initialized_) return;
  initialized_ = true;

  MachineOperatorBuilder::Flags supported =
      InstructionSelector::SupportedMachineOperatorFlags();
#define SET_SUPPORTED(name, machine_name) \
  instance_.name##_ = supported & MachineOperatorBuilder::Flag::k##machine_name;

  SUPPORTED_OPERATIONS_LIST(SET_SUPPORTED)
#undef SET_SUPPORTED
}

base::SmallVector<Block*, 4> SuccessorBlocks(const Block& block,
                                             const Graph& graph) {
  return SuccessorBlocks(block.LastOperation(graph));
}

// static
bool SupportedOperations::IsUnalignedLoadSupported(MemoryRepresentation repr) {
  return InstructionSelector::AlignmentRequirements().IsUnalignedLoadSupported(
      repr.ToMachineType().representation());
}

// static
bool SupportedOperations::IsUnalignedStoreSupported(MemoryRepresentation repr) {
  return InstructionSelector::AlignmentRequirements().IsUnalignedStoreSupported(
      repr.ToMachineType().representation());
}

void CheckExceptionOp::Validate(const Graph& graph) const {
  DCHECK_NE(didnt_throw_block, catch_block);
  // `CheckException` should follow right after the throwing operation.
  DCHECK_EQ(throwing_operation(),
            V<Any>::Cast(graph.PreviousIndex(graph.Index(*this))));
}

namespace {
BlockIndex index_for_bound_block(const Block* block) {
  DCHECK_NOT_NULL(block);
  const BlockIndex index = block->index();
  DCHECK(index.valid());
  return index;
}
}  // namespace

size_t CallOp::hash_value(HashingStrategy strategy) const {
  if (strategy == HashingStrategy::kMakeSnapshotStable) {
    // Destructure here to cause a compilation error in case `options` is
    // changed.
    auto [descriptor_value, callee_effects] = options();
    return HashWithOptions(*descriptor_value, callee_effects);
  } else {
    return Base::hash_value(strategy);
  }
}

size_t CheckExceptionOp::hash_value(HashingStrategy strategy) const {
  if (strategy == HashingStrategy::kMakeSnapshotStable) {
    // Destructure here to cause a compilation error in case `options` is
    // changed.
    auto [didnt_throw_block_value, catch_block_value] = options();
    return HashWithOptions(index_for_bound_block(didnt_throw_block_value),
                           index_for_bound_block(catch_block_value));
  } else {
    return Base::hash_value(strategy);
  }
}

size_t GotoOp::hash_value(HashingStrategy strategy) const {
  if (strategy == HashingStrategy::kMakeSnapshotStable) {
    // Destructure here to cause a compilation error in case `options` is
    // changed.
    auto [destination_value, is_backedge_value] = options();
    return HashWithOptions(index_for_bound_block(destination_value),
                           is_backedge_value);
  } else {
    return Base::hash_value(strategy);
  }
}

size_t BranchOp::hash_value(HashingStrategy strategy) const {
  if (strategy == HashingStrategy::kMakeSnapshotStable) {
    // Destructure here to cause a compilation error in case `options` is
    // changed.
    auto [if_true_value, if_false_value, hint_value] = options();
    return HashWithOptions(index_for_bound_block(if_true_value),
                           index_for_bound_block(if_false_value), hint_value);
  } else {
    return Base::hash_value(strategy);
  }
}

size_t SwitchOp::hash_value(HashingStrategy strategy) const {
  if (strategy == HashingStrategy::kMakeSnapshotStable) {
    // Destructure here to cause a compilation error in case `options` is
    // changed.
    auto [cases_value, default_case_value, default_hint_value] = options();
    DCHECK_NOT_NULL(default_case_value);
    size_t hash = HashWithOptions(index_for_bound_block(default_case_value),
                                  default_hint_value);
    for (const auto& c : cases_value) {
      hash = fast_hash_combine(hash, c.value,
                               index_for_bound_block(c.destination), c.hint);
    }
    return hash;
  } else {
    return Base::hash_value(strategy);
  }
}

namespace {
// Ensures basic consistency of representation mapping.
class InputsRepFactoryCheck : InputsRepFactory {
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Word32()) ==
                MaybeRegisterRepresentation::Word32());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Word64()) ==
                MaybeRegisterRepresentation::Word64());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Float32()) ==
                MaybeRegisterRepresentation::Float32());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Float64()) ==
                MaybeRegisterRepresentation::Float64());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Tagged()) ==
                MaybeRegisterRepresentation::Tagged());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Compressed()) ==
                MaybeRegisterRepresentation::Compressed());
  static_assert(*ToMaybeRepPointer(RegisterRepresentation::Simd128()) ==
                MaybeRegisterRepresentation::Simd128());
};
}  // namespace

bool IsUnlikelySuccessor(const Block* block, const Block* successor,
                         const Graph& graph) {
  DCHECK(base::contains(successor->Predecessors(), block));
  const Operation& terminator = block->LastOperation(graph);
  switch (terminator.opcode) {
    case Opcode::kCheckException: {
      const CheckExceptionOp& check_exception =
          terminator.Cast<CheckExceptionOp>();
      return successor == check_exception.catch_block;
    }
    case Opcode::kGoto:
      return false;
    case Opcode::kBranch: {
      const BranchOp& branch = terminator.Cast<BranchOp>();
      return (branch.hint == BranchHint::kTrue &&
              successor == branch.if_false) ||
             (branch.hint == BranchHint::kFalse && successor == branch.if_true);
    }
    case Opcode::kSwitch: {
      const SwitchOp& swtch = terminator.Cast<SwitchOp>();
      if (successor == swtch.default_case) {
        return swtch.default_hint == BranchHint::kFalse;
      }
      auto it = std::find_if(swtch.cases.begin(), swtch.cases.end(),
                             [successor](const SwitchOp::Case& c) {
                               return c.destination == successor;
                             });
      DCHECK_NE(it, swtch.cases.end());
      return it->hint == BranchHint::kFalse;
    }
    case Opcode::kDeoptimize:
    case Opcode::kTailCall:
    case Opcode::kUnreachable:
    case Opcode::kReturn:
      UNREACHABLE();

#define NON_TERMINATOR_CASE(op) case Opcode::k##op:
      TURBOSHAFT_OPERATION_LIST_NOT_BLOCK_TERMINATOR(NON_TERMINATOR_CASE)
      UNREACHABLE();
#undef NON_TERMINATOR_CASE
  }
}

bool Operation::IsOnlyUserOf(const Operation& value, const Graph& graph) const {
  DCHECK_GE(std::count(inputs().begin(), inputs().end(), graph.Index(value)),
            1);
  if (value.saturated_use_count.IsOne()) return true;
  return std::count(inputs().begin(), inputs().end(), graph.Index(value)) ==
         value.saturated_use_count.Get();
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                               node-23.7.0/deps/v8/src/compiler/turboshaft/operations.h                                            0000664 0000000 0000000 00001220524 14746647661 0023171 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_OPERATIONS_H_
#define V8_COMPILER_TURBOSHAFT_OPERATIONS_H_

#include <cmath>
#include <cstdint>
#include <cstring>
#include <limits>
#include <optional>
#include <tuple>
#include <type_traits>
#include <utility>

#include "src/base/logging.h"
#include "src/base/macros.h"
#include "src/base/platform/mutex.h"
#include "src/base/small-vector.h"
#include "src/base/template-utils.h"
#include "src/base/vector.h"
#include "src/codegen/external-reference.h"
#include "src/common/globals.h"
#include "src/compiler/common-operator.h"
#include "src/compiler/fast-api-calls.h"
#include "src/compiler/globals.h"
#include "src/compiler/simplified-operator.h"
#include "src/compiler/turboshaft/deopt-data.h"
#include "src/compiler/turboshaft/fast-hash.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/snapshot-table.h"
#include "src/compiler/turboshaft/types.h"
#include "src/compiler/turboshaft/utils.h"
#include "src/compiler/turboshaft/zone-with-name.h"
#include "src/compiler/write-barrier-kind.h"
#include "src/flags/flags.h"

#if V8_ENABLE_WEBASSEMBLY
#include "src/wasm/wasm-module.h"
#include "src/wasm/wasm-objects.h"
#endif

namespace v8::internal {
class HeapObject;
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           AbortReason reason);
}  // namespace v8::internal
namespace v8::internal::compiler {
class CallDescriptor;
class DeoptimizeParameters;
class FrameStateInfo;
class Node;
enum class TrapId : int32_t;
}  // namespace v8::internal::compiler
namespace v8::internal::compiler::turboshaft {

inline constexpr char kCompilationZoneName[] = "compilation-zone";

class Block;
struct FrameStateData;
class Graph;
struct FrameStateOp;

enum class HashingStrategy {
  kDefault,
  // This strategy requires that hashing a graph during builtin construction
  // (mksnapshot) produces the same hash for repeated runs of mksnapshot. This
  // requires that no pointers and external constants are used in hashes.
  kMakeSnapshotStable,
};

// This belongs to `VariableReducer` in `variable-reducer.h`. It is defined here
// because of cyclic header dependencies.
struct VariableData {
  MaybeRegisterRepresentation rep;
  bool loop_invariant;
  IntrusiveSetIndex active_loop_variables_index = {};
};
using Variable = SnapshotTable<OpIndex, VariableData>::Key;

// DEFINING NEW OPERATIONS
// =======================
// For each operation `Foo`, we define:
// - An entry V(Foo) in one of the TURBOSHAFT*OPERATION list (eg,
//   TURBOSHAFT_OPERATION_LIST_BLOCK_TERMINATOR,
//   TURBOSHAFT_SIMPLIFIED_OPERATION_LIST etc), which defines
//   `Opcode::kFoo` and whether the operation is a block terminator.
// - A `struct FooOp`, which derives from either `OperationT<FooOp>` or
//   `FixedArityOperationT<k, FooOp>` if the op always has excactly `k` inputs.
// Furthermore, the struct has to contain:
// - A bunch of options directly as public fields.
// - A getter `options()` returning a tuple of all these options. This is used
//   for default printing and hashing. Alternatively, `void
//   PrintOptions(std::ostream& os) const` and `size_t hash_value() const` can
//   also be defined manually.
// - Getters for named inputs.
// - A constructor that first takes all the inputs and then all the options. For
//   a variable arity operation where the constructor doesn't take the inputs as
//   a single base::Vector<OpIndex> argument, it's also necessary to overwrite
//   the static `New` function, see `CallOp` for an example.
// - An `Explode` method that unpacks an operation and invokes the passed
//   callback. If the operation inherits from FixedArityOperationT, the base
//   class already provides the required implementation.
// - `OpEffects` as either a static constexpr member `effects` or a
//   non-static method `Effects()` if the effects depend on the particular
//   operation and not just the opcode.
// - outputs_rep/inputs_rep methods, which should return a vector describing the
//   representation of the outputs and inputs of this operations.
// After defining the struct here, you'll also need to integrate it in
// Turboshaft:
// - If Foo is not in not lowered before reaching the instruction selector, add
//   a overload of ProcessOperation for FooOp in recreate-schedule.cc, and
//   handle Opcode::kFoo in the Turboshaft VisitNode of instruction-selector.cc.

#ifdef V8_INTL_SUPPORT
#define TURBOSHAFT_INTL_OPERATION_LIST(V) V(StringToCaseIntl)
#else
#define TURBOSHAFT_INTL_OPERATION_LIST(V)
#endif  // V8_INTL_SUPPORT

#ifdef V8_ENABLE_WEBASSEMBLY
// These operations should be lowered to Machine operations during
// WasmLoweringPhase.
#define TURBOSHAFT_WASM_OPERATION_LIST(V) \
  V(WasmStackCheck)                       \
  V(GlobalGet)                            \
  V(GlobalSet)                            \
  V(Null)                                 \
  V(IsNull)                               \
  V(AssertNotNull)                        \
  V(RttCanon)                             \
  V(WasmTypeCheck)                        \
  V(WasmTypeCast)                         \
  V(AnyConvertExtern)                     \
  V(ExternConvertAny)                     \
  V(WasmTypeAnnotation)                   \
  V(StructGet)                            \
  V(StructSet)                            \
  V(ArrayGet)                             \
  V(ArraySet)                             \
  V(ArrayLength)                          \
  V(WasmAllocateArray)                    \
  V(WasmAllocateStruct)                   \
  V(WasmRefFunc)                          \
  V(StringAsWtf16)                        \
  V(StringPrepareForGetCodeUnit)

#if V8_ENABLE_WASM_SIMD256_REVEC
#define TURBOSHAFT_SIMD256_COMMOM_OPERATION_LIST(V) \
  V(Simd256Constant)                                \
  V(Simd256Extract128Lane)                          \
  V(Simd256LoadTransform)                           \
  V(Simd256Unary)                                   \
  V(Simd256Binop)                                   \
  V(Simd256Shift)                                   \
  V(Simd256Ternary)                                 \
  V(Simd256Splat)                                   \
  V(SimdPack128To256)

#if V8_TARGET_ARCH_X64
#define TURBOSHAFT_SIMD256_X64_OPERATION_LIST(V) \
  V(Simd256Shufd)                                \
  V(Simd256Shufps)                               \
  V(Simd256Unpack)

#define TURBOSHAFT_SIMD256_OPERATION_LIST(V)  \
  TURBOSHAFT_SIMD256_COMMOM_OPERATION_LIST(V) \
  TURBOSHAFT_SIMD256_X64_OPERATION_LIST(V)
#else
#define TURBOSHAFT_SIMD256_OPERATION_LIST(V) \
  TURBOSHAFT_SIMD256_COMMOM_OPERATION_LIST(V)
#endif  // V8_TARGET_ARCH_X64

#else
#define TURBOSHAFT_SIMD256_OPERATION_LIST(V)
#endif

#define TURBOSHAFT_SIMD_OPERATION_LIST(V) \
  V(Simd128Constant)                      \
  V(Simd128Binop)                         \
  V(Simd128Unary)                         \
  V(Simd128Reduce)                        \
  V(Simd128Shift)                         \
  V(Simd128Test)                          \
  V(Simd128Splat)                         \
  V(Simd128Ternary)                       \
  V(Simd128ExtractLane)                   \
  V(Simd128ReplaceLane)                   \
  V(Simd128LaneMemory)                    \
  V(Simd128LoadTransform)                 \
  V(Simd128Shuffle)                       \
  TURBOSHAFT_SIMD256_OPERATION_LIST(V)

#else
#define TURBOSHAFT_WASM_OPERATION_LIST(V)
#define TURBOSHAFT_SIMD_OPERATION_LIST(V)
#endif

#define TURBOSHAFT_OPERATION_LIST_BLOCK_TERMINATOR(V) \
  V(CheckException)                                   \
  V(Goto)                                             \
  V(TailCall)                                         \
  V(Unreachable)                                      \
  V(Return)                                           \
  V(Branch)                                           \
  V(Switch)                                           \
  V(Deoptimize)

#ifdef V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA
#define TURBOSHAFT_CPED_OPERATION_LIST(V) \
  V(GetContinuationPreservedEmbedderData) \
  V(SetContinuationPreservedEmbedderData)
#else
#define TURBOSHAFT_CPED_OPERATION_LIST(V)
#endif  // V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA

// These operations should be lowered to Machine operations during
// MachineLoweringPhase.
#define TURBOSHAFT_SIMPLIFIED_OPERATION_LIST(V) \
  TURBOSHAFT_INTL_OPERATION_LIST(V)             \
  TURBOSHAFT_CPED_OPERATION_LIST(V)             \
  V(ArgumentsLength)                            \
  V(BigIntBinop)                                \
  V(BigIntComparison)                           \
  V(BigIntUnary)                                \
  V(CheckedClosure)                             \
  V(WordBinopDeoptOnOverflow)                   \
  V(CheckEqualsInternalizedString)              \
  V(CheckMaps)                                  \
  V(CompareMaps)                                \
  V(Float64Is)                                  \
  V(ObjectIs)                                   \
  V(ObjectIsNumericValue)                       \
  V(Float64SameValue)                           \
  V(SameValue)                                  \
  V(ChangeOrDeopt)                              \
  V(Convert)                                    \
  V(ConvertJSPrimitiveToObject)                 \
  V(ConvertJSPrimitiveToUntagged)               \
  V(ConvertJSPrimitiveToUntaggedOrDeopt)        \
  V(ConvertUntaggedToJSPrimitive)               \
  V(ConvertUntaggedToJSPrimitiveOrDeopt)        \
  V(TruncateJSPrimitiveToUntagged)              \
  V(TruncateJSPrimitiveToUntaggedOrDeopt)       \
  V(DoubleArrayMinMax)                          \
  V(EnsureWritableFastElements)                 \
  V(FastApiCall)                                \
  V(FindOrderedHashEntry)                       \
  V(LoadDataViewElement)                        \
  V(LoadFieldByIndex)                           \
  V(LoadMessage)                                \
  V(LoadStackArgument)                          \
  V(LoadTypedElement)                           \
  V(StoreDataViewElement)                       \
  V(StoreMessage)                               \
  V(StoreTypedElement)                          \
  V(MaybeGrowFastElements)                      \
  V(NewArgumentsElements)                       \
  V(NewArray)                                   \
  V(RuntimeAbort)                               \
  V(StaticAssert)                               \
  V(StringAt)                                   \
  V(StringComparison)                           \
  V(StringConcat)                               \
  V(StringFromCodePointAt)                      \
  V(StringIndexOf)                              \
  V(StringLength)                               \
  V(StringSubstring)                            \
  V(NewConsString)                              \
  V(TransitionAndStoreArrayElement)             \
  V(TransitionElementsKind)                     \
  V(DebugPrint)                                 \
  V(CheckTurboshaftTypeOf)

// These Operations are the lowest level handled by Turboshaft, and are
// supported by the InstructionSelector.
#define TURBOSHAFT_MACHINE_OPERATION_LIST(V) \
  V(WordBinop)                               \
  V(FloatBinop)                              \
  V(Word32PairBinop)                         \
  V(OverflowCheckedBinop)                    \
  V(WordUnary)                               \
  V(OverflowCheckedUnary)                    \
  V(FloatUnary)                              \
  V(Shift)                                   \
  V(Comparison)                              \
  V(Change)                                  \
  V(TryChange)                               \
  V(BitcastWord32PairToFloat64)              \
  V(TaggedBitcast)                           \
  V(Select)                                  \
  V(PendingLoopPhi)                          \
  V(Constant)                                \
  V(LoadRootRegister)                        \
  V(Load)                                    \
  V(Store)                                   \
  V(Retain)                                  \
  V(Parameter)                               \
  V(OsrValue)                                \
  V(StackPointerGreaterThan)                 \
  V(StackSlot)                               \
  V(FrameConstant)                           \
  V(DeoptimizeIf)                            \
  IF_WASM(V, TrapIf)                         \
  IF_WASM(V, LoadStackPointer)               \
  IF_WASM(V, SetStackPointer)                \
  V(Phi)                                     \
  V(FrameState)                              \
  V(Call)                                    \
  V(CatchBlockBegin)                         \
  V(DidntThrow)                              \
  V(Tuple)                                   \
  V(Projection)                              \
  V(DebugBreak)                              \
  V(AssumeMap)                               \
  V(AtomicRMW)                               \
  V(AtomicWord32Pair)                        \
  V(MemoryBarrier)                           \
  V(Comment)                                 \
  V(Dead)                                    \
  V(AbortCSADcheck)

// These are operations used in the frontend and are mostly tied to JS
// semantics.
#define TURBOSHAFT_JS_NON_THROWING_OPERATION_LIST(V) V(SpeculativeNumberBinop)

#define TURBOSHAFT_JS_THROWING_OPERATION_LIST(V) \
  V(GenericBinop)                                \
  V(GenericUnop)                                 \
  V(ToNumberOrNumeric)

#define TURBOSHAFT_JS_OPERATION_LIST(V)        \
  TURBOSHAFT_JS_NON_THROWING_OPERATION_LIST(V) \
  TURBOSHAFT_JS_THROWING_OPERATION_LIST(V)

// These are operations that are not Machine operations and need to be lowered
// before Instruction Selection, but they are not lowered during the
// MachineLoweringPhase.
#define TURBOSHAFT_OTHER_OPERATION_LIST(V) \
  V(Allocate)                              \
  V(DecodeExternalPointer)                 \
  V(JSStackCheck)

#define TURBOSHAFT_OPERATION_LIST_NOT_BLOCK_TERMINATOR(V) \
  TURBOSHAFT_WASM_OPERATION_LIST(V)                       \
  TURBOSHAFT_SIMD_OPERATION_LIST(V)                       \
  TURBOSHAFT_MACHINE_OPERATION_LIST(V)                    \
  TURBOSHAFT_SIMPLIFIED_OPERATION_LIST(V)                 \
  TURBOSHAFT_JS_OPERATION_LIST(V)                         \
  TURBOSHAFT_OTHER_OPERATION_LIST(V)

#define TURBOSHAFT_OPERATION_LIST(V)            \
  TURBOSHAFT_OPERATION_LIST_BLOCK_TERMINATOR(V) \
  TURBOSHAFT_OPERATION_LIST_NOT_BLOCK_TERMINATOR(V)

enum class Opcode : uint8_t {
#define ENUM_CONSTANT(Name) k##Name,
  TURBOSHAFT_OPERATION_LIST(ENUM_CONSTANT)
#undef ENUM_CONSTANT
};

const char* OpcodeName(Opcode opcode);
constexpr std::underlying_type_t<Opcode> OpcodeIndex(Opcode x) {
  return static_cast<std::underlying_type_t<Opcode>>(x);
}

#define FORWARD_DECLARE(Name) struct Name##Op;
TURBOSHAFT_OPERATION_LIST(FORWARD_DECLARE)
#undef FORWARD_DECLARE

namespace detail {
template <class Op>
struct operation_to_opcode_map {};

#define OPERATION_OPCODE_MAP_CASE(Name)    \
  template <>                              \
  struct operation_to_opcode_map<Name##Op> \
      : std::integral_constant<Opcode, Opcode::k##Name> {};
TURBOSHAFT_OPERATION_LIST(OPERATION_OPCODE_MAP_CASE)
#undef OPERATION_OPCODE_MAP_CASE
}  // namespace detail

template <typename Op>
struct operation_to_opcode
    : detail::operation_to_opcode_map<std::remove_cvref_t<Op>> {};
template <typename Op>
constexpr Opcode operation_to_opcode_v = operation_to_opcode<Op>::value;

template <typename Op, uint64_t Mask, uint64_t Value>
struct OpMaskT {
  using operation = Op;
  static constexpr uint64_t mask = Mask;
  static constexpr uint64_t value = Value;
};

#define COUNT_OPCODES(Name) +1
constexpr uint16_t kNumberOfBlockTerminatorOpcodes =
    0 TURBOSHAFT_OPERATION_LIST_BLOCK_TERMINATOR(COUNT_OPCODES);
#undef COUNT_OPCODES

#define COUNT_OPCODES(Name) +1
constexpr uint16_t kNumberOfOpcodes =
    0 TURBOSHAFT_OPERATION_LIST(COUNT_OPCODES);
#undef COUNT_OPCODES

inline constexpr bool IsBlockTerminator(Opcode opcode) {
  return OpcodeIndex(opcode) < kNumberOfBlockTerminatorOpcodes;
}

// Operations that can throw and that have static output representations.
#define TURBOSHAFT_THROWING_STATIC_OUTPUTS_OPERATIONS_LIST(V) \
  TURBOSHAFT_JS_THROWING_OPERATION_LIST(V)                    \
  V(FastApiCall)

// This list repeats the operations that may throw and need to be followed by
// `DidntThrow`.
#define TURBOSHAFT_THROWING_OPERATIONS_LIST(V)          \
  TURBOSHAFT_THROWING_STATIC_OUTPUTS_OPERATIONS_LIST(V) \
  V(Call)

// Operations that need to be followed by `DidntThrowOp`.
inline constexpr bool MayThrow(Opcode opcode) {
#define CASE(Name) case Opcode::k##Name:
  switch (opcode) {
    TURBOSHAFT_THROWING_OPERATIONS_LIST(CASE)
    return true;
    default:
      return false;
  }
#undef CASE
}

// For Throwing operations, outputs_rep() are empty, because the values are
// produced by the subsequent DidntThrow. Nevertheless, the operation has to
// define its output representations in an array that DidntThrow can then reuse
// to know what its outputs are. Additionally, when using Maglev as a frontend,
// catch handlers that have never been reach so far are not emitted, and instead
// the throwing operations lazy deopt instead of throwing.
//
// That's where the THROWING_OP_BOILERPLATE macro comes in: it creates  an array
// of representations that DidntThrow can use, and will define outputs_rep() to
// be empty, and takes care of creating a LazyDeoptOnThrow member. For instance:
//
//    THROWING_OP_BOILERPLATE(RegisterRepresentation::Tagged(),
//                            RegisterRepresentation::Word32())
//
// Warning: don't forget to add `lazy_deopt_on_throw` to the `options` of your
// Operation (you'll get a compile-time error if you forget it).
#define THROWING_OP_BOILERPLATE(...)                                         \
  static constexpr RegisterRepresentation kOutputRepsStorage[]{__VA_ARGS__}; \
  static constexpr base::Vector<const RegisterRepresentation> kOutReps =     \
      base::VectorOf(kOutputRepsStorage, arraysize(kOutputRepsStorage));     \
  base::Vector<const RegisterRepresentation> outputs_rep() const {           \
    return {};                                                               \
  }                                                                          \
  LazyDeoptOnThrow lazy_deopt_on_throw;

template <typename T>
inline base::Vector<T> InitVectorOf(
    ZoneVector<T>& storage,
    std::initializer_list<RegisterRepresentation> values) {
  storage.resize(values.size());
  size_t i = 0;
  for (auto&& value : values) {
    storage[i++] = value;
  }
  return base::VectorOf(storage);
}

class InputsRepFactory {
 public:
  constexpr static base::Vector<const MaybeRegisterRepresentation> SingleRep(
      RegisterRepresentation rep) {
    return base::VectorOf(ToMaybeRepPointer(rep), 1);
  }

  constexpr static base::Vector<const MaybeRegisterRepresentation> PairOf(
      RegisterRepresentation rep) {
    return base::VectorOf(ToMaybeRepPointer(rep), 2);
  }

 protected:
  constexpr static const MaybeRegisterRepresentation* ToMaybeRepPointer(
      RegisterRepresentation rep) {
    size_t index = static_cast<size_t>(rep.value()) * 2;
    DCHECK_LT(index, arraysize(rep_map));
    return &rep_map[index];
  }

 private:
  constexpr static MaybeRegisterRepresentation rep_map[] = {
      MaybeRegisterRepresentation::Word32(),
      MaybeRegisterRepresentation::Word32(),
      MaybeRegisterRepresentation::Word64(),
      MaybeRegisterRepresentation::Word64(),
      MaybeRegisterRepresentation::Float32(),
      MaybeRegisterRepresentation::Float32(),
      MaybeRegisterRepresentation::Float64(),
      MaybeRegisterRepresentation::Float64(),
      MaybeRegisterRepresentation::Tagged(),
      MaybeRegisterRepresentation::Tagged(),
      MaybeRegisterRepresentation::Compressed(),
      MaybeRegisterRepresentation::Compressed(),
      MaybeRegisterRepresentation::Simd128(),
      MaybeRegisterRepresentation::Simd128(),
#ifdef V8_ENABLE_WASM_SIMD256_REVEC
      MaybeRegisterRepresentation::Simd256(),
      MaybeRegisterRepresentation::Simd256(),
#endif  // V8_ENABLE_WASM_SIMD256_REVEC
  };
};

struct EffectDimensions {
  // Produced by loads, consumed by operations that should not move before loads
  // because they change memory.
  bool load_heap_memory : 1;
  bool load_off_heap_memory : 1;

  // Produced by stores, consumed by operations that should not move before
  // stores because they load or store memory.
  bool store_heap_memory : 1;
  bool store_off_heap_memory : 1;

  // Operations that perform raw heap access (like initialization) consume
  // `before_raw_heap_access` and produce `after_raw_heap_access`.
  // Operations that need the heap to be in a consistent state produce
  // `before_raw_heap_access` and consume `after_raw_heap_access`.
  bool before_raw_heap_access : 1;
  // Produced by operations that access raw/untagged pointers into the
  // heap or keep such a pointer alive, consumed by operations that can GC to
  // ensure they don't move before the raw access.
  bool after_raw_heap_access : 1;

  // Produced by any operation that can affect whether subsequent operations are
  // executed, for example by branching, deopting, throwing or aborting.
  // Consumed by all operations that should not be hoisted before a check
  // because they rely on it. For example, loads usually rely on the shape of
  // the heap object or the index being in bounds.
  bool control_flow : 1;
  // We need to ensure that the padding bits have a specified value, as they are
  // observable in bitwise operations.
  uint8_t unused_padding : 1;

  using Bits = uint8_t;
  constexpr EffectDimensions()
      : load_heap_memory(false),
        load_off_heap_memory(false),
        store_heap_memory(false),
        store_off_heap_memory(false),
        before_raw_heap_access(false),
        after_raw_heap_access(false),
        control_flow(false),
        unused_padding(0) {}
  Bits bits() const { return base::bit_cast<Bits>(*this); }
  static EffectDimensions FromBits(Bits bits) {
    return base::bit_cast<EffectDimensions>(bits);
  }
  bool operator==(EffectDimensions other) const {
    return bits() == other.bits();
  }
  bool operator!=(EffectDimensions other) const {
    return bits() != other.bits();
  }
};
static_assert(sizeof(EffectDimensions) == sizeof(EffectDimensions::Bits));

// Possible reorderings are restricted using two bit vectors: `produces` and
// `consumes`. Two operations cannot be reordered if the first operation
// produces an effect dimension that the second operation consumes. This is not
// necessarily symmetric. For example, it is possible to reorder
//     Load(x)
//     CheckMaps(y)
// to become
//     CheckMaps(x)
//     Load(y)
// because the load cannot affect the map check. But the other direction could
// be unsound, if the load depends on the map check having been executed. The
// former reordering is useful to push a load across a check into a branch if
// it is only needed there. The effect system expresses this by having the map
// check produce `EffectDimensions::control_flow` and the load consuming
// `EffectDimensions::control_flow`. If the producing operation comes before the
// consuming operation, then this order has to be preserved. But if the
// consuming operation comes first, then we are free to reorder them. Operations
// that produce and consume the same effect dimension always have a fixed order
// among themselves. For example, stores produce and consume the store
// dimensions. It is possible for operations to be reorderable unless certain
// other operations appear in-between. This way, the IR can be generous with
// reorderings as long as all operations are high-level, but become more
// restrictive as soon as low-level operations appear. For example, allocations
// can be freely reordered. Tagged bitcasts can be reordered with other tagged
// bitcasts. But a tagged bitcast cannot be reordered with allocations, as this
// would mean that an untagged pointer can be alive while a GC is happening. The
// way this works is that allocations produce the `before_raw_heap_access`
// dimension and consume the `after_raw_heap_access` dimension to stay either
// before or after a raw heap access. This means that there are no ordering
// constraints between allocations themselves. Bitcasts should not
// be moved accross an allocation. We treat them as raw heap access by letting
// them consume `before_raw_heap_access` and produce `after_raw_heap_access`.
// This way, allocations cannot be moved across bitcasts. Similarily,
// initializing stores and uninitialized allocations are classified as raw heap
// access, to prevent any operation that relies on a consistent heap state to be
// scheduled in the middle of an inline allocation. As long as we didn't lower
// to raw heap accesses yet, pure allocating operations or operations reading
// immutable memory can float freely. As soon as there are raw heap accesses,
// they become more restricted in their movement. Note that calls are not the
// most side-effectful operations, as they do not leave the heap in an
// inconsistent state, so they do not need to be marked as raw heap access.
struct OpEffects {
  EffectDimensions produces;
  EffectDimensions consumes;

  // Operations that cannot be merged because they produce identity.  That is,
  // every repetition can produce a different result, but the order in which
  // they are executed does not matter. All we care about is that they are
  // different. Producing a random number or allocating an object with
  // observable pointer equality are examples. Producing identity doesn't
  // restrict reordering in straight-line code, but we must prevent using GVN or
  // moving identity-producing operations in- or out of loops.
  bool can_create_identity : 1;
  // If the operation can allocate and therefore can trigger GC.
  bool can_allocate : 1;
  // Instructions that have no uses but are `required_when_unused` should not be
  // removed.
  bool required_when_unused : 1;
  // We need to ensure that the padding bits have a specified value, as they are
  // observable in bitwise operations.  This is split into two fields so that
  // also MSVC creates the correct object layout.
  uint8_t unused_padding_1 : 5;
  uint8_t unused_padding_2;

  constexpr OpEffects()
      : can_create_identity(false),
        can_allocate(false),
        required_when_unused(false),
        unused_padding_1(0),
        unused_padding_2(0) {}

  using Bits = uint32_t;
  Bits bits() const { return base::bit_cast<Bits>(*this); }
  static OpEffects FromBits(Bits bits) {
    return base::bit_cast<OpEffects>(bits);
  }

  bool operator==(OpEffects other) const { return bits() == other.bits(); }
  bool operator!=(OpEffects other) const { return bits() != other.bits(); }
  OpEffects operator|(OpEffects other) const {
    return FromBits(bits() | other.bits());
  }
  OpEffects operator&(OpEffects other) const {
    return FromBits(bits() & other.bits());
  }
  bool IsSubsetOf(OpEffects other) const {
    return (bits() & ~other.bits()) == 0;
  }

  constexpr OpEffects AssumesConsistentHeap() const {
    OpEffects result = *this;
    // Do not move the operation into a region with raw heap access.
    result.produces.before_raw_heap_access = true;
    result.consumes.after_raw_heap_access = true;
    return result;
  }
  // Like `CanAllocate()`, but allocated values must be immutable and not have
  // identity (for example `HeapNumber`).
  // Note that if we first allocate something as mutable and later make it
  // immutable, we have to allocate it with identity.
  constexpr OpEffects CanAllocateWithoutIdentity() const {
    OpEffects result = AssumesConsistentHeap();
    result.can_allocate = true;
    return result;
  }
  // Allocations change the GC state and can trigger GC, as well as produce a
  // fresh identity.
  constexpr OpEffects CanAllocate() const {
    return CanAllocateWithoutIdentity().CanCreateIdentity();
  }
  // The operation can leave the heap in an incosistent state or have untagged
  // pointers into the heap as input or output.
  constexpr OpEffects CanDoRawHeapAccess() const {
    OpEffects result = *this;
    // Do not move any operation that relies on a consistent heap state accross.
    result.produces.after_raw_heap_access = true;
    result.consumes.before_raw_heap_access = true;
    return result;
  }
  // Reading mutable heap memory. Reading immutable memory doesn't count.
  constexpr OpEffects CanReadHeapMemory() const {
    OpEffects result = *this;
    result.produces.load_heap_memory = true;
    // Do not reorder before stores.
    result.consumes.store_heap_memory = true;
    return result;
  }
  // Reading mutable off-heap memory or other input. Reading immutable memory
  // doesn't count.
  constexpr OpEffects CanReadOffHeapMemory() const {
    OpEffects result = *this;
    result.produces.load_off_heap_memory = true;
    // Do not reorder before stores.
    result.consumes.store_off_heap_memory = true;
    return result;
  }
  // Writing any off-memory or other output.
  constexpr OpEffects CanWriteOffHeapMemory() const {
    OpEffects result = *this;
    result.required_when_unused = true;
    result.produces.store_off_heap_memory = true;
    // Do not reorder before stores.
    result.consumes.store_off_heap_memory = true;
    // Do not reorder before loads.
    result.consumes.load_off_heap_memory = true;
    // Do not move before deopting or aborting operations.
    result.consumes.control_flow = true;
    return result;
  }
  // Writing heap memory that existed before the operation started. Initializing
  // newly allocated memory doesn't count.
  constexpr OpEffects CanWriteHeapMemory() const {
    OpEffects result = *this;
    result.required_when_unused = true;
    result.produces.store_heap_memory = true;
    // Do not reorder before stores.
    result.consumes.store_heap_memory = true;
    // Do not reorder before loads.
    result.consumes.load_heap_memory = true;
    // Do not move before deopting or aborting operations.
    result.consumes.control_flow = true;
    return result;
  }
  // Writing any memory or other output, on- or off-heap.
  constexpr OpEffects CanWriteMemory() const {
    return CanWriteHeapMemory().CanWriteOffHeapMemory();
  }
  // Reading any memory or other input, on- or off-heap.
  constexpr OpEffects CanReadMemory() const {
    return CanReadHeapMemory().CanReadOffHeapMemory();
  }
  // The operation might read immutable data from the heap, so it can be freely
  // reordered with operations that keep the heap in a consistent state. But we
  // must prevent the operation from observing an incompletely initialized
  // object.
  constexpr OpEffects CanReadImmutableMemory() const {
    OpEffects result = AssumesConsistentHeap();
    return result;
  }
  // Partial operations that are only safe to execute after we performed certain
  // checks, for example loads may only be safe after a corresponding bound or
  // map checks.
  constexpr OpEffects CanDependOnChecks() const {
    OpEffects result = *this;
    result.consumes.control_flow = true;
    return result;
  }
  // The operation can affect control flow (like branch, deopt, throw or crash).
  constexpr OpEffects CanChangeControlFlow() const {
    OpEffects result = *this;
    result.required_when_unused = true;
    // Signal that this changes control flow. Prevents stores or operations
    // relying on checks from flowing before this operation.
    result.produces.control_flow = true;
    // Stores must not flow past something that affects control flow.
    result.consumes.store_heap_memory = true;
    result.consumes.store_off_heap_memory = true;
    return result;
  }
  // Execution of the current function may end with this operation, for example
  // because of return, deopt, exception throw or abort/trap.
  constexpr OpEffects CanLeaveCurrentFunction() const {
    // All memory becomes observable.
    return CanChangeControlFlow().CanReadMemory().RequiredWhenUnused();
  }
  // The operation can deopt.
  constexpr OpEffects CanDeopt() const {
    return CanLeaveCurrentFunction()
        // We might depend on previous checks to avoid deopting.
        .CanDependOnChecks();
  }
  // Producing identity doesn't prevent reorderings, but it prevents GVN from
  // de-duplicating identical operations.
  constexpr OpEffects CanCreateIdentity() const {
    OpEffects result = *this;
    result.can_create_identity = true;
    return result;
  }
  // The set of all possible effects.
  constexpr OpEffects CanCallAnything() const {
    return CanReadMemory()
        .CanWriteMemory()
        .CanAllocate()
        .CanChangeControlFlow()
        .CanDependOnChecks()
        .RequiredWhenUnused();
  }
  constexpr OpEffects RequiredWhenUnused() const {
    OpEffects result = *this;
    result.required_when_unused = true;
    return result;
  }

  // Operations that can be removed if their result is not used. Unused
  // allocations can be removed.
  constexpr bool is_required_when_unused() const {
    return required_when_unused;
  }
  // Operations that can be moved before a preceding branch or check.
  bool hoistable_before_a_branch() const {
    // Since this excludes `CanDependOnChecks()`, most loads actually cannot be
    // hoisted.
    return IsSubsetOf(OpEffects().CanReadMemory());
  }
  // Operations that can be eliminated via value numbering, which means that if
  // there are two identical operations where one dominates the other, then the
  // second can be replaced with the first one. This is safe for deopting or
  // throwing operations, because the absence of read effects guarantees
  // deterministic behavior.
  bool repetition_is_eliminatable() const {
    return IsSubsetOf(OpEffects()
                          .CanDependOnChecks()
                          .CanChangeControlFlow()
                          .CanAllocateWithoutIdentity());
  }
  bool can_read_mutable_memory() const {
    return produces.load_heap_memory | produces.load_off_heap_memory;
  }
  bool requires_consistent_heap() const {
    return produces.before_raw_heap_access | consumes.after_raw_heap_access;
  }
  bool can_write() const {
    return produces.store_heap_memory | produces.store_off_heap_memory;
  }
  bool can_be_constant_folded() const {
    // Operations that CanDependOnChecks can still be constant-folded. If they
    // did indeed depend on a check, then their result will only be used after
    // said check has been executed anyways.
    return IsSubsetOf(OpEffects().CanDependOnChecks());
  }
};
static_assert(sizeof(OpEffects) == sizeof(OpEffects::Bits));

V8_INLINE size_t hash_value(OpEffects effects) {
  return static_cast<size_t>(effects.bits());
}

inline bool CannotSwapOperations(OpEffects first, OpEffects second) {
  return first.produces.bits() & (second.consumes.bits());
}

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           OpEffects op_effects);

// SaturatedUint8 is a wrapper around a uint8_t, which can be incremented and
// decremented with the `Incr` and `Decr` methods. These methods prevent over-
// and underflow, and saturate once the uint8_t reaches the maximum (255):
// future increment and decrement will not change the value then.
// We purposefuly do not expose the uint8_t directly, so that users go through
// Incr/Decr/SetToZero/SetToOne to manipulate it, so that the saturation and
// lack of over/underflow is always respected.
class SaturatedUint8 {
 public:
  SaturatedUint8() = default;

  void Incr() {
    if (V8_LIKELY(val != kMax)) {
      val++;
    }
  }
  void Decr() {
    if (V8_LIKELY(val != 0 && val != kMax)) {
      val--;
    }
  }

  void SetToZero() { val = 0; }
  void SetToOne() { val = 1; }

  bool IsZero() const { return val == 0; }
  bool IsOne() const { return val == 1; }
  bool IsSaturated() const { return val == kMax; }
  uint8_t Get() const { return val; }

  SaturatedUint8& operator+=(const SaturatedUint8& other) {
    uint32_t sum = val;
    sum += other.val;
    val = static_cast<uint8_t>(std::min<uint32_t>(sum, kMax));
    return *this;
  }

  static SaturatedUint8 FromSize(size_t value) {
    uint8_t val = static_cast<uint8_t>(std::min<size_t>(value, kMax));
    return SaturatedUint8{val};
  }

 private:
  explicit SaturatedUint8(uint8_t val) : val(val) {}
  uint8_t val = 0;
  static constexpr uint8_t kMax = std::numeric_limits<uint8_t>::max();
};

// underlying_operation<> is used to extract the operation type from OpMaskT
// classes used in Operation::Is<> and Operation::TryCast<>.
template <typename T>
struct underlying_operation {
  using type = T;
};
template <typename T, uint64_t M, uint64_t V>
struct underlying_operation<OpMaskT<T, M, V>> {
  using type = T;
};
template <typename T>
using underlying_operation_t = typename underlying_operation<T>::type;

// Baseclass for all Turboshaft operations.
// The `alignas(OpIndex)` is necessary because it is followed by an array of
// `OpIndex` inputs.
struct alignas(OpIndex) Operation {
  struct IdentityMapper {
    OpIndex Map(OpIndex index) { return index; }
    OptionalOpIndex Map(OptionalOpIndex index) { return index; }
    template <size_t N>
    base::SmallVector<OpIndex, N> Map(base::Vector<const OpIndex> indices) {
      return base::SmallVector<OpIndex, N>{indices};
    }
  };

  const Opcode opcode;

  // The number of uses of this operation in the current graph.
  // Instead of overflowing, we saturate the value if it reaches the maximum. In
  // this case, the true number of uses is unknown.
  // We use such a small type to save memory and because nodes with a high
  // number of uses are rare. Additionally, we usually only care if the number
  // of uses is 0, 1 or bigger than 1.
  SaturatedUint8 saturated_use_count;

  const uint16_t input_count;

  // The inputs are stored adjacent in memory, right behind the `Operation`
  // object.
  base::Vector<const OpIndex> inputs() const;
  V8_INLINE OpIndex input(size_t i) const { return inputs()[i]; }

  static size_t StorageSlotCount(Opcode opcode, size_t input_count);
  size_t StorageSlotCount() const {
    return StorageSlotCount(opcode, input_count);
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const;
  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const;

  template <class Op>
  bool Is() const {
    if constexpr (std::is_base_of_v<Operation, Op>) {
      return opcode == Op::opcode;
    } else {
      // Otherwise this must be OpMaskT.
      return IsOpmask<Op>();
    }
  }
  template <class Op>
  underlying_operation_t<Op>& Cast() {
    DCHECK(Is<Op>());
    return *static_cast<underlying_operation_t<Op>*>(this);
  }
  template <class Op>
  const underlying_operation_t<Op>& Cast() const {
    DCHECK(Is<Op>());
    return *static_cast<const underlying_operation_t<Op>*>(this);
  }
  template <class Op>
  const underlying_operation_t<Op>* TryCast() const {
    if (!Is<Op>()) return nullptr;
    return static_cast<const underlying_operation_t<Op>*>(this);
  }
  template <class Op>
  underlying_operation_t<Op>* TryCast() {
    if (!Is<Op>()) return nullptr;
    return static_cast<underlying_operation_t<Op>*>(this);
  }
  OpEffects Effects() const;
  bool IsBlockTerminator() const {
    return turboshaft::IsBlockTerminator(opcode);
  }
  bool IsRequiredWhenUnused() const {
    DCHECK_IMPLIES(IsBlockTerminator(), Effects().is_required_when_unused());
    return Effects().is_required_when_unused();
  }

  std::string ToString() const;
  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
  void PrintOptions(std::ostream& os) const;

  // Returns true if {this} is the only operation using {value}.
  bool IsOnlyUserOf(const Operation& value, const Graph& graph) const;

  void Print() const;

 protected:
  // Operation objects store their inputs behind the object. Therefore, they can
  // only be constructed as part of a Graph.
  explicit Operation(Opcode opcode, size_t input_count)
      : opcode(opcode), input_count(input_count) {
    DCHECK_LE(input_count,
              std::numeric_limits<decltype(this->input_count)>::max());
  }

  template <class OpmaskT>
  // A Turboshaft operation can be as small as 4 Bytes while Opmasks can span up
  // to 8 Bytes. Any mask larger than the operation it is compared with will
  // always have a mismatch in the initialized memory. Still, there can be some
  // uninitialized memory being compared as part of the 8 Byte comparison that
  // this function performs.
  V8_CLANG_NO_SANITIZE("memory") bool IsOpmask() const {
    static_assert(std::is_same_v<
                  underlying_operation_t<OpmaskT>,
                  typename OpMaskT<typename OpmaskT::operation, OpmaskT::mask,
                                   OpmaskT::value>::operation>);
    // We check with the given mask.
    uint64_t b;
    memcpy(&b, this, sizeof(uint64_t));
    b &= OpmaskT::mask;
    return b == OpmaskT::value;
  }

  Operation(const Operation&) = delete;
  Operation& operator=(const Operation&) = delete;
};

struct OperationPrintStyle {
  const Operation& op;
  const char* op_index_prefix = "#";
};

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           OperationPrintStyle op);
inline std::ostream& operator<<(std::ostream& os, const Operation& op) {
  return os << OperationPrintStyle{op};
}

V8_EXPORT_PRIVATE Zone* get_zone(Graph* graph);

OperationStorageSlot* AllocateOpStorage(Graph* graph, size_t slot_count);
V8_EXPORT_PRIVATE const Operation& Get(const Graph& graph, OpIndex index);

// Determine if an operation declares `effects`, which means that its
// effects are static and don't depend on inputs or options.
template <class Op, class = void>
struct HasStaticEffects : std::bool_constant<false> {};
template <class Op>
struct HasStaticEffects<Op, std::void_t<decltype(Op::effects)>>
    : std::bool_constant<true> {};

// This template knows the complete type of the operation and is plugged into
// the inheritance hierarchy. It removes boilerplate from the concrete
// `Operation` subclasses, defining everything that can be expressed
// generically. It overshadows many methods from `Operation` with ones that
// exploit additional static information.
template <class Derived>
struct OperationT : Operation {
  // Enable concise base-constructor call in derived struct.
  using Base = OperationT;

  static const Opcode opcode;

  static constexpr OpEffects Effects() { return Derived::effects; }
  static constexpr bool IsBlockTerminator() {
    return turboshaft::IsBlockTerminator(opcode);
  }
  bool IsRequiredWhenUnused() const {
    return IsBlockTerminator() ||
           derived_this().Effects().is_required_when_unused();
  }

  static constexpr std::optional<OpEffects> EffectsIfStatic() {
    if constexpr (HasStaticEffects<Derived>::value) {
      return Derived::Effects();
    }
    return std::nullopt;
  }

  Derived& derived_this() { return *static_cast<Derived*>(this); }
  const Derived& derived_this() const {
    return *static_cast<const Derived*>(this);
  }

  // Shadow Operation::inputs to exploit static knowledge about object size.
  base::Vector<OpIndex> inputs() {
    return {reinterpret_cast<OpIndex*>(reinterpret_cast<char*>(this) +
                                       sizeof(Derived)),
            derived_this().input_count};
  }
  base::Vector<const OpIndex> inputs() const {
    return {reinterpret_cast<const OpIndex*>(
                reinterpret_cast<const char*>(this) + sizeof(Derived)),
            derived_this().input_count};
  }

  V8_INLINE OpIndex& input(size_t i) { return derived_this().inputs()[i]; }
  // TODO(chromium:331100916): remove this V<Any> overload once all users use
  // the more specific V<T> overload.
  V8_INLINE V<Any> input(size_t i) const { return derived_this().inputs()[i]; }
  template <typename T>
  V8_INLINE V<T> input(size_t i) const {
    return V<T>::Cast(derived_this().inputs()[i]);
  }

  static size_t StorageSlotCount(size_t input_count) {
    // The operation size in bytes is:
    //   `sizeof(Derived) + input_count*sizeof(OpIndex)`.
    // This is an optimized computation of:
    //   round_up(size_in_bytes / sizeof(StorageSlot))
    constexpr size_t r = sizeof(OperationStorageSlot) / sizeof(OpIndex);
    static_assert(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
    static_assert(sizeof(Derived) % sizeof(OpIndex) == 0);
    size_t result = std::max<size_t>(
        2, (r - 1 + sizeof(Derived) / sizeof(OpIndex) + input_count) / r);
    DCHECK_EQ(result, Operation::StorageSlotCount(opcode, input_count));
    return result;
  }
  size_t StorageSlotCount() const { return StorageSlotCount(input_count); }

  template <class... Args>
  static Derived& New(Graph* graph, size_t input_count, Args... args) {
    OperationStorageSlot* ptr =
        AllocateOpStorage(graph, StorageSlotCount(input_count));
    Derived* result = new (ptr) Derived(args...);
#ifdef DEBUG
    result->Validate(*graph);
    ZoneVector<MaybeRegisterRepresentation> storage(get_zone(graph));
    base::Vector<const MaybeRegisterRepresentation> expected =
        result->inputs_rep(storage);
    // TODO(mliedtke): DCHECK that expected and inputs are of the same size
    // and adapt inputs_rep() to always emit a representation for all inputs.
    size_t end = std::min<size_t>(expected.size(), result->input_count);
    for (size_t i = 0; i < end; ++i) {
      if (expected[i] == MaybeRegisterRepresentation::None()) continue;
      DCHECK(ValidOpInputRep(*graph, result->inputs()[i],
                             RegisterRepresentation(expected[i])));
    }
#endif
    // If this DCHECK fails, then the number of inputs specified in the
    // operation constructor and in the static New function disagree.
    DCHECK_EQ(input_count, result->Operation::input_count);
    return *result;
  }

  template <class... Args>
  static Derived& New(Graph* graph, ShadowyOpIndexVectorWrapper inputs,
                      Args... args) {
    return New(graph, inputs.size(), inputs, args...);
  }

  explicit OperationT(size_t input_count) : Operation(opcode, input_count) {
    static_assert((std::is_base_of<OperationT, Derived>::value));
#if !V8_CC_MSVC
    static_assert(std::is_trivially_copyable<Derived>::value);
#endif  // !V8_CC_MSVC
    static_assert(std::is_trivially_destructible<Derived>::value);
  }
  explicit OperationT(ShadowyOpIndexVectorWrapper inputs)
      : OperationT(inputs.size()) {
    this->inputs().OverwriteWith(
        static_cast<base::Vector<const OpIndex>>(inputs));
  }

  bool EqualsForGVN(const Base& other) const {
    // By default, GVN only removed identical Operations. However, some
    // Operations (like DeoptimizeIf) can be GVNed when a dominating
    // similar-but-not-identical one exists. In that case, the Operation should
    // redefine EqualsForGVN, so that GVN knows which inputs or options of the
    // Operation to ignore (you should also probably redefine hash_value,
    // otherwise GVN won't even try to call EqualsForGVN).
    return derived_this() == other.derived_this();
  }
  bool operator==(const Base& other) const {
    return derived_this().inputs() == other.derived_this().inputs() &&
           derived_this().options() == other.derived_this().options();
  }
  template <typename... Args>
  size_t HashWithOptions(const Args&... args) const {
    return fast_hash_combine(opcode, derived_this().inputs(), args...);
  }
  size_t hash_value(
      HashingStrategy strategy = HashingStrategy::kDefault) const {
    return HashWithOptions(derived_this().options());
  }

  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const {
    os << "(";
    bool first = true;
    for (OpIndex input : inputs()) {
      if (!first) os << ", ";
      first = false;
      os << op_index_prefix << input.id();
    }
    os << ")";
  }

  void PrintOptions(std::ostream& os) const {
    const auto& options = derived_this().options();
    constexpr size_t options_count =
        std::tuple_size<std::remove_reference_t<decltype(options)>>::value;
    if (options_count == 0) {
      return;
    }
    PrintOptionsHelper(os, options, std::make_index_sequence<options_count>());
  }

  // Check graph invariants for this operation. Will be invoked in debug mode
  // immediately upon construction.
  // Concrete Operator classes are expected to re-define it.
  void Validate(const Graph& graph) const = delete;

 private:
  template <class... T, size_t... I>
  static void PrintOptionsHelper(std::ostream& os,
                                 const std::tuple<T...>& options,
                                 std::index_sequence<I...>) {
    os << "[";
    bool first = true;
    USE(first);
    ((first ? (first = false, os << std::get<I>(options))
            : os << ", " << std::get<I>(options)),
     ...);
    os << "]";
  }

  // All Operations have to define the outputs_rep function, to which
  // Operation::outputs_rep() will forward, based on their opcode. If you forget
  // to define it, then Operation::outputs_rep() would forward to itself,
  // resulting in an infinite loop. To avoid this, we define here in OperationT
  // a private version outputs_rep (with no implementation): if an operation
  // forgets to define outputs_rep, then Operation::outputs_rep() tries to call
  // this private version, which fails at compile time.
  base::Vector<const RegisterRepresentation> outputs_rep() const;

  // Returns a vector of the input representations.
  // The passed in {storage} can be used to store the underlying data.
  // The returned vector might be smaller than the input_count in which case the
  // additional inputs are assumed to have no register representation.
  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const;
};

template <size_t InputCount, class Derived>
struct FixedArityOperationT : OperationT<Derived> {
  // Enable concise base access in derived struct.
  using Base = FixedArityOperationT;

  // Shadow Operation::input_count to exploit static knowledge.
  static constexpr uint16_t input_count = InputCount;

  template <class... Args>
  explicit FixedArityOperationT(Args... args)
      : OperationT<Derived>(InputCount) {
    static_assert(sizeof...(Args) == InputCount, "wrong number of inputs");
    size_t i = 0;
    OpIndex* inputs = this->inputs().begin();
    ((inputs[i++] = args), ...);
  }

  // Redefine the input initialization to tell C++ about the static input size.
  template <class... Args>
  static Derived& New(Graph* graph, Args... args) {
    Derived& result =
        OperationT<Derived>::New(graph, InputCount, std::move(args)...);
    return result;
  }

  template <typename Fn, typename Mapper, size_t... InputI, size_t... OptionI>
  V8_INLINE auto ExplodeImpl(Fn fn, Mapper& mapper,
                             std::index_sequence<InputI...>,
                             std::index_sequence<OptionI...>) const {
    auto options = this->derived_this().options();
    USE(options);
    return fn(mapper.Map(this->input(InputI))...,
              std::get<OptionI>(options)...);
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return ExplodeImpl(
        fn, mapper, std::make_index_sequence<input_count>(),
        std::make_index_sequence<
            std::tuple_size_v<decltype(this->derived_this().options())>>());
  }
};

#define SUPPORTED_OPERATIONS_LIST(V)               \
  V(float32_round_down, Float32RoundDown)          \
  V(float64_round_down, Float64RoundDown)          \
  V(float32_round_up, Float32RoundUp)              \
  V(float64_round_up, Float64RoundUp)              \
  V(float32_round_to_zero, Float32RoundTruncate)   \
  V(float64_round_to_zero, Float64RoundTruncate)   \
  V(float32_round_ties_even, Float32RoundTiesEven) \
  V(float64_round_ties_even, Float64RoundTiesEven) \
  V(float64_round_ties_away, Float64RoundTiesAway) \
  V(int32_div_is_safe, Int32DivIsSafe)             \
  V(uint32_div_is_safe, Uint32DivIsSafe)           \
  V(word32_shift_is_safe, Word32ShiftIsSafe)       \
  V(word32_ctz, Word32Ctz)                         \
  V(word64_ctz, Word64Ctz)                         \
  V(word64_ctz_lowerable, Word64CtzLowerable)      \
  V(word32_popcnt, Word32Popcnt)                   \
  V(word64_popcnt, Word64Popcnt)                   \
  V(word32_reverse_bits, Word32ReverseBits)        \
  V(word64_reverse_bits, Word64ReverseBits)        \
  V(float32_select, Float32Select)                 \
  V(float64_select, Float64Select)                 \
  V(int32_abs_with_overflow, Int32AbsWithOverflow) \
  V(int64_abs_with_overflow, Int64AbsWithOverflow) \
  V(word32_rol, Word32Rol)                         \
  V(word64_rol, Word64Rol)                         \
  V(word64_rol_lowerable, Word64RolLowerable)      \
  V(sat_conversion_is_safe, SatConversionIsSafe)   \
  V(word32_select, Word32Select)                   \
  V(word64_select, Word64Select)                   \
  V(float64_to_float16, Float64ToFloat16)          \
  V(float16, Float16)

class V8_EXPORT_PRIVATE SupportedOperations {
#define DECLARE_FIELD(name, machine_name) bool name##_;
#define DECLARE_GETTER(name, machine_name)     \
  static bool name() {                         \
    if constexpr (DEBUG_BOOL) {                \
      base::MutexGuard lock(mutex_.Pointer()); \
      DCHECK(initialized_);                    \
    }                                          \
    return instance_.name##_;                  \
  }

 public:
  static void Initialize();
  static bool IsUnalignedLoadSupported(MemoryRepresentation repr);
  static bool IsUnalignedStoreSupported(MemoryRepresentation repr);
  SUPPORTED_OPERATIONS_LIST(DECLARE_GETTER)

 private:
  SUPPORTED_OPERATIONS_LIST(DECLARE_FIELD)

  static bool initialized_;
  static base::LazyMutex mutex_;
  static SupportedOperations instance_;

#undef DECLARE_FIELD
#undef DECLARE_GETTER
};

template <RegisterRepresentation::Enum... reps>
base::Vector<const RegisterRepresentation> RepVector() {
  static constexpr std::array<RegisterRepresentation, sizeof...(reps)>
      rep_array{RegisterRepresentation{reps}...};
  return base::VectorOf(rep_array);
}

template <MaybeRegisterRepresentation::Enum... reps>
base::Vector<const MaybeRegisterRepresentation> MaybeRepVector() {
  static constexpr std::array<MaybeRegisterRepresentation, sizeof...(reps)>
      rep_array{MaybeRegisterRepresentation{reps}...};
  return base::VectorOf(rep_array);
}

#if DEBUG
V8_EXPORT_PRIVATE bool ValidOpInputRep(
    const Graph& graph, OpIndex input,
    std::initializer_list<RegisterRepresentation> expected_rep,
    std::optional<size_t> projection_index = {});
V8_EXPORT_PRIVATE bool ValidOpInputRep(
    const Graph& graph, OpIndex input, RegisterRepresentation expected_rep,
    std::optional<size_t> projection_index = {});
#endif  // DEBUG

// DeadOp is a special operation that can be used by analyzers to mark
// operations as being dead (typically, it should be used by calling the Graph's
// KillOperation method, which will Replace the old operation by a DeadOp).
// CopyingPhase and Analyzers should ignore Dead operations. A Dead operation
// should never be the input of a non-dead operation.
struct DeadOp : FixedArityOperationT<0, DeadOp> {
  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

struct AbortCSADcheckOp : FixedArityOperationT<1, AbortCSADcheckOp> {
  static constexpr OpEffects effects =
      OpEffects().RequiredWhenUnused().CanLeaveCurrentFunction();

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<String> message() { return Base::input<String>(0); }

  explicit AbortCSADcheckOp(V<String> message) : Base(message) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

struct GenericBinopOp : FixedArityOperationT<4, GenericBinopOp> {
#define GENERIC_BINOP_LIST(V) \
  V(Add)                      \
  V(Multiply)                 \
  V(Subtract)                 \
  V(Divide)                   \
  V(Modulus)                  \
  V(Exponentiate)             \
  V(BitwiseAnd)               \
  V(BitwiseOr)                \
  V(BitwiseXor)               \
  V(ShiftLeft)                \
  V(ShiftRight)               \
  V(ShiftRightLogical)        \
  V(Equal)                    \
  V(StrictEqual)              \
  V(LessThan)                 \
  V(LessThanOrEqual)          \
  V(GreaterThan)              \
  V(GreaterThanOrEqual)
  enum class Kind : uint8_t {
#define DEFINE_KIND(Name) k##Name,
    GENERIC_BINOP_LIST(DEFINE_KIND)
#undef DEFINE_KIND
  };
  Kind kind;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  THROWING_OP_BOILERPLATE(RegisterRepresentation::Tagged())

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> left() const { return input<Object>(0); }
  V<Object> right() const { return input<Object>(1); }
  V<FrameState> frame_state() const { return input<FrameState>(2); }
  V<Context> context() const { return input<Context>(3); }

  GenericBinopOp(V<Object> left, V<Object> right, V<FrameState> frame_state,
                 V<Context> context, Kind kind,
                 LazyDeoptOnThrow lazy_deopt_on_throw)
      : Base(left, right, frame_state, context),
        kind(kind),
        lazy_deopt_on_throw(lazy_deopt_on_throw) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, lazy_deopt_on_throw}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           GenericBinopOp::Kind kind);

struct GenericUnopOp : FixedArityOperationT<3, GenericUnopOp> {
#define GENERIC_UNOP_LIST(V) \
  V(BitwiseNot)              \
  V(Negate)                  \
  V(Increment)               \
  V(Decrement)
  enum class Kind : uint8_t {
#define DEFINE_KIND(Name) k##Name,
    GENERIC_UNOP_LIST(DEFINE_KIND)
#undef DEFINE_KIND
  };
  Kind kind;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  THROWING_OP_BOILERPLATE(RegisterRepresentation::Tagged())

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }
  V<Context> context() const { return Base::input<Context>(2); }

  GenericUnopOp(V<Object> input, V<FrameState> frame_state, V<Context> context,
                Kind kind, LazyDeoptOnThrow lazy_deopt_on_throw)
      : Base(input, frame_state, context),
        kind(kind),
        lazy_deopt_on_throw(lazy_deopt_on_throw) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, lazy_deopt_on_throw}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           GenericUnopOp::Kind kind);

struct ToNumberOrNumericOp : FixedArityOperationT<3, ToNumberOrNumericOp> {
  Object::Conversion kind;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  THROWING_OP_BOILERPLATE(RegisterRepresentation::Tagged())

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }
  OpIndex frame_state() const { return Base::input(1); }
  V<Context> context() const { return Base::input<Context>(2); }

  ToNumberOrNumericOp(V<Object> input, OpIndex frame_state, V<Context> context,
                      Object::Conversion kind,
                      LazyDeoptOnThrow lazy_deopt_on_throw)
      : Base(input, frame_state, context),
        kind(kind),
        lazy_deopt_on_throw(lazy_deopt_on_throw) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, lazy_deopt_on_throw}; }
};

struct WordBinopOp : FixedArityOperationT<2, WordBinopOp> {
  enum class Kind : uint8_t {
    kAdd,
    kMul,
    kSignedMulOverflownBits,
    kUnsignedMulOverflownBits,
    kBitwiseAnd,
    kBitwiseOr,
    kBitwiseXor,
    kSub,
    kSignedDiv,
    kUnsignedDiv,
    kSignedMod,
    kUnsignedMod,
  };
  Kind kind;
  WordRepresentation rep;

  // We must avoid division by 0.
  static constexpr OpEffects effects = OpEffects().CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::PairOf(rep);
  }

  template <class WordType = Word,
            typename = std::enable_if_t<IsWord<WordType>()>>
  V<WordType> left() const {
    return input<WordType>(0);
  }
  template <class WordType = Word,
            typename = std::enable_if_t<IsWord<WordType>()>>
  V<WordType> right() const {
    return input<WordType>(1);
  }

  bool IsCommutative() const { return IsCommutative(kind); }

  static bool IsCommutative(Kind kind) {
    switch (kind) {
      case Kind::kAdd:
      case Kind::kMul:
      case Kind::kSignedMulOverflownBits:
      case Kind::kUnsignedMulOverflownBits:
      case Kind::kBitwiseAnd:
      case Kind::kBitwiseOr:
      case Kind::kBitwiseXor:
        return true;
      case Kind::kSub:
      case Kind::kSignedDiv:
      case Kind::kUnsignedDiv:
      case Kind::kSignedMod:
      case Kind::kUnsignedMod:
        return false;
    }
  }

  static bool IsAssociative(Kind kind) {
    switch (kind) {
      case Kind::kAdd:
      case Kind::kMul:
      case Kind::kBitwiseAnd:
      case Kind::kBitwiseOr:
      case Kind::kBitwiseXor:
        return true;
      case Kind::kSignedMulOverflownBits:
      case Kind::kUnsignedMulOverflownBits:
      case Kind::kSub:
      case Kind::kSignedDiv:
      case Kind::kUnsignedDiv:
      case Kind::kSignedMod:
      case Kind::kUnsignedMod:
        return false;
    }
  }
  // The Word32 and Word64 versions of the operator compute the same result when
  // truncated to 32 bit.
  static bool AllowsWord64ToWord32Truncation(Kind kind) {
    switch (kind) {
      case Kind::kAdd:
      case Kind::kMul:
      case Kind::kBitwiseAnd:
      case Kind::kBitwiseOr:
      case Kind::kBitwiseXor:
      case Kind::kSub:
        return true;
      case Kind::kSignedMulOverflownBits:
      case Kind::kUnsignedMulOverflownBits:
      case Kind::kSignedDiv:
      case Kind::kUnsignedDiv:
      case Kind::kSignedMod:
      case Kind::kUnsignedMod:
        return false;
    }
  }

  WordBinopOp(V<Word> left, V<Word> right, Kind kind, WordRepresentation rep)
      : Base(left, right), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, rep}; }
  void PrintOptions(std::ostream& os) const;
};

struct FloatBinopOp : FixedArityOperationT<2, FloatBinopOp> {
  enum class Kind : uint8_t {
    kAdd,
    kMul,
    kMin,
    kMax,
    kSub,
    kDiv,
    kMod,
    kPower,
    kAtan2,
  };
  Kind kind;
  FloatRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::PairOf(rep);
  }

  V<Float> left() const { return input<Float>(0); }
  V<Float> right() const { return input<Float>(1); }

  static bool IsCommutative(Kind kind) {
    switch (kind) {
      case Kind::kAdd:
      case Kind::kMul:
      case Kind::kMin:
      case Kind::kMax:
        return true;
      case Kind::kSub:
      case Kind::kDiv:
      case Kind::kMod:
      case Kind::kPower:
      case Kind::kAtan2:
        return false;
    }
  }

  FloatBinopOp(V<Float> left, V<Float> right, Kind kind,
               FloatRepresentation rep)
      : Base(left, right), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
    DCHECK_IMPLIES(kind == any_of(Kind::kPower, Kind::kAtan2, Kind::kMod),
                   rep == FloatRepresentation::Float64());
  }
  auto options() const { return std::tuple{kind, rep}; }
  void PrintOptions(std::ostream& os) const;
};

struct Word32PairBinopOp : FixedArityOperationT<4, Word32PairBinopOp> {
  enum class Kind : uint8_t {
    kAdd,
    kSub,
    kMul,
    kShiftLeft,
    kShiftRightArithmetic,
    kShiftRightLogical,
  };
  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32(),
                     RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      const ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  OpIndex left_low() const { return input(0); }
  OpIndex left_high() const { return input(1); }
  OpIndex right_low() const { return input(2); }
  OpIndex right_high() const { return input(3); }

  Word32PairBinopOp(OpIndex left_low, OpIndex left_high, OpIndex right_low,
                    OpIndex right_high, Kind kind)
      : Base(left_low, left_high, right_low, right_high), kind(kind) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind}; }
  void PrintOptions(std::ostream& os) const;
};

struct WordBinopDeoptOnOverflowOp
    : FixedArityOperationT<3, WordBinopDeoptOnOverflowOp> {
  enum class Kind : uint8_t {
    kSignedAdd,
    kSignedMul,
    kSignedSub,
    kSignedDiv,
    kSignedMod,
    kUnsignedDiv,
    kUnsignedMod
  };
  Kind kind;
  WordRepresentation rep;
  FeedbackSource feedback;
  CheckForMinusZeroMode mode;

  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::PairOf(rep);
  }

  V<Word> left() const { return input<Word>(0); }
  V<Word> right() const { return input<Word>(1); }
  V<FrameState> frame_state() const { return input<FrameState>(2); }

  WordBinopDeoptOnOverflowOp(V<Word> left, V<Word> right,
                             V<FrameState> frame_state, Kind kind,
                             WordRepresentation rep, FeedbackSource feedback,
                             CheckForMinusZeroMode mode)
      : Base(left, right, frame_state),
        kind(kind),
        rep(rep),
        feedback(feedback),
        mode(mode) {}

  void Validate(const Graph& graph) const {
    DCHECK_IMPLIES(kind == Kind::kUnsignedDiv || kind == Kind::kUnsignedMod,
                   rep == WordRepresentation::Word32());
  }
  auto options() const { return std::tuple{kind, rep, feedback, mode}; }
  void PrintOptions(std::ostream& os) const;
};

struct OverflowCheckedBinopOp
    : FixedArityOperationT<2, OverflowCheckedBinopOp> {
  static constexpr int kValueIndex = 0;
  static constexpr int kOverflowIndex = 1;

  enum class Kind : uint8_t {
    kSignedAdd,
    kSignedMul,
    kSignedSub,
  };
  Kind kind;
  WordRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (rep.value()) {
      case WordRepresentation::Word32():
        return RepVector<RegisterRepresentation::Word32(),
                         RegisterRepresentation::Word32()>();
      case WordRepresentation::Word64():
        return RepVector<RegisterRepresentation::Word64(),
                         RegisterRepresentation::Word32()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::PairOf(rep);
  }

  V<Word> left() const { return input<Word>(0); }
  V<Word> right() const { return input<Word>(1); }

  static bool IsCommutative(Kind kind) {
    switch (kind) {
      case Kind::kSignedAdd:
      case Kind::kSignedMul:
        return true;
      case Kind::kSignedSub:
        return false;
    }
  }

  OverflowCheckedBinopOp(V<Word> left, V<Word> right, Kind kind,
                         WordRepresentation rep)
      : Base(left, right), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, rep}; }
  void PrintOptions(std::ostream& os) const;
};

struct WordUnaryOp : FixedArityOperationT<1, WordUnaryOp> {
  enum class Kind : uint8_t {
    kReverseBytes,
    kCountLeadingZeros,
    kCountTrailingZeros,
    kPopCount,
    kSignExtend8,
    kSignExtend16,
  };
  Kind kind;
  WordRepresentation rep;
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  V<Word> input() const { return Base::input<Word>(0); }

  V8_EXPORT_PRIVATE static bool IsSupported(Kind kind, WordRepresentation rep);

  explicit WordUnaryOp(V<Word> input, Kind kind, WordRepresentation rep)
      : Base(input), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, rep}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           WordUnaryOp::Kind kind);

struct OverflowCheckedUnaryOp
    : FixedArityOperationT<1, OverflowCheckedUnaryOp> {
  static constexpr int kValueIndex = 0;
  static constexpr int kOverflowIndex = 1;

  enum class Kind : uint8_t { kAbs };
  Kind kind;
  WordRepresentation rep;
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (rep.value()) {
      case WordRepresentation::Word32():
        return RepVector<RegisterRepresentation::Word32(),
                         RegisterRepresentation::Word32()>();
      case WordRepresentation::Word64():
        return RepVector<RegisterRepresentation::Word64(),
                         RegisterRepresentation::Word32()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  V<Word> input() const { return Base::input<Word>(0); }

  explicit OverflowCheckedUnaryOp(V<Word> input, Kind kind,
                                  WordRepresentation rep)
      : Base(input), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, rep}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           OverflowCheckedUnaryOp::Kind kind);

struct FloatUnaryOp : FixedArityOperationT<1, FloatUnaryOp> {
  enum class Kind : uint8_t {
    kAbs,
    kNegate,
    kSilenceNaN,
    kRoundDown,      // round towards -infinity
    kRoundUp,        // round towards +infinity
    kRoundToZero,    // round towards 0
    kRoundTiesEven,  // break ties by rounding towards the next even number
    kLog,
    kLog2,
    kLog10,
    kLog1p,
    kSqrt,
    kCbrt,
    kExp,
    kExpm1,
    kSin,
    kCos,
    kSinh,
    kCosh,
    kAcos,
    kAsin,
    kAsinh,
    kAcosh,
    kTan,
    kTanh,
    kAtan,
    kAtanh,
  };

  Kind kind;
  FloatRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  V<Float> input() const { return Base::input<Float>(0); }

  V8_EXPORT_PRIVATE static bool IsSupported(Kind kind, FloatRepresentation rep);

  explicit FloatUnaryOp(V<Float> input, Kind kind, FloatRepresentation rep)
      : Base(input), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, rep}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           FloatUnaryOp::Kind kind);

struct ShiftOp : FixedArityOperationT<2, ShiftOp> {
  enum class Kind : uint8_t {
    kShiftRightArithmeticShiftOutZeros,
    kShiftRightArithmetic,
    kShiftRightLogical,
    kShiftLeft,
    kRotateRight,
    kRotateLeft
  };
  Kind kind;
  WordRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(static_cast<const RegisterRepresentation*>(&rep), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(storage,
                        {static_cast<const RegisterRepresentation&>(rep),
                         RegisterRepresentation::Word32()});
  }

  OpIndex left() const { return input(0); }
  OpIndex right() const { return input(1); }

  bool IsRightShift() const { return IsRightShift(kind); }

  static bool IsRightShift(Kind kind) {
    switch (kind) {
      case Kind::kShiftRightArithmeticShiftOutZeros:
      case Kind::kShiftRightArithmetic:
      case Kind::kShiftRightLogical:
        return true;
      case Kind::kShiftLeft:
      case Kind::kRotateRight:
      case Kind::kRotateLeft:
        return false;
    }
  }
  // The Word32 and Word64 versions of the operator compute the same result when
  // truncated to 32 bit.
  static bool AllowsWord64ToWord32Truncation(Kind kind) {
    switch (kind) {
      case Kind::kShiftLeft:
        return true;
      case Kind::kShiftRightArithmeticShiftOutZeros:
      case Kind::kShiftRightArithmetic:
      case Kind::kShiftRightLogical:
      case Kind::kRotateRight:
      case Kind::kRotateLeft:
        return false;
    }
  }

  ShiftOp(OpIndex left, OpIndex right, Kind kind, WordRepresentation rep)
      : Base(left, right), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, rep}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ShiftOp::Kind kind);

struct ComparisonOp : FixedArityOperationT<2, ComparisonOp> {
  enum class Kind : uint8_t {
    kEqual,
    kSignedLessThan,
    kSignedLessThanOrEqual,
    kUnsignedLessThan,
    kUnsignedLessThanOrEqual
  };
  Kind kind;
  RegisterRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::PairOf(rep);
  }

  static bool IsCommutative(Kind kind) { return kind == Kind::kEqual; }

  V<Any> left() const { return input<Any>(0); }
  V<Any> right() const { return input<Any>(1); }

  ComparisonOp(V<Any> left, V<Any> right, Kind kind, RegisterRepresentation rep)
      : Base(left, right), kind(kind), rep(rep) {}

  void Validate(const Graph& graph) const {
    if (kind == Kind::kEqual) {
      DCHECK(rep == any_of(RegisterRepresentation::Word32(),
                           RegisterRepresentation::Word64(),
                           RegisterRepresentation::Float32(),
                           RegisterRepresentation::Float64(),
                           RegisterRepresentation::Tagged()));

      RegisterRepresentation input_rep = rep;
#ifdef V8_COMPRESS_POINTERS
      // In the presence of pointer compression, we only compare the lower
      // 32bit.
      if (input_rep == RegisterRepresentation::Tagged()) {
        input_rep = RegisterRepresentation::Compressed();
      }
#endif  // V8_COMPRESS_POINTERS
      DCHECK(ValidOpInputRep(graph, left(), input_rep));
      DCHECK(ValidOpInputRep(graph, right(), input_rep));
      USE(input_rep);
    } else {
      DCHECK_EQ(rep, any_of(RegisterRepresentation::Word32(),
                            RegisterRepresentation::Word64(),
                            RegisterRepresentation::Float32(),
                            RegisterRepresentation::Float64()));
      DCHECK_IMPLIES(
          rep == any_of(RegisterRepresentation::Float32(),
                        RegisterRepresentation::Float64()),
          kind == any_of(Kind::kSignedLessThan, Kind::kSignedLessThanOrEqual));
    }
  }
  auto options() const { return std::tuple{kind, rep}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ComparisonOp::Kind kind);
DEFINE_MULTI_SWITCH_INTEGRAL(ComparisonOp::Kind, 8)

struct ChangeOp : FixedArityOperationT<1, ChangeOp> {
  enum class Kind : uint8_t {
    // convert between different floating-point types. Note that the
    // Float64->Float32 conversion is truncating.
    kFloatConversion,
    // overflow guaranteed to result in the minimal integer
    kSignedFloatTruncateOverflowToMin,
    kUnsignedFloatTruncateOverflowToMin,
    // JS semantics float64 to word32 truncation
    // https://tc39.es/ecma262/#sec-touint32
    kJSFloatTruncate,
    // convert (un)signed integer to floating-point value
    kSignedToFloat,
    kUnsignedToFloat,
    // extract half of a float64 value
    kExtractHighHalf,
    kExtractLowHalf,
    // increase bit-width for unsigned integer values
    kZeroExtend,
    // increase bid-width for signed integer values
    kSignExtend,
    // truncate word64 to word32
    kTruncate,
    // preserve bits, change meaning
    kBitcast
  };
  // Violated assumptions result in undefined behavior.
  enum class Assumption : uint8_t {
    kNoAssumption,
    // Used for conversions from floating-point to integer, assumes that the
    // value doesn't exceed the integer range.
    kNoOverflow,
    // Assume that the original value can be recovered by a corresponding
    // reverse transformation.
    kReversible,
  };
  Kind kind;
  // Reversible means undefined behavior if value cannot be represented
  // precisely.
  Assumption assumption;
  RegisterRepresentation from;
  RegisterRepresentation to;

  // Returns true if change<kind>(change<reverse_kind>(a)) == a for all a.
  // This assumes that change<reverse_kind> uses the inverted {from} and {to}
  // representations, i.e. the input to the inner change op has the same
  // representation as the result of the outer change op.
  static bool IsReversible(Kind kind, Assumption assumption,
                           RegisterRepresentation from,
                           RegisterRepresentation to, Kind reverse_kind,
                           bool signalling_nan_possible) {
    switch (kind) {
      case Kind::kFloatConversion:
        return from == RegisterRepresentation::Float32() &&
               to == RegisterRepresentation::Float64() &&
               reverse_kind == Kind::kFloatConversion &&
               !signalling_nan_possible;
      case Kind::kSignedFloatTruncateOverflowToMin:
        return assumption == Assumption::kReversible &&
               reverse_kind == Kind::kSignedToFloat;
      case Kind::kUnsignedFloatTruncateOverflowToMin:
        return assumption == Assumption::kReversible &&
               reverse_kind == Kind::kUnsignedToFloat;
      case Kind::kJSFloatTruncate:
        return false;
      case Kind::kSignedToFloat:
        if (from == RegisterRepresentation::Word32() &&
            to == RegisterRepresentation::Float64()) {
          return reverse_kind == any_of(Kind::kSignedFloatTruncateOverflowToMin,
                                        Kind::kJSFloatTruncate);
        } else {
          return assumption == Assumption::kReversible &&
                 reverse_kind ==
                     any_of(Kind::kSignedFloatTruncateOverflowToMin);
        }
      case Kind::kUnsignedToFloat:
        if (from == RegisterRepresentation::Word32() &&
            to == RegisterRepresentation::Float64()) {
          return reverse_kind ==
                 any_of(Kind::kUnsignedFloatTruncateOverflowToMin,
                        Kind::kJSFloatTruncate);
        } else {
          return assumption == Assumption::kReversible &&
                 reverse_kind == Kind::kUnsignedFloatTruncateOverflowToMin;
        }
      case Kind::kExtractHighHalf:
      case Kind::kExtractLowHalf:
        return false;
      case Kind::kZeroExtend:
      case Kind::kSignExtend:
        DCHECK_EQ(from, RegisterRepresentation::Word32());
        DCHECK_EQ(to, RegisterRepresentation::Word64());
        return reverse_kind == Kind::kTruncate;
      case Kind::kTruncate:
        DCHECK_EQ(from, RegisterRepresentation::Word64());
        DCHECK_EQ(to, RegisterRepresentation::Word32());
        return reverse_kind == Kind::kBitcast;
      case Kind::kBitcast:
        return reverse_kind == Kind::kBitcast;
    }
  }

  bool IsReversibleBy(Kind reverse_kind, bool signalling_nan_possible) const {
    return IsReversible(kind, assumption, from, to, reverse_kind,
                        signalling_nan_possible);
  }

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&to, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(from);
  }

  V<Untagged> input() const { return Base::input<Untagged>(0); }

  ChangeOp(V<Untagged> input, Kind kind, Assumption assumption,
           RegisterRepresentation from, RegisterRepresentation to)
      : Base(input), kind(kind), assumption(assumption), from(from), to(to) {}

  void Validate(const Graph& graph) const {
    // Bitcasts from and to Tagged should use a TaggedBitcast instead (which has
    // different effects, since it's unsafe to reorder such bitcasts accross
    // GCs).
    DCHECK_IMPLIES(kind == Kind::kBitcast,
                   from != RegisterRepresentation::Tagged() &&
                       to != RegisterRepresentation::Tagged());
  }
  auto options() const { return std::tuple{kind, assumption, from, to}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ChangeOp::Kind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ChangeOp::Assumption assumption);
DEFINE_MULTI_SWITCH_INTEGRAL(ChangeOp::Kind, 16)
DEFINE_MULTI_SWITCH_INTEGRAL(ChangeOp::Assumption, 4)

struct ChangeOrDeoptOp : FixedArityOperationT<2, ChangeOrDeoptOp> {
  enum class Kind : uint8_t {
    kUint32ToInt32,
    kInt64ToInt32,
    kUint64ToInt32,
    kUint64ToInt64,
    kFloat64ToInt32,
    kFloat64ToUint32,
    kFloat64ToInt64,
    kFloat64NotHole,
  };
  Kind kind;
  CheckForMinusZeroMode minus_zero_mode;
  FeedbackSource feedback;

  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case Kind::kUint32ToInt32:
      case Kind::kInt64ToInt32:
      case Kind::kUint64ToInt32:
      case Kind::kFloat64ToInt32:
      case Kind::kFloat64ToUint32:
        return RepVector<RegisterRepresentation::Word32()>();
      case Kind::kUint64ToInt64:
      case Kind::kFloat64ToInt64:
        return RepVector<RegisterRepresentation::Word64()>();
      case Kind::kFloat64NotHole:
        return RepVector<RegisterRepresentation::Float64()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    switch (kind) {
      case Kind::kUint32ToInt32:
        return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
      case Kind::kInt64ToInt32:
      case Kind::kUint64ToInt32:
      case Kind::kUint64ToInt64:
        return MaybeRepVector<MaybeRegisterRepresentation::Word64()>();
      case Kind::kFloat64ToInt32:
      case Kind::kFloat64ToUint32:
      case Kind::kFloat64ToInt64:
      case Kind::kFloat64NotHole:
        return MaybeRepVector<MaybeRegisterRepresentation::Float64()>();
    }
  }

  V<Untagged> input() const { return Base::input<Untagged>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  ChangeOrDeoptOp(V<Untagged> input, V<FrameState> frame_state, Kind kind,
                  CheckForMinusZeroMode minus_zero_mode,
                  const FeedbackSource& feedback)
      : Base(input, frame_state),
        kind(kind),
        minus_zero_mode(minus_zero_mode),
        feedback(feedback) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{kind, minus_zero_mode, feedback}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ChangeOrDeoptOp::Kind kind);

// Perform a conversion and return a pair of the result and a bit if it was
// successful.
struct TryChangeOp : FixedArityOperationT<1, TryChangeOp> {
  static constexpr uint32_t kSuccessValue = 1;
  static constexpr uint32_t kFailureValue = 0;
  enum class Kind : uint8_t {
    // The result of the truncation is undefined if the result is out of range.
    kSignedFloatTruncateOverflowUndefined,
    kUnsignedFloatTruncateOverflowUndefined,
  };
  Kind kind;
  FloatRepresentation from;
  WordRepresentation to;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (to.value()) {
      case WordRepresentation::Word32():
        return RepVector<RegisterRepresentation::Word32(),
                         RegisterRepresentation::Word32()>();
      case WordRepresentation::Word64():
        return RepVector<RegisterRepresentation::Word64(),
                         RegisterRepresentation::Word32()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(from);
  }

  OpIndex input() const { return Base::input(0); }

  TryChangeOp(OpIndex input, Kind kind, FloatRepresentation from,
              WordRepresentation to)
      : Base(input), kind(kind), from(from), to(to) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, from, to}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           TryChangeOp::Kind kind);

struct BitcastWord32PairToFloat64Op
    : FixedArityOperationT<2, BitcastWord32PairToFloat64Op> {
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Float64()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  V<Word32> high_word32() const { return input<Word32>(0); }
  V<Word32> low_word32() const { return input<Word32>(1); }

  BitcastWord32PairToFloat64Op(V<Word32> high_word32, V<Word32> low_word32)
      : Base(high_word32, low_word32) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{}; }
};

struct TaggedBitcastOp : FixedArityOperationT<1, TaggedBitcastOp> {
  enum class Kind : uint8_t {
    kSmi,  // This is a bitcast from a Word to a Smi or from a Smi to a Word
    kHeapObject,     // This is a bitcast from or to a Heap Object
    kTagAndSmiBits,  // This is a bitcast where only access to the tag and the
                     // smi bits (if it's a smi) are valid
    kAny
  };
  Kind kind;
  RegisterRepresentation from;
  RegisterRepresentation to;

  OpEffects Effects() const {
    switch (kind) {
      case Kind::kSmi:
      case Kind::kTagAndSmiBits:
        return OpEffects();
      case Kind::kHeapObject:
      case Kind::kAny:
        // Due to moving GC, converting from or to pointers doesn't commute with
        // GC.
        return OpEffects().CanDoRawHeapAccess();
    }
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&to, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(from);
  }

  OpIndex input() const { return Base::input(0); }

  TaggedBitcastOp(OpIndex input, RegisterRepresentation from,
                  RegisterRepresentation to, Kind kind)
      : Base(input), kind(kind), from(from), to(to) {}

  void Validate(const Graph& graph) const {
    if (kind == Kind::kSmi) {
      DCHECK((from.IsWord() && to.IsTaggedOrCompressed()) ||
             (from.IsTaggedOrCompressed() && to.IsWord()));
      DCHECK_IMPLIES(from == RegisterRepresentation::Word64() ||
                         to == RegisterRepresentation::Word64(),
                     Is64());
    } else {
      // TODO(nicohartmann@): Without implicit trucation, the first case might
      // not be correct anymore.
      DCHECK((from.IsWord() && to == RegisterRepresentation::Tagged()) ||
             (from == RegisterRepresentation::Tagged() &&
              to == RegisterRepresentation::WordPtr()) ||
             (from == RegisterRepresentation::Compressed() &&
              to == RegisterRepresentation::Word32()));
    }
  }
  auto options() const { return std::tuple{from, to, kind}; }
};
std::ostream& operator<<(std::ostream& os, TaggedBitcastOp::Kind assumption);

struct SelectOp : FixedArityOperationT<3, SelectOp> {
  enum class Implementation : uint8_t { kBranch, kCMove };

  RegisterRepresentation rep;
  BranchHint hint;
  Implementation implem;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(storage, {RegisterRepresentation::Word32(), rep, rep});
  }

  SelectOp(V<Word32> cond, V<Any> vtrue, V<Any> vfalse,
           RegisterRepresentation rep, BranchHint hint, Implementation implem)
      : Base(cond, vtrue, vfalse), rep(rep), hint(hint), implem(implem) {}

  void Validate(const Graph& graph) const {
    DCHECK_IMPLIES(implem == Implementation::kCMove,
                   (rep == RegisterRepresentation::Word32() &&
                    SupportedOperations::word32_select()) ||
                       (rep == RegisterRepresentation::Word64() &&
                        SupportedOperations::word64_select()) ||
                       (rep == RegisterRepresentation::Float32() &&
                        SupportedOperations::float32_select()) ||
                       (rep == RegisterRepresentation::Float64() &&
                        SupportedOperations::float64_select()));
  }

  V<Word32> cond() const { return input<Word32>(0); }
  V<Any> vtrue() const { return input<Any>(1); }
  V<Any> vfalse() const { return input<Any>(2); }

  auto options() const { return std::tuple{rep, hint, implem}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           SelectOp::Implementation kind);

struct PhiOp : OperationT<PhiOp> {
  RegisterRepresentation rep;

  // Phis have to remain at the beginning of the current block. As effects
  // cannot express this completely, we just mark them as having no effects but
  // treat them specially when scheduling operations.
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(input_count);
    for (size_t i = 0; i < input_count; ++i) {
      storage[i] = rep;
    }
    return base::VectorOf(storage);
  }

  static constexpr size_t kLoopPhiBackEdgeIndex = 1;

  explicit PhiOp(base::Vector<const OpIndex> inputs, RegisterRepresentation rep)
      : Base(inputs), rep(rep) {}

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    auto mapped_inputs = mapper.template Map<64>(inputs());
    return fn(base::VectorOf(mapped_inputs), rep);
  }

  void Validate(const Graph& graph) const { DCHECK_GT(input_count, 0); }
  auto options() const { return std::tuple{rep}; }
};

// Used as a placeholder for a loop-phi while building the graph, replaced with
// a normal `PhiOp` before graph building is over, so it should never appear in
// a complete graph.
struct PendingLoopPhiOp : FixedArityOperationT<1, PendingLoopPhiOp> {
  RegisterRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  OpIndex first() const { return input(0); }
  PendingLoopPhiOp(OpIndex first, RegisterRepresentation rep)
      : Base(first), rep(rep) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{rep}; }
};

struct ConstantOp : FixedArityOperationT<0, ConstantOp> {
  enum class Kind : uint8_t {
    kWord32,
    kWord64,
    kFloat32,
    kFloat64,
    kSmi,
    kNumber,  // TODO(tebbi): See if we can avoid number constants.
    kTaggedIndex,
    kExternal,
    kHeapObject,
    kCompressedHeapObject,
    kTrustedHeapObject,
    kRelocatableWasmCall,
    kRelocatableWasmStubCall,
    kRelocatableWasmCanonicalSignatureId
  };

  Kind kind;
  RegisterRepresentation rep = Representation(kind);
  union Storage {
    uint64_t integral;
    i::Float32 float32;
    i::Float64 float64;
    ExternalReference external;
    Handle<HeapObject> handle;

    Storage(uint64_t integral = 0) : integral(integral) {}
    Storage(i::Tagged<Smi> smi) : integral(smi.ptr()) {}
    Storage(i::Float64 constant) : float64(constant) {}
    Storage(i::Float32 constant) : float32(constant) {}
    Storage(ExternalReference constant) : external(constant) {}
    Storage(Handle<HeapObject> constant) : handle(constant) {}

    inline bool operator==(const ConstantOp::Storage&) const {
      // It is tricky to implement this properly. We currently need to define
      // this for the matchers, but this should never be called.
      UNREACHABLE();
    }
  } storage;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  static RegisterRepresentation Representation(Kind kind) {
    switch (kind) {
      case Kind::kRelocatableWasmCanonicalSignatureId:
      case Kind::kWord32:
        return RegisterRepresentation::Word32();
      case Kind::kWord64:
        return RegisterRepresentation::Word64();
      case Kind::kFloat32:
        return RegisterRepresentation::Float32();
      case Kind::kFloat64:
        return RegisterRepresentation::Float64();
      case Kind::kExternal:
      case Kind::kTaggedIndex:
      case Kind::kTrustedHeapObject:
      case Kind::kRelocatableWasmCall:
      case Kind::kRelocatableWasmStubCall:
        return RegisterRepresentation::WordPtr();
      case Kind::kSmi:
      case Kind::kHeapObject:
      case Kind::kNumber:
        return RegisterRepresentation::Tagged();
      case Kind::kCompressedHeapObject:
        return RegisterRepresentation::Compressed();
    }
  }

  ConstantOp(Kind kind, Storage storage)
      : Base(), kind(kind), storage(storage) {}

  void Validate(const Graph& graph) const {
    DCHECK_IMPLIES(
        kind == Kind::kWord32,
        storage.integral <= WordRepresentation::Word32().MaxUnsignedValue());
    DCHECK_IMPLIES(
        kind == Kind::kRelocatableWasmCanonicalSignatureId,
        storage.integral <= WordRepresentation::Word32().MaxSignedValue());
  }

  uint64_t integral() const {
    DCHECK(IsIntegral());
    return storage.integral;
  }

  int64_t signed_integral() const {
    DCHECK(IsIntegral());
    switch (kind) {
      case Kind::kWord32:
      case Kind::kRelocatableWasmCanonicalSignatureId:
        return static_cast<int32_t>(storage.integral);
      case Kind::kWord64:
        return static_cast<int64_t>(storage.integral);
      default:
        UNREACHABLE();
    }
  }

  uint32_t word32() const {
    DCHECK(kind == Kind::kWord32 || kind == Kind::kWord64);
    return static_cast<uint32_t>(storage.integral);
  }

  uint64_t word64() const {
    DCHECK_EQ(kind, Kind::kWord64);
    return static_cast<uint64_t>(storage.integral);
  }

  i::Tagged<Smi> smi() const {
    DCHECK_EQ(kind, Kind::kSmi);
    return i::Tagged<Smi>(storage.integral);
  }

  i::Float64 number() const {
    DCHECK_EQ(kind, Kind::kNumber);
    return storage.float64;
  }

  i::Float32 float32() const {
    DCHECK_EQ(kind, Kind::kFloat32);
    return storage.float32;
  }

  i::Float64 float64() const {
    DCHECK_EQ(kind, Kind::kFloat64);
    return storage.float64;
  }

  int32_t tagged_index() const {
    DCHECK_EQ(kind, Kind::kTaggedIndex);
    return static_cast<int32_t>(static_cast<uint32_t>(storage.integral));
  }

  ExternalReference external_reference() const {
    DCHECK_EQ(kind, Kind::kExternal);
    return storage.external;
  }

  Handle<i::HeapObject> handle() const {
    DCHECK(kind == Kind::kHeapObject || kind == Kind::kCompressedHeapObject ||
           kind == Kind::kTrustedHeapObject);
    return storage.handle;
  }

  bool IsWord(uint64_t value) const {
    switch (kind) {
      case Kind::kWord32:
        return static_cast<uint32_t>(value) == word32();
      case Kind::kWord64:
        return value == word64();
      default:
        UNREACHABLE();
    }
  }

  bool IsIntegral() const {
    return kind == Kind::kWord32 || kind == Kind::kWord64 ||
           kind == Kind::kRelocatableWasmCall ||
           kind == Kind::kRelocatableWasmStubCall ||
           kind == Kind::kRelocatableWasmCanonicalSignatureId;
  }

  auto options() const { return std::tuple{kind, storage}; }

  void PrintOptions(std::ostream& os) const;
  size_t hash_value(
      HashingStrategy strategy = HashingStrategy::kDefault) const {
    switch (kind) {
      case Kind::kWord32:
      case Kind::kWord64:
      case Kind::kSmi:
      case Kind::kTaggedIndex:
      case Kind::kRelocatableWasmCall:
      case Kind::kRelocatableWasmStubCall:
      case Kind::kRelocatableWasmCanonicalSignatureId:
        return HashWithOptions(storage.integral);
      case Kind::kFloat32:
        return HashWithOptions(storage.float32.get_bits());
      case Kind::kFloat64:
      case Kind::kNumber:
        return HashWithOptions(storage.float64.get_bits());
      case Kind::kExternal:
        return HashWithOptions(strategy == HashingStrategy::kMakeSnapshotStable
                                   ? 0
                                   : storage.external.raw());
      case Kind::kHeapObject:
      case Kind::kCompressedHeapObject:
      case Kind::kTrustedHeapObject:
        if (strategy == HashingStrategy::kMakeSnapshotStable) {
          return HashWithOptions();
        }
        return HashWithOptions(storage.handle.address());
    }
  }
  bool operator==(const ConstantOp& other) const {
    if (kind != other.kind) return false;
    switch (kind) {
      case Kind::kWord32:
      case Kind::kWord64:
      case Kind::kSmi:
      case Kind::kTaggedIndex:
      case Kind::kRelocatableWasmCall:
      case Kind::kRelocatableWasmStubCall:
      case Kind::kRelocatableWasmCanonicalSignatureId:
        return storage.integral == other.storage.integral;
      case Kind::kFloat32:
        // Using a bit_cast to uint32_t in order to return false when comparing
        // +0 and -0.
        // Note: for JavaScript, it would be fine to return true when both
        // values are NaNs, but for Wasm we must not merge NaNs that way.
        // Since we canonicalize NaNs for JS anyway, we don't need to treat
        // them specially here.
        return base::bit_cast<uint32_t>(storage.float32) ==
               base::bit_cast<uint32_t>(other.storage.float32);
      case Kind::kFloat64:
      case Kind::kNumber:
        // Using a bit_cast to uint64_t in order to return false when comparing
        // +0 and -0.
        // Note: for JavaScript, it would be fine to return true when both
        // values are NaNs, but for Wasm we must not merge NaNs that way.
        // Since we canonicalize NaNs for JS anyway, we don't need to treat
        // them specially here.
        return base::bit_cast<uint64_t>(storage.float64) ==
               base::bit_cast<uint64_t>(other.storage.float64);
      case Kind::kExternal:
        return storage.external.raw() == other.storage.external.raw();
      case Kind::kHeapObject:
      case Kind::kCompressedHeapObject:
      case Kind::kTrustedHeapObject:
        return storage.handle.address() == other.storage.handle.address();
    }
  }
};

// Load `loaded_rep` from: base + offset + index * 2^element_size_log2.
// For Kind::tagged_base: subtract kHeapObjectTag,
//                        `base` has to be the object start.
// For (u)int8/16, the value will be sign- or zero-extended to Word32.
// When result_rep is RegisterRepresentation::Compressed(), then the load does
// not decompress the value.
struct LoadOp : OperationT<LoadOp> {
  struct Kind {
    // The `base` input is a tagged pointer to a HeapObject.
    bool tagged_base : 1;
    // The effective address might be unaligned. This is only set to true if
    // the platform does not support unaligned loads for the given
    // MemoryRepresentation natively.
    bool maybe_unaligned : 1;
    // There is a Wasm trap handler for out-of-bounds accesses.
    bool with_trap_handler : 1;
    // The wasm trap handler is used for null accesses. Note that this requires
    // with_trap_handler as well.
    bool trap_on_null : 1;
    // If {load_eliminable} is true, then:
    //   - Stores/Loads at this address cannot overlap. Concretely, it means
    //     that something like this cannot happen:
    //
    //         const u32s = Uint32Array.of(3, 8);
    //         const u8s = new Uint8Array(u32s.buffer);
    //         u32s[0] = 0xffffffff;
    //         u8s[1] = 0; // Overlaps with previous store!
    //
    //   - Stores/Loads at this address have a canonical base. Concretely, it
    //     means that something like this cannot happen:
    //
    //         let buffer = new ArrayBuffer(10000);
    //         let ta1 = new Int32Array(buffer, 0/*offset*/);
    //         let ta2 = new Int32Array(buffer, 100*4/*offset*/);
    //         ta2[0] = 0xff;
    //         ta1[100] = 42; // Same destination as the previous store!
    //
    //   - No other thread can modify the underlying value. E.g. in the case of
    //     loading the wasm stack limit, other threads can modify the loaded
    //     value, so we always have to reload it.
    //
    // This is mainly used for load elimination: when stores/loads don't have
    // the {load_eliminable} bit set to true, more things need to be
    // invalidated.
    // In the main JS pipeline, only ArrayBuffers (= TypedArray/DataView)
    // loads/stores have this {load_eliminable} set to false,
    // and all other loads have it to true.
    bool load_eliminable : 1;
    // The loaded value may not change.
    bool is_immutable : 1;
    // The load should be atomic.
    bool is_atomic : 1;

    static constexpr Kind Aligned(BaseTaggedness base_is_tagged) {
      switch (base_is_tagged) {
        case BaseTaggedness::kTaggedBase:
          return TaggedBase();
        case BaseTaggedness::kUntaggedBase:
          return RawAligned();
      }
    }

    // TODO(dmercadier): use designed initializers once we move to C++20.
    static constexpr Kind TaggedBase() {
      return {true, false, false, false, true, false, false};
    }
    static constexpr Kind RawAligned() {
      return {false, false, false, false, true, false, false};
    }
    static constexpr Kind RawUnaligned() {
      return {false, true, false, false, true, false, false};
    }
    static constexpr Kind Protected() {
      return {false, false, true, false, true, false, false};
    }
    static constexpr Kind TrapOnNull() {
      return {true, false, true, true, true, false, false};
    }
    static constexpr Kind MaybeUnaligned(MemoryRepresentation rep) {
      return rep == MemoryRepresentation::Int8() ||
                     rep == MemoryRepresentation::Uint8() ||
                     SupportedOperations::IsUnalignedLoadSupported(rep)
                 ? LoadOp::Kind::RawAligned()
                 : LoadOp::Kind::RawUnaligned();
    }

    constexpr Kind NotLoadEliminable() {
      Kind kind = *this;
      kind.load_eliminable = false;
      return kind;
    }

    constexpr Kind Immutable() const {
      Kind kind(*this);
      kind.is_immutable = true;
      return kind;
    }

    constexpr Kind Atomic() const {
      Kind kind(*this);
      kind.is_atomic = true;
      return kind;
    }

    bool operator==(const Kind& other) const {
      return tagged_base == other.tagged_base &&
             maybe_unaligned == other.maybe_unaligned &&
             with_trap_handler == other.with_trap_handler &&
             load_eliminable == other.load_eliminable &&
             is_immutable == other.is_immutable &&
             is_atomic == other.is_atomic && trap_on_null == other.trap_on_null;
    }
  };
  Kind kind;
  MemoryRepresentation loaded_rep;
  RegisterRepresentation result_rep;
  uint8_t element_size_log2;  // multiply index with 2^element_size_log2
  int32_t offset;             // add offset to scaled index

  OpEffects Effects() const {
    // Loads might depend on checks for pointer validity, object layout, bounds
    // checks, etc.
    // TODO(tebbi): Distinguish between on-heap and off-heap loads.
    OpEffects effects = OpEffects().CanReadMemory().CanDependOnChecks();
    if (kind.with_trap_handler) effects = effects.CanLeaveCurrentFunction();
    if (kind.is_atomic) {
      // Atomic load should not be reordered with other loads.
      effects = effects.CanWriteMemory();
    }
    return effects;
  }
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&result_rep, 1);
  }

  MachineType machine_type() const;

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    base::Vector<const MaybeRegisterRepresentation> result =
        kind.tagged_base
            ? MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                             MaybeRegisterRepresentation::WordPtr()>()
            : MaybeRepVector<MaybeRegisterRepresentation::WordPtr(),
                             MaybeRegisterRepresentation::WordPtr()>();
    return index().valid() ? result : base::VectorOf(result.data(), 1);
  }

  OpIndex base() const { return input(0); }
  OptionalOpIndex index() const {
    return input_count == 2 ? input(1) : OpIndex::Invalid();
  }

  static constexpr bool OffsetIsValid(int32_t offset, bool tagged_base) {
    if (tagged_base) {
      // When a Load has the tagged_base Kind, it means that {offset} will
      // eventually need a "-kHeapObjectTag". If the {offset} is
      // min_int, then subtracting kHeapObjectTag will underflow.
      return offset >= std::numeric_limits<int32_t>::min() + kHeapObjectTag;
    }
    return true;
  }

  LoadOp(OpIndex base, OptionalOpIndex index, Kind kind,
         MemoryRepresentation loaded_rep, RegisterRepresentation result_rep,
         int32_t offset, uint8_t element_size_log2)
      : Base(1 + index.valid()),
        kind(kind),
        loaded_rep(loaded_rep),
        result_rep(result_rep),
        element_size_log2(element_size_log2),
        offset(offset) {
    input(0) = base;
    if (index.valid()) {
      input(1) = index.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(base()), mapper.Map(index()), kind, loaded_rep,
              result_rep, offset, element_size_log2);
  }

  void Validate(const Graph& graph) const {
    DCHECK(loaded_rep.ToRegisterRepresentation() == result_rep ||
           (loaded_rep.IsCompressibleTagged() &&
            result_rep == RegisterRepresentation::Compressed()) ||
           kind.is_atomic);
    DCHECK_IMPLIES(element_size_log2 > 0, index().valid());
    DCHECK_IMPLIES(kind.maybe_unaligned,
                   !SupportedOperations::IsUnalignedLoadSupported(loaded_rep));
    DCHECK(OffsetIsValid(offset, kind.tagged_base));
  }
  static LoadOp& New(Graph* graph, OpIndex base, OptionalOpIndex index,
                     Kind kind, MemoryRepresentation loaded_rep,
                     RegisterRepresentation result_rep, int32_t offset,
                     uint8_t element_size_log2) {
    return Base::New(graph, 1 + index.valid(), base, index, kind, loaded_rep,
                     result_rep, offset, element_size_log2);
  }
  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
  void PrintOptions(std::ostream& os) const;
  auto options() const {
    return std::tuple{kind, loaded_rep, result_rep, offset, element_size_log2};
  }
};

V8_INLINE size_t hash_value(LoadOp::Kind kind) {
  return base::hash_value(
      static_cast<int>(kind.tagged_base) | (kind.maybe_unaligned << 1) |
      (kind.load_eliminable << 2) | (kind.is_immutable << 3) |
      (kind.with_trap_handler << 4) | (kind.is_atomic << 5));
}

struct AtomicRMWOp : OperationT<AtomicRMWOp> {
  enum class BinOp : uint8_t {
    kAdd,
    kSub,
    kAnd,
    kOr,
    kXor,
    kExchange,
    kCompareExchange
  };
  BinOp bin_op;
  RegisterRepresentation in_out_rep;
  MemoryRepresentation memory_rep;
  MemoryAccessKind memory_access_kind;
  OpEffects Effects() const {
    OpEffects effects =
        OpEffects().CanWriteMemory().CanDependOnChecks().CanReadMemory();
    if (memory_access_kind == MemoryAccessKind::kProtected) {
      effects = effects.CanLeaveCurrentFunction();
    }
    return effects;
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&in_out_rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    if (bin_op == BinOp::kCompareExchange) {
      return InitVectorOf(
          storage, {RegisterRepresentation::WordPtr(),
                    RegisterRepresentation::WordPtr(), in_out_rep, in_out_rep});
    }
    return InitVectorOf(storage,
                        {RegisterRepresentation::WordPtr(),
                         RegisterRepresentation::WordPtr(), in_out_rep});
  }

  V<WordPtr> base() const { return input<WordPtr>(0); }
  V<WordPtr> index() const { return input<WordPtr>(1); }
  OpIndex value() const { return input(2); }
  OptionalOpIndex expected() const {
    return (input_count == 4) ? input(3) : OpIndex::Invalid();
  }

  void Validate(const Graph& graph) const {
    DCHECK_EQ(bin_op == BinOp::kCompareExchange, expected().valid());
  }

  AtomicRMWOp(OpIndex base, OpIndex index, OpIndex value,
              OptionalOpIndex expected, BinOp bin_op,
              RegisterRepresentation in_out_rep,
              MemoryRepresentation memory_rep, MemoryAccessKind kind)
      : Base(3 + expected.valid()),
        bin_op(bin_op),
        in_out_rep(in_out_rep),
        memory_rep(memory_rep),
        memory_access_kind(kind) {
    input(0) = base;
    input(1) = index;
    input(2) = value;
    if (expected.valid()) {
      input(3) = expected.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(base()), mapper.Map(index()), mapper.Map(value()),
              mapper.Map(expected()), bin_op, in_out_rep, memory_rep,
              memory_access_kind);
  }

  static AtomicRMWOp& New(Graph* graph, OpIndex base, OpIndex index,
                          OpIndex value, OptionalOpIndex expected, BinOp bin_op,
                          RegisterRepresentation result_rep,
                          MemoryRepresentation input_rep,
                          MemoryAccessKind kind) {
    return Base::New(graph, 3 + expected.valid(), base, index, value, expected,
                     bin_op, result_rep, input_rep, kind);
  }

  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;

  void PrintOptions(std::ostream& os) const;

  auto options() const {
    return std::tuple{bin_op, in_out_rep, memory_rep, memory_access_kind};
  }
};
DEFINE_MULTI_SWITCH_INTEGRAL(AtomicRMWOp::BinOp, 8)

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           AtomicRMWOp::BinOp kind);

struct AtomicWord32PairOp : OperationT<AtomicWord32PairOp> {
  enum class Kind : uint8_t {
    kAdd,
    kSub,
    kAnd,
    kOr,
    kXor,
    kExchange,
    kCompareExchange,
    kLoad,
    kStore
  };

  Kind kind;
  int32_t offset;

  static Kind KindFromBinOp(AtomicRMWOp::BinOp bin_op) {
    switch (bin_op) {
      case AtomicRMWOp::BinOp::kAdd:
        return Kind::kAdd;
      case AtomicRMWOp::BinOp::kSub:
        return Kind::kSub;
      case AtomicRMWOp::BinOp::kAnd:
        return Kind::kAnd;
      case AtomicRMWOp::BinOp::kOr:
        return Kind::kOr;
      case AtomicRMWOp::BinOp::kXor:
        return Kind::kXor;
      case AtomicRMWOp::BinOp::kExchange:
        return Kind::kExchange;
      case AtomicRMWOp::BinOp::kCompareExchange:
        return Kind::kCompareExchange;
    }
  }

  OpEffects Effects() const {
    OpEffects effects = OpEffects().CanDependOnChecks();
    if (kind == Kind::kStore) {
      return effects.CanWriteMemory();
    }
    // Atomic loads are marked as "can write memory" as they should not be
    // reordered with other loads. Secondly, they may not be removed even if
    // unused as they might make writes of other threads visible.
    return effects.CanReadMemory().CanWriteMemory();
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    if (kind == Kind::kStore) return {};
    return RepVector<RegisterRepresentation::Word32(),
                     RegisterRepresentation::Word32()>();
  }
  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(input_count);

    const bool has_index = HasIndex();
    storage[0] = RegisterRepresentation::WordPtr();  // base
    if (has_index) {
      storage[1] = RegisterRepresentation::WordPtr();  // index
    }
    if (kind != Kind::kLoad) {
      storage[1 + has_index] = RegisterRepresentation::Word32();  // value_low
      storage[2 + has_index] = RegisterRepresentation::Word32();  // value_high
      if (kind == Kind::kCompareExchange) {
        storage[3 + has_index] =
            RegisterRepresentation::Word32();  // expected_low
        storage[4 + has_index] =
            RegisterRepresentation::Word32();  // expected_high
      }
    }
    return base::VectorOf(storage);
  }

  V<WordPtr> base() const { return input<WordPtr>(0); }
  OptionalV<WordPtr> index() const {
    return HasIndex() ? input<WordPtr>(1) : V<WordPtr>::Invalid();
  }
  OptionalV<Word32> value_low() const {
    return kind != Kind::kLoad ? input<Word32>(1 + HasIndex())
                               : V<Word32>::Invalid();
  }
  OptionalV<Word32> value_high() const {
    return kind != Kind::kLoad ? input<Word32>(2 + HasIndex())
                               : V<Word32>::Invalid();
  }
  OptionalV<Word32> expected_low() const {
    return kind == Kind::kCompareExchange ? input<Word32>(3 + HasIndex())
                                          : V<Word32>::Invalid();
  }
  OptionalV<Word32> expected_high() const {
    return kind == Kind::kCompareExchange ? input<Word32>(4 + HasIndex())
                                          : V<Word32>::Invalid();
  }

  void Validate(const Graph& graph) const {}

  AtomicWord32PairOp(V<WordPtr> base, OptionalV<WordPtr> index,
                     OptionalV<Word32> value_low, OptionalV<Word32> value_high,
                     OptionalV<Word32> expected_low,
                     OptionalV<Word32> expected_high, Kind kind, int32_t offset)
      : Base(InputCount(kind, index.has_value())), kind(kind), offset(offset) {
    DCHECK_EQ(value_low.valid(), value_high.valid());
    DCHECK_EQ(expected_low.valid(), expected_high.valid());
    DCHECK_EQ(kind == Kind::kCompareExchange, expected_low.valid());
    DCHECK_EQ(kind != Kind::kLoad, value_low.valid());

    const bool has_index = index.has_value();
    DCHECK_EQ(has_index, HasIndex());

    input(0) = base;
    if (has_index) input(1) = index.value();
    if (kind != Kind::kLoad) {
      input(1 + has_index) = value_low.value();
      input(2 + has_index) = value_high.value();
      if (kind == Kind::kCompareExchange) {
        input(3 + has_index) = expected_low.value();
        input(4 + has_index) = expected_high.value();
      }
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(base()), mapper.Map(index()), mapper.Map(value_low()),
              mapper.Map(value_high()), mapper.Map(expected_low()),
              mapper.Map(expected_high()), kind, offset);
  }

  static constexpr size_t InputCount(Kind kind, bool has_index) {
    switch (kind) {
      case Kind::kLoad:
        return 1 + has_index;  // base, index?
      case Kind::kAdd:
      case Kind::kSub:
      case Kind::kAnd:
      case Kind::kOr:
      case Kind::kXor:
      case Kind::kExchange:
      case Kind::kStore:
        return 3 + has_index;  // base, index?, value_low, value_high
      case Kind::kCompareExchange:
        return 5 + has_index;  // base, index?, value_low, value_high,
                               // expected_low, expected_high
    }
  }
  bool HasIndex() const { return input_count == InputCount(kind, true); }

  static AtomicWord32PairOp& New(Graph* graph, V<WordPtr> base,
                                 OptionalV<WordPtr> index,
                                 OptionalV<Word32> value_low,
                                 OptionalV<Word32> value_high,
                                 OptionalV<Word32> expected_low,
                                 OptionalV<Word32> expected_high, Kind kind,
                                 int32_t offset) {
    return Base::New(graph, InputCount(kind, index.has_value()), base, index,
                     value_low, value_high, expected_low, expected_high, kind,
                     offset);
  }

  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;

  void PrintOptions(std::ostream& os) const;

  auto options() const { return std::tuple{kind, offset}; }
};

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           AtomicWord32PairOp::Kind kind);

struct MemoryBarrierOp : FixedArityOperationT<0, MemoryBarrierOp> {
  AtomicMemoryOrder memory_order;

  static constexpr OpEffects effects =
      OpEffects().CanReadHeapMemory().CanWriteMemory();

  explicit MemoryBarrierOp(AtomicMemoryOrder memory_order)
      : Base(), memory_order(memory_order) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{memory_order}; }
  void PrintOptions(std::ostream& os) const;
};

// Store `value` to: base + offset + index * 2^element_size_log2.
// For Kind::tagged_base: subtract kHeapObjectTag,
//                        `base` has to be the object start.
struct StoreOp : OperationT<StoreOp> {
  using Kind = LoadOp::Kind;
  Kind kind;
  MemoryRepresentation stored_rep;
  WriteBarrierKind write_barrier;
  uint8_t element_size_log2;  // multiply index with 2^element_size_log2
  int32_t offset;             // add offset to scaled index
  bool maybe_initializing_or_transitioning;
  uint16_t
      shifted_indirect_pointer_tag;  // for indirect pointer stores, the
                                     // IndirectPointerTag of the store shifted
                                     // to the right by kIndirectPointerTagShift
                                     // (so it fits into 16 bits).
  // TODO(saelo): now that we have a pointer tag in these low-level operations,
  // we could also consider passing the external pointer tag (for external
  // pointers) through to the macro assembler (where we have routines to work
  // with external pointers) instead of handling those earlier in the compiler.
  // We might lose the ability to hardcode the table address though.

  OpEffects Effects() const {
    // Stores might depend on checks for pointer validity, object layout, bounds
    // checks, etc.
    // TODO(tebbi): Distinghish between on-heap and off-heap stores.
    OpEffects effects = OpEffects().CanWriteMemory().CanDependOnChecks();
    if (kind.with_trap_handler) effects = effects.CanLeaveCurrentFunction();
    if (maybe_initializing_or_transitioning) {
      effects = effects.CanDoRawHeapAccess();
    }
    if (kind.is_atomic) {
      // Atomic stores should not be eliminated away, even if the situation
      // seems to allow e.g. store-store elimination. Elimination is avoided by
      // setting the `CanReadMemory` effect.
      effects = effects.CanReadMemory();
    }
    return effects;
  }
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    RegisterRepresentation base = kind.tagged_base
                                      ? RegisterRepresentation::Tagged()
                                      : RegisterRepresentation::WordPtr();
    if (index() == OpIndex::Invalid()) {
      return InitVectorOf(
          storage, {base, stored_rep.ToRegisterRepresentationForStore()});
    }
    return InitVectorOf(storage,
                        {base, stored_rep.ToRegisterRepresentationForStore(),
                         RegisterRepresentation::WordPtr()});
  }

  OpIndex base() const { return input(0); }
  OpIndex value() const { return input(1); }
  OptionalOpIndex index() const {
    return input_count == 3 ? input(2) : OpIndex::Invalid();
  }

  IndirectPointerTag indirect_pointer_tag() const {
    uint64_t shifted = shifted_indirect_pointer_tag;
    return static_cast<IndirectPointerTag>(shifted << kIndirectPointerTagShift);
  }

  StoreOp(
      OpIndex base, OptionalOpIndex index, OpIndex value, Kind kind,
      MemoryRepresentation stored_rep, WriteBarrierKind write_barrier,
      int32_t offset, uint8_t element_size_log2,
      bool maybe_initializing_or_transitioning,
      IndirectPointerTag maybe_indirect_pointer_tag = kIndirectPointerNullTag)
      : Base(2 + index.valid()),
        kind(kind),
        stored_rep(stored_rep),
        write_barrier(write_barrier),
        element_size_log2(element_size_log2),
        offset(offset),
        maybe_initializing_or_transitioning(
            maybe_initializing_or_transitioning),
        shifted_indirect_pointer_tag(maybe_indirect_pointer_tag >>
                                     kIndirectPointerTagShift) {
    DCHECK_EQ(indirect_pointer_tag(), maybe_indirect_pointer_tag);
    input(0) = base;
    input(1) = value;
    if (index.valid()) {
      input(2) = index.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(base()), mapper.Map(index()), mapper.Map(value()),
              kind, stored_rep, write_barrier, offset, element_size_log2,
              maybe_initializing_or_transitioning, indirect_pointer_tag());
  }

  void Validate(const Graph& graph) const {
    DCHECK_IMPLIES(element_size_log2 > 0, index().valid());
    DCHECK_IMPLIES(kind.maybe_unaligned,
                   !SupportedOperations::IsUnalignedLoadSupported(stored_rep));
    DCHECK(LoadOp::OffsetIsValid(offset, kind.tagged_base));
  }
  static StoreOp& New(
      Graph* graph, OpIndex base, OptionalOpIndex index, OpIndex value,
      Kind kind, MemoryRepresentation stored_rep,
      WriteBarrierKind write_barrier, int32_t offset, uint8_t element_size_log2,
      bool maybe_initializing_or_transitioning,
      IndirectPointerTag maybe_indirect_pointer_tag = kIndirectPointerNullTag) {
    return Base::New(graph, 2 + index.valid(), base, index, value, kind,
                     stored_rep, write_barrier, offset, element_size_log2,
                     maybe_initializing_or_transitioning,
                     maybe_indirect_pointer_tag);
  }

  void PrintInputs(std::ostream& os, const std::string& op_index_prefix) const;
  void PrintOptions(std::ostream& os) const;
  auto options() const {
    return std::tuple{
        kind,   stored_rep,        write_barrier,
        offset, element_size_log2, maybe_initializing_or_transitioning};
  }
};

struct AllocateOp : FixedArityOperationT<1, AllocateOp> {
  AllocationType type;

  static constexpr OpEffects effects =
      OpEffects()
          .CanAllocate()
          // The resulting object is unitialized, which leaves the heap in an
          // inconsistent state.
          .CanDoRawHeapAccess()
          // Do not move allocations before checks, to avoid OOM or invalid
          // size.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr()>();
  }

  V<WordPtr> size() const { return input<WordPtr>(0); }

  AllocateOp(V<WordPtr> size, AllocationType type) : Base(size), type(type) {}

  void Validate(const Graph& graph) const {}
  void PrintOptions(std::ostream& os) const;

  auto options() const { return std::tuple{type}; }
};

struct DecodeExternalPointerOp
    : FixedArityOperationT<1, DecodeExternalPointerOp> {
  ExternalPointerTag tag;

  // Accessing external pointers is only safe if the garbage collected pointer
  // keeping the external pointer alive is retained for the length of the
  // operation. For this, it is essential that we use a `Retain` operation
  // placed after the last access to the external data.
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::WordPtr()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  OpIndex handle() const { return input(0); }

  DecodeExternalPointerOp(OpIndex handle, ExternalPointerTag tag)
      : Base(handle), tag(tag) {}

  void Validate(const Graph& graph) const {
    DCHECK_NE(tag, kExternalPointerNullTag);
  }
  void PrintOptions(std::ostream& os) const;
  auto options() const { return std::tuple{tag}; }
};

struct JSStackCheckOp : OperationT<JSStackCheckOp> {
  enum class Kind : uint8_t { kFunctionEntry, kBuiltinEntry, kLoop };
  Kind kind;

  OpEffects Effects() const {
    switch (kind) {
      case Kind::kFunctionEntry:
        return OpEffects().CanCallAnything();
      case Kind::kBuiltinEntry:
        return OpEffects().CanCallAnything();
      case Kind::kLoop:
        // Loop body iteration stack checks can't write memory.
        // TODO(dmercadier): we could prevent this from allocating. In
        // particular, we'd need to:
        //   - forbid GC interrupts from being processed in loop stack checks.
        //   - make sure that the debugger always deopts the current function
        //     when it triggers a loop interrupt.
        return OpEffects()
            .CanDependOnChecks()
            .CanDeopt()
            .CanReadHeapMemory()
            .CanAllocate();
    }
  }

  V<Context> native_context() const { return Base::input<Context>(0); }
  OptionalV<FrameState> frame_state() const {
    return input_count > 1 ? Base::input<FrameState>(1)
                           : OptionalV<FrameState>::Nullopt();
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit JSStackCheckOp(V<Context> context, OptionalV<FrameState> frame_state,
                          Kind kind)
      : Base(1 + frame_state.has_value()), kind(kind) {
    input(0) = context;
    if (frame_state.has_value()) {
      input(1) = frame_state.value();
    }
  }

  static JSStackCheckOp& New(Graph* graph, V<Context> context,
                             OptionalV<FrameState> frame_state, Kind kind) {
    return Base::New(graph, 1 + frame_state.has_value(), context, frame_state,
                     kind);
  }

  void Validate(const Graph& graph) const {
    DCHECK_EQ(kind == Kind::kBuiltinEntry, !frame_state().has_value());
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(native_context()), mapper.Map(frame_state()), kind);
  }

  auto options() const { return std::tuple{kind}; }
};

// Retain a HeapObject to prevent it from being garbage collected too early.
struct RetainOp : FixedArityOperationT<1, RetainOp> {
  V<Object> retained() const { return input<Object>(0); }

  // Retain doesn't actually write, it just keeps a value alive. However, since
  // this must not be reordered with reading operations, we mark it as writing.
  static constexpr OpEffects effects = OpEffects().CanWriteMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  explicit RetainOp(V<Object> retained) : Base(retained) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{}; }
};

// We compare the stack pointer register with the given limit and a
// codegen-dependant adjustment.
struct StackPointerGreaterThanOp
    : FixedArityOperationT<1, StackPointerGreaterThanOp> {
  StackCheckKind kind;

  // Since the frame size of optimized functions is constant, this behaves like
  // a pure operation.
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr()>();
  }

  V<WordPtr> stack_limit() const { return input<WordPtr>(0); }

  StackPointerGreaterThanOp(V<WordPtr> stack_limit, StackCheckKind kind)
      : Base(stack_limit), kind(kind) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind}; }
};

// Allocate a piece of memory in the current stack frame. Every operation
// in the IR is a separate stack slot, but repeated execution in a loop
// produces the same stack slot.
struct StackSlotOp : FixedArityOperationT<0, StackSlotOp> {
  int size;
  int alignment;
  bool is_tagged;

  // We can freely reorder stack slot operations, but must not de-duplicate
  // them.
  static constexpr OpEffects effects = OpEffects().CanCreateIdentity();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::WordPtr()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  StackSlotOp(int size, int alignment, bool is_tagged = false)
      : size(size), alignment(alignment), is_tagged(is_tagged) {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{size, alignment, is_tagged}; }
};

// Values that are constant for the current stack frame/invocation.
// Therefore, they behaves like a constant, even though they are different for
// every invocation.
struct FrameConstantOp : FixedArityOperationT<0, FrameConstantOp> {
  enum class Kind : uint8_t {
    kStackCheckOffset,
    kFramePointer,
    kParentFramePointer
  };
  Kind kind;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case Kind::kStackCheckOffset:
        return RepVector<RegisterRepresentation::Tagged()>();
      case Kind::kFramePointer:
      case Kind::kParentFramePointer:
        return RepVector<RegisterRepresentation::WordPtr()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit FrameConstantOp(Kind kind) : Base(), kind(kind) {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           FrameConstantOp::Kind kind);

struct FrameStateOp : OperationT<FrameStateOp> {
  bool inlined;
  const FrameStateData* data;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  OpIndex parent_frame_state() const {
    DCHECK(inlined);
    return input(0);
  }
  base::Vector<const OpIndex> state_values() const {
    base::Vector<const OpIndex> result = inputs();
    if (inlined) result += 1;
    return result;
  }
  uint16_t state_values_count() const {
    DCHECK_EQ(input_count - inlined, state_values().size());
    return input_count - inlined;
  }
  const OpIndex state_value(size_t idx) const { return state_values()[idx]; }

  RegisterRepresentation state_value_rep(size_t idx) const {
    return RegisterRepresentation::FromMachineRepresentation(
        data->machine_types[idx].representation());
  }

  FrameStateOp(base::Vector<const OpIndex> inputs, bool inlined,
               const FrameStateData* data)
      : Base(inputs), inlined(inlined), data(data) {}

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    auto mapped_inputs = mapper.template Map<32>(inputs());
    return fn(base::VectorOf(mapped_inputs), inlined, data);
  }

  V8_EXPORT_PRIVATE void Validate(const Graph& graph) const;
  V8_EXPORT_PRIVATE void PrintOptions(std::ostream& os) const;
  auto options() const { return std::tuple{inlined, data}; }
};

struct DeoptimizeOp : FixedArityOperationT<1, DeoptimizeOp> {
  const DeoptimizeParameters* parameters;

  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  V<FrameState> frame_state() const { return input<FrameState>(0); }

  DeoptimizeOp(V<FrameState> frame_state,
               const DeoptimizeParameters* parameters)
      : Base(frame_state), parameters(parameters) {}
  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }
  auto options() const { return std::tuple{parameters}; }
};

struct DeoptimizeIfOp : FixedArityOperationT<2, DeoptimizeIfOp> {
  bool negated;
  const DeoptimizeParameters* parameters;

  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  V<Word32> condition() const { return input<Word32>(0); }
  V<FrameState> frame_state() const { return input<FrameState>(1); }

  DeoptimizeIfOp(V<Word32> condition, V<FrameState> frame_state, bool negated,
                 const DeoptimizeParameters* parameters)
      : Base(condition, frame_state),
        negated(negated),
        parameters(parameters) {}

  bool EqualsForGVN(const DeoptimizeIfOp& other) const {
    // As far as GVN is concerned, the `frame_state` and `parameters` don't
    // matter: 2 DeoptimizeIf can be GVNed if they have the same `condition` and
    // same `negated`, regardless of their `frame_state` and `parameters`.
    return condition() == other.condition() && negated == other.negated;
  }
  size_t hash_value(
      HashingStrategy strategy = HashingStrategy::kDefault) const {
    // To enable GVNing as described above in `EqualsForGVN`, `hash_value` has
    // to ignore the `frame_state` and the `parameters`.
    return fast_hash_combine(Opcode::kDeoptimizeIf, condition(), negated);
  }
  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }
  auto options() const { return std::tuple{negated, parameters}; }
  void PrintOptions(std::ostream& os) const;
};

#if V8_ENABLE_WEBASSEMBLY

struct WasmStackCheckOp : FixedArityOperationT<0, WasmStackCheckOp> {
  using Kind = JSStackCheckOp::Kind;
  Kind kind;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  explicit WasmStackCheckOp(Kind kind) : Base(), kind(kind) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           WasmStackCheckOp::Kind kind);
struct TrapIfOp : OperationT<TrapIfOp> {
  bool negated;
  const TrapId trap_id;

  static constexpr OpEffects effects =
      OpEffects()
          // Traps must not float above a protective check.
          .CanDependOnChecks()
          // Subsequent code can rely on the trap not having happened.
          .CanLeaveCurrentFunction();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  V<Word32> condition() const { return input<Word32>(0); }
  OptionalV<FrameState> frame_state() const {
    return input_count > 1 ? input<FrameState>(1)
                           : OptionalV<FrameState>::Nullopt();
  }

  TrapIfOp(V<Word32> condition, OptionalV<FrameState> frame_state, bool negated,
           const TrapId trap_id)
      : Base(1 + frame_state.valid()), negated(negated), trap_id(trap_id) {
    input(0) = condition;
    if (frame_state.valid()) {
      input(1) = frame_state.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(condition()), mapper.Map(frame_state()), negated,
              trap_id);
  }

  static TrapIfOp& New(Graph* graph, V<Word32> condition,
                       OptionalV<FrameState> frame_state, bool negated,
                       const TrapId trap_id) {
    return Base::New(graph, 1 + frame_state.valid(), condition, frame_state,
                     negated, trap_id);
  }

  void Validate(const Graph& graph) const {
    if (frame_state().valid()) {
      DCHECK(Get(graph, frame_state().value()).Is<FrameStateOp>());
    }
  }
  auto options() const { return std::tuple{negated, trap_id}; }
};
#endif  // V8_ENABLE_WEBASSEMBLY

struct StaticAssertOp : FixedArityOperationT<1, StaticAssertOp> {
  const char* source;
  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().RequiredWhenUnused();

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  V<Word32> condition() const { return Base::input<Word32>(0); }

  StaticAssertOp(V<Word32> condition, const char* source)
      : Base(condition), source(source) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{source}; }
};

struct ParameterOp : FixedArityOperationT<0, ParameterOp> {
  int32_t parameter_index;
  RegisterRepresentation rep;
  const char* debug_name;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return {&rep, 1};
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};  // On the callee side a parameter doesn't have an input.
  }

  explicit ParameterOp(int32_t parameter_index, RegisterRepresentation rep,
                       const char* debug_name = "")
      : Base(),
        parameter_index(parameter_index),
        rep(rep),
        debug_name(debug_name) {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{parameter_index, rep, debug_name}; }
  void PrintOptions(std::ostream& os) const;
};

struct OsrValueOp : FixedArityOperationT<0, OsrValueOp> {
  int32_t index;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit OsrValueOp(int32_t index) : Base(), index(index) {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{index}; }
};

struct TSCallDescriptor : public NON_EXPORTED_BASE(ZoneObject) {
  const CallDescriptor* descriptor;
  base::Vector<const RegisterRepresentation> in_reps;
  base::Vector<const RegisterRepresentation> out_reps;
  CanThrow can_throw;
  LazyDeoptOnThrow lazy_deopt_on_throw;

  TSCallDescriptor(const CallDescriptor* descriptor,
                   base::Vector<const RegisterRepresentation> in_reps,
                   base::Vector<const RegisterRepresentation> out_reps,
                   CanThrow can_throw, LazyDeoptOnThrow lazy_deopt_on_throw)
      : descriptor(descriptor),
        in_reps(in_reps),
        out_reps(out_reps),
        can_throw(can_throw),
        lazy_deopt_on_throw(lazy_deopt_on_throw) {}

  static const TSCallDescriptor* Create(const CallDescriptor* descriptor,
                                        CanThrow can_throw,
                                        LazyDeoptOnThrow lazy_deopt_on_throw,
                                        Zone* graph_zone) {
    DCHECK_IMPLIES(can_throw == CanThrow::kNo,
                   lazy_deopt_on_throw == LazyDeoptOnThrow::kNo);
    base::Vector<RegisterRepresentation> in_reps =
        graph_zone->AllocateVector<RegisterRepresentation>(
            descriptor->ParameterCount());
    for (size_t i = 0; i < descriptor->ParameterCount(); ++i) {
      in_reps[i] = RegisterRepresentation::FromMachineRepresentation(
          descriptor->GetParameterType(i).representation());
    }
    base::Vector<RegisterRepresentation> out_reps =
        graph_zone->AllocateVector<RegisterRepresentation>(
            descriptor->ReturnCount());
    for (size_t i = 0; i < descriptor->ReturnCount(); ++i) {
      out_reps[i] = RegisterRepresentation::FromMachineRepresentation(
          descriptor->GetReturnType(i).representation());
    }
    return graph_zone->New<TSCallDescriptor>(descriptor, in_reps, out_reps,
                                             can_throw, lazy_deopt_on_throw);
  }
};

template <>
struct fast_hash<TSCallDescriptor> {
  size_t operator()(const TSCallDescriptor& v) {
    const CallDescriptor& d = *v.descriptor;
    // This does not include all fields of the call descriptor, but it should be
    // sufficient to differentiate between different calls (and collisions are
    // not too critical).
    return fast_hash_combine(d.kind(), d.tag(), d.ReturnCount(),
                             d.ParameterCount(), d.GPParameterCount(),
                             d.FPParameterCount(), d.ParameterSlotCount(),
                             d.ReturnSlotCount(), d.flags());
  }
};

// If {target} is a HeapObject representing a builtin, return that builtin's ID.
std::optional<Builtin> TryGetBuiltinId(const ConstantOp* target,
                                       JSHeapBroker* broker);

struct CallOp : OperationT<CallOp> {
  const TSCallDescriptor* descriptor;
  OpEffects callee_effects;

  OpEffects Effects() const { return callee_effects; }

  // The outputs are produced by the `DidntThrow` operation.
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }
  base::Vector<const RegisterRepresentation> results_rep() const {
    return descriptor->out_reps;
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(input_count);
    size_t i = 0;
    if (descriptor->descriptor->IsCodeObjectCall() ||
        descriptor->descriptor->IsJSFunctionCall() ||
        descriptor->descriptor->IsBuiltinPointerCall()) {
      storage[i++] = MaybeRegisterRepresentation::Tagged();
    } else {
      storage[i++] = MaybeRegisterRepresentation::WordPtr();
    }
    if (HasFrameState()) {
      storage[i++] = MaybeRegisterRepresentation::None();
    }
    for (auto rep : descriptor->in_reps) {
      // In JavaScript, parameters are optional.
      if (i >= input_count) break;
      storage[i++] = rep;
    }
    storage.resize(i);
    return base::VectorOf(storage);
  }

  bool HasFrameState() const {
    return descriptor->descriptor->NeedsFrameState();
  }

  V<CallTarget> callee() const { return input<CallTarget>(0); }
  OptionalV<FrameState> frame_state() const {
    return HasFrameState() ? input<FrameState>(1)
                           : OptionalV<FrameState>::Nullopt();
  }
  base::Vector<const OpIndex> arguments() const {
    return inputs().SubVector(1 + HasFrameState(), input_count);
  }
  // Returns true if this call is a JS (but not wasm) stack check.
  V8_EXPORT_PRIVATE bool IsStackCheck(const Graph& graph, JSHeapBroker* broker,
                                      StackCheckKind kind) const;

  CallOp(V<CallTarget> callee, OptionalV<FrameState> frame_state,
         base::Vector<const OpIndex> arguments,
         const TSCallDescriptor* descriptor, OpEffects effects)
      : Base(1 + frame_state.valid() + arguments.size()),
        descriptor(descriptor),
        callee_effects(effects) {
    base::Vector<OpIndex> inputs = this->inputs();
    inputs[0] = callee;
    if (frame_state.valid()) {
      inputs[1] = frame_state.value();
    }
    inputs.SubVector(1 + frame_state.valid(), inputs.size())
        .OverwriteWith(arguments);
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    V<CallTarget> mapped_callee = mapper.Map(callee());
    OptionalV<FrameState> mapped_frame_state = mapper.Map(frame_state());
    auto mapped_arguments = mapper.template Map<16>(arguments());
    return fn(mapped_callee, mapped_frame_state,
              base::VectorOf(mapped_arguments), descriptor, Effects());
  }

  void Validate(const Graph& graph) const {
    if (frame_state().valid()) {
      DCHECK(Get(graph, frame_state().value()).Is<FrameStateOp>());
    }
  }

  static CallOp& New(Graph* graph, V<CallTarget> callee,
                     OptionalV<FrameState> frame_state,
                     base::Vector<const OpIndex> arguments,
                     const TSCallDescriptor* descriptor, OpEffects effects) {
    return Base::New(graph, 1 + frame_state.valid() + arguments.size(), callee,
                     frame_state, arguments, descriptor, effects);
  }
  // TODO(mliedtke): Should the hash function be overwritten, so that calls (and
  // potentially tail calls) can participate in GVN? Right now this is prevented
  // by every call descriptor being a different pointer.
  auto options() const { return std::tuple{descriptor, callee_effects}; }
  size_t hash_value(HashingStrategy strategy = HashingStrategy::kDefault) const;
  void PrintOptions(std::ostream& os) const;
};

// Catch an exception from the first operation of the `successor` block and
// continue execution in `catch_block` in this case.
struct CheckExceptionOp : FixedArityOperationT<1, CheckExceptionOp> {
  Block* didnt_throw_block;
  Block* catch_block;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  V<Any> throwing_operation() const { return input<Any>(0); }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  CheckExceptionOp(V<Any> throwing_operation, Block* successor,
                   Block* catch_block)
      : Base(throwing_operation),
        didnt_throw_block(successor),
        catch_block(catch_block) {}

  V8_EXPORT_PRIVATE void Validate(const Graph& graph) const;

  size_t hash_value(HashingStrategy strategy = HashingStrategy::kDefault) const;
  auto options() const { return std::tuple{didnt_throw_block, catch_block}; }
};

// This is a pseudo-operation that marks the beginning of a catch block. It
// returns the caught exception.
struct CatchBlockBeginOp : FixedArityOperationT<0, CatchBlockBeginOp> {
  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  CatchBlockBeginOp() : Base() {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

// Throwing operations always appear together with `DidntThrowOp`, which
// produces the value in case that no exception was thrown. If the callsite is
// non-catching, then `DidntThrowOp` follows right after the throwing operation:
//
//   100: Call(...)
//   101: DidntThrow(100)
//   102: Foo(101)
//
// If the callsite can catch, then the
// pattern is as follows:
//
//   100: Call(...)
//   101: CheckException(B10, B11)
//
//   B10:
//   102: DidntThrow(100)
//   103: Foo(102)
//
//   B11:
//   200: CatchBlockBegin()
//   201: ...
//
// This complexity is mostly hidden from graph creation, with
// `DidntThrowOp` and `CheckExceptionOp` being inserted automatically.
// The correct way to produce `CheckExceptionOp` is to create an
// `Assembler::CatchScope`, which will cause all throwing operations
// to add a `CheckExceptionOp` automatically while the scope is active.
// Since `CopyingPhase` does this automatically, lowering throwing
// operations into an arbitrary subgraph works automatically.
struct DidntThrowOp : FixedArityOperationT<1, DidntThrowOp> {
  OpEffects throwing_op_effects;

  // If there is a `CheckException` operation with a catch block for
  // `throwing_operation`.
  bool has_catch_block;
  // This is a pointer to a vector instead of a vector to save a bit of memory,
  // using optimal 16 bytes instead of 24.
  const base::Vector<const RegisterRepresentation>* results_rep;

  OpEffects Effects() const { return throwing_op_effects; }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return *results_rep;
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::None()>();
  }

  OpIndex throwing_operation() const { return input(0); }

  explicit DidntThrowOp(
      OpIndex throwing_operation, bool has_catch_block,
      const base::Vector<const RegisterRepresentation>* results_rep,
      OpEffects throwing_op_effects)
      : Base(throwing_operation),
        throwing_op_effects(throwing_op_effects),
        has_catch_block(has_catch_block),
        results_rep(results_rep) {}
  V8_EXPORT_PRIVATE void Validate(const Graph& graph) const;
  auto options() const {
    return std::tuple{throwing_op_effects, has_catch_block};
  }
};

struct TailCallOp : OperationT<TailCallOp> {
  const TSCallDescriptor* descriptor;

  static constexpr OpEffects effects = OpEffects().CanLeaveCurrentFunction();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    // While TailCalls do return some values, those values are returned to the
    // caller rather than to the current function (and a TailCallOp thus never
    // has any uses), so we set the outputs_rep to empty. If you need to know
    // what a TailCallOp returns, you can find out in `descriptor->outputs_rep`.
    return {};
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(input_count);
    size_t i = 0;
    storage[i++] = MaybeRegisterRepresentation::Tagged();  // True for wasm?
    for (auto rep : descriptor->in_reps) {
      storage[i++] = rep;
    }
    storage.resize(i);
    return base::VectorOf(storage);
  }

  OpIndex callee() const { return input(0); }
  base::Vector<const OpIndex> arguments() const {
    return inputs().SubVector(1, input_count);
  }

  TailCallOp(OpIndex callee, base::Vector<const OpIndex> arguments,
             const TSCallDescriptor* descriptor)
      : Base(1 + arguments.size()), descriptor(descriptor) {
    base::Vector<OpIndex> inputs = this->inputs();
    inputs[0] = callee;
    inputs.SubVector(1, inputs.size()).OverwriteWith(arguments);
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    OpIndex mapped_callee = mapper.Map(callee());
    auto mapped_arguments = mapper.template Map<16>(arguments());
    return fn(mapped_callee, base::VectorOf(mapped_arguments), descriptor);
  }

  void Validate(const Graph& graph) const {}
  static TailCallOp& New(Graph* graph, OpIndex callee,
                         base::Vector<const OpIndex> arguments,
                         const TSCallDescriptor* descriptor) {
    return Base::New(graph, 1 + arguments.size(), callee, arguments,
                     descriptor);
  }
  auto options() const { return std::tuple{descriptor}; }
  void PrintOptions(std::ostream& os) const;
};

// Control-flow should never reach here.
struct UnreachableOp : FixedArityOperationT<0, UnreachableOp> {
  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().CanLeaveCurrentFunction();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  UnreachableOp() : Base() {}
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

struct ReturnOp : OperationT<ReturnOp> {
  static constexpr OpEffects effects = OpEffects().CanLeaveCurrentFunction();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    // TODO(mliedtke): Ideally, a return op would expect to get the correct
    // types for all its return values, not just the pop count.
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  // Number of additional stack slots to be removed.
  V<Word32> pop_count() const { return input<Word32>(0); }

  base::Vector<const OpIndex> return_values() const {
    return inputs().SubVector(1, input_count);
  }

  ReturnOp(V<Word32> pop_count, base::Vector<const OpIndex> return_values)
      : Base(1 + return_values.size()) {
    base::Vector<OpIndex> inputs = this->inputs();
    inputs[0] = pop_count;
    inputs.SubVector(1, inputs.size()).OverwriteWith(return_values);
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    OpIndex mapped_pop_count = mapper.Map(pop_count());
    auto mapped_return_values = mapper.template Map<4>(return_values());
    return fn(mapped_pop_count, base::VectorOf(mapped_return_values));
  }

  void Validate(const Graph& graph) const {
  }
  static ReturnOp& New(Graph* graph, V<Word32> pop_count,
                       base::Vector<const OpIndex> return_values) {
    return Base::New(graph, 1 + return_values.size(), pop_count, return_values);
  }
  auto options() const { return std::tuple{}; }
};

struct GotoOp : FixedArityOperationT<0, GotoOp> {
  bool is_backedge;
  Block* destination;

  static constexpr OpEffects effects = OpEffects().CanChangeControlFlow();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit GotoOp(Block* destination, bool is_backedge)
      : Base(), is_backedge(is_backedge), destination(destination) {}
  void Validate(const Graph& graph) const {}
  size_t hash_value(HashingStrategy strategy = HashingStrategy::kDefault) const;
  auto options() const { return std::tuple{destination, is_backedge}; }
};

struct BranchOp : FixedArityOperationT<1, BranchOp> {
  BranchHint hint;
  Block* if_true;
  Block* if_false;

  static constexpr OpEffects effects = OpEffects().CanChangeControlFlow();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  OpIndex condition() const { return input(0); }

  BranchOp(OpIndex condition, Block* if_true, Block* if_false, BranchHint hint)
      : Base(condition), hint(hint), if_true(if_true), if_false(if_false) {}

  void Validate(const Graph& graph) const {
  }
  size_t hash_value(HashingStrategy strategy = HashingStrategy::kDefault) const;
  auto options() const { return std::tuple{if_true, if_false, hint}; }
};

struct SwitchOp : FixedArityOperationT<1, SwitchOp> {
  struct Case {
    BranchHint hint;
    int32_t value;
    Block* destination;

    Case(int32_t value, Block* destination, BranchHint hint)
        : hint(hint), value(value), destination(destination) {}

    bool operator==(const Case& other) const {
      return value == other.value && destination == other.destination &&
             hint == other.hint;
    }
  };
  BranchHint default_hint;
  base::Vector<Case> cases;
  Block* default_case;

  static constexpr OpEffects effects = OpEffects().CanChangeControlFlow();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32()>();
  }

  V<Word32> input() const { return Base::input<Word32>(0); }

  SwitchOp(V<Word32> input, base::Vector<Case> cases, Block* default_case,
           BranchHint default_hint)
      : Base(input),
        default_hint(default_hint),
        cases(cases),
        default_case(default_case) {}

  void Validate(const Graph& graph) const {}
  void PrintOptions(std::ostream& os) const;
  size_t hash_value(HashingStrategy strategy = HashingStrategy::kDefault) const;
  auto options() const { return std::tuple{cases, default_case, default_hint}; }
};

template <>
struct fast_hash<SwitchOp::Case> {
  size_t operator()(SwitchOp::Case v) {
    return fast_hash_combine(v.value, v.destination);
  }
};

inline base::SmallVector<Block*, 4> SuccessorBlocks(const Operation& op) {
  switch (op.opcode) {
    case Opcode::kCheckException: {
      auto& casted = op.Cast<CheckExceptionOp>();
      return {casted.didnt_throw_block, casted.catch_block};
    }
    case Opcode::kGoto: {
      auto& casted = op.Cast<GotoOp>();
      return {casted.destination};
    }
    case Opcode::kBranch: {
      auto& casted = op.Cast<BranchOp>();
      return {casted.if_true, casted.if_false};
    }
    case Opcode::kReturn:
    case Opcode::kTailCall:
    case Opcode::kDeoptimize:
    case Opcode::kUnreachable:
      return base::SmallVector<Block*, 4>{};
    case Opcode::kSwitch: {
      auto& casted = op.Cast<SwitchOp>();
      base::SmallVector<Block*, 4> result;
      for (const SwitchOp::Case& c : casted.cases) {
        result.push_back(c.destination);
      }
      result.push_back(casted.default_case);
      return result;
    }
#define NON_TERMINATOR_CASE(op) case Opcode::k##op:
      TURBOSHAFT_OPERATION_LIST_NOT_BLOCK_TERMINATOR(NON_TERMINATOR_CASE)
      UNREACHABLE();
#undef NON_TERMINATOR_CASE
  }
}

V8_EXPORT_PRIVATE base::SmallVector<Block*, 4> SuccessorBlocks(
    const Block& block, const Graph& graph);

// Tuples are only used to lower operations with multiple outputs.
// `TupleOp` should be folded away by subsequent `ProjectionOp`s.
struct TupleOp : OperationT<TupleOp> {
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit TupleOp(base::Vector<const V<Any>> inputs) : Base(inputs) {}

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    auto mapped_inputs = mapper.template Map<4>(inputs());
    return fn(base::VectorOf(mapped_inputs));
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

// For operations that produce multiple results, we use `ProjectionOp` to
// distinguish them.
struct ProjectionOp : FixedArityOperationT<1, ProjectionOp> {
  uint16_t index;
  RegisterRepresentation rep;

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&rep, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  V<Any> input() const { return Base::input<Any>(0); }

  ProjectionOp(V<Any> input, uint16_t index, RegisterRepresentation rep)
      : Base(input), index(index), rep(rep) {}

  void Validate(const Graph& graph) const {
    DCHECK(ValidOpInputRep(graph, input(), rep, index));
  }
  auto options() const { return std::tuple{index, rep}; }
};

struct CheckTurboshaftTypeOfOp
    : FixedArityOperationT<1, CheckTurboshaftTypeOfOp> {
  RegisterRepresentation rep;
  Type type;
  bool successful;

  static constexpr OpEffects effects = OpEffects()
                                           .CanDependOnChecks()
                                           .CanReadImmutableMemory()
                                           .RequiredWhenUnused();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  OpIndex input() const { return Base::input(0); }

  CheckTurboshaftTypeOfOp(OpIndex input, RegisterRepresentation rep, Type type,
                          bool successful)
      : Base(input), rep(rep), type(std::move(type)), successful(successful) {}

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{rep, type, successful}; }
};

struct ObjectIsOp : FixedArityOperationT<1, ObjectIsOp> {
  enum class Kind : uint8_t {
    kArrayBufferView,
    kBigInt,
    kBigInt64,
    kCallable,
    kConstructor,
    kDetectableCallable,
    kInternalizedString,
    kNonCallable,
    kNumber,
    kNumberOrBigInt,
    kReceiver,
    kReceiverOrNullOrUndefined,
    kSmi,
    kString,
    kStringOrStringWrapper,
    kSymbol,
    kUndetectable,
  };
  enum class InputAssumptions : uint8_t {
    kNone,
    kHeapObject,
    kBigInt,
  };
  Kind kind;
  InputAssumptions input_assumptions;

  // All type checks performed by this operator are regarding immutable
  // properties. Therefore, it can be considered pure. Input assumptions,
  // howerever, can rely on being scheduled after checks.
  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().CanReadImmutableMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }

  ObjectIsOp(V<Object> input, Kind kind, InputAssumptions input_assumptions)
      : Base(input), kind(kind), input_assumptions(input_assumptions) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind, input_assumptions}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ObjectIsOp::Kind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, ObjectIsOp::InputAssumptions input_assumptions);

enum class NumericKind : uint8_t {
  kFloat64Hole,
  kFinite,
  kInteger,
  kSafeInteger,
  kSmi,
  kMinusZero,
  kNaN,
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os, NumericKind kind);

struct Float64IsOp : FixedArityOperationT<1, Float64IsOp> {
  NumericKind kind;

  Float64IsOp(V<Float64> input, NumericKind kind) : Base(input), kind(kind) {}

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Float64()>();
  }

  V<Float64> input() const { return Base::input<Float64>(0); }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{kind}; }
};

struct ObjectIsNumericValueOp
    : FixedArityOperationT<1, ObjectIsNumericValueOp> {
  NumericKind kind;
  FloatRepresentation input_rep;

  ObjectIsNumericValueOp(V<Object> input, NumericKind kind,
                         FloatRepresentation input_rep)
      : Base(input), kind(kind), input_rep(input_rep) {}

  // Heap numbers are immutable, so reading from them is pure. We might rely on
  // checks to assume that the input is a heap number.
  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().CanReadImmutableMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }

  void Validate(const Graph& graph) const {
  }
  auto options() const { return std::tuple{kind, input_rep}; }
};

struct ConvertOp : FixedArityOperationT<1, ConvertOp> {
  enum class Kind : uint8_t {
    kObject,
    kBoolean,
    kNumber,
    kNumberOrOddball,
    kPlainPrimitive,
    kString,
    kSmi,
  };
  Kind from;
  Kind to;

  // All properties/values we read are immutable.
  static constexpr OpEffects effects =
      OpEffects()
          // We only allocate identityless primitives here.
          .CanAllocateWithoutIdentity()
          // We might use preceding checks to ensure the input has the right
          // type.
          .CanDependOnChecks()
          .CanReadImmutableMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }

  ConvertOp(V<Object> input, Kind from, Kind to)
      : Base(input), from(from), to(to) {}

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{from, to}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ConvertOp::Kind kind);

struct ConvertUntaggedToJSPrimitiveOp
    : FixedArityOperationT<1, ConvertUntaggedToJSPrimitiveOp> {
  enum class JSPrimitiveKind : uint8_t {
    kBigInt,
    kBoolean,
    kHeapNumber,
    kHeapNumberOrUndefined,
    kNumber,
    kSmi,
    kString,
  };
  enum class InputInterpretation : uint8_t {
    kSigned,
    kUnsigned,
    kCharCode,
    kCodePoint,
  };
  JSPrimitiveKind kind;
  RegisterRepresentation input_rep;
  InputInterpretation input_interpretation;
  CheckForMinusZeroMode minus_zero_mode;

  // The input is untagged and the results are identityless primitives.
  static constexpr OpEffects effects = OpEffects().CanAllocateWithoutIdentity();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(input_rep);
  }

  V<Untagged> input() const { return Base::input<Untagged>(0); }

  ConvertUntaggedToJSPrimitiveOp(V<Untagged> input, JSPrimitiveKind kind,
                                 RegisterRepresentation input_rep,
                                 InputInterpretation input_interpretation,
                                 CheckForMinusZeroMode minus_zero_mode)
      : Base(input),
        kind(kind),
        input_rep(input_rep),
        input_interpretation(input_interpretation),
        minus_zero_mode(minus_zero_mode) {}

  void Validate(const Graph& graph) const {
    switch (kind) {
      case JSPrimitiveKind::kBigInt:
        DCHECK_EQ(input_rep, RegisterRepresentation::Word64());
        DCHECK_EQ(minus_zero_mode,
                  CheckForMinusZeroMode::kDontCheckForMinusZero);
        break;
      case JSPrimitiveKind::kBoolean:
        DCHECK_EQ(input_rep, RegisterRepresentation::Word32());
        DCHECK_EQ(minus_zero_mode,
                  CheckForMinusZeroMode::kDontCheckForMinusZero);
        break;
      case JSPrimitiveKind::kNumber:
      case JSPrimitiveKind::kHeapNumber:
        DCHECK_IMPLIES(
            minus_zero_mode == CheckForMinusZeroMode::kCheckForMinusZero,
            input_rep == RegisterRepresentation::Float64());
        break;
      case JSPrimitiveKind::kHeapNumberOrUndefined:
        DCHECK_IMPLIES(
            minus_zero_mode == CheckForMinusZeroMode::kDontCheckForMinusZero,
            input_rep == RegisterRepresentation::Float64());
        break;
      case JSPrimitiveKind::kSmi:
        DCHECK_EQ(input_rep, WordRepresentation::Word32());
        DCHECK_EQ(minus_zero_mode,
                  CheckForMinusZeroMode::kDontCheckForMinusZero);
        break;
      case JSPrimitiveKind::kString:
        DCHECK_EQ(input_rep, WordRepresentation::Word32());
        DCHECK_EQ(input_interpretation,
                  any_of(InputInterpretation::kCharCode,
                         InputInterpretation::kCodePoint));
        break;
    }
  }

  auto options() const {
    return std::tuple{kind, input_rep, input_interpretation, minus_zero_mode};
  }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, ConvertUntaggedToJSPrimitiveOp::JSPrimitiveKind kind);

struct ConvertUntaggedToJSPrimitiveOrDeoptOp
    : FixedArityOperationT<2, ConvertUntaggedToJSPrimitiveOrDeoptOp> {
  enum class JSPrimitiveKind : uint8_t {
    kSmi,
  };
  enum class InputInterpretation : uint8_t {
    kSigned,
    kUnsigned,
  };
  JSPrimitiveKind kind;
  RegisterRepresentation input_rep;
  InputInterpretation input_interpretation;
  FeedbackSource feedback;

  // We currently only convert Word representations to Smi or deopt. We need to
  // change the effects if we add more kinds.
  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(input_rep);
  }

  V<Untagged> input() const { return Base::input<Untagged>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  ConvertUntaggedToJSPrimitiveOrDeoptOp(
      V<Untagged> input, V<FrameState> frame_state, JSPrimitiveKind kind,
      RegisterRepresentation input_rep,
      InputInterpretation input_interpretation, const FeedbackSource& feedback)
      : Base(input, frame_state),
        kind(kind),
        input_rep(input_rep),
        input_interpretation(input_interpretation),
        feedback(feedback) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const {
    return std::tuple{kind, input_rep, input_interpretation, feedback};
  }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os,
    ConvertUntaggedToJSPrimitiveOrDeoptOp::JSPrimitiveKind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, ConvertUntaggedToJSPrimitiveOrDeoptOp::InputInterpretation
                          input_interpretation);

struct ConvertJSPrimitiveToUntaggedOp
    : FixedArityOperationT<1, ConvertJSPrimitiveToUntaggedOp> {
  enum class UntaggedKind : uint8_t {
    kInt32,
    kInt64,
    kUint32,
    kBit,
    kFloat64,
  };
  enum class InputAssumptions : uint8_t {
    kBoolean,
    kSmi,
    kNumberOrOddball,
    kPlainPrimitive,
  };
  UntaggedKind kind;
  InputAssumptions input_assumptions;

  // This operation can read memory, but only immutable aspects, so it counts as
  // pure.
  static constexpr OpEffects effects =
      OpEffects()
          // We might rely on preceding checks to avoid deopts.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case UntaggedKind::kInt32:
      case UntaggedKind::kUint32:
      case UntaggedKind::kBit:
        return RepVector<RegisterRepresentation::Word32()>();
      case UntaggedKind::kInt64:
        return RepVector<RegisterRepresentation::Word64()>();
      case UntaggedKind::kFloat64:
        return RepVector<RegisterRepresentation::Float64()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<JSPrimitive> input() const { return Base::input<JSPrimitive>(0); }

  ConvertJSPrimitiveToUntaggedOp(V<JSPrimitive> input, UntaggedKind kind,
                                 InputAssumptions input_assumptions)
      : Base(input), kind(kind), input_assumptions(input_assumptions) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind, input_assumptions}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, ConvertJSPrimitiveToUntaggedOp::UntaggedKind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os,
    ConvertJSPrimitiveToUntaggedOp::InputAssumptions input_assumptions);

struct ConvertJSPrimitiveToUntaggedOrDeoptOp
    : FixedArityOperationT<2, ConvertJSPrimitiveToUntaggedOrDeoptOp> {
  enum class UntaggedKind : uint8_t {
    kInt32,
    kInt64,
    kFloat64,
    kArrayIndex,
  };
  enum class JSPrimitiveKind : uint8_t {
    kNumber,
    kNumberOrBoolean,
    kNumberOrOddball,
    kNumberOrString,
    kSmi,
  };
  JSPrimitiveKind from_kind;
  UntaggedKind to_kind;
  CheckForMinusZeroMode minus_zero_mode;
  FeedbackSource feedback;

  // This operation can read memory, but only immutable aspects, so it counts as
  // pure.
  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (to_kind) {
      case UntaggedKind::kInt32:
        return RepVector<RegisterRepresentation::Word32()>();
      case UntaggedKind::kInt64:
        return RepVector<RegisterRepresentation::Word64()>();
      case UntaggedKind::kFloat64:
        return RepVector<RegisterRepresentation::Float64()>();
      case UntaggedKind::kArrayIndex:
        return Is64() ? RepVector<RegisterRepresentation::Word64()>()
                      : RepVector<RegisterRepresentation::Word32()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  ConvertJSPrimitiveToUntaggedOrDeoptOp(V<Object> input,
                                        V<FrameState> frame_state,
                                        JSPrimitiveKind from_kind,
                                        UntaggedKind to_kind,
                                        CheckForMinusZeroMode minus_zero_mode,
                                        const FeedbackSource& feedback)
      : Base(input, frame_state),
        from_kind(from_kind),
        to_kind(to_kind),
        minus_zero_mode(minus_zero_mode),
        feedback(feedback) {}
  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const {
    return std::tuple{from_kind, to_kind, minus_zero_mode, feedback};
  }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os,
    ConvertJSPrimitiveToUntaggedOrDeoptOp::JSPrimitiveKind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, ConvertJSPrimitiveToUntaggedOrDeoptOp::UntaggedKind kind);

struct TruncateJSPrimitiveToUntaggedOp
    : FixedArityOperationT<1, TruncateJSPrimitiveToUntaggedOp> {
  enum class UntaggedKind : uint8_t {
    kInt32,
    kInt64,
    kBit,
  };
  enum class InputAssumptions : uint8_t {
    kBigInt,
    kNumberOrOddball,
    kHeapObject,
    kObject,
  };
  UntaggedKind kind;
  InputAssumptions input_assumptions;

  // This operation can read memory, but only immutable aspects, so it counts as
  // pure.
  static constexpr OpEffects effects =
      OpEffects()
          // We might rely on preceding checks to ensure the input type.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case UntaggedKind::kInt32:
      case UntaggedKind::kBit:
        return RepVector<RegisterRepresentation::Word32()>();
      case UntaggedKind::kInt64:
        return RepVector<RegisterRepresentation::Word64()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<JSPrimitive> input() const { return Base::input<JSPrimitive>(0); }

  TruncateJSPrimitiveToUntaggedOp(V<JSPrimitive> input, UntaggedKind kind,
                                  InputAssumptions input_assumptions)
      : Base(input), kind(kind), input_assumptions(input_assumptions) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind, input_assumptions}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, TruncateJSPrimitiveToUntaggedOp::UntaggedKind kind);
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os,
    TruncateJSPrimitiveToUntaggedOp::InputAssumptions input_assumptions);

struct TruncateJSPrimitiveToUntaggedOrDeoptOp
    : FixedArityOperationT<2, TruncateJSPrimitiveToUntaggedOrDeoptOp> {
  enum class UntaggedKind : uint8_t {
    kInt32,
  };
  using InputRequirement =
      ConvertJSPrimitiveToUntaggedOrDeoptOp::JSPrimitiveKind;
  UntaggedKind kind;
  InputRequirement input_requirement;
  FeedbackSource feedback;

  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case UntaggedKind::kInt32:
        return RepVector<RegisterRepresentation::Word32()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<JSPrimitive> input() const { return Base::input<JSPrimitive>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  TruncateJSPrimitiveToUntaggedOrDeoptOp(V<JSPrimitive> input,
                                         V<FrameState> frame_state,
                                         UntaggedKind kind,
                                         InputRequirement input_requirement,
                                         const FeedbackSource& feedback)
      : Base(input, frame_state),
        kind(kind),
        input_requirement(input_requirement),
        feedback(feedback) {}
  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{kind, input_requirement, feedback}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os,
    TruncateJSPrimitiveToUntaggedOrDeoptOp::UntaggedKind kind);

struct ConvertJSPrimitiveToObjectOp
    : FixedArityOperationT<3, ConvertJSPrimitiveToObjectOp> {
  ConvertReceiverMode mode;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<JSPrimitive> value() const { return Base::input<JSPrimitive>(0); }
  V<Context> native_context() const { return Base::input<Context>(1); }
  V<JSGlobalProxy> global_proxy() const {
    return Base::input<JSGlobalProxy>(2);
  }

  ConvertJSPrimitiveToObjectOp(V<JSPrimitive> value, V<Context> native_context,
                               V<JSGlobalProxy> global_proxy,
                               ConvertReceiverMode mode)
      : Base(value, native_context, global_proxy), mode(mode) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{mode}; }
};

struct NewConsStringOp : FixedArityOperationT<3, NewConsStringOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // Strings are conceptually immutable and don't have identity.
          .CanAllocateWithoutIdentity()
          // We might rely on preceding checks to ensure the input is a string
          // and on their combined length being between ConsString::kMinLength
          // and ConsString::kMaxLength.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<Word32> length() const { return Base::input<Word32>(0); }
  V<String> first() const { return Base::input<String>(1); }
  V<String> second() const { return Base::input<String>(2); }

  NewConsStringOp(V<Word32> length, V<String> first, V<String> second)
      : Base(length, first, second) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct NewArrayOp : FixedArityOperationT<1, NewArrayOp> {
  enum class Kind : uint8_t {
    kDouble,
    kObject,
  };
  Kind kind;
  AllocationType allocation_type;

  static constexpr OpEffects effects =
      OpEffects()
          // Allocate the result, which has identity.
          .CanAllocate()
          // We might have checks to ensure the array length is valid and not
          // too big.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr()>();
  }

  OpIndex length() const { return Base::input(0); }

  NewArrayOp(OpIndex length, Kind kind, AllocationType allocation_type)
      : Base(length), kind(kind), allocation_type(allocation_type) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind, allocation_type}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           NewArrayOp::Kind kind);

struct DoubleArrayMinMaxOp : FixedArityOperationT<1, DoubleArrayMinMaxOp> {
  enum class Kind : uint8_t {
    kMin,
    kMax,
  };
  Kind kind;

  static constexpr OpEffects effects =
      OpEffects()
          // Read the array contents.
          .CanReadHeapMemory()
          // Allocate the HeapNumber result.
          .CanAllocateWithoutIdentity()
          // We might depend on checks to ensure the input is an array.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex array() const { return Base::input(0); }

  DoubleArrayMinMaxOp(OpIndex array, Kind kind) : Base(array), kind(kind) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           DoubleArrayMinMaxOp::Kind kind);

// TODO(nicohartmann@): We should consider getting rid of the LoadFieldByIndex
// operation.
struct LoadFieldByIndexOp : FixedArityOperationT<2, LoadFieldByIndexOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // Read the possibly mutable property.
          .CanReadHeapMemory()
          // We may allocate heap number for the result.
          .CanAllocateWithoutIdentity()
          // We assume the input is an object.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  OpIndex object() const { return Base::input(0); }
  // Index encoding (see `src/objects/field-index-inl.h`):
  // For efficiency, the LoadByFieldIndex instruction takes an index that is
  // optimized for quick access. If the property is inline, the index is
  // positive. If it's out-of-line, the encoded index is -raw_index - 1 to
  // disambiguate the zero out-of-line index from the zero inobject case.
  // The index itself is shifted up by one bit, the lower-most bit
  // signifying if the field is a mutable double box (1) or not (0).
  OpIndex index() const { return Base::input(1); }

  LoadFieldByIndexOp(OpIndex object, OpIndex index) : Base(object, index) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct DebugBreakOp : FixedArityOperationT<0, DebugBreakOp> {
  // Prevent any reordering.
  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  DebugBreakOp() : Base() {}
  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct DebugPrintOp : FixedArityOperationT<1, DebugPrintOp> {
  RegisterRepresentation rep;

  // We just need to ensure that the debug print stays in the same block and
  // observes the right memory state. It doesn't actually change control flow,
  // but pretending so ensures the we do not remove the debug print even though
  // it is unused. We assume that the debug print doesn't affect memory so that
  // the scheduling of loads is not affected.
  static constexpr OpEffects effects = OpEffects()
                                           .CanChangeControlFlow()
                                           .CanDependOnChecks()
                                           .CanReadMemory()
                                           .RequiredWhenUnused();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InputsRepFactory::SingleRep(rep);
  }

  OpIndex input() const { return Base::input(0); }

  DebugPrintOp(OpIndex input, RegisterRepresentation rep)
      : Base(input), rep(rep) {}
  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{rep}; }
};

struct BigIntBinopOp : FixedArityOperationT<3, BigIntBinopOp> {
  enum class Kind : uint8_t {
    kAdd,
    kSub,
    kMul,
    kDiv,
    kMod,
    kBitwiseAnd,
    kBitwiseOr,
    kBitwiseXor,
    kShiftLeft,
    kShiftRightArithmetic,
  };
  Kind kind;

  // These operations can deopt (abort), allocate and read immutable data.
  static constexpr OpEffects effects =
      OpEffects()
          // Allocate the resulting BigInt, which does not have identity.
          .CanAllocateWithoutIdentity()
          .CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<BigInt> left() const { return Base::input<BigInt>(0); }
  V<BigInt> right() const { return Base::input<BigInt>(1); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(2); }

  BigIntBinopOp(V<BigInt> left, V<BigInt> right, V<FrameState> frame_state,
                Kind kind)
      : Base(left, right, frame_state), kind(kind) {}
  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           BigIntBinopOp::Kind kind);

struct BigIntComparisonOp : FixedArityOperationT<2, BigIntComparisonOp> {
  enum class Kind : uint8_t {
    kEqual,
    kLessThan,
    kLessThanOrEqual,
  };
  Kind kind;

  static constexpr OpEffects effects =
      OpEffects()
          // We rely on the inputs having BigInt type.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  static bool IsCommutative(Kind kind) { return kind == Kind::kEqual; }

  V<BigInt> left() const { return Base::input<BigInt>(0); }
  V<BigInt> right() const { return Base::input<BigInt>(1); }

  BigIntComparisonOp(V<BigInt> left, V<BigInt> right, Kind kind)
      : Base(left, right), kind(kind) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           BigIntComparisonOp::Kind kind);

struct BigIntUnaryOp : FixedArityOperationT<1, BigIntUnaryOp> {
  enum class Kind : uint8_t {
    kNegate,
  };
  Kind kind;

  static constexpr OpEffects effects =
      OpEffects()
          // BigInt content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the input being a BigInt.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<BigInt> input() const { return Base::input<BigInt>(0); }

  BigIntUnaryOp(V<BigInt> input, Kind kind) : Base(input), kind(kind) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           BigIntUnaryOp::Kind kind);

struct LoadRootRegisterOp : FixedArityOperationT<0, LoadRootRegisterOp> {
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::WordPtr()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  LoadRootRegisterOp() : Base() {}
  void Validate(const Graph& graph) const {}
  std::tuple<> options() const { return {}; }
};

struct StringAtOp : FixedArityOperationT<2, StringAtOp> {
  enum class Kind : uint8_t {
    kCharCode,
    kCodePoint,
  };
  Kind kind;

  static constexpr OpEffects effects =
      // String content is immutable, so this operation is pure.
      OpEffects()
          // We rely on the input being a string.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::WordPtr()>();
  }

  V<String> string() const { return Base::input<String>(0); }
  V<WordPtr> position() const { return Base::input<WordPtr>(1); }

  StringAtOp(V<String> string, V<WordPtr> position, Kind kind)
      : Base(string, position), kind(kind) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           StringAtOp::Kind kind);

#ifdef V8_INTL_SUPPORT
struct StringToCaseIntlOp : FixedArityOperationT<1, StringToCaseIntlOp> {
  enum class Kind : uint8_t {
    kLower,
    kUpper,
  };
  Kind kind;

  static constexpr OpEffects effects =
      OpEffects()
          // String content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the input being a string.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<String> string() const { return Base::input<String>(0); }

  StringToCaseIntlOp(V<String> string, Kind kind) : Base(string), kind(kind) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           StringToCaseIntlOp::Kind kind);
#endif  // V8_INTL_SUPPORT

struct StringLengthOp : FixedArityOperationT<1, StringLengthOp> {
  static constexpr OpEffects effects =
      // String content is immutable, so this operation is pure.
      OpEffects()
          // We rely on the input being a string.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<String> string() const { return Base::input<String>(0); }

  explicit StringLengthOp(V<String> string) : Base(string) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct StringIndexOfOp : FixedArityOperationT<3, StringIndexOfOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // String content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the inputs being strings.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  // Search the string `search` within the string `string` starting at
  // `position`.
  V<String> string() const { return Base::input<String>(0); }
  V<String> search() const { return Base::input<String>(1); }
  V<Smi> position() const { return Base::input<Smi>(2); }

  StringIndexOfOp(V<String> string, V<String> search, V<Smi> position)
      : Base(string, search, position) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct StringFromCodePointAtOp
    : FixedArityOperationT<2, StringFromCodePointAtOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // String content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the input being in a certain char range.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::WordPtr()>();
  }

  V<String> string() const { return Base::input<String>(0); }
  V<WordPtr> index() const { return Base::input<WordPtr>(1); }

  StringFromCodePointAtOp(V<String> string, V<WordPtr> index)
      : Base(string, index) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct StringSubstringOp : FixedArityOperationT<3, StringSubstringOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // String content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the input being a string.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  V<String> string() const { return Base::input<String>(0); }
  V<Word32> start() const { return Base::input<Word32>(1); }
  V<Word32> end() const { return Base::input<Word32>(2); }

  StringSubstringOp(V<String> string, V<Word32> start, V<Word32> end)
      : Base(string, start, end) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct StringConcatOp : FixedArityOperationT<2, StringConcatOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // String content is immutable, the allocated result does not have
          // identity.
          .CanAllocateWithoutIdentity()
          // We rely on the inputs being strings.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<String> left() const { return Base::input<String>(0); }
  V<String> right() const { return Base::input<String>(1); }

  StringConcatOp(V<String> left, V<String> right) : Base(left, right) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct StringComparisonOp : FixedArityOperationT<2, StringComparisonOp> {
  enum class Kind : uint8_t {
    kEqual,
    kLessThan,
    kLessThanOrEqual,
  };
  Kind kind;

  static constexpr OpEffects effects =
      // String content is immutable, so the operation is pure.
      OpEffects()
          // We rely on the input being strings.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  static bool IsCommutative(Kind kind) { return kind == Kind::kEqual; }

  V<String> left() const { return Base::input<String>(0); }
  V<String> right() const { return Base::input<String>(1); }

  StringComparisonOp(V<String> left, V<String> right, Kind kind)
      : Base(left, right), kind(kind) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           StringComparisonOp::Kind kind);

struct ArgumentsLengthOp : FixedArityOperationT<0, ArgumentsLengthOp> {
  enum class Kind : uint8_t {
    kArguments,
    kRest,
  };
  Kind kind;
  int formal_parameter_count =
      0;  // This field is unused for kind == kArguments.

  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit ArgumentsLengthOp(Kind kind, int formal_parameter_count)
      : Base(), kind(kind), formal_parameter_count(formal_parameter_count) {
    DCHECK_IMPLIES(kind == Kind::kArguments, formal_parameter_count == 0);
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind, formal_parameter_count}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           ArgumentsLengthOp::Kind kind);

struct NewArgumentsElementsOp
    : FixedArityOperationT<1, NewArgumentsElementsOp> {
  CreateArgumentsType type;
  int formal_parameter_count;

  static constexpr OpEffects effects =
      OpEffects()
          // Allocate the fixed array, which has identity.
          .CanAllocate()
          // Do not move the allocation before checks/branches.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex arguments_count() const { return Base::input(0); }

  NewArgumentsElementsOp(OpIndex arguments_count, CreateArgumentsType type,
                         int formal_parameter_count)
      : Base(arguments_count),
        type(type),
        formal_parameter_count(formal_parameter_count) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{type, formal_parameter_count}; }
};

inline constexpr RegisterRepresentation RegisterRepresentationForArrayType(
    ExternalArrayType array_type) {
  switch (array_type) {
    case kExternalInt8Array:
    case kExternalUint8Array:
    case kExternalUint8ClampedArray:
    case kExternalInt16Array:
    case kExternalUint16Array:
    case kExternalInt32Array:
    case kExternalUint32Array:
      return RegisterRepresentation::Word32();
    case kExternalFloat32Array:
      return RegisterRepresentation::Float32();
    case kExternalFloat64Array:
      return RegisterRepresentation::Float64();
    case kExternalBigInt64Array:
    case kExternalBigUint64Array:
      return RegisterRepresentation::Word64();
    case kExternalFloat16Array:
      UNIMPLEMENTED();
  }
}

inline base::Vector<const RegisterRepresentation> VectorForRep(
    RegisterRepresentation rep) {
  static constexpr std::array<RegisterRepresentation, 6> table{
      RegisterRepresentation::Word32(),  RegisterRepresentation::Word64(),
      RegisterRepresentation::Float32(), RegisterRepresentation::Float64(),
      RegisterRepresentation::Tagged(),  RegisterRepresentation::Compressed()};
  return base::VectorOf(&table[static_cast<size_t>(rep.value())], 1);
}

struct LoadTypedElementOp : FixedArityOperationT<4, LoadTypedElementOp> {
  ExternalArrayType array_type;

  static constexpr OpEffects effects =
      OpEffects()
          // We read mutable memory.
          .CanReadMemory()
          // We rely on the input type and a valid index.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return VectorForRep(RegisterRepresentationForArrayType(array_type));
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::WordPtr(),
                          MaybeRegisterRepresentation::WordPtr()>();
  }

  OpIndex buffer() const { return Base::input(0); }
  OpIndex base() const { return Base::input(1); }
  OpIndex external() const { return Base::input(2); }
  OpIndex index() const { return Base::input(3); }

  LoadTypedElementOp(OpIndex buffer, OpIndex base, OpIndex external,
                     OpIndex index, ExternalArrayType array_type)
      : Base(buffer, base, external, index), array_type(array_type) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{array_type}; }
};

struct LoadDataViewElementOp : FixedArityOperationT<4, LoadDataViewElementOp> {
  ExternalArrayType element_type;

  static constexpr OpEffects effects = OpEffects()
                                           // We read mutable memory.
                                           .CanReadMemory()
                                           // We rely on the input type.
                                           .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return VectorForRep(RegisterRepresentationForArrayType(element_type));
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::WordPtr(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  OpIndex object() const { return Base::input(0); }
  OpIndex storage() const { return Base::input(1); }
  OpIndex index() const { return Base::input(2); }
  OpIndex is_little_endian() const { return Base::input(3); }

  LoadDataViewElementOp(OpIndex object, OpIndex storage, OpIndex index,
                        OpIndex is_little_endian,
                        ExternalArrayType element_type)
      : Base(object, storage, index, is_little_endian),
        element_type(element_type) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{element_type}; }
};

struct LoadStackArgumentOp : FixedArityOperationT<2, LoadStackArgumentOp> {
  // Stack arguments are immutable, so reading them is pure.
  static constexpr OpEffects effects =
      OpEffects()
          // We rely on the input being in bounds.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr(),
                          MaybeRegisterRepresentation::WordPtr()>();
  }

  OpIndex base() const { return Base::input(0); }
  OpIndex index() const { return Base::input(1); }

  LoadStackArgumentOp(OpIndex base, OpIndex index) : Base(base, index) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct StoreTypedElementOp : FixedArityOperationT<5, StoreTypedElementOp> {
  ExternalArrayType array_type;

  static constexpr OpEffects effects =
      OpEffects()
          // We are reading the backing store pointer and writing into it.
          .CanReadMemory()
          .CanWriteMemory()
          // We rely on the input type and a valid index.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(
        storage,
        {RegisterRepresentation::Tagged(), RegisterRepresentation::Tagged(),
         RegisterRepresentation::WordPtr(), RegisterRepresentation::WordPtr(),
         RegisterRepresentationForArrayType(array_type)});
  }

  OpIndex buffer() const { return Base::input(0); }
  OpIndex base() const { return Base::input(1); }
  OpIndex external() const { return Base::input(2); }
  OpIndex index() const { return Base::input(3); }
  OpIndex value() const { return Base::input(4); }

  StoreTypedElementOp(OpIndex buffer, OpIndex base, OpIndex external,
                      OpIndex index, OpIndex value,
                      ExternalArrayType array_type)
      : Base(buffer, base, external, index, value), array_type(array_type) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{array_type}; }
};

struct StoreDataViewElementOp
    : FixedArityOperationT<5, StoreDataViewElementOp> {
  ExternalArrayType element_type;

  static constexpr OpEffects effects =
      OpEffects()
          // We are reading the backing store pointer and writing into it.
          .CanReadMemory()
          .CanWriteMemory()
          // We rely on the input type and a valid index.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(
        storage,
        {RegisterRepresentation::Tagged(), RegisterRepresentation::Tagged(),
         RegisterRepresentation::WordPtr(),
         RegisterRepresentationForArrayType(element_type),
         RegisterRepresentation::Word32()});
  }

  OpIndex object() const { return Base::input(0); }
  OpIndex storage() const { return Base::input(1); }
  OpIndex index() const { return Base::input(2); }
  OpIndex value() const { return Base::input(3); }
  OpIndex is_little_endian() const { return Base::input(4); }

  StoreDataViewElementOp(OpIndex object, OpIndex storage, OpIndex index,
                         OpIndex value, OpIndex is_little_endian,
                         ExternalArrayType element_type)
      : Base(object, storage, index, value, is_little_endian),
        element_type(element_type) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{element_type}; }
};

struct TransitionAndStoreArrayElementOp
    : FixedArityOperationT<3, TransitionAndStoreArrayElementOp> {
  enum class Kind : uint8_t {
    kElement,
    kNumberElement,
    kOddballElement,
    kNonNumberElement,
    kSignedSmallElement,
  };
  Kind kind;
  MaybeHandle<Map> fast_map;
  MaybeHandle<Map> double_map;

  static constexpr OpEffects effects =
      OpEffects()
          // We are reading and writing mutable memory.
          .CanReadHeapMemory()
          .CanWriteHeapMemory()
          // We rely on the input type and a valid index.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(
        storage, {RegisterRepresentation::Tagged(),
                  RegisterRepresentation::WordPtr(), value_representation()});
  }

  OpIndex array() const { return Base::input(0); }
  OpIndex index() const { return Base::input(1); }
  OpIndex value() const { return Base::input(2); }

  TransitionAndStoreArrayElementOp(OpIndex array, OpIndex index, OpIndex value,
                                   Kind kind, MaybeHandle<Map> fast_map,
                                   MaybeHandle<Map> double_map)
      : Base(array, index, value),
        kind(kind),
        fast_map(fast_map),
        double_map(double_map) {}

  void Validate(const Graph& graph) const {}

  RegisterRepresentation value_representation() const {
    switch (kind) {
      case Kind::kElement:
      case Kind::kNonNumberElement:
      case Kind::kOddballElement:
        return RegisterRepresentation::Tagged();
      case Kind::kNumberElement:
        return RegisterRepresentation::Float64();
      case Kind::kSignedSmallElement:
        return RegisterRepresentation::Word32();
    }
  }

  size_t hash_value(
      HashingStrategy strategy = HashingStrategy::kDefault) const {
    DCHECK_EQ(strategy, HashingStrategy::kDefault);
    return HashWithOptions(fast_map.address(), double_map.address());
  }

  bool operator==(const TransitionAndStoreArrayElementOp& other) const {
    return kind == other.kind && fast_map.equals(other.fast_map) &&
           double_map.equals(other.double_map);
  }

  auto options() const { return std::tuple{kind, fast_map, double_map}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(
    std::ostream& os, TransitionAndStoreArrayElementOp::Kind kind);

struct CompareMapsOp : FixedArityOperationT<1, CompareMapsOp> {
  ZoneRefSet<Map> maps;

  static constexpr OpEffects effects = OpEffects().CanReadHeapMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<HeapObject> heap_object() const { return Base::input<HeapObject>(0); }

  CompareMapsOp(V<HeapObject> heap_object, ZoneRefSet<Map> maps)
      : Base(heap_object), maps(std::move(maps)) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{maps}; }
};

struct CheckMapsOp : FixedArityOperationT<2, CheckMapsOp> {
  CheckMapsFlags flags;
  ZoneRefSet<Map> maps;
  FeedbackSource feedback;

  // TODO(tebbi): Map checks without map transitions have less effects.
  static constexpr OpEffects effects = OpEffects()
                                           .CanDependOnChecks()
                                           .CanDeopt()
                                           .CanReadHeapMemory()
                                           .CanWriteHeapMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<HeapObject> heap_object() const { return Base::input<HeapObject>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  CheckMapsOp(V<HeapObject> heap_object, V<FrameState> frame_state,
              ZoneRefSet<Map> maps, CheckMapsFlags flags,
              const FeedbackSource& feedback)
      : Base(heap_object, frame_state),
        flags(flags),
        maps(std::move(maps)),
        feedback(feedback) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{maps, flags, feedback}; }
};

// AssumeMaps are inserted after CheckMaps have been lowered, in order to keep
// map information around and easily accessible for subsequent optimization
// passes (Load Elimination for instance can then use those AssumeMap to
// determine that some objects don't alias because they have different maps).
struct AssumeMapOp : FixedArityOperationT<1, AssumeMapOp> {
  ZoneRefSet<Map> maps;
  // AssumeMap should not be scheduled before the preceding CheckMaps
  static constexpr OpEffects effects = OpEffects()
                                           .CanDependOnChecks()
                                           .CanReadHeapMemory()
                                           .CanChangeControlFlow();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<HeapObject> heap_object() const { return Base::input<HeapObject>(0); }

  AssumeMapOp(V<HeapObject> heap_object, ZoneRefSet<Map> maps)
      : Base(heap_object), maps(std::move(maps)) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{maps}; }
};

struct CheckedClosureOp : FixedArityOperationT<2, CheckedClosureOp> {
  Handle<FeedbackCell> feedback_cell;

  // We only check immutable aspects of the incoming value.
  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> input() const { return Base::input<Object>(0); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(1); }

  CheckedClosureOp(V<Object> input, V<FrameState> frame_state,
                   Handle<FeedbackCell> feedback_cell)
      : Base(input, frame_state), feedback_cell(feedback_cell) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  bool operator==(const CheckedClosureOp& other) const {
    return feedback_cell.address() == other.feedback_cell.address();
  }
  size_t hash_value(
      HashingStrategy strategy = HashingStrategy::kDefault) const {
    DCHECK_EQ(strategy, HashingStrategy::kDefault);
    return HashWithOptions(feedback_cell.address());
  }

  auto options() const { return std::tuple{feedback_cell}; }
};

struct CheckEqualsInternalizedStringOp
    : FixedArityOperationT<3, CheckEqualsInternalizedStringOp> {
  static constexpr OpEffects effects = OpEffects().CanDeopt();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<Object> expected() const { return Base::input<Object>(0); }
  V<Object> value() const { return Base::input<Object>(1); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(2); }

  CheckEqualsInternalizedStringOp(V<Object> expected, V<Object> value,
                                  V<FrameState> frame_state)
      : Base(expected, value, frame_state) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{}; }
};

struct LoadMessageOp : FixedArityOperationT<1, LoadMessageOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // We are reading the message from the isolate.
          .CanReadOffHeapMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr()>();
  }

  V<WordPtr> offset() const { return Base::input<WordPtr>(0); }

  explicit LoadMessageOp(V<WordPtr> offset) : Base(offset) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct StoreMessageOp : FixedArityOperationT<2, StoreMessageOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // We are writing the message in the isolate.
          .CanWriteOffHeapMemory();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  V<WordPtr> offset() const { return Base::input<WordPtr>(0); }
  V<Object> object() const { return Base::input<Object>(1); }

  explicit StoreMessageOp(V<WordPtr> offset, V<Object> object)
      : Base(offset, object) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{}; }
};

struct SameValueOp : FixedArityOperationT<2, SameValueOp> {
  enum class Mode : uint8_t {
    kSameValue,
    kSameValueNumbersOnly,
  };
  Mode mode;

  static constexpr OpEffects effects =
      OpEffects()
          // We might depend on the inputs being numbers.
          .CanDependOnChecks();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex left() const { return Base::input(0); }
  OpIndex right() const { return Base::input(1); }

  SameValueOp(OpIndex left, OpIndex right, Mode mode)
      : Base(left, right), mode(mode) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{mode}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           SameValueOp::Mode mode);

struct Float64SameValueOp : FixedArityOperationT<2, Float64SameValueOp> {
  static constexpr OpEffects effects = OpEffects();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Float64(),
                          MaybeRegisterRepresentation::Float64()>();
  }

  V<Float64> left() const { return Base::input<Float64>(0); }
  V<Float64> right() const { return Base::input<Float64>(1); }

  Float64SameValueOp(V<Float64> left, V<Float64> right) : Base(left, right) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct FastApiCallParameters : public NON_EXPORTED_BASE(ZoneObject) {
  const FastApiCallFunctionVector c_functions;
  fast_api_call::OverloadsResolutionResult resolution_result;

  const CFunctionInfo* c_signature() const { return c_functions[0].signature; }

  FastApiCallParameters(
      const FastApiCallFunctionVector& c_functions,
      const fast_api_call::OverloadsResolutionResult& resolution_result)
      : c_functions(c_functions), resolution_result(resolution_result) {
    DCHECK_LT(0, c_functions.size());
  }

  static const FastApiCallParameters* Create(
      const FastApiCallFunctionVector& c_functions,
      const fast_api_call::OverloadsResolutionResult& resolution_result,
      Zone* graph_zone) {
    return graph_zone->New<FastApiCallParameters>(std::move(c_functions),
                                                  resolution_result);
  }
};

struct FastApiCallOp : OperationT<FastApiCallOp> {
  static constexpr uint32_t kSuccessValue = 1;
  static constexpr uint32_t kFailureValue = 0;

  const FastApiCallParameters* parameters;

  // FastApiCallOp has two outputs so far:
  // (1) a `should_fallback` flag, indicating that a slow call should be done;
  // (2) the actual return value, which is always tagged.
  // TODO(ahaas) Remove the `should_fallback` flag once fast api functions don't
  // use it anymore.
  THROWING_OP_BOILERPLATE(RegisterRepresentation::Word32(),
                          RegisterRepresentation::Tagged())

  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  // There are three inputs that are not parameters, the frame state, the data
  // argument, and the context.
  static constexpr int kNumNonParamInputs = 3;

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    DCHECK_EQ(inputs().size(),
              kNumNonParamInputs + parameters->c_signature()->ArgumentCount());
    storage.resize(inputs().size());
    storage[0] = MaybeRegisterRepresentation::None();
    storage[1] = MaybeRegisterRepresentation::Tagged();
    storage[2] = MaybeRegisterRepresentation::Tagged();
    for (unsigned i = 0; i < parameters->c_signature()->ArgumentCount(); ++i) {
      storage[i + kNumNonParamInputs] = argument_representation(i);
    }
    return base::VectorOf(storage);
  }

  MaybeRegisterRepresentation argument_representation(
      unsigned argument_index) const {
    const CTypeInfo& arg_type =
        parameters->c_signature()->ArgumentInfo(argument_index);
    uint8_t flags = static_cast<uint8_t>(arg_type.GetFlags());
    switch (arg_type.GetSequenceType()) {
      case CTypeInfo::SequenceType::kScalar:
        if (flags & (static_cast<uint8_t>(CTypeInfo::Flags::kEnforceRangeBit) |
                     static_cast<uint8_t>(CTypeInfo::Flags::kClampBit))) {
          return MaybeRegisterRepresentation::Float64();
        }
        switch (arg_type.GetType()) {
          case CTypeInfo::Type::kVoid:
            UNREACHABLE();
          case CTypeInfo::Type::kBool:
          case CTypeInfo::Type::kUint8:
          case CTypeInfo::Type::kInt32:
          case CTypeInfo::Type::kUint32:
            return MaybeRegisterRepresentation::Word32();
          case CTypeInfo::Type::kInt64:
          case CTypeInfo::Type::kUint64:
            return MaybeRegisterRepresentation::Word64();
          case CTypeInfo::Type::kV8Value:
          case CTypeInfo::Type::kApiObject:
          case CTypeInfo::Type::kPointer:
          case CTypeInfo::Type::kSeqOneByteString:
            return MaybeRegisterRepresentation::Tagged();
          case CTypeInfo::Type::kFloat32:
          case CTypeInfo::Type::kFloat64:
            return MaybeRegisterRepresentation::Float64();
          case CTypeInfo::Type::kAny:
            // As the register representation is unknown, just treat it as None
            // to prevent any validation.
            return MaybeRegisterRepresentation::None();
        }
      case CTypeInfo::SequenceType::kIsSequence:
      case CTypeInfo::SequenceType::kIsTypedArray:
        return MaybeRegisterRepresentation::Tagged();
      case CTypeInfo::SequenceType::kIsArrayBuffer:
        UNREACHABLE();
    }
  }

  V<FrameState> frame_state() const { return input<FrameState>(0); }

  V<Object> data_argument() const { return input<Object>(1); }

  V<Context> context() const { return input<Context>(2); }

  base::Vector<const OpIndex> arguments() const {
    return inputs().SubVector(kNumNonParamInputs, inputs().size());
  }

  FastApiCallOp(V<FrameState> frame_state, V<Object> data_argument,
                V<Context> context, base::Vector<const OpIndex> arguments,
                const FastApiCallParameters* parameters)
      : Base(kNumNonParamInputs + arguments.size()),
        parameters(parameters),
        lazy_deopt_on_throw(LazyDeoptOnThrow::kNo) {
    base::Vector<OpIndex> inputs = this->inputs();
    inputs[0] = frame_state;
    inputs[1] = data_argument;
    inputs[2] = context;
    inputs.SubVector(kNumNonParamInputs, kNumNonParamInputs + arguments.size())
        .OverwriteWith(arguments);
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    V<FrameState> mapped_frame_state = mapper.Map(frame_state());
    OpIndex mapped_data_argument = mapper.Map(data_argument());
    V<Context> mapped_context = mapper.Map(context());
    auto mapped_arguments = mapper.template Map<8>(arguments());
    return fn(mapped_frame_state, mapped_data_argument, mapped_context,
              base::VectorOf(mapped_arguments), parameters);
  }

  void Validate(const Graph& graph) const {
  }

  static FastApiCallOp& New(Graph* graph, V<FrameState> frame_state,
                            V<Object> data_argument, V<Context> context,
                            base::Vector<const OpIndex> arguments,
                            const FastApiCallParameters* parameters) {
    return Base::New(graph, kNumNonParamInputs + arguments.size(), frame_state,
                     data_argument, context, arguments, parameters);
  }

  auto options() const { return std::tuple{parameters, lazy_deopt_on_throw}; }
};

struct RuntimeAbortOp : FixedArityOperationT<0, RuntimeAbortOp> {
  AbortReason reason;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  explicit RuntimeAbortOp(AbortReason reason) : reason(reason) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{reason}; }
};

struct EnsureWritableFastElementsOp
    : FixedArityOperationT<2, EnsureWritableFastElementsOp> {
  // TODO(tebbi): Can we have more precise effects here?
  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex object() const { return Base::input(0); }
  OpIndex elements() const { return Base::input(1); }

  EnsureWritableFastElementsOp(OpIndex object, OpIndex elements)
      : Base(object, elements) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct MaybeGrowFastElementsOp
    : FixedArityOperationT<5, MaybeGrowFastElementsOp> {
  GrowFastElementsMode mode;
  FeedbackSource feedback;

  // TODO(tebbi): Can we have more precise effects here?
  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Word32(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  V<Object> object() const { return Base::input<Object>(0); }
  V<Object> elements() const { return Base::input<Object>(1); }
  V<Word32> index() const { return Base::input<Word32>(2); }
  V<Word32> elements_length() const { return Base::input<Word32>(3); }
  V<FrameState> frame_state() const { return Base::input<FrameState>(4); }

  MaybeGrowFastElementsOp(V<Object> object, V<Object> elements, V<Word32> index,
                          V<Word32> elements_length, V<FrameState> frame_state,
                          GrowFastElementsMode mode,
                          const FeedbackSource& feedback)
      : Base(object, elements, index, elements_length, frame_state),
        mode(mode),
        feedback(feedback) {}

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{mode, feedback}; }
};

struct TransitionElementsKindOp
    : FixedArityOperationT<1, TransitionElementsKindOp> {
  ElementsTransition transition;

  static constexpr OpEffects effects = OpEffects().CanCallAnything();
  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex object() const { return Base::input(0); }

  TransitionElementsKindOp(OpIndex object, const ElementsTransition& transition)
      : Base(object), transition(transition) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{transition}; }
};

struct FindOrderedHashEntryOp
    : FixedArityOperationT<2, FindOrderedHashEntryOp> {
  enum class Kind : uint8_t {
    kFindOrderedHashMapEntry,
    kFindOrderedHashMapEntryForInt32Key,
    kFindOrderedHashSetEntry,
  };
  Kind kind;

  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().CanReadMemory().CanAllocate();
  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case Kind::kFindOrderedHashMapEntry:
      case Kind::kFindOrderedHashSetEntry:
        return RepVector<RegisterRepresentation::Tagged()>();
      case Kind::kFindOrderedHashMapEntryForInt32Key:
        return RepVector<RegisterRepresentation::WordPtr()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return kind == Kind::kFindOrderedHashMapEntryForInt32Key
               ? MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                                MaybeRegisterRepresentation::Word32()>()
               : MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                                MaybeRegisterRepresentation::Tagged()>();
  }

  OpIndex data_structure() const { return Base::input(0); }
  OpIndex key() const { return Base::input(1); }

  FindOrderedHashEntryOp(OpIndex data_structure, OpIndex key, Kind kind)
      : Base(data_structure, key), kind(kind) {}

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           FindOrderedHashEntryOp::Kind kind);

struct CommentOp : FixedArityOperationT<0, CommentOp> {
  const char* message;

  // Comments should not be removed.
  static constexpr OpEffects effects = OpEffects().RequiredWhenUnused();

  explicit CommentOp(const char* message) : message(message) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{message}; }
};

struct SpeculativeNumberBinopOp
    : FixedArityOperationT<3, SpeculativeNumberBinopOp> {
  enum class Kind : uint8_t {
    kSafeIntegerAdd,
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects().CanDeopt().CanAllocate();

  OpIndex left() const { return Base::input(0); }
  OpIndex right() const { return Base::input(1); }
  OpIndex frame_state() const { return Base::input(2); }

  SpeculativeNumberBinopOp(OpIndex left, OpIndex right, OpIndex frame_state,
                           Kind kind)
      : Base(left, right, frame_state), kind(kind) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
    DCHECK(Get(graph, frame_state()).Is<FrameStateOp>());
  }

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           SpeculativeNumberBinopOp::Kind kind);

#if V8_ENABLE_WEBASSEMBLY

V8_EXPORT_PRIVATE const RegisterRepresentation& RepresentationFor(
    wasm::ValueType type);

struct GlobalGetOp : FixedArityOperationT<1, GlobalGetOp> {
  const wasm::WasmGlobal* global;
  static constexpr OpEffects effects = OpEffects().CanReadMemory();

  V<WasmTrustedInstanceData> instance() const {
    return input<WasmTrustedInstanceData>(0);
  }

  GlobalGetOp(V<WasmTrustedInstanceData> instance,
              const wasm::WasmGlobal* global)
      : Base(instance), global(global) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    const RegisterRepresentation& repr = RepresentationFor(global->type);
    return base::VectorOf(&repr, 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{global}; }
};

struct GlobalSetOp : FixedArityOperationT<2, GlobalSetOp> {
  const wasm::WasmGlobal* global;
  static constexpr OpEffects effects = OpEffects().CanWriteMemory();

  V<WasmTrustedInstanceData> instance() const {
    return input<WasmTrustedInstanceData>(0);
  }
  V<Any> value() const { return input<Any>(1); }

  explicit GlobalSetOp(V<WasmTrustedInstanceData> instance, V<Any> value,
                       const wasm::WasmGlobal* global)
      : Base(instance, value), global(global) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(2);
    storage[0] = MaybeRegisterRepresentation::Tagged();
    storage[1] = MaybeRegisterRepresentation(RepresentationFor(global->type));
    return base::VectorOf(storage);
  }

  void Validate(const Graph& graph) const { DCHECK(global->mutability); }

  auto options() const { return std::tuple{global}; }
};

struct NullOp : FixedArityOperationT<0, NullOp> {
  wasm::ValueType type;
  static constexpr OpEffects effects = OpEffects();

  explicit NullOp(wasm::ValueType type) : Base(), type(type) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {
    DCHECK(type.is_object_reference() && type.is_nullable());
  }

  auto options() const { return std::tuple{type}; }
};

struct IsNullOp : FixedArityOperationT<1, IsNullOp> {
  wasm::ValueType type;
  static constexpr OpEffects effects = OpEffects();

  V<Object> object() const { return input<Object>(0); }

  IsNullOp(V<Object> object, wasm::ValueType type) : Base(object), type(type) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
  }

  auto options() const { return std::tuple{type}; }
};

// Traps on a null input, otherwise returns the input, type-cast to the
// respective non-nullable type.
struct AssertNotNullOp : FixedArityOperationT<1, AssertNotNullOp> {
  wasm::ValueType type;
  TrapId trap_id;

  // Lowers to a trap and inherits {TrapIf}'s effects.
  static constexpr OpEffects effects =
      OpEffects().CanDependOnChecks().CanLeaveCurrentFunction();

  V<Object> object() const { return input<Object>(0); }

  AssertNotNullOp(V<Object> object, wasm::ValueType type, TrapId trap_id)
      : Base(object), type(type), trap_id(trap_id) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
    // TODO(14108): Validate.
  }

  auto options() const { return std::tuple{type, trap_id}; }
};

// The runtime type (RTT) is a value representing a conrete type (in this case
// heap-type). The canonical RTTs are implicitly created values and invisible to
// the user in wasm-gc MVP. (See
// https://github.com/WebAssembly/gc/blob/main/proposals/gc/MVP.md#runtime-types)
struct RttCanonOp : FixedArityOperationT<1, RttCanonOp> {
  uint32_t type_index;

  static constexpr OpEffects effects = OpEffects();

  explicit RttCanonOp(V<FixedArray> rtts, uint32_t type_index)
      : Base(rtts), type_index(type_index) {}

  V<FixedArray> rtts() const { return input<FixedArray>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }
  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{type_index}; }
};

struct WasmTypeCheckOp : OperationT<WasmTypeCheckOp> {
  WasmTypeCheckConfig config;

  static constexpr OpEffects effects = OpEffects().AssumesConsistentHeap();

  WasmTypeCheckOp(V<Object> object, OptionalV<Map> rtt,
                  WasmTypeCheckConfig config)
      : Base(1 + rtt.valid()), config(config) {
    input(0) = object;
    if (rtt.valid()) {
      input(1) = rtt.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(object()), mapper.Map(rtt()), config);
  }

  V<Object> object() const { return Base::input<Object>(0); }
  OptionalV<Map> rtt() const {
    return input_count > 1 ? input<Map>(1) : OptionalV<Map>::Nullopt();
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return input_count > 1
               ? MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                                MaybeRegisterRepresentation::Tagged()>()
               : MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{config}; }

  static WasmTypeCheckOp& New(Graph* graph, V<Object> object,
                              OptionalV<Map> rtt, WasmTypeCheckConfig config) {
    return Base::New(graph, 1 + rtt.valid(), object, rtt, config);
  }
};

struct WasmTypeCastOp : OperationT<WasmTypeCastOp> {
  WasmTypeCheckConfig config;

  static constexpr OpEffects effects = OpEffects().CanLeaveCurrentFunction();

  WasmTypeCastOp(V<Object> object, OptionalV<Map> rtt,
                 WasmTypeCheckConfig config)
      : Base(1 + rtt.valid()), config(config) {
    input(0) = object;
    if (rtt.valid()) {
      input(1) = rtt.value();
    }
  }

  template <typename Fn, typename Mapper>
  V8_INLINE auto Explode(Fn fn, Mapper& mapper) const {
    return fn(mapper.Map(object()), mapper.Map(rtt()), config);
  }

  V<Object> object() const { return Base::input<Object>(0); }
  OptionalV<Map> rtt() const {
    return input_count > 1 ? input<Map>(1) : OptionalV<Map>::Nullopt();
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return input_count > 1
               ? MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                                MaybeRegisterRepresentation::Tagged()>()
               : MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{config}; }

  static WasmTypeCastOp& New(Graph* graph, V<Object> object, OptionalV<Map> rtt,
                             WasmTypeCheckConfig config) {
    return Base::New(graph, 1 + rtt.valid(), object, rtt, config);
  }
};

// Annotate a value with a wasm type.
// This is a helper operation to propagate type information from the graph
// builder to type-based optimizations and will then be removed.
struct WasmTypeAnnotationOp : FixedArityOperationT<1, WasmTypeAnnotationOp> {
  static constexpr OpEffects effects = OpEffects();
  wasm::ValueType type;

  explicit WasmTypeAnnotationOp(V<Object> value, wasm::ValueType type)
      : Base(value), type(type) {}

  V<Object> value() const { return Base::input<Object>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
    // In theory, the operation could be used for non-reference types as well.
    // This would require updating inputs_rep and outputs_rep to be based on
    // the wasm type.
    DCHECK(type.is_object_reference());
  }

  auto options() const { return std::tuple(type); }
};

struct AnyConvertExternOp : FixedArityOperationT<1, AnyConvertExternOp> {
  static constexpr OpEffects effects =
      SmiValuesAre31Bits() ? OpEffects().CanReadMemory()
                           : OpEffects().CanReadMemory().CanAllocate();

  explicit AnyConvertExternOp(V<Object> object) : Base(object) {}

  V<Object> object() const { return Base::input<Object>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple(); }
};

struct ExternConvertAnyOp : FixedArityOperationT<1, ExternConvertAnyOp> {
  static constexpr OpEffects effects = OpEffects();

  explicit ExternConvertAnyOp(V<Object> object) : Base(object) {}

  V<Object> object() const { return Base::input<Object>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple(); }
};

struct StructGetOp : FixedArityOperationT<1, StructGetOp> {
  bool is_signed;  // `false` only for unsigned packed type accesses.
  CheckForNull null_check;
  const wasm::StructType* type;
  uint32_t type_index;
  int field_index;

  OpEffects Effects() const {
    OpEffects result =
        OpEffects()
            // This should not float above a protective null check.
            .CanDependOnChecks()
            .CanReadMemory();
    if (null_check == kWithNullCheck) {
      // This may trap.
      result = result.CanLeaveCurrentFunction();
    }
    return result;
  }

  StructGetOp(V<WasmStructNullable> object, const wasm::StructType* type,
              uint32_t type_index, int field_index, bool is_signed,
              CheckForNull null_check)
      : Base(object),
        is_signed(is_signed),
        null_check(null_check),
        type(type),
        type_index(type_index),
        field_index(field_index) {}

  V<WasmStructNullable> object() const { return input<WasmStructNullable>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&RepresentationFor(type->field(field_index)), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {
    DCHECK_LT(field_index, type->field_count());
    DCHECK_IMPLIES(!is_signed, type->field(field_index).is_packed());
  }

  auto options() const {
    return std::tuple{type, type_index, field_index, is_signed, null_check};
  }
};

struct StructSetOp : FixedArityOperationT<2, StructSetOp> {
  CheckForNull null_check;
  const wasm::StructType* type;
  uint32_t type_index;
  int field_index;

  OpEffects Effects() const {
    OpEffects result =
        OpEffects()
            // This should not float above a protective null check.
            .CanDependOnChecks()
            .CanWriteMemory();
    if (null_check == kWithNullCheck) {
      // This may trap.
      result = result.CanLeaveCurrentFunction();
    }
    return result;
  }

  StructSetOp(V<WasmStructNullable> object, V<Any> value,
              const wasm::StructType* type, uint32_t type_index,
              int field_index, CheckForNull null_check)
      : Base(object, value),
        null_check(null_check),
        type(type),
        type_index(type_index),
        field_index(field_index) {}

  V<WasmStructNullable> object() const { return input<WasmStructNullable>(0); }
  V<Any> value() const { return input(1); }

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    storage.resize(2);
    storage[0] = RegisterRepresentation::Tagged();
    storage[1] = RepresentationFor(type->field(field_index));
    return base::VectorOf(storage);
  }

  void Validate(const Graph& graph) const {
    DCHECK_LT(field_index, type->field_count());
  }

  auto options() const {
    return std::tuple{type, type_index, field_index, null_check};
  }
};

struct ArrayGetOp : FixedArityOperationT<2, ArrayGetOp> {
  bool is_signed;
  const wasm::ArrayType* array_type;

  // ArrayGetOp may never trap as it is always protected by a length check.
  static constexpr OpEffects effects =
      OpEffects()
          // This should not float above a protective null/length check.
          .CanDependOnChecks()
          .CanReadMemory();

  ArrayGetOp(V<WasmArrayNullable> array, V<Word32> index,
             const wasm::ArrayType* array_type, bool is_signed)
      : Base(array, index), is_signed(is_signed), array_type(array_type) {}

  V<WasmArrayNullable> array() const { return input<WasmArrayNullable>(0); }
  V<Word32> index() const { return input<Word32>(1); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return base::VectorOf(&RepresentationFor(array_type->element_type()), 1);
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{array_type, is_signed}; }
  void PrintOptions(std::ostream& os) const;
};

struct ArraySetOp : FixedArityOperationT<3, ArraySetOp> {
  wasm::ValueType element_type;

  // ArraySetOp may never trap as it is always protected by a length check.
  static constexpr OpEffects effects =
      OpEffects()
          // This should not float above a protective null/length check.
          .CanDependOnChecks()
          .CanWriteMemory();

  ArraySetOp(V<WasmArrayNullable> array, V<Word32> index, V<Any> value,
             wasm::ValueType element_type)
      : Base(array, index, value), element_type(element_type) {}

  V<WasmArrayNullable> array() const { return input<WasmArrayNullable>(0); }
  V<Word32> index() const { return input<Word32>(1); }
  V<Any> value() const { return input<Any>(2); }

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(storage, {RegisterRepresentation::Tagged(),
                                  RegisterRepresentation::Word32(),
                                  RepresentationFor(element_type)});
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{element_type}; }
};

struct ArrayLengthOp : FixedArityOperationT<1, ArrayLengthOp> {
  CheckForNull null_check;

  OpEffects Effects() const {
    OpEffects result =
        OpEffects()
            // This should not float above a protective null check.
            .CanDependOnChecks()
            .CanReadMemory();
    if (null_check == kWithNullCheck) {
      // This may trap.
      result = result.CanLeaveCurrentFunction();
    }
    return result;
  }

  explicit ArrayLengthOp(V<WasmArrayNullable> array, CheckForNull null_check)
      : Base(array), null_check(null_check) {}

  V<WasmArrayNullable> array() const { return input<WasmArrayNullable>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{null_check}; }
};

struct WasmAllocateArrayOp : FixedArityOperationT<2, WasmAllocateArrayOp> {
  static constexpr OpEffects effects =
      OpEffects().CanAllocate().CanLeaveCurrentFunction();

  const wasm::ArrayType* array_type;

  explicit WasmAllocateArrayOp(V<Map> rtt, V<Word32> length,
                               const wasm::ArrayType* array_type)
      : Base(rtt, length), array_type(array_type) {}

  V<Map> rtt() const { return Base::input<Map>(0); }
  V<Word32> length() const { return Base::input<Word32>(1); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged(),
                          MaybeRegisterRepresentation::Word32()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{array_type}; }
  void PrintOptions(std::ostream& os) const;
};

struct WasmAllocateStructOp : FixedArityOperationT<1, WasmAllocateStructOp> {
  static constexpr OpEffects effects =
      OpEffects().CanAllocate().CanLeaveCurrentFunction();

  const wasm::StructType* struct_type;

  explicit WasmAllocateStructOp(V<Map> rtt, const wasm::StructType* struct_type)
      : Base(rtt), struct_type(struct_type) {}

  V<Map> rtt() const { return Base::input<Map>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{struct_type}; }
};

struct WasmRefFuncOp : FixedArityOperationT<1, WasmRefFuncOp> {
  static constexpr OpEffects effects = OpEffects().CanAllocate();
  uint32_t function_index;

  explicit WasmRefFuncOp(V<WasmTrustedInstanceData> wasm_instance,
                         uint32_t function_index)
      : Base(wasm_instance), function_index(function_index) {}

  V<WasmTrustedInstanceData> instance() const {
    return input<WasmTrustedInstanceData>(0);
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{function_index}; }
};

// Casts a JavaScript string to a flattened wtf16 string.
// TODO(14108): Can we optimize stringref operations without adding this as a
// special operations?
struct StringAsWtf16Op : FixedArityOperationT<1, StringAsWtf16Op> {
  static constexpr OpEffects effects =
      OpEffects()
          // This should not float above a protective null/length check.
          .CanDependOnChecks()
          .CanReadMemory();

  explicit StringAsWtf16Op(V<String> string) : Base(string) {}

  V<String> string() const { return input<String>(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

// Takes a flattened string and extracts the first string pointer, the base
// offset and the character width shift.
struct StringPrepareForGetCodeUnitOp
    : FixedArityOperationT<1, StringPrepareForGetCodeUnitOp> {
  static constexpr OpEffects effects =
      OpEffects()
          // This should not float above a protective null/length check.
          .CanDependOnChecks();

  explicit StringPrepareForGetCodeUnitOp(V<Object> string) : Base(string) {}

  OpIndex string() const { return input(0); }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged(),
                     RegisterRepresentation::WordPtr(),
                     RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Tagged()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

struct Simd128ConstantOp : FixedArityOperationT<0, Simd128ConstantOp> {
  static constexpr uint8_t kZero[kSimd128Size] = {};
  uint8_t value[kSimd128Size];

  static constexpr OpEffects effects = OpEffects();

  explicit Simd128ConstantOp(const uint8_t incoming_value[kSimd128Size])
      : Base() {
    std::copy(incoming_value, incoming_value + kSimd128Size, value);
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {
    // TODO(14108): Validate.
  }

  bool IsZero() const { return std::memcmp(kZero, value, kSimd128Size) == 0; }

  auto options() const { return std::tuple{value}; }
  void PrintOptions(std::ostream& os) const;
};

#define FOREACH_SIMD_128_BINARY_SIGN_EXTENSION_OPCODE(V) \
  V(I16x8ExtMulLowI8x16S)                                \
  V(I16x8ExtMulHighI8x16S)                               \
  V(I16x8ExtMulLowI8x16U)                                \
  V(I16x8ExtMulHighI8x16U)                               \
  V(I32x4ExtMulLowI16x8S)                                \
  V(I32x4ExtMulHighI16x8S)                               \
  V(I32x4ExtMulLowI16x8U)                                \
  V(I32x4ExtMulHighI16x8U)                               \
  V(I64x2ExtMulLowI32x4S)                                \
  V(I64x2ExtMulHighI32x4S)                               \
  V(I64x2ExtMulLowI32x4U)                                \
  V(I64x2ExtMulHighI32x4U)

#define FOREACH_SIMD_128_BINARY_BASIC_OPCODE(V) \
  V(I8x16Eq)                                    \
  V(I8x16Ne)                                    \
  V(I8x16GtS)                                   \
  V(I8x16GtU)                                   \
  V(I8x16GeS)                                   \
  V(I8x16GeU)                                   \
  V(I16x8Eq)                                    \
  V(I16x8Ne)                                    \
  V(I16x8GtS)                                   \
  V(I16x8GtU)                                   \
  V(I16x8GeS)                                   \
  V(I16x8GeU)                                   \
  V(I32x4Eq)                                    \
  V(I32x4Ne)                                    \
  V(I32x4GtS)                                   \
  V(I32x4GtU)                                   \
  V(I32x4GeS)                                   \
  V(I32x4GeU)                                   \
  V(F32x4Eq)                                    \
  V(F32x4Ne)                                    \
  V(F32x4Lt)                                    \
  V(F32x4Le)                                    \
  V(F64x2Eq)                                    \
  V(F64x2Ne)                                    \
  V(F64x2Lt)                                    \
  V(F64x2Le)                                    \
  V(S128And)                                    \
  V(S128AndNot)                                 \
  V(S128Or)                                     \
  V(S128Xor)                                    \
  V(I8x16SConvertI16x8)                         \
  V(I8x16UConvertI16x8)                         \
  V(I8x16Add)                                   \
  V(I8x16AddSatS)                               \
  V(I8x16AddSatU)                               \
  V(I8x16Sub)                                   \
  V(I8x16SubSatS)                               \
  V(I8x16SubSatU)                               \
  V(I8x16MinS)                                  \
  V(I8x16MinU)                                  \
  V(I8x16MaxS)                                  \
  V(I8x16MaxU)                                  \
  V(I8x16RoundingAverageU)                      \
  V(I16x8Q15MulRSatS)                           \
  V(I16x8SConvertI32x4)                         \
  V(I16x8UConvertI32x4)                         \
  V(I16x8Add)                                   \
  V(I16x8AddSatS)                               \
  V(I16x8AddSatU)                               \
  V(I16x8Sub)                                   \
  V(I16x8SubSatS)                               \
  V(I16x8SubSatU)                               \
  V(I16x8Mul)                                   \
  V(I16x8MinS)                                  \
  V(I16x8MinU)                                  \
  V(I16x8MaxS)                                  \
  V(I16x8MaxU)                                  \
  V(I16x8RoundingAverageU)                      \
  V(I32x4Add)                                   \
  V(I32x4Sub)                                   \
  V(I32x4Mul)                                   \
  V(I32x4MinS)                                  \
  V(I32x4MinU)                                  \
  V(I32x4MaxS)                                  \
  V(I32x4MaxU)                                  \
  V(I32x4DotI16x8S)                             \
  V(I64x2Add)                                   \
  V(I64x2Sub)                                   \
  V(I64x2Mul)                                   \
  V(I64x2Eq)                                    \
  V(I64x2Ne)                                    \
  V(I64x2GtS)                                   \
  V(I64x2GeS)                                   \
  V(F32x4Add)                                   \
  V(F32x4Sub)                                   \
  V(F32x4Mul)                                   \
  V(F32x4Div)                                   \
  V(F32x4Min)                                   \
  V(F32x4Max)                                   \
  V(F32x4Pmin)                                  \
  V(F32x4Pmax)                                  \
  V(F64x2Add)                                   \
  V(F64x2Sub)                                   \
  V(F64x2Mul)                                   \
  V(F64x2Div)                                   \
  V(F64x2Min)                                   \
  V(F64x2Max)                                   \
  V(F64x2Pmin)                                  \
  V(F64x2Pmax)                                  \
  V(F32x4RelaxedMin)                            \
  V(F32x4RelaxedMax)                            \
  V(F64x2RelaxedMin)                            \
  V(F64x2RelaxedMax)                            \
  V(I16x8RelaxedQ15MulRS)                       \
  V(I16x8DotI8x16I7x16S)                        \
  FOREACH_SIMD_128_BINARY_SIGN_EXTENSION_OPCODE(V)

#define FOREACH_SIMD_128_BINARY_SPECIAL_OPCODE(V) \
  V(I8x16Swizzle)                                 \
  V(I8x16RelaxedSwizzle)

#define FOREACH_SIMD_128_BINARY_MANDATORY_OPCODE(V) \
  FOREACH_SIMD_128_BINARY_BASIC_OPCODE(V)           \
  FOREACH_SIMD_128_BINARY_SPECIAL_OPCODE(V)

#define FOREACH_SIMD_128_BINARY_OPTIONAL_OPCODE(V) \
  V(F16x8Add)                                      \
  V(F16x8Sub)                                      \
  V(F16x8Mul)                                      \
  V(F16x8Div)                                      \
  V(F16x8Min)                                      \
  V(F16x8Max)                                      \
  V(F16x8Pmin)                                     \
  V(F16x8Pmax)                                     \
  V(F16x8Eq)                                       \
  V(F16x8Ne)                                       \
  V(F16x8Lt)                                       \
  V(F16x8Le)

#define FOREACH_SIMD_128_BINARY_OPCODE(V)     \
  FOREACH_SIMD_128_BINARY_MANDATORY_OPCODE(V) \
  FOREACH_SIMD_128_BINARY_OPTIONAL_OPCODE(V)

struct Simd128BinopOp : FixedArityOperationT<2, Simd128BinopOp> {
  // clang-format off
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_BINARY_OPCODE(DEFINE_KIND)
    kFirstSignExtensionOp = kI16x8ExtMulLowI8x16S,
    kLastSignExtensionOp = kI64x2ExtMulHighI32x4U,
#undef DEFINE_KIND
  };
  // clang-format on

  Kind kind;

  static bool IsCommutative(Kind kind) {
    switch (kind) {
      // TODO(14108): Explicitly list all commutative SIMD operations.
      case Kind::kI64x2Add:
      case Kind::kI32x4Add:
      case Kind::kI16x8Add:
      case Kind::kI8x16Add:
      case Kind::kF64x2Add:
      case Kind::kF32x4Add:

      case Kind::kI64x2Mul:
      case Kind::kI32x4Mul:
      case Kind::kI16x8Mul:
      case Kind::kF64x2Mul:
      case Kind::kF32x4Mul:
        return true;
      default:
        return false;
    }
  }

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Simd128()>();
  }

  Simd128BinopOp(V<Simd128> left, V<Simd128> right, Kind kind)
      : Base(left, right), kind(kind) {}

  V<Simd128> left() const { return input<Simd128>(0); }
  V<Simd128> right() const { return input<Simd128>(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128BinopOp::Kind kind);

#define FOREACH_SIMD_128_UNARY_SIGN_EXTENSION_OPCODE(V) \
  V(I16x8SConvertI8x16Low)                              \
  V(I16x8SConvertI8x16High)                             \
  V(I16x8UConvertI8x16Low)                              \
  V(I16x8UConvertI8x16High)                             \
  V(I32x4SConvertI16x8Low)                              \
  V(I32x4SConvertI16x8High)                             \
  V(I32x4UConvertI16x8Low)                              \
  V(I32x4UConvertI16x8High)                             \
  V(I64x2SConvertI32x4Low)                              \
  V(I64x2SConvertI32x4High)                             \
  V(I64x2UConvertI32x4Low)                              \
  V(I64x2UConvertI32x4High)

#define FOREACH_SIMD_128_UNARY_NON_OPTIONAL_OPCODE(V) \
  V(S128Not)                                          \
  V(F32x4DemoteF64x2Zero)                             \
  V(F64x2PromoteLowF32x4)                             \
  V(I8x16Abs)                                         \
  V(I8x16Neg)                                         \
  V(I8x16Popcnt)                                      \
  V(I16x8ExtAddPairwiseI8x16S)                        \
  V(I16x8ExtAddPairwiseI8x16U)                        \
  V(I32x4ExtAddPairwiseI16x8S)                        \
  V(I32x4ExtAddPairwiseI16x8U)                        \
  V(I16x8Abs)                                         \
  V(I16x8Neg)                                         \
  V(I32x4Abs)                                         \
  V(I32x4Neg)                                         \
  V(I64x2Abs)                                         \
  V(I64x2Neg)                                         \
  V(F32x4Abs)                                         \
  V(F32x4Neg)                                         \
  V(F32x4Sqrt)                                        \
  V(F64x2Abs)                                         \
  V(F64x2Neg)                                         \
  V(F64x2Sqrt)                                        \
  V(I32x4SConvertF32x4)                               \
  V(I32x4UConvertF32x4)                               \
  V(F32x4SConvertI32x4)                               \
  V(F32x4UConvertI32x4)                               \
  V(I32x4TruncSatF64x2SZero)                          \
  V(I32x4TruncSatF64x2UZero)                          \
  V(F64x2ConvertLowI32x4S)                            \
  V(F64x2ConvertLowI32x4U)                            \
  V(I32x4RelaxedTruncF32x4S)                          \
  V(I32x4RelaxedTruncF32x4U)                          \
  V(I32x4RelaxedTruncF64x2SZero)                      \
  V(I32x4RelaxedTruncF64x2UZero)                      \
  FOREACH_SIMD_128_UNARY_SIGN_EXTENSION_OPCODE(V)

#define FOREACH_SIMD_128_UNARY_OPTIONAL_OPCODE(V)                             \
  V(F16x8Abs)                                                                 \
  V(F16x8Neg)                                                                 \
  V(F16x8Sqrt)                                                                \
  V(F16x8Ceil)                                                                \
  V(F16x8Floor)                                                               \
  V(F16x8Trunc)                                                               \
  V(F16x8NearestInt)                                                          \
  V(I16x8SConvertF16x8)                                                       \
  V(I16x8UConvertF16x8)                                                       \
  V(F16x8SConvertI16x8)                                                       \
  V(F16x8UConvertI16x8)                                                       \
  V(F16x8DemoteF32x4Zero)                                                     \
  V(F16x8DemoteF64x2Zero)                                                     \
  V(F32x4PromoteLowF16x8)                                                     \
  V(F32x4Ceil)                                                                \
  V(F32x4Floor)                                                               \
  V(F32x4Trunc)                                                               \
  V(F32x4NearestInt)                                                          \
  V(F64x2Ceil)                                                                \
  V(F64x2Floor)                                                               \
  V(F64x2Trunc)                                                               \
  V(F64x2NearestInt)                                                          \
  /* TODO(mliedtke): Rename to ReverseBytes once the naming is decoupled from \
   * Turbofan naming. */                                                      \
  V(Simd128ReverseBytes)

#define FOREACH_SIMD_128_UNARY_OPCODE(V)        \
  FOREACH_SIMD_128_UNARY_NON_OPTIONAL_OPCODE(V) \
  FOREACH_SIMD_128_UNARY_OPTIONAL_OPCODE(V)

struct Simd128UnaryOp : FixedArityOperationT<1, Simd128UnaryOp> {
  // clang-format off
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_UNARY_OPCODE(DEFINE_KIND)
    kFirstSignExtensionOp = kI16x8SConvertI8x16Low,
    kLastSignExtensionOp = kI64x2UConvertI32x4High,
#undef DEFINE_KIND
  };
  // clang-format on

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128()>();
  }

  Simd128UnaryOp(V<Simd128> input, Kind kind) : Base(input), kind(kind) {}

  V<Simd128> input() const { return Base::input<Simd128>(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128UnaryOp::Kind kind);

#define FOREACH_SIMD_128_REDUCE_OPTIONAL_OPCODE(V) \
  V(I8x16AddReduce)                                \
  V(I16x8AddReduce)                                \
  V(I32x4AddReduce)                                \
  V(I64x2AddReduce)                                \
  V(F32x4AddReduce)                                \
  V(F64x2AddReduce)

struct Simd128ReduceOp : FixedArityOperationT<1, Simd128ReduceOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_REDUCE_OPTIONAL_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128()>();
  }

  Simd128ReduceOp(V<Simd128> input, Kind kind) : Base(input), kind(kind) {}

  V<Simd128> input() const { return Base::input<Simd128>(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128ReduceOp::Kind kind);

#define FOREACH_SIMD_128_SHIFT_OPCODE(V) \
  V(I8x16Shl)                            \
  V(I8x16ShrS)                           \
  V(I8x16ShrU)                           \
  V(I16x8Shl)                            \
  V(I16x8ShrS)                           \
  V(I16x8ShrU)                           \
  V(I32x4Shl)                            \
  V(I32x4ShrS)                           \
  V(I32x4ShrU)                           \
  V(I64x2Shl)                            \
  V(I64x2ShrS)                           \
  V(I64x2ShrU)

struct Simd128ShiftOp : FixedArityOperationT<2, Simd128ShiftOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_SHIFT_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Word32()>();
  }

  Simd128ShiftOp(V<Simd128> input, V<Word32> shift, Kind kind)
      : Base(input, shift), kind(kind) {}

  V<Simd128> input() const { return Base::input<Simd128>(0); }
  V<Word32> shift() const { return Base::input<Word32>(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128ShiftOp::Kind kind);

#define FOREACH_SIMD_128_TEST_OPCODE(V) \
  V(V128AnyTrue)                        \
  V(I8x16AllTrue)                       \
  V(I8x16BitMask)                       \
  V(I16x8AllTrue)                       \
  V(I16x8BitMask)                       \
  V(I32x4AllTrue)                       \
  V(I32x4BitMask)                       \
  V(I64x2AllTrue)                       \
  V(I64x2BitMask)

struct Simd128TestOp : FixedArityOperationT<1, Simd128TestOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_TEST_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Word32()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128()>();
  }

  Simd128TestOp(V<Simd128> input, Kind kind) : Base(input), kind(kind) {}

  V<Simd128> input() const { return Base::input<Simd128>(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128TestOp::Kind kind);

#define FOREACH_SIMD_128_SPLAT_MANDATORY_OPCODE(V) \
  V(I8x16)                                         \
  V(I16x8)                                         \
  V(I32x4)                                         \
  V(I64x2)                                         \
  V(F32x4)                                         \
  V(F64x2)

#define FOREACH_SIMD_128_SPLAT_OPCODE(V)     \
  FOREACH_SIMD_128_SPLAT_MANDATORY_OPCODE(V) \
  V(F16x8)
struct Simd128SplatOp : FixedArityOperationT<1, Simd128SplatOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_SPLAT_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    switch (kind) {
      case Kind::kI8x16:
      case Kind::kI16x8:
      case Kind::kI32x4:
        return MaybeRepVector<RegisterRepresentation::Word32()>();
      case Kind::kI64x2:
        return MaybeRepVector<RegisterRepresentation::Word64()>();
      case Kind::kF16x8:
      case Kind::kF32x4:
        return MaybeRepVector<RegisterRepresentation::Float32()>();
      case Kind::kF64x2:
        return MaybeRepVector<RegisterRepresentation::Float64()>();
    }
  }

  Simd128SplatOp(V<Any> input, Kind kind) : Base(input), kind(kind) {}

  V<Any> input() const { return Base::input<Any>(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128SplatOp::Kind kind);

#define FOREACH_SIMD_128_TERNARY_MASK_OPCODE(V) \
  V(S128Select)                                 \
  V(I8x16RelaxedLaneSelect)                     \
  V(I16x8RelaxedLaneSelect)                     \
  V(I32x4RelaxedLaneSelect)                     \
  V(I64x2RelaxedLaneSelect)

#define FOREACH_SIMD_128_TERNARY_OTHER_OPCODE(V) \
  V(F32x4Qfma)                                   \
  V(F32x4Qfms)                                   \
  V(F64x2Qfma)                                   \
  V(F64x2Qfms)                                   \
  V(I32x4DotI8x16I7x16AddS)

#define FOREACH_SIMD_128_TERNARY_OPTIONAL_OPCODE(V) \
  V(F16x8Qfma)                                      \
  V(F16x8Qfms)

#define FOREACH_SIMD_128_TERNARY_OPCODE(V) \
  FOREACH_SIMD_128_TERNARY_MASK_OPCODE(V)  \
  FOREACH_SIMD_128_TERNARY_OTHER_OPCODE(V) \
  FOREACH_SIMD_128_TERNARY_OPTIONAL_OPCODE(V)

struct Simd128TernaryOp : FixedArityOperationT<3, Simd128TernaryOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_TERNARY_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Simd128()>();
  }

  Simd128TernaryOp(V<Simd128> first, V<Simd128> second, V<Simd128> third,
                   Kind kind)
      : Base(first, second, third), kind(kind) {}

  V<Simd128> first() const { return input<Simd128>(0); }
  V<Simd128> second() const { return input<Simd128>(1); }
  V<Simd128> third() const { return input<Simd128>(2); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd128TernaryOp::Kind kind);

struct Simd128ExtractLaneOp : FixedArityOperationT<1, Simd128ExtractLaneOp> {
  enum class Kind : uint8_t {
    kI8x16S,
    kI8x16U,
    kI16x8S,
    kI16x8U,
    kI32x4,
    kI64x2,
    kF16x8,
    kF32x4,
    kF64x2,
  };

  Kind kind;
  uint8_t lane;

  static constexpr OpEffects effects = OpEffects();

  static MachineRepresentation element_rep(Kind kind) {
    switch (kind) {
      case Kind::kI8x16S:
      case Kind::kI8x16U:
        return MachineRepresentation::kWord8;
      case Kind::kI16x8S:
      case Kind::kI16x8U:
        return MachineRepresentation::kWord16;
      case Kind::kI32x4:
        return MachineRepresentation::kWord32;
      case Kind::kI64x2:
        return MachineRepresentation::kWord64;
      case Kind::kF16x8:
      case Kind::kF32x4:
        return MachineRepresentation::kFloat32;
      case Kind::kF64x2:
        return MachineRepresentation::kFloat64;
    }
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    switch (kind) {
      case Kind::kI8x16S:
      case Kind::kI8x16U:
      case Kind::kI16x8S:
      case Kind::kI16x8U:
      case Kind::kI32x4:
        return RepVector<RegisterRepresentation::Word32()>();
      case Kind::kI64x2:
        return RepVector<RegisterRepresentation::Word64()>();
      case Kind::kF16x8:
      case Kind::kF32x4:
        return RepVector<RegisterRepresentation::Float32()>();
      case Kind::kF64x2:
        return RepVector<RegisterRepresentation::Float64()>();
    }
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128()>();
  }

  Simd128ExtractLaneOp(V<Simd128> input, Kind kind, uint8_t lane)
      : Base(input), kind(kind), lane(lane) {}

  V<Simd128> input() const { return Base::input<Simd128>(0); }

  void Validate(const Graph& graph) const {
#if DEBUG
    uint8_t lane_count;
    switch (kind) {
      case Kind::kI8x16S:
      case Kind::kI8x16U:
        lane_count = 16;
        break;
      case Kind::kI16x8S:
      case Kind::kI16x8U:
      case Kind::kF16x8:
        lane_count = 8;
        break;
      case Kind::kI32x4:
      case Kind::kF32x4:
        lane_count = 4;
        break;
      case Kind::kI64x2:
      case Kind::kF64x2:
        lane_count = 2;
        break;
    }
    DCHECK_LT(lane, lane_count);
#endif
  }

  auto options() const { return std::tuple{kind, lane}; }
  void PrintOptions(std::ostream& os) const;
};

struct Simd128ReplaceLaneOp : FixedArityOperationT<2, Simd128ReplaceLaneOp> {
  enum class Kind : uint8_t {
    kI8x16,
    kI16x8,
    kI32x4,
    kI64x2,
    kF16x8,
    kF32x4,
    kF64x2,
  };

  Kind kind;
  uint8_t lane;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }
  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return InitVectorOf(storage,
                        {RegisterRepresentation::Simd128(), new_lane_rep()});
  }

  Simd128ReplaceLaneOp(V<Simd128> into, V<Any> new_lane, Kind kind,
                       uint8_t lane)
      : Base(into, new_lane), kind(kind), lane(lane) {}

  V<Simd128> into() const { return input<Simd128>(0); }
  V<Any> new_lane() const { return input<Any>(1); }

  void Validate(const Graph& graph) const {
#if DEBUG
    uint8_t lane_count;
    switch (kind) {
      case Kind::kI8x16:
        lane_count = 16;
        break;
      case Kind::kI16x8:
      case Kind::kF16x8:
        lane_count = 8;
        break;
      case Kind::kI32x4:
      case Kind::kF32x4:
        lane_count = 4;
        break;
      case Kind::kI64x2:
      case Kind::kF64x2:
        lane_count = 2;
        break;
    }
    DCHECK_LT(lane, lane_count);
#endif
  }

  auto options() const { return std::tuple{kind, lane}; }
  void PrintOptions(std::ostream& os) const;

  RegisterRepresentation new_lane_rep() const {
    switch (kind) {
      case Kind::kI8x16:
      case Kind::kI16x8:
      case Kind::kI32x4:
        return RegisterRepresentation::Word32();
      case Kind::kI64x2:
        return RegisterRepresentation::Word64();
      case Kind::kF16x8:
      case Kind::kF32x4:
        return RegisterRepresentation::Float32();
      case Kind::kF64x2:
        return RegisterRepresentation::Float64();
    }
  }
};

// If `mode` is `kLoad`, load a value from `base() + index() + offset`, whose
// size is determinded by `lane_kind`, and return the Simd128 `value()` with
// the lane specified by `lane_kind` and `lane` replaced with the loaded value.
// If `mode` is `kStore`, extract the lane specified by `lane` with size
// `lane_kind` from `value()`, and store it to `base() + index() + offset`.
struct Simd128LaneMemoryOp : FixedArityOperationT<3, Simd128LaneMemoryOp> {
  enum class Mode : bool { kLoad, kStore };
  using Kind = LoadOp::Kind;
  // The values encode the element_size_log2.
  enum class LaneKind : uint8_t { k8 = 0, k16 = 1, k32 = 2, k64 = 3 };

  Mode mode;
  Kind kind;
  LaneKind lane_kind;
  uint8_t lane;
  int offset;

  OpEffects Effects() const {
    OpEffects effects = mode == Mode::kLoad ? OpEffects().CanReadMemory()
                                            : OpEffects().CanWriteMemory();
    effects = effects.CanDependOnChecks();
    if (kind.with_trap_handler) effects = effects.CanLeaveCurrentFunction();
    return effects;
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return mode == Mode::kLoad ? RepVector<RegisterRepresentation::Simd128()>()
                               : RepVector<>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::WordPtr(),
                          RegisterRepresentation::WordPtr(),
                          RegisterRepresentation::Simd128()>();
  }

  Simd128LaneMemoryOp(OpIndex base, OpIndex index, OpIndex value, Mode mode,
                      Kind kind, LaneKind lane_kind, uint8_t lane, int offset)
      : Base(base, index, value),
        mode(mode),
        kind(kind),
        lane_kind(lane_kind),
        lane(lane),
        offset(offset) {}

  OpIndex base() const { return input(0); }
  OpIndex index() const { return input(1); }
  OpIndex value() const { return input(2); }
  uint8_t lane_size() const { return 1 << static_cast<uint8_t>(lane_kind); }

  void Validate(const Graph& graph) {
    DCHECK(!kind.tagged_base);
#if DEBUG
    uint8_t lane_count;
    switch (lane_kind) {
      case LaneKind::k8:
        lane_count = 16;
        break;
      case LaneKind::k16:
        lane_count = 8;
        break;
      case LaneKind::k32:
        lane_count = 4;
        break;
      case LaneKind::k64:
        lane_count = 2;
        break;
    }
    DCHECK_LT(lane, lane_count);
#endif
  }

  auto options() const {
    return std::tuple{mode, kind, lane_kind, lane, offset};
  }
  void PrintOptions(std::ostream& os) const;
};

#define FOREACH_SIMD_128_LOAD_TRANSFORM_OPCODE(V) \
  V(8x8S)                                         \
  V(8x8U)                                         \
  V(16x4S)                                        \
  V(16x4U)                                        \
  V(32x2S)                                        \
  V(32x2U)                                        \
  V(8Splat)                                       \
  V(16Splat)                                      \
  V(32Splat)                                      \
  V(64Splat)                                      \
  V(32Zero)                                       \
  V(64Zero)

// Load a value from `base() + index() + offset`, whose size is determinded by
// `transform_kind`, and generate a Simd128 value as follows:
// - From 8x8S to 32x2U (extend kinds), the loaded value has size 8. It is
//   interpreted as a vector of values according to the size of the kind, which
//   populate the even lanes of the generated value. The odd lanes are zero- or
//   sign-extended according to the kind.
// - For splat kinds, the loaded value's size is determined by the kind, all
//   lanes of the generated value are populated with the loaded value.
// - For "zero" kinds, the loaded value's size is determined by the kind, and
//   the generated value zero-extends the loaded value.
struct Simd128LoadTransformOp
    : FixedArityOperationT<2, Simd128LoadTransformOp> {
  using LoadKind = LoadOp::Kind;
  enum class TransformKind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_128_LOAD_TRANSFORM_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  LoadKind load_kind;
  TransformKind transform_kind;
  int offset;

  OpEffects Effects() const {
    OpEffects effects = OpEffects().CanReadMemory().CanDependOnChecks();
    if (load_kind.with_trap_handler) {
      effects = effects.CanLeaveCurrentFunction();
    }
    return effects;
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::WordPtr(),
                          RegisterRepresentation::WordPtr()>();
  }

  Simd128LoadTransformOp(V<WordPtr> base, V<WordPtr> index, LoadKind load_kind,
                         TransformKind transform_kind, int offset)
      : Base(base, index),
        load_kind(load_kind),
        transform_kind(transform_kind),
        offset(offset) {}

  V<WordPtr> base() const { return input<WordPtr>(0); }
  V<WordPtr> index() const { return input<WordPtr>(1); }

  void Validate(const Graph& graph) { DCHECK(!load_kind.tagged_base); }

  auto options() const { return std::tuple{load_kind, transform_kind, offset}; }
  void PrintOptions(std::ostream& os) const;
};

// Takes two Simd128 inputs and generates a Simd128 value. The 8-bit lanes of
// both inputs are numbered 0-31, and each output 8-bit lane is selected from
// among the input lanes according to `shuffle`.
struct Simd128ShuffleOp : FixedArityOperationT<2, Simd128ShuffleOp> {
  uint8_t shuffle[kSimd128Size];

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Simd128()>();
  }

  Simd128ShuffleOp(V<Simd128> left, V<Simd128> right,
                   const uint8_t incoming_shuffle[kSimd128Size])
      : Base(left, right) {
    std::copy(incoming_shuffle, incoming_shuffle + kSimd128Size, shuffle);
  }

  V<Simd128> left() const { return input<Simd128>(0); }
  V<Simd128> right() const { return input<Simd128>(1); }

  void Validate(const Graph& graph) {
#if DEBUG
    constexpr uint8_t kNumberOfLanesForShuffle = 32;
    for (uint8_t index : shuffle) {
      DCHECK_LT(index, kNumberOfLanesForShuffle);
    }
#endif
  }

  auto options() const { return std::tuple{shuffle}; }
  void PrintOptions(std::ostream& os) const;
};

#if V8_ENABLE_WASM_SIMD256_REVEC
struct Simd256ConstantOp : FixedArityOperationT<0, Simd256ConstantOp> {
  static constexpr uint8_t kZero[kSimd256Size] = {};
  uint8_t value[kSimd256Size];

  static constexpr OpEffects effects = OpEffects();

  explicit Simd256ConstantOp(const uint8_t incoming_value[kSimd256Size])
      : Base() {
    std::copy(incoming_value, incoming_value + kSimd256Size, value);
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {
    // TODO(14108): Validate.
  }

  bool IsZero() const { return std::memcmp(kZero, value, kSimd256Size) == 0; }

  auto options() const { return std::tuple{value}; }
  void PrintOptions(std::ostream& os) const;
};

struct Simd256Extract128LaneOp
    : FixedArityOperationT<1, Simd256Extract128LaneOp> {
  uint8_t lane;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd128()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256()>();
  }

  Simd256Extract128LaneOp(OpIndex input, uint8_t lane)
      : Base(input), lane(lane) {}

  OpIndex input() const { return Base::input(0); }

  void Validate(const Graph& graph) const {
#if DEBUG
    DCHECK_LT(lane, 2);
#endif
  }

  auto options() const { return std::tuple{lane}; }
  void PrintOptions(std::ostream& os) const;
};

#define FOREACH_SIMD_256_LOAD_TRANSFORM_OPCODE(V) \
  V(8x16S)                                        \
  V(8x16U)                                        \
  V(16x8S)                                        \
  V(16x8U)                                        \
  V(32x4S)                                        \
  V(32x4U)                                        \
  V(8Splat)                                       \
  V(16Splat)                                      \
  V(32Splat)                                      \
  V(64Splat)

struct Simd256LoadTransformOp
    : FixedArityOperationT<2, Simd256LoadTransformOp> {
  using LoadKind = LoadOp::Kind;
  enum class TransformKind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_LOAD_TRANSFORM_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  LoadKind load_kind;
  TransformKind transform_kind;
  int offset;

  OpEffects Effects() const {
    OpEffects effects = OpEffects().CanReadMemory().CanDependOnChecks();
    if (load_kind.with_trap_handler) {
      effects = effects.CanLeaveCurrentFunction();
    }
    return effects;
  }

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::WordPtr(),
                          RegisterRepresentation::WordPtr()>();
  }

  Simd256LoadTransformOp(V<WordPtr> base, V<WordPtr> index, LoadKind load_kind,
                         TransformKind transform_kind, int offset)
      : Base(base, index),
        load_kind(load_kind),
        transform_kind(transform_kind),
        offset(offset) {}

  V<WordPtr> base() const { return input<WordPtr>(0); }
  V<WordPtr> index() const { return input<WordPtr>(1); }

  void Validate(const Graph& graph) { DCHECK(!load_kind.tagged_base); }

  auto options() const { return std::tuple{load_kind, transform_kind, offset}; }
  void PrintOptions(std::ostream& os) const;
};

#define FOREACH_SIMD_256_UNARY_SIGN_EXTENSION_OPCODE(V) \
  V(I16x16SConvertI8x16)                                \
  V(I16x16UConvertI8x16)                                \
  V(I32x8SConvertI16x8)                                 \
  V(I32x8UConvertI16x8)                                 \
  V(I64x4SConvertI32x4)                                 \
  V(I64x4UConvertI32x4)

#define FOREACH_SIMD_256_UNARY_OPCODE(V) \
  V(S256Not)                             \
  V(I8x32Abs)                            \
  V(I8x32Neg)                            \
  V(I16x16ExtAddPairwiseI8x32S)          \
  V(I16x16ExtAddPairwiseI8x32U)          \
  V(I32x8ExtAddPairwiseI16x16S)          \
  V(I32x8ExtAddPairwiseI16x16U)          \
  V(I16x16Abs)                           \
  V(I16x16Neg)                           \
  V(I32x8Abs)                            \
  V(I32x8Neg)                            \
  V(F32x8Abs)                            \
  V(F32x8Neg)                            \
  V(F32x8Sqrt)                           \
  V(F64x4Sqrt)                           \
  V(I32x8UConvertF32x8)                  \
  V(I32x8SConvertF32x8)                  \
  V(F32x8UConvertI32x8)                  \
  V(F32x8SConvertI32x8)                  \
  FOREACH_SIMD_256_UNARY_SIGN_EXTENSION_OPCODE(V)

struct Simd256UnaryOp : FixedArityOperationT<1, Simd256UnaryOp> {
  // clang-format off
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_UNARY_OPCODE(DEFINE_KIND)
    kFirstSignExtensionOp = kI16x16SConvertI8x16,
    kLastSignExtensionOp = kI64x4UConvertI32x4,
#undef DEFINE_KIND
  };
  // clang-format on

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    if (kind >= Kind::kFirstSignExtensionOp) {
      return MaybeRepVector<RegisterRepresentation::Simd128()>();
    } else {
      return MaybeRepVector<RegisterRepresentation::Simd256()>();
    }
  }

  Simd256UnaryOp(OpIndex input, Kind kind) : Base(input), kind(kind) {}

  OpIndex input() const { return Base::input(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
std::ostream& operator<<(std::ostream& os, Simd256UnaryOp::Kind kind);

#define FOREACH_SIMD_256_BINARY_SIGN_EXTENSION_OPCODE(V) \
  V(I64x4ExtMulI32x4S)                                   \
  V(I64x4ExtMulI32x4U)                                   \
  V(I32x8ExtMulI16x8S)                                   \
  V(I32x8ExtMulI16x8U)                                   \
  V(I16x16ExtMulI8x16S)                                  \
  V(I16x16ExtMulI8x16U)

#define FOREACH_SIMD_256_BINARY_OPCODE(V) \
  V(I8x32Eq)                              \
  V(I8x32Ne)                              \
  V(I8x32GtS)                             \
  V(I8x32GtU)                             \
  V(I8x32GeS)                             \
  V(I8x32GeU)                             \
  V(I16x16Eq)                             \
  V(I16x16Ne)                             \
  V(I16x16GtS)                            \
  V(I16x16GtU)                            \
  V(I16x16GeS)                            \
  V(I16x16GeU)                            \
  V(I32x8Eq)                              \
  V(I32x8Ne)                              \
  V(I32x8GtS)                             \
  V(I32x8GtU)                             \
  V(I32x8GeS)                             \
  V(I32x8GeU)                             \
  V(F32x8Eq)                              \
  V(F32x8Ne)                              \
  V(F32x8Lt)                              \
  V(F32x8Le)                              \
  V(F64x4Eq)                              \
  V(F64x4Ne)                              \
  V(F64x4Lt)                              \
  V(F64x4Le)                              \
  V(S256And)                              \
  V(S256AndNot)                           \
  V(S256Or)                               \
  V(S256Xor)                              \
  V(I8x32SConvertI16x16)                  \
  V(I8x32UConvertI16x16)                  \
  V(I8x32Add)                             \
  V(I8x32AddSatS)                         \
  V(I8x32AddSatU)                         \
  V(I8x32Sub)                             \
  V(I8x32SubSatS)                         \
  V(I8x32SubSatU)                         \
  V(I8x32MinS)                            \
  V(I8x32MinU)                            \
  V(I8x32MaxS)                            \
  V(I8x32MaxU)                            \
  V(I8x32RoundingAverageU)                \
  V(I16x16SConvertI32x8)                  \
  V(I16x16UConvertI32x8)                  \
  V(I16x16Add)                            \
  V(I16x16AddSatS)                        \
  V(I16x16AddSatU)                        \
  V(I16x16Sub)                            \
  V(I16x16SubSatS)                        \
  V(I16x16SubSatU)                        \
  V(I16x16Mul)                            \
  V(I16x16MinS)                           \
  V(I16x16MinU)                           \
  V(I16x16MaxS)                           \
  V(I16x16MaxU)                           \
  V(I16x16RoundingAverageU)               \
  V(I32x8Add)                             \
  V(I32x8Sub)                             \
  V(I32x8Mul)                             \
  V(I32x8MinS)                            \
  V(I32x8MinU)                            \
  V(I32x8MaxS)                            \
  V(I32x8MaxU)                            \
  V(I32x8DotI16x16S)                      \
  V(I64x4Add)                             \
  V(I64x4Sub)                             \
  V(I64x4Mul)                             \
  V(I64x4Eq)                              \
  V(I64x4Ne)                              \
  V(I64x4GtS)                             \
  V(I64x4GeS)                             \
  V(F32x8Add)                             \
  V(F32x8Sub)                             \
  V(F32x8Mul)                             \
  V(F32x8Div)                             \
  V(F32x8Min)                             \
  V(F32x8Max)                             \
  V(F32x8Pmin)                            \
  V(F32x8Pmax)                            \
  V(F64x4Add)                             \
  V(F64x4Sub)                             \
  V(F64x4Mul)                             \
  V(F64x4Div)                             \
  V(F64x4Min)                             \
  V(F64x4Max)                             \
  V(F64x4Pmin)                            \
  V(F64x4Pmax)                            \
  V(F32x8RelaxedMin)                      \
  V(F32x8RelaxedMax)                      \
  V(F64x4RelaxedMin)                      \
  V(F64x4RelaxedMax)                      \
  V(I16x16DotI8x32I7x32S)                 \
  FOREACH_SIMD_256_BINARY_SIGN_EXTENSION_OPCODE(V)

struct Simd256BinopOp : FixedArityOperationT<2, Simd256BinopOp> {
  // clang-format off
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_BINARY_OPCODE(DEFINE_KIND)
    kFirstSignExtensionOp = kI64x4ExtMulI32x4S,
    kLastSignExtensionOp = kI16x16ExtMulI8x16U,
#undef DEFINE_KIND
  };
  // clang-format on

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    if (kind >= Kind::kFirstSignExtensionOp) {
      return MaybeRepVector<RegisterRepresentation::Simd128(),
                            RegisterRepresentation::Simd128()>();
    } else {
      return MaybeRepVector<RegisterRepresentation::Simd256(),
                            RegisterRepresentation::Simd256()>();
    }
  }

  Simd256BinopOp(OpIndex left, OpIndex right, Kind kind)
      : Base(left, right), kind(kind) {}

  OpIndex left() const { return input(0); }
  OpIndex right() const { return input(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};

std::ostream& operator<<(std::ostream& os, Simd256BinopOp::Kind kind);

#define FOREACH_SIMD_256_SHIFT_OPCODE(V) \
  V(I16x16Shl)                           \
  V(I16x16ShrS)                          \
  V(I16x16ShrU)                          \
  V(I32x8Shl)                            \
  V(I32x8ShrS)                           \
  V(I32x8ShrU)                           \
  V(I64x4Shl)                            \
  V(I64x4ShrU)

struct Simd256ShiftOp : FixedArityOperationT<2, Simd256ShiftOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_SHIFT_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256(),
                          RegisterRepresentation::Word32()>();
  }

  Simd256ShiftOp(V<Simd256> input, V<Word32> shift, Kind kind)
      : Base(input, shift), kind(kind) {}

  V<Simd256> input() const { return Base::input<Simd256>(0); }
  V<Word32> shift() const { return Base::input<Word32>(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           Simd256ShiftOp::Kind kind);

#define FOREACH_SIMD_256_TERNARY_MASK_OPCODE(V) \
  V(S256Select)                                 \
  V(I8x32RelaxedLaneSelect)                     \
  V(I16x16RelaxedLaneSelect)                    \
  V(I32x8RelaxedLaneSelect)                     \
  V(I64x4RelaxedLaneSelect)

#define FOREACH_SIMD_256_TERNARY_OTHER_OPCODE(V) \
  V(F32x8Qfma)                                   \
  V(F32x8Qfms)                                   \
  V(F64x4Qfma)                                   \
  V(F64x4Qfms)                                   \
  V(I32x8DotI8x32I7x32AddS)

#define FOREACH_SIMD_256_TERNARY_OPCODE(V) \
  FOREACH_SIMD_256_TERNARY_MASK_OPCODE(V)  \
  FOREACH_SIMD_256_TERNARY_OTHER_OPCODE(V)

struct Simd256TernaryOp : FixedArityOperationT<3, Simd256TernaryOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_TERNARY_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256(),
                          RegisterRepresentation::Simd256(),
                          RegisterRepresentation::Simd256()>();
  }

  Simd256TernaryOp(V<Simd256> first, V<Simd256> second, V<Simd256> third,
                   Kind kind)
      : Base(first, second, third), kind(kind) {}

  V<Simd256> first() const { return input<Simd256>(0); }
  V<Simd256> second() const { return input<Simd256>(1); }
  V<Simd256> third() const { return input<Simd256>(2); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
std::ostream& operator<<(std::ostream& os, Simd256TernaryOp::Kind kind);

#define FOREACH_SIMD_256_SPLAT_OPCODE(V) \
  V(I8x32)                               \
  V(I16x16)                              \
  V(I32x8)                               \
  V(I64x4)                               \
  V(F32x8)                               \
  V(F64x4)

struct Simd256SplatOp : FixedArityOperationT<1, Simd256SplatOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_SPLAT_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };

  Kind kind;

  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    switch (kind) {
      case Kind::kI8x32:
      case Kind::kI16x16:
      case Kind::kI32x8:
        return MaybeRepVector<RegisterRepresentation::Word32()>();
      case Kind::kI64x4:
        return MaybeRepVector<RegisterRepresentation::Word64()>();
      case Kind::kF32x8:
        return MaybeRepVector<RegisterRepresentation::Float32()>();
      case Kind::kF64x4:
        return MaybeRepVector<RegisterRepresentation::Float64()>();
    }
  }

  Simd256SplatOp(OpIndex input, Kind kind) : Base(input), kind(kind) {}

  OpIndex input() const { return Base::input(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
std::ostream& operator<<(std::ostream& os, Simd256SplatOp::Kind kind);

struct SimdPack128To256Op : FixedArityOperationT<2, SimdPack128To256Op> {
  static constexpr OpEffects effects = OpEffects();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd128(),
                          RegisterRepresentation::Simd128()>();
  }

  SimdPack128To256Op(V<Simd128> left, V<Simd128> right) : Base(left, right) {}
  V<Simd128> left() const { return Base::input<Simd128>(0); }
  V<Simd128> right() const { return Base::input<Simd128>(1); }
  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

#ifdef V8_TARGET_ARCH_X64
struct Simd256ShufdOp : FixedArityOperationT<1, Simd256ShufdOp> {
  static constexpr OpEffects effects = OpEffects();
  uint8_t control;

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256()>();
  }

  Simd256ShufdOp(V<Simd256> input, uint8_t control)
      : Base(input), control(control) {}

  V<Simd256> input() const { return Base::input<Simd256>(0); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{control}; }

  void PrintOptions(std::ostream& os) const;
};

struct Simd256ShufpsOp : FixedArityOperationT<2, Simd256ShufpsOp> {
  static constexpr OpEffects effects = OpEffects();
  uint8_t control;

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256(),
                          RegisterRepresentation::Simd256()>();
  }

  Simd256ShufpsOp(V<Simd256> left, V<Simd256> right, uint8_t control)
      : Base(left, right), control(control) {}

  V<Simd256> left() const { return Base::input<Simd256>(0); }
  V<Simd256> right() const { return Base::input<Simd256>(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{control}; }

  void PrintOptions(std::ostream& os) const;
};

#define FOREACH_SIMD_256_UNPACK_OPCODE(V) \
  V(32x8Low)                              \
  V(32x8High)
struct Simd256UnpackOp : FixedArityOperationT<2, Simd256UnpackOp> {
  enum class Kind : uint8_t {
#define DEFINE_KIND(kind) k##kind,
    FOREACH_SIMD_256_UNPACK_OPCODE(DEFINE_KIND)
#undef DEFINE_KIND
  };
  static constexpr OpEffects effects = OpEffects();
  Kind kind;

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Simd256()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<RegisterRepresentation::Simd256(),
                          RegisterRepresentation::Simd256()>();
  }

  Simd256UnpackOp(V<Simd256> left, V<Simd256> right, Kind kind)
      : Base(left, right), kind(kind) {}

  V<Simd256> left() const { return Base::input<Simd256>(0); }
  V<Simd256> right() const { return Base::input<Simd256>(1); }

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{kind}; }
};
std::ostream& operator<<(std::ostream& os, Simd256UnpackOp::Kind kind);
#endif  // V8_TARGET_ARCH_X64

#endif  // V8_ENABLE_WASM_SIMD256_REVEC

struct LoadStackPointerOp : FixedArityOperationT<0, LoadStackPointerOp> {
  // TODO(nicohartmann@): Review effects.
  static constexpr OpEffects effects = OpEffects().CanReadMemory();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::WordPtr()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

struct SetStackPointerOp : FixedArityOperationT<1, SetStackPointerOp> {
  // TODO(nicohartmann@): Review effects.
  static constexpr OpEffects effects = OpEffects().CanCallAnything();

  OpIndex value() const { return Base::input(0); }

  explicit SetStackPointerOp(OpIndex value) : Base(value) {}

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::WordPtr()>();
  }

  void Validate(const Graph& graph) const {}
  auto options() const { return std::tuple{}; }
};

#endif  // V8_ENABLE_WEBASSEMBLY

#ifdef V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA
struct GetContinuationPreservedEmbedderDataOp
    : FixedArityOperationT<0, GetContinuationPreservedEmbedderDataOp> {
  static constexpr OpEffects effects = OpEffects().CanReadOffHeapMemory();

  base::Vector<const RegisterRepresentation> outputs_rep() const {
    return RepVector<RegisterRepresentation::Tagged()>();
  }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return {};
  }

  GetContinuationPreservedEmbedderDataOp() : Base() {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};

struct SetContinuationPreservedEmbedderDataOp
    : FixedArityOperationT<1, SetContinuationPreservedEmbedderDataOp> {
  static constexpr OpEffects effects = OpEffects().CanWriteOffHeapMemory();

  base::Vector<const RegisterRepresentation> outputs_rep() const { return {}; }

  base::Vector<const MaybeRegisterRepresentation> inputs_rep(
      ZoneVector<MaybeRegisterRepresentation>& storage) const {
    return MaybeRepVector<MaybeRegisterRepresentation::Tagged()>();
  }

  explicit SetContinuationPreservedEmbedderDataOp(V<Object> value)
      : Base(value) {}

  void Validate(const Graph& graph) const {}

  auto options() const { return std::tuple{}; }
};
#endif  // V8_ENABLE_CONTINUATION_PRESERVED_EMBEDDER_DATA

#define OPERATION_EFFECTS_CASE(Name) Name##Op::EffectsIfStatic(),
static constexpr std::optional<OpEffects>
    kOperationEffectsTable[kNumberOfOpcodes] = {
        TURBOSHAFT_OPERATION_LIST(OPERATION_EFFECTS_CASE)};
#undef OPERATION_EFFECTS_CASE

template <class Op>
const Opcode OperationT<Op>::opcode = operation_to_opcode<Op>::value;

template <Opcode opcode>
struct opcode_to_operation_map {};

#define OPERATION_OPCODE_MAP_CASE(Name)             \
  template <>                                       \
  struct opcode_to_operation_map<Opcode::k##Name> { \
    using Op = Name##Op;                            \
  };
TURBOSHAFT_OPERATION_LIST(OPERATION_OPCODE_MAP_CASE)
#undef OPERATION_OPCODE_MAP_CASE

template <class Op, class = void>
struct static_operation_input_count : std::integral_constant<uint32_t, 0> {};
template <class Op>
struct static_operation_input_count<Op, std::void_t<decltype(Op::inputs)>>
    : std::integral_constant<uint32_t, sizeof(Op::inputs) / sizeof(OpIndex)> {};
constexpr size_t kOperationSizeTable[kNumberOfOpcodes] = {
#define OPERATION_SIZE(Name) sizeof(Name##Op),
    TURBOSHAFT_OPERATION_LIST(OPERATION_SIZE)
#undef OPERATION_SIZE
};
constexpr size_t kOperationSizeDividedBySizeofOpIndexTable[kNumberOfOpcodes] = {
#define OPERATION_SIZE(Name) (sizeof(Name##Op) / sizeof(OpIndex)),
    TURBOSHAFT_OPERATION_LIST(OPERATION_SIZE)
#undef OPERATION_SIZE
};

inline base::Vector<const OpIndex> Operation::inputs() const {
  // This is actually undefined behavior, since we use the `this` pointer to
  // access an adjacent object.
  const OpIndex* ptr = reinterpret_cast<const OpIndex*>(
      reinterpret_cast<const char*>(this) +
      kOperationSizeTable[OpcodeIndex(opcode)]);
  return {ptr, input_count};
}

inline OpEffects Operation::Effects() const {
  if (auto prop = kOperationEffectsTable[OpcodeIndex(opcode)]) {
    return *prop;
  }
  switch (opcode) {
    case Opcode::kLoad:
      return Cast<LoadOp>().Effects();
    case Opcode::kStore:
      return Cast<StoreOp>().Effects();
    case Opcode::kCall:
      return Cast<CallOp>().Effects();
    case Opcode::kDidntThrow:
      return Cast<DidntThrowOp>().Effects();
    case Opcode::kTaggedBitcast:
      return Cast<TaggedBitcastOp>().Effects();
    case Opcode::kAtomicRMW:
      return Cast<AtomicRMWOp>().Effects();
    case Opcode::kAtomicWord32Pair:
      return Cast<AtomicWord32PairOp>().Effects();
    case Opcode::kJSStackCheck:
      return Cast<JSStackCheckOp>().Effects();
#if V8_ENABLE_WEBASSEMBLY
    case Opcode::kWasmStackCheck:
      return Cast<WasmStackCheckOp>().Effects();
    case Opcode::kStructGet:
      return Cast<StructGetOp>().Effects();
    case Opcode::kStructSet:
      return Cast<StructSetOp>().Effects();
    case Opcode::kArrayLength:
      return Cast<ArrayLengthOp>().Effects();
    case Opcode::kSimd128LaneMemory:
      return Cast<Simd128LaneMemoryOp>().Effects();
    case Opcode::kSimd128LoadTransform:
      return Cast<Simd128LoadTransformOp>().Effects();
#if V8_ENABLE_WASM_SIMD256_REVEC
    case Opcode::kSimd256LoadTransform:
      return Cast<Simd256LoadTransformOp>().Effects();
#endif  // V8_ENABLE_WASM_SIMD256_REVEC
#endif
    default:
      UNREACHABLE();
  }
}

// static
inline size_t Operation::StorageSlotCount(Opcode opcode, size_t input_count) {
  size_t size = kOperationSizeDividedBySizeofOpIndexTable[OpcodeIndex(opcode)];
  constexpr size_t r = sizeof(OperationStorageSlot) / sizeof(OpIndex);
  static_assert(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
  return std::max<size_t>(2, (r - 1 + size + input_count) / r);
}

V8_INLINE bool CanBeUsedAsInput(const Operation& op) {
  return op.Is<FrameStateOp>() || op.outputs_rep().size() > 0;
}

inline base::Vector<const RegisterRepresentation> Operation::outputs_rep()
    const {
  switch (opcode) {
#define CASE(type)                         \
  case Opcode::k##type: {                  \
    const type##Op& op = Cast<type##Op>(); \
    return op.outputs_rep();               \
  }
    TURBOSHAFT_OPERATION_LIST(CASE)
#undef CASE
  }
}

inline base::Vector<const MaybeRegisterRepresentation> Operation::inputs_rep(
    ZoneVector<MaybeRegisterRepresentation>& storage) const {
  switch (opcode) {
#define CASE(type)                         \
  case Opcode::k##type: {                  \
    const type##Op& op = Cast<type##Op>(); \
    return op.inputs_rep(storage);         \
  }
    TURBOSHAFT_OPERATION_LIST(CASE)
#undef CASE
  }
}

bool IsUnlikelySuccessor(const Block* block, const Block* successor,
                         const Graph& graph);

// Analyzers should skip (= ignore) operations for which ShouldSkipOperation
// returns true. This happens for:
//  - DeadOp: this means that a previous Analyzer decided that this operation is
//    dead.
//  - Operations that are not RequiredWhenUnused, and whose saturated_use_count
//    is 0: this corresponds to pure operations that have no uses.
V8_EXPORT_PRIVATE V8_INLINE bool ShouldSkipOperation(const Operation& op) {
  if (op.Is<DeadOp>()) return true;
  return op.saturated_use_count.IsZero() && !op.IsRequiredWhenUnused();
}

namespace detail {
// Defining `input_count` to compute the number of OpIndex inputs of an
// operation.

// There is one overload for each possible type of parameters for all
// Operations rather than a default generic overload, so that we don't
// accidentally forget some types (eg, if a new Operation takes its inputs as a
// std::vector<OpIndex>, we shouldn't count this as "0 inputs because it's
// neither raw OpIndex nor base::Vector<OpIndex>", which a generic overload
// might do).

// Base case
constexpr size_t input_count() { return 0; }

// All parameters that are not OpIndex and should thus not count towards the
// "input_count" of the operations.
template <typename T, typename = std::enable_if_t<std::is_enum_v<T> ||
                                                  std::is_integral_v<T> ||
                                                  std::is_floating_point_v<T>>>
constexpr size_t input_count(T) {
  return 0;
}
// TODO(42203211): The first parameter should be just DirectHandle<T> and
// MaybeDirectHandle<T> but now it does not compile with implicit Handle to
// DirectHandle conversions.
template <template <typename T> typename HandleType, typename T,
          typename = std::enable_if_t<std::disjunction_v<
              std::is_convertible<HandleType<T>, DirectHandle<T>>,
              std::is_convertible<HandleType<T>, MaybeDirectHandle<T>>>>>
constexpr size_t input_count(const HandleType<T>) {
  return 0;
}
template <typename T>
constexpr size_t input_count(const base::Flags<T>) {
  return 0;
}
constexpr size_t input_count(const Block*) { return 0; }
constexpr size_t input_count(const TSCallDescriptor*) { return 0; }
constexpr size_t input_count(const char*) { return 0; }
constexpr size_t input_count(const DeoptimizeParameters*) { return 0; }
constexpr size_t input_count(const FastApiCallParameters*) { return 0; }
constexpr size_t input_count(const FrameStateData*) { return 0; }
constexpr size_t input_count(const base::Vector<SwitchOp::Case>) { return 0; }
constexpr size_t input_count(LoadOp::Kind) { return 0; }
constexpr size_t input_count(RegisterRepresentation) { return 0; }
constexpr size_t input_count(MemoryRepresentation) { return 0; }
constexpr size_t input_count(OpEffects) { return 0; }
inline size_t input_count(const ElementsTransition) { return 0; }
inline size_t input_count(const FeedbackSource) { return 0; }
inline size_t input_count(const ZoneRefSet<Map>) { return 0; }
inline size_t input_count(ConstantOp::Storage) { return 0; }
inline size_t input_count(Type) { return 0; }
#ifdef V8_ENABLE_WEBASSEMBLY
constexpr size_t input_count(const wasm::WasmGlobal*) { return 0; }
constexpr size_t input_count(const wasm::StructType*) { return 0; }
constexpr size_t input_count(const wasm::ArrayType*) { return 0; }
constexpr size_t input_count(wasm::ValueType) { return 0; }
constexpr size_t input_count(WasmTypeCheckConfig) { return 0; }
#endif

// All parameters that are OpIndex-like (ie, OpIndex, and OpIndex containers)
constexpr size_t input_count(OpIndex) { return 1; }
constexpr size_t input_count(OptionalOpIndex) { return 1; }
constexpr size_t input_count(base::Vector<const OpIndex> inputs) {
  return inputs.size();
}
template <typename T>
constexpr size_t input_count(base::Vector<const V<T>> inputs) {
  return inputs.size();
}
}  // namespace detail

template <typename Op, typename... Args>
Op* CreateOperation(base::SmallVector<OperationStorageSlot, 32>& storage,
                    Args... args) {
  size_t input_count = (0 + ... + detail::input_count(args));
  size_t size = Operation::StorageSlotCount(Op::opcode, input_count);
  storage.resize_no_init(size);
  Op* op = new (storage.data()) Op(args...);
  // Checking that the {input_count} we computed is at least the actual
  // input_count of the operation. {input_count} could be greater in the case of
  // OptionalOpIndex: they count for 1 input when computing {input_count} here,
  // but in Operations, they only count for 1 input when they are valid.
  DCHECK_GE(input_count, op->input_count);
  return op;
}

template <typename F>
auto VisitOperation(const Operation& op, F&& f) {
  switch (op.opcode) {
#define CASE(name)      \
  case Opcode::k##name: \
    return f(op.Cast<name##Op>());
    TURBOSHAFT_OPERATION_LIST(CASE)
#undef CASE
  }
}

// Checking that throwing operations have the required members and options.
namespace details {

template <typename T, typename Tuple>
struct TupleHasType;

template <typename T, typename... Ts>
struct TupleHasType<T, std::tuple<Ts...>> {
  static constexpr bool value = (std::is_same_v<T, Ts> || ...);
};

template <typename Op, typename = void>
struct ThrowingOpHasProperMembers : std::false_type {};
template <typename Op>
struct ThrowingOpHasProperMembers<
    Op, std::void_t<std::conjunction<decltype(Op::kOutputRepsStorage),
                                     decltype(Op::lazy_deopt_on_throw)>>>
    : std::true_type {};

template <typename Op, typename = void>
struct ThrowingOpHasLazyDeoptOption : std::false_type {};

template <typename Op>
struct ThrowingOpHasLazyDeoptOption<
    Op, std::enable_if_t<TupleHasType<
            LazyDeoptOnThrow, decltype(std::declval<Op>().options())>::value>>
    : std::true_type {};

// CallOp has special handling because its outputs_rep are dynamic (and found on
// its call descriptor).
template <>
struct ThrowingOpHasLazyDeoptOption<CallOp, void> : std::true_type {};
template <>
struct ThrowingOpHasProperMembers<CallOp, void> : std::true_type {};

}  // namespace details

#define THROWING_OP_LOOKS_VALID(Name)                             \
  static_assert(details::ThrowingOpHasProperMembers<Name##Op>()); \
  static_assert(details::ThrowingOpHasLazyDeoptOption<Name##Op>());
TURBOSHAFT_THROWING_OPERATIONS_LIST(THROWING_OP_LOOKS_VALID)
#undef THROWING_OP_LOOKS_VALID

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_OPERATIONS_H_
                                                                                                                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/opmasks.h                                               0000664 0000000 0000000 00000036540 14746647661 0022465 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_OPMASKS_H_
#define V8_COMPILER_TURBOSHAFT_OPMASKS_H_

#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"

// The Opmasks allow performing a type check or cast with an operation mask
// that doesn't only encode the opcode but also additional properties, i.e.
// fields of an operation.
// The type check will be expressed by masking out the first 8 bytes of the
// object based on a generic Opmask and then comparing it against a specific
// shape of that mask.
//
// Given the following operation and mask definitions:
//
//   struct ConvertOp : FixedArityOperationT<1, ConvertOp> {
//     enum Type : int8_t {kBool, kInt, kFloat};
//     Type from;
//     Type to;
//   };
//
//   using ConvertOpMask =
//     MaskBuilder<ConvertOp, FIELD(ConvertOp, from), FIELD(ConvertOp, to)>;
//   using ConvertOpTargetMask = MaskBuilder<ConvertOp, FIELD(ConvertOp, to)>;
//
//   using ConvertFloatToInt =
//     ConvertOpMask::For<ConvertOp::kFloat, ConvertOp::kInt>;
//   using ConvertToInt =
//     ConvertOpTargetMask::For<ConvertOp::kInt>;
//
// The masks can be used in the following way:
//
//    const Operation& my_op = ...;
//    bool is_float_to_int = my_op.Is<ConvertFloatToInt>();
//    const ConvertOp* to_int = my_op.TryCast<ConvertToInt>();
//
// Where to_int will be non-null iff my_op is a ConvertOp *and* the target type
// is int.

namespace v8::internal::compiler::turboshaft::Opmask {

template <typename T, size_t Offset>
struct OpMaskField {
  using type = T;
  static constexpr size_t offset = Offset;
  static constexpr size_t size = sizeof(T);

  static_assert(offset + size <= sizeof(uint64_t));
};

template <typename T>
constexpr uint64_t encode_for_mask(T value) {
  return static_cast<uint64_t>(value);
}

template <typename T>
struct UnwrapRepresentation {
  using type = T;
};
template <>
struct UnwrapRepresentation<WordRepresentation> {
  using type = WordRepresentation::Enum;
};
template <>
struct UnwrapRepresentation<FloatRepresentation> {
  using type = FloatRepresentation::Enum;
};
template <>
struct UnwrapRepresentation<RegisterRepresentation> {
  using type = RegisterRepresentation::Enum;
};

template <typename Op, typename... Fields>
struct MaskBuilder {
  static constexpr uint64_t BuildBaseMask() {
    static_assert(OFFSET_OF(Operation, opcode) == 0);
    static_assert(sizeof(Operation::opcode) == sizeof(uint8_t));
    static_assert(sizeof(Operation) == 4);
#if V8_TARGET_BIG_ENDIAN
    return static_cast<uint64_t>(0xFF)
           << ((sizeof(uint64_t) - sizeof(uint8_t)) * kBitsPerByte);
#else
    return static_cast<uint64_t>(0xFF);
#endif
  }

  static constexpr uint64_t EncodeBaseValue(Opcode opcode) {
    static_assert(OFFSET_OF(Operation, opcode) == 0);
#if V8_TARGET_BIG_ENDIAN
    return static_cast<uint64_t>(opcode)
           << ((sizeof(uint64_t) - sizeof(Operation::opcode)) * kBitsPerByte);
#else
    return static_cast<uint64_t>(opcode);
#endif
  }

  static constexpr uint64_t BuildMask() {
    constexpr uint64_t base_mask = BuildBaseMask();
    return (base_mask | ... | BuildFieldMask<Fields>());
  }

  static constexpr uint64_t EncodeValue(typename Fields::type... args) {
    constexpr uint64_t base_mask = EncodeBaseValue(operation_to_opcode_v<Op>);
    return (base_mask | ... | EncodeFieldValue<Fields>(args));
  }

  template <typename F>
  static constexpr uint64_t BuildFieldMask() {
    static_assert(F::size < sizeof(uint64_t));
    static_assert(F::offset + F::size <= sizeof(uint64_t));
    constexpr uint64_t ones = static_cast<uint64_t>(-1) >>
                              ((sizeof(uint64_t) - F::size) * kBitsPerByte);
#if V8_TARGET_BIG_ENDIAN
    return ones << ((sizeof(uint64_t) - F::size - F::offset) * kBitsPerByte);
#else
    return ones << (F::offset * kBitsPerByte);
#endif
  }

  template <typename F>
  static constexpr uint64_t EncodeFieldValue(typename F::type value) {
#if V8_TARGET_BIG_ENDIAN
    return encode_for_mask(value)
           << ((sizeof(uint64_t) - F::size - F::offset) * kBitsPerByte);
#else
    return encode_for_mask(value) << (F::offset * kBitsPerByte);
#endif
  }

  template <typename Fields::type... Args>
  using For = OpMaskT<Op, BuildMask(), EncodeValue(Args...)>;
};

#define FIELD(op, field_name)                                       \
  OpMaskField<UnwrapRepresentation<decltype(op::field_name)>::type, \
              OFFSET_OF(op, field_name)>

// === Definitions of masks for Turboshaft operations === //

using WordBinopMask =
    MaskBuilder<WordBinopOp, FIELD(WordBinopOp, kind), FIELD(WordBinopOp, rep)>;
using WordBinopKindMask = MaskBuilder<WordBinopOp, FIELD(WordBinopOp, kind)>;

using kWord32Add =
    WordBinopMask::For<WordBinopOp::Kind::kAdd, WordRepresentation::Word32()>;
using kWord32Sub =
    WordBinopMask::For<WordBinopOp::Kind::kSub, WordRepresentation::Word32()>;
using kWord32Mul =
    WordBinopMask::For<WordBinopOp::Kind::kMul, WordRepresentation::Word32()>;
using kWord32SignedMulOverflownBits =
    WordBinopMask::For<WordBinopOp::Kind::kSignedMulOverflownBits,
                       WordRepresentation::Word32()>;
using kWord32UnsignedMulOverflownBits =
    WordBinopMask::For<WordBinopOp::Kind::kUnsignedMulOverflownBits,
                       WordRepresentation::Word32()>;

using kWord32BitwiseAnd = WordBinopMask::For<WordBinopOp::Kind::kBitwiseAnd,
                                             WordRepresentation::Word32()>;
using kWord32BitwiseOr = WordBinopMask::For<WordBinopOp::Kind::kBitwiseOr,
                                            WordRepresentation::Word32()>;
using kWord32BitwiseXor = WordBinopMask::For<WordBinopOp::Kind::kBitwiseXor,
                                             WordRepresentation::Word32()>;
using kWord64Add =
    WordBinopMask::For<WordBinopOp::Kind::kAdd, WordRepresentation::Word64()>;
using kWord64Sub =
    WordBinopMask::For<WordBinopOp::Kind::kSub, WordRepresentation::Word64()>;
using kWord64Mul =
    WordBinopMask::For<WordBinopOp::Kind::kMul, WordRepresentation::Word64()>;
using kWord64BitwiseAnd = WordBinopMask::For<WordBinopOp::Kind::kBitwiseAnd,
                                             WordRepresentation::Word64()>;
using kWord64BitwiseOr = WordBinopMask::For<WordBinopOp::Kind::kBitwiseOr,
                                            WordRepresentation::Word64()>;
using kWord64BitwiseXor = WordBinopMask::For<WordBinopOp::Kind::kBitwiseXor,
                                             WordRepresentation::Word64()>;

using kBitwiseAnd = WordBinopKindMask::For<WordBinopOp::Kind::kBitwiseAnd>;
using kBitwiseXor = WordBinopKindMask::For<WordBinopOp::Kind::kBitwiseXor>;

using WordUnaryMask =
    MaskBuilder<WordUnaryOp, FIELD(WordUnaryOp, kind), FIELD(WordUnaryOp, rep)>;
using kWord32ReverseBytes = WordUnaryMask::For<WordUnaryOp::Kind::kReverseBytes,
                                               WordRepresentation::Word32()>;
using kWord64ReverseBytes = WordUnaryMask::For<WordUnaryOp::Kind::kReverseBytes,
                                               WordRepresentation::Word64()>;

using FloatUnaryMask = MaskBuilder<FloatUnaryOp, FIELD(FloatUnaryOp, kind),
                                   FIELD(FloatUnaryOp, rep)>;

using kFloat32Negate = FloatUnaryMask::For<FloatUnaryOp::Kind::kNegate,
                                           FloatRepresentation::Float32()>;
using kFloat64Abs = FloatUnaryMask::For<FloatUnaryOp::Kind::kAbs,
                                        FloatRepresentation::Float64()>;
using kFloat64Negate = FloatUnaryMask::For<FloatUnaryOp::Kind::kNegate,
                                           FloatRepresentation::Float64()>;

using FloatBinopMask = MaskBuilder<FloatBinopOp, FIELD(FloatBinopOp, kind),
                                   FIELD(FloatBinopOp, rep)>;

using kFloat32Sub = FloatBinopMask::For<FloatBinopOp::Kind::kSub,
                                        FloatRepresentation::Float32()>;
using kFloat32Mul = FloatBinopMask::For<FloatBinopOp::Kind::kMul,
                                        FloatRepresentation::Float32()>;
using kFloat64Sub = FloatBinopMask::For<FloatBinopOp::Kind::kSub,
                                        FloatRepresentation::Float64()>;
using kFloat64Mul = FloatBinopMask::For<FloatBinopOp::Kind::kMul,
                                        FloatRepresentation::Float64()>;

using ShiftMask =
    MaskBuilder<ShiftOp, FIELD(ShiftOp, kind), FIELD(ShiftOp, rep)>;
using ShiftKindMask = MaskBuilder<ShiftOp, FIELD(ShiftOp, kind)>;

using kWord32ShiftLeft =
    ShiftMask::For<ShiftOp::Kind::kShiftLeft, WordRepresentation::Word32()>;
using kWord32ShiftRightArithmetic =
    ShiftMask::For<ShiftOp::Kind::kShiftRightArithmetic,
                   WordRepresentation::Word32()>;
using kWord32ShiftRightArithmeticShiftOutZeros =
    ShiftMask::For<ShiftOp::Kind::kShiftRightArithmeticShiftOutZeros,
                   WordRepresentation::Word32()>;
using kWord32ShiftRightLogical =
    ShiftMask::For<ShiftOp::Kind::kShiftRightLogical,
                   WordRepresentation::Word32()>;
using kWord32RotateRight =
    ShiftMask::For<ShiftOp::Kind::kRotateRight, WordRepresentation::Word32()>;
using kWord64ShiftLeft =
    ShiftMask::For<ShiftOp::Kind::kShiftLeft, WordRepresentation::Word64()>;
using kWord64ShiftRightArithmetic =
    ShiftMask::For<ShiftOp::Kind::kShiftRightArithmetic,
                   WordRepresentation::Word64()>;
using kWord64ShiftRightLogical =
    ShiftMask::For<ShiftOp::Kind::kShiftRightLogical,
                   WordRepresentation::Word64()>;
using kShiftLeft = ShiftKindMask::For<ShiftOp::Kind::kShiftLeft>;

using PhiMask = MaskBuilder<PhiOp, FIELD(PhiOp, rep)>;
using kTaggedPhi = PhiMask::For<RegisterRepresentation::Tagged()>;

using ConstantMask = MaskBuilder<ConstantOp, FIELD(ConstantOp, kind)>;

using kWord32Constant = ConstantMask::For<ConstantOp::Kind::kWord32>;
using kWord64Constant = ConstantMask::For<ConstantOp::Kind::kWord64>;
using kExternalConstant = ConstantMask::For<ConstantOp::Kind::kExternal>;

using ProjectionMask = MaskBuilder<ProjectionOp, FIELD(ProjectionOp, index)>;

using kProjection0 = ProjectionMask::For<0>;
using kProjection1 = ProjectionMask::For<1>;

using ComparisonMask = MaskBuilder<ComparisonOp, FIELD(ComparisonOp, kind),
                                   FIELD(ComparisonOp, rep)>;

using kWord32Equal = ComparisonMask::For<ComparisonOp::Kind::kEqual,
                                         WordRepresentation::Word32()>;
using kWord64Equal = ComparisonMask::For<ComparisonOp::Kind::kEqual,
                                         WordRepresentation::Word64()>;
using ComparisonKindMask = MaskBuilder<ComparisonOp, FIELD(ComparisonOp, kind)>;
using kComparisonEqual = ComparisonKindMask::For<ComparisonOp::Kind::kEqual>;

using ChangeOpMask =
    MaskBuilder<ChangeOp, FIELD(ChangeOp, kind), FIELD(ChangeOp, assumption),
                FIELD(ChangeOp, from), FIELD(ChangeOp, to)>;

using kChangeInt32ToInt64 = ChangeOpMask::For<
    ChangeOp::Kind::kSignExtend, ChangeOp::Assumption::kNoAssumption,
    RegisterRepresentation::Word32(), RegisterRepresentation::Word64()>;
using kChangeUint32ToUint64 = ChangeOpMask::For<
    ChangeOp::Kind::kZeroExtend, ChangeOp::Assumption::kNoAssumption,
    RegisterRepresentation::Word32(), RegisterRepresentation::Word64()>;
using kFloat64ExtractHighWord32 = ChangeOpMask::For<
    ChangeOp::Kind::kExtractHighHalf, ChangeOp::Assumption::kNoAssumption,
    RegisterRepresentation::Float64(), RegisterRepresentation::Word32()>;
using kTruncateFloat64ToInt64OverflowToMin =
    ChangeOpMask::For<ChangeOp::Kind::kSignedFloatTruncateOverflowToMin,
                      ChangeOp::Assumption::kNoAssumption,
                      RegisterRepresentation::Float64(),
                      RegisterRepresentation::Word64()>;
using kTruncateFloat32ToInt32OverflowToMin =
    ChangeOpMask::For<ChangeOp::Kind::kSignedFloatTruncateOverflowToMin,
                      ChangeOp::Assumption::kNoAssumption,
                      RegisterRepresentation::Float32(),
                      RegisterRepresentation::Word32()>;
using kTruncateFloat32ToUint32OverflowToMin =
    ChangeOpMask::For<ChangeOp::Kind::kUnsignedFloatTruncateOverflowToMin,
                      ChangeOp::Assumption::kNoAssumption,
                      RegisterRepresentation::Float32(),
                      RegisterRepresentation::Word32()>;

using kTruncateWord64ToWord32 = ChangeOpMask::For<
    ChangeOp::Kind::kTruncate, ChangeOp::Assumption::kNoAssumption,
    RegisterRepresentation::Word64(), RegisterRepresentation::Word32()>;

using OverflowCheckedBinopMask =
    MaskBuilder<OverflowCheckedBinopOp, FIELD(OverflowCheckedBinopOp, kind),
                FIELD(OverflowCheckedBinopOp, rep)>;
using kOverflowCheckedWord32Add =
    OverflowCheckedBinopMask::For<OverflowCheckedBinopOp::Kind::kSignedAdd,
                                  WordRepresentation::Word32()>;

using TaggedBitcastMask =
    MaskBuilder<TaggedBitcastOp, FIELD(TaggedBitcastOp, from),
                FIELD(TaggedBitcastOp, to), FIELD(TaggedBitcastOp, kind)>;
using kBitcastTaggedToWordPtrForTagAndSmiBits =
    TaggedBitcastMask::For<RegisterRepresentation::Tagged(),
                           RegisterRepresentation::WordPtr(),
                           TaggedBitcastOp::Kind::kTagAndSmiBits>;
using kBitcastWordPtrToSmi =
    TaggedBitcastMask::For<RegisterRepresentation::WordPtr(),
                           RegisterRepresentation::Tagged(),
                           TaggedBitcastOp::Kind::kSmi>;

using TaggedBitcastKindMask =
    MaskBuilder<TaggedBitcastOp, FIELD(TaggedBitcastOp, kind)>;
using kTaggedBitcastSmi =
    TaggedBitcastKindMask::For<TaggedBitcastOp::Kind::kSmi>;
using kTaggedBitcastHeapObject =
    TaggedBitcastKindMask::For<TaggedBitcastOp::Kind::kHeapObject>;

#if V8_ENABLE_WEBASSEMBLY

using Simd128BinopMask =
    MaskBuilder<Simd128BinopOp, FIELD(Simd128BinopOp, kind)>;
using kSimd128I32x4Mul = Simd128BinopMask::For<Simd128BinopOp::Kind::kI32x4Mul>;
using kSimd128I16x8Mul = Simd128BinopMask::For<Simd128BinopOp::Kind::kI16x8Mul>;

#define SIMD_SIGN_EXTENSION_BINOP_MASK(kind) \
  using kSimd128##kind = Simd128BinopMask::For<Simd128BinopOp::Kind::k##kind>;
FOREACH_SIMD_128_BINARY_SIGN_EXTENSION_OPCODE(SIMD_SIGN_EXTENSION_BINOP_MASK)
#undef SIMD_SIGN_EXTENSION_BINOP_MASK

using Simd128UnaryMask =
    MaskBuilder<Simd128UnaryOp, FIELD(Simd128UnaryOp, kind)>;
#define SIMD_UNARY_MASK(kind) \
  using kSimd128##kind = Simd128UnaryMask::For<Simd128UnaryOp::Kind::k##kind>;
FOREACH_SIMD_128_UNARY_OPCODE(SIMD_UNARY_MASK)
#undef SIMD_UNARY_MASK

using Simd128ShiftMask =
    MaskBuilder<Simd128ShiftOp, FIELD(Simd128ShiftOp, kind)>;
#define SIMD_SHIFT_MASK(kind) \
  using kSimd128##kind = Simd128ShiftMask::For<Simd128ShiftOp::Kind::k##kind>;
FOREACH_SIMD_128_SHIFT_OPCODE(SIMD_SHIFT_MASK)
#undef SIMD_SHIFT_MASK

using Simd128LoadTransformMask =
    MaskBuilder<Simd128LoadTransformOp,
                FIELD(Simd128LoadTransformOp, transform_kind)>;
#define SIMD_LOAD_TRANSFORM_MASK(kind)                               \
  using kSimd128LoadTransform##kind = Simd128LoadTransformMask::For< \
      Simd128LoadTransformOp::TransformKind::k##kind>;
FOREACH_SIMD_128_LOAD_TRANSFORM_OPCODE(SIMD_LOAD_TRANSFORM_MASK)
#undef SIMD_LOAD_TRANSFORM_MASK

#endif  // V8_ENABLE_WEBASSEMBLY

#ifndef TURBOSHAFT_OPMASK_EXPORT_FIELD_MACRO_FOR_UNITTESTS
#undef FIELD
#endif

}  // namespace v8::internal::compiler::turboshaft::Opmask

#endif  // V8_COMPILER_TURBOSHAFT_OPMASKS_H_
                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/optimize-phase.cc                                       0000664 0000000 0000000 00000003266 14746647661 0024103 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/optimize-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/late-escape-analysis-reducer.h"
#include "src/compiler/turboshaft/machine-optimization-reducer.h"
#include "src/compiler/turboshaft/memory-optimization-reducer.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/pretenuring-propagation-reducer.h"
#include "src/compiler/turboshaft/required-optimization-reducer.h"
#include "src/compiler/turboshaft/structural-optimization-reducer.h"
#include "src/compiler/turboshaft/value-numbering-reducer.h"
#include "src/compiler/turboshaft/variable-reducer.h"
#include "src/numbers/conversions-inl.h"
#include "src/roots/roots-inl.h"

namespace v8::internal::compiler::turboshaft {

void OptimizePhase::Run(PipelineData* data, Zone* temp_zone) {
  UnparkedScopeIfNeeded scope(data->broker(),
                              v8_flags.turboshaft_trace_reduction);
  turboshaft::CopyingPhase<turboshaft::StructuralOptimizationReducer,
                           turboshaft::LateEscapeAnalysisReducer,
                           turboshaft::PretenuringPropagationReducer,
                           turboshaft::MemoryOptimizationReducer,
                           turboshaft::MachineOptimizationReducer,
                           turboshaft::ValueNumberingReducer>::Run(data,
                                                                   temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/compiler/turboshaft/optimize-phase.h                                        0000664 0000000 0000000 00000001110 14746647661 0023727 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_OPTIMIZE_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_OPTIMIZE_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct OptimizePhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(Optimize)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_OPTIMIZE_PHASE_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                        node-23.7.0/deps/v8/src/compiler/turboshaft/phase.cc                                                0000664 0000000 0000000 00000011306 14746647661 0022237 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/phase.h"

#include "src/compiler/backend/register-allocator.h"
#include "src/compiler/graph-visualizer.h"
#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/graph-visualizer.h"
#include "src/diagnostics/code-tracer.h"
#include "src/utils/ostreams.h"
#ifdef V8_ENABLE_WEBASSEMBLY
#include "src/wasm/wasm-engine.h"
#endif

namespace v8::internal::compiler::turboshaft {

void PipelineData::InitializeRegisterComponent(
    const RegisterConfiguration* config, CallDescriptor* call_descriptor) {
  DCHECK(!register_component_.has_value());
  register_component_.emplace(zone_stats());
  auto& zone = register_component_->zone;
  register_component_->allocation_data = zone.New<RegisterAllocationData>(
      config, zone, frame(), sequence(), &info()->tick_counter(),
      debug_name_.get());
}

AccountingAllocator* PipelineData::allocator() const {
  if (isolate_) return isolate_->allocator();
#ifdef V8_ENABLE_WEBASSEMBLY
  if (auto e = wasm::GetWasmEngine()) {
    return e->allocator();
  }
#endif
  return nullptr;
}

void PrintTurboshaftGraph(PipelineData* data, Zone* temp_zone,
                          CodeTracer* code_tracer, const char* phase_name) {
  if (data->info()->trace_turbo_json()) {
    UnparkedScopeIfNeeded scope(data->broker());
    AllowHandleDereference allow_deref;
    turboshaft::Graph& graph = data->graph();

    TurboJsonFile json_of(data->info(), std::ios_base::app);
    PrintTurboshaftGraphForTurbolizer(json_of, graph, phase_name,
                                      data->node_origins(), temp_zone);
  }

  if (data->info()->trace_turbo_graph()) {
    DCHECK(code_tracer);
    UnparkedScopeIfNeeded scope(data->broker());
    AllowHandleDereference allow_deref;

    CodeTracer::StreamScope tracing_scope(code_tracer);
    tracing_scope.stream() << "\n----- " << phase_name << " -----\n"
                           << data->graph();
  }
}

void PrintTurboshaftGraphForTurbolizer(std::ofstream& stream,
                                       const Graph& graph,
                                       const char* phase_name,
                                       NodeOriginTable* node_origins,
                                       Zone* temp_zone) {
  stream << "{\"name\":\"" << phase_name
         << "\",\"type\":\"turboshaft_graph\",\"data\":"
         << AsJSON(graph, node_origins, temp_zone) << "},\n";

  PrintTurboshaftCustomDataPerOperation(
      stream, "Properties", graph,
      [](std::ostream& stream, const turboshaft::Graph& graph,
         turboshaft::OpIndex index) -> bool {
        const auto& op = graph.Get(index);
        op.PrintOptions(stream);
        return true;
      });
  PrintTurboshaftCustomDataPerOperation(
      stream, "Types", graph,
      [](std::ostream& stream, const turboshaft::Graph& graph,
         turboshaft::OpIndex index) -> bool {
        turboshaft::Type type = graph.operation_types()[index];
        if (!type.IsInvalid() && !type.IsNone()) {
          type.PrintTo(stream);
          return true;
        }
        return false;
      });
  PrintTurboshaftCustomDataPerOperation(
      stream, "Representations", graph,
      [](std::ostream& stream, const turboshaft::Graph& graph,
         turboshaft::OpIndex index) -> bool {
        const Operation& op = graph.Get(index);
        stream << PrintCollection(op.outputs_rep());
        return true;
      });
  PrintTurboshaftCustomDataPerOperation(
      stream, "Use Count (saturated)", graph,
      [](std::ostream& stream, const turboshaft::Graph& graph,
         turboshaft::OpIndex index) -> bool {
        stream << static_cast<int>(graph.Get(index).saturated_use_count.Get());
        return true;
      });
#ifdef DEBUG
  PrintTurboshaftCustomDataPerBlock(
      stream, "Type Refinements", graph,
      [](std::ostream& stream, const turboshaft::Graph& graph,
         turboshaft::BlockIndex index) -> bool {
        const std::vector<std::pair<turboshaft::OpIndex, turboshaft::Type>>&
            refinements = graph.block_type_refinement()[index];
        if (refinements.empty()) return false;
        stream << "\\n";
        for (const auto& [op, type] : refinements) {
          stream << op << " : " << type << "\\n";
        }
        return true;
      });
#endif  // DEBUG
}

CodeTracer* PipelineData::GetCodeTracer() const {
#if V8_ENABLE_WEBASSEMBLY
  if (info_->IsWasm() || info_->IsWasmBuiltin()) {
    return wasm::GetWasmEngine()->GetCodeTracer();
  }
#endif  // V8_ENABLE_WEBASSEMBLY
  DCHECK_NOT_NULL(isolate_);
  return isolate_->GetCodeTracer();
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/compiler/turboshaft/phase.h                                                 0000664 0000000 0000000 00000045450 14746647661 0022110 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_PHASE_H_

#include <optional>
#include <type_traits>

#include "src/base/contextual.h"
#include "src/base/template-meta-programming/functional.h"
#include "src/codegen/assembler.h"
#include "src/codegen/optimized-compilation-info.h"
#include "src/common/globals.h"
#include "src/compiler/access-info.h"
#include "src/compiler/backend/instruction.h"
#include "src/compiler/compilation-dependencies.h"
#include "src/compiler/compiler-source-position-table.h"
#include "src/compiler/node-origin-table.h"
#include "src/compiler/osr.h"
#include "src/compiler/phase.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/zone-with-name.h"
#include "src/logging/runtime-call-stats.h"
#include "src/zone/accounting-allocator.h"
#include "src/zone/zone.h"

#ifdef HAS_CPP_CONCEPTS
#define STATIC_ASSERT_IF_CONCEPTS(cond) static_assert(cond)
#else
#define STATIC_ASSERT_IF_CONCEPTS(cond)
#endif  // HAS_CPP_CONCEPTS

#define DECL_TURBOSHAFT_PHASE_CONSTANTS_IMPL(Name, CallStatsName)             \
  DECL_PIPELINE_PHASE_CONSTANTS_HELPER(CallStatsName, PhaseKind::kTurboshaft, \
                                       RuntimeCallStats::kThreadSpecific)     \
  static constexpr char kPhaseName[] = "V8.TF" #CallStatsName;                \
  static void AssertTurboshaftPhase() {                                       \
    STATIC_ASSERT_IF_CONCEPTS(TurboshaftPhase<Name##Phase>);                  \
  }

#define DECL_TURBOSHAFT_PHASE_CONSTANTS(Name) \
  DECL_TURBOSHAFT_PHASE_CONSTANTS_IMPL(Name, Turboshaft##Name)
#define DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(Name) \
  DECL_TURBOSHAFT_PHASE_CONSTANTS_IMPL(Name, Name)

#define DECL_TURBOSHAFT_MAIN_THREAD_PIPELINE_PHASE_CONSTANTS_WITH_LEGACY_NAME( \
    Name)                                                                      \
  DECL_PIPELINE_PHASE_CONSTANTS_HELPER(Name, PhaseKind::kTurboshaft,           \
                                       RuntimeCallStats::kExact)               \
  static constexpr char kPhaseName[] = "V8.TF" #Name;                          \
  static void AssertTurboshaftPhase() {                                        \
    STATIC_ASSERT_IF_CONCEPTS(TurboshaftPhase<Name##Phase>);                   \
  }

namespace v8::internal::compiler {
class RegisterAllocationData;
class Schedule;
class TurbofanPipelineStatistics;
}  // namespace v8::internal::compiler

namespace v8::internal::compiler::turboshaft {

class PipelineData;

#ifdef HAS_CPP_CONCEPTS
template <typename Phase>
struct HasProperRunMethod {
  using parameters = base::tmp::call_parameters_t<decltype(&Phase::Run)>;
  static_assert(
      base::tmp::length_v<parameters> >= 2,
      "Phase::Run needs at least two parameters (PipelineData* and Zone*)");
  using parameter0 = base::tmp::element_t<parameters, 0>;
  using parameter1 = base::tmp::element_t<parameters, 1>;
  static constexpr bool value = std::is_same_v<parameter0, PipelineData*> &&
                                std::is_same_v<parameter1, Zone*>;
};

template <typename Phase, typename... Args>
concept TurboshaftPhase =
    HasProperRunMethod<Phase>::value &&
    requires(Phase p) { p.kKind == PhaseKind::kTurboshaft; };

template <typename Phase>
concept TurbofanPhase = requires(Phase p) { p.kKind == PhaseKind::kTurbofan; };

template <typename Phase>
concept CompilerPhase = TurboshaftPhase<Phase> || TurbofanPhase<Phase>;

#define CONCEPT(name) name
#else  // HAS_CPP_CONCEPTS
#define CONCEPT(name) typename
#endif  // HAS_CPP_CONCEPTS

namespace detail {
template <typename, typename = void>
struct produces_printable_graph_impl : std::true_type {};

template <typename P>
struct produces_printable_graph_impl<
    P, std::void_t<decltype(P::kOutputIsTraceableGraph)>>
    : std::bool_constant<P::kOutputIsTraceableGraph> {};

#ifdef HAS_CPP_CLASS_TYPES_AS_TEMPLATE_ARGS
template <base::tmp::StringLiteral ZoneName>
#else
template <auto ZoneName>
#endif
struct ComponentWithZone {
  template <typename T>
  using Pointer = ZoneWithNamePointer<T, ZoneName>;

  explicit ComponentWithZone(ZoneStats* zone_stats)
      : zone(zone_stats,
#ifdef HAS_CPP_CLASS_TYPES_AS_TEMPLATE_ARGS
             ZoneName.c_str()
#else
             ZONE_NAME
#endif
        ) {
  }
  explicit ComponentWithZone(ZoneWithName<ZoneName> existing_zone)
      : zone(std::move(existing_zone)) {}

  ZoneWithName<ZoneName> zone;
};

struct BuiltinComponent {
  const CallDescriptor* call_descriptor;

  BuiltinComponent(const CallDescriptor* call_descriptor)
      : call_descriptor(call_descriptor) {}
};

struct GraphComponent : public ComponentWithZone<kGraphZoneName> {
  using ComponentWithZone::ComponentWithZone;

  Pointer<Graph> graph = nullptr;
  Pointer<SourcePositionTable> source_positions = nullptr;
  Pointer<NodeOriginTable> node_origins = nullptr;
  bool graph_has_special_rpo = false;
};

struct CodegenComponent : public ComponentWithZone<kCodegenZoneName> {
  using ComponentWithZone::ComponentWithZone;

  Pointer<Frame> frame = nullptr;
  std::unique_ptr<CodeGenerator> code_generator;
  Pointer<CompilationDependency> dependencies = nullptr;
  // TODO(nicohartmann): Make {osr_helper} an optional once TurboFan's
  // PipelineData is gone.
  std::shared_ptr<OsrHelper> osr_helper;
  JumpOptimizationInfo* jump_optimization_info = nullptr;
  size_t max_unoptimized_frame_height = 0;
  size_t max_pushed_argument_count = 0;
};

struct InstructionComponent : public ComponentWithZone<kInstructionZoneName> {
  using ComponentWithZone::ComponentWithZone;

  Pointer<InstructionSequence> sequence = nullptr;
};

struct RegisterComponent
    : public ComponentWithZone<kRegisterAllocationZoneName> {
  using ComponentWithZone::ComponentWithZone;

  Pointer<RegisterAllocationData> allocation_data = nullptr;
};
}  // namespace detail

template <typename P>
struct produces_printable_graph
    : public detail::produces_printable_graph_impl<P> {};

enum class TurboshaftPipelineKind { kJS, kWasm, kCSA, kTSABuiltin, kJSToWasm };

class LoopUnrollingAnalyzer;
class WasmRevecAnalyzer;

class V8_EXPORT_PRIVATE PipelineData {
  using BuiltinComponent = detail::BuiltinComponent;
  using GraphComponent = detail::GraphComponent;
  using CodegenComponent = detail::CodegenComponent;
  using InstructionComponent = detail::InstructionComponent;
  using RegisterComponent = detail::RegisterComponent;

 public:
  explicit PipelineData(ZoneStats* zone_stats,
                        TurboshaftPipelineKind pipeline_kind, Isolate* isolate,
                        OptimizedCompilationInfo* info,
                        const AssemblerOptions& assembler_options,
                        int start_source_position = kNoSourcePosition)
      : zone_stats_(zone_stats),
        compilation_zone_(zone_stats, kCompilationZoneName),
        pipeline_kind_(pipeline_kind),
        isolate_(isolate),
        info_(info),
        debug_name_(info_ ? info_->GetDebugName() : std::unique_ptr<char[]>{}),
        start_source_position_(start_source_position),
        assembler_options_(assembler_options) {
#if V8_ENABLE_WEBASSEMBLY
    if (info != nullptr) {
      DCHECK_EQ(assembler_options_.is_wasm,
                info->IsWasm() || info->IsWasmBuiltin());
    }
#endif
  }

  void InitializeBrokerAndDependencies(std::shared_ptr<JSHeapBroker> broker,
                                       CompilationDependencies* dependencies) {
    DCHECK_NULL(broker_.get());
    DCHECK_NULL(dependencies_);
    DCHECK_NOT_NULL(broker);
    DCHECK_NOT_NULL(dependencies);
    broker_ = std::move(broker);
    dependencies_ = dependencies;
  }

  void InitializeBuiltinComponent(const CallDescriptor* call_descriptor) {
    DCHECK(!builtin_component_.has_value());
    builtin_component_.emplace(call_descriptor);
  }

  void InitializeGraphComponent(SourcePositionTable* source_positions) {
    DCHECK(!graph_component_.has_value());
    graph_component_.emplace(zone_stats_);
    auto& zone = graph_component_->zone;
    graph_component_->graph = zone.New<Graph>(zone);
    graph_component_->source_positions =
        GraphComponent::Pointer<SourcePositionTable>(source_positions);
    if (info_ && info_->trace_turbo_json()) {
      graph_component_->node_origins = zone.New<NodeOriginTable>(zone);
    }
  }

  void InitializeGraphComponentWithGraphZone(
      ZoneWithName<kGraphZoneName> graph_zone,
      ZoneWithNamePointer<SourcePositionTable, kGraphZoneName> source_positions,
      ZoneWithNamePointer<NodeOriginTable, kGraphZoneName> node_origins) {
    DCHECK(!graph_component_.has_value());
    graph_component_.emplace(std::move(graph_zone));
    auto& zone = graph_component_->zone;
    graph_component_->graph = zone.New<Graph>(zone);
    graph_component_->source_positions = source_positions;
    graph_component_->node_origins = node_origins;
    if (!graph_component_->node_origins && info_ && info_->trace_turbo_json()) {
      graph_component_->node_origins = zone.New<NodeOriginTable>(zone);
    }
  }

  void ClearGraphComponent() {
    DCHECK(graph_component_.has_value());
    graph_component_.reset();
  }

  void InitializeCodegenComponent(
      std::shared_ptr<OsrHelper> osr_helper,
      JumpOptimizationInfo* jump_optimization_info = nullptr) {
    DCHECK(!codegen_component_.has_value());
    codegen_component_.emplace(zone_stats_);
    codegen_component_->osr_helper = std::move(osr_helper);
    codegen_component_->jump_optimization_info = jump_optimization_info;
  }

  void ClearCodegenComponent() {
    DCHECK(codegen_component_.has_value());
    codegen_component_.reset();
  }

  void InitializeCodeGenerator(Linkage* linkage) {
    DCHECK(codegen_component_.has_value());
    CodegenComponent& cg = *codegen_component_;
    DCHECK_NULL(codegen_component_->code_generator);
#if V8_ENABLE_WEBASSEMBLY
    DCHECK_EQ(assembler_options_.is_wasm,
              info()->IsWasm() || info()->IsWasmBuiltin());
#endif
    std::optional<OsrHelper> osr_helper;
    if (cg.osr_helper) osr_helper = *cg.osr_helper;
    cg.code_generator = std::make_unique<CodeGenerator>(
        cg.zone, cg.frame, linkage, sequence(), info_, isolate_,
        std::move(osr_helper), start_source_position_,
        cg.jump_optimization_info, assembler_options_, info_->builtin(),
        cg.max_unoptimized_frame_height, cg.max_pushed_argument_count,
        v8_flags.trace_turbo_stack_accesses ? debug_name_.get() : nullptr);
  }

  void InitializeInstructionComponent(const CallDescriptor* call_descriptor) {
    DCHECK(!instruction_component_.has_value());
    instruction_component_.emplace(zone_stats());
    auto& zone = instruction_component_->zone;
    InstructionBlocks* instruction_blocks =
        InstructionSequence::InstructionBlocksFor(zone, graph());
    instruction_component_->sequence =
        zone.New<InstructionSequence>(isolate(), zone, instruction_blocks);
    if (call_descriptor && call_descriptor->RequiresFrameAsIncoming()) {
      instruction_component_->sequence->instruction_blocks()[0]
          ->mark_needs_frame();
    } else {
      DCHECK(call_descriptor->CalleeSavedFPRegisters().is_empty());
    }
  }

  void ClearInstructionComponent() {
    DCHECK(instruction_component_.has_value());
    instruction_component_.reset();
  }

  void InitializeRegisterComponent(const RegisterConfiguration* config,
                                   CallDescriptor* call_descriptor);

  void ClearRegisterComponent() {
    DCHECK(register_component_.has_value());
    register_component_.reset();
  }

  AccountingAllocator* allocator() const;
  ZoneStats* zone_stats() const { return zone_stats_; }
  TurboshaftPipelineKind pipeline_kind() const { return pipeline_kind_; }
  Isolate* isolate() const { return isolate_; }
  OptimizedCompilationInfo* info() const { return info_; }
  const char* debug_name() const { return debug_name_.get(); }
  JSHeapBroker* broker() const { return broker_.get(); }
  CompilationDependencies* depedencies() const { return dependencies_; }
  const AssemblerOptions& assembler_options() const {
    return assembler_options_;
  }
  JumpOptimizationInfo* jump_optimization_info() {
    if (!codegen_component_.has_value()) return nullptr;
    return codegen_component_->jump_optimization_info;
  }
  const CallDescriptor* builtin_call_descriptor() const {
    DCHECK(builtin_component_.has_value());
    return builtin_component_->call_descriptor;
  }

  bool has_graph() const {
    DCHECK_IMPLIES(graph_component_.has_value(),
                   graph_component_->graph != nullptr);
    return graph_component_.has_value();
  }
  ZoneWithName<kGraphZoneName>& graph_zone() { return graph_component_->zone; }
  turboshaft::Graph& graph() const { return *graph_component_->graph; }
  GraphComponent::Pointer<SourcePositionTable> source_positions() const {
    return graph_component_->source_positions;
  }
  GraphComponent::Pointer<NodeOriginTable> node_origins() const {
    if (!graph_component_.has_value()) return nullptr;
    return graph_component_->node_origins;
  }
  RegisterAllocationData* register_allocation_data() const {
    return register_component_->allocation_data;
  }
  ZoneWithName<kRegisterAllocationZoneName>& register_allocation_zone() {
    return register_component_->zone;
  }
  CodeGenerator* code_generator() const {
    return codegen_component_->code_generator.get();
  }
  void set_code(MaybeHandle<Code> code) {
    DCHECK(code_.is_null());
    code_ = code;
  }
  MaybeHandle<Code> code() const { return code_; }
  InstructionSequence* sequence() const {
    return instruction_component_->sequence;
  }
  Frame* frame() const { return codegen_component_->frame; }
  CodeTracer* GetCodeTracer() const;
  size_t& max_unoptimized_frame_height() {
    return codegen_component_->max_unoptimized_frame_height;
  }
  size_t& max_pushed_argument_count() {
    return codegen_component_->max_pushed_argument_count;
  }
  RuntimeCallStats* runtime_call_stats() const { return runtime_call_stats_; }
  void set_runtime_call_stats(RuntimeCallStats* stats) {
    runtime_call_stats_ = stats;
  }

  // The {compilation_zone} outlives the entire compilation pipeline. It is
  // shared between all phases (including code gen where the graph zone is gone
  // already).
  ZoneWithName<kCompilationZoneName>& compilation_zone() {
    return compilation_zone_;
  }

  TurbofanPipelineStatistics* pipeline_statistics() const {
    return pipeline_statistics_;
  }
  void set_pipeline_statistics(
      TurbofanPipelineStatistics* pipeline_statistics) {
    pipeline_statistics_ = pipeline_statistics;
  }

#if V8_ENABLE_WEBASSEMBLY
  const wasm::FunctionSig* wasm_sig() const {
    DCHECK(wasm_sig_ != nullptr);
    return wasm_sig_;
  }

  const wasm::WasmModule* wasm_module() const { return wasm_module_; }

  bool wasm_shared() const { return wasm_shared_; }

  void SetIsWasm(const wasm::WasmModule* module, const wasm::FunctionSig* sig,
                 bool shared) {
    wasm_module_ = module;
    wasm_sig_ = sig;
    wasm_shared_ = shared;
    DCHECK(pipeline_kind() == TurboshaftPipelineKind::kWasm ||
           pipeline_kind() == TurboshaftPipelineKind::kJSToWasm);
  }
#ifdef V8_ENABLE_WASM_SIMD256_REVEC
  WasmRevecAnalyzer* wasm_revec_analyzer() const {
    DCHECK_NOT_NULL(wasm_revec_analyzer_);
    return wasm_revec_analyzer_;
  }

  void set_wasm_revec_analyzer(WasmRevecAnalyzer* wasm_revec_analyzer) {
    DCHECK_NULL(wasm_revec_analyzer_);
    wasm_revec_analyzer_ = wasm_revec_analyzer;
  }

  void clear_wasm_revec_analyzer() { wasm_revec_analyzer_ = nullptr; }
#endif  // V8_ENABLE_WASM_SIMD256_REVEC
#endif  // V8_ENABLE_WEBASSEMBLY

  bool is_wasm() const {
    return pipeline_kind() == TurboshaftPipelineKind::kWasm ||
           pipeline_kind() == TurboshaftPipelineKind::kJSToWasm;
  }
  bool is_js_to_wasm() const {
    return pipeline_kind() == TurboshaftPipelineKind::kJSToWasm;
  }

  void InitializeFrameData(CallDescriptor* call_descriptor) {
    DCHECK(codegen_component_.has_value());
    DCHECK_NULL(codegen_component_->frame);
    int fixed_frame_size = 0;
    if (call_descriptor != nullptr) {
      fixed_frame_size =
          call_descriptor->CalculateFixedFrameSize(info()->code_kind());
    }
    codegen_component_->frame = codegen_component_->zone.New<Frame>(
        fixed_frame_size, codegen_component_->zone);
    if (codegen_component_->osr_helper) {
      codegen_component_->osr_helper->SetupFrame(codegen_component_->frame);
    }
  }

  void set_source_position_output(std::string source_position_output) {
    source_position_output_ = std::move(source_position_output);
  }
  std::string source_position_output() const { return source_position_output_; }

  bool graph_has_special_rpo() const {
    return graph_component_->graph_has_special_rpo;
  }
  void set_graph_has_special_rpo() {
    graph_component_->graph_has_special_rpo = true;
  }

 private:
  ZoneStats* zone_stats_;
  // The {compilation_zone_} outlives the entire compilation pipeline. It is
  // shared between all phases (including code gen where the graph zone is gone
  // already).
  ZoneWithName<kCompilationZoneName> compilation_zone_;
  TurboshaftPipelineKind pipeline_kind_;
  Isolate* const isolate_ = nullptr;
  OptimizedCompilationInfo* info_ = nullptr;
  std::unique_ptr<char[]> debug_name_;
  // TODO(nicohartmann): Use unique_ptr once TurboFan's pipeline data is gone.
  std::shared_ptr<JSHeapBroker> broker_;
  TurbofanPipelineStatistics* pipeline_statistics_ = nullptr;
  CompilationDependencies* dependencies_ = nullptr;
  int start_source_position_ = kNoSourcePosition;
  const AssemblerOptions assembler_options_;
  MaybeHandle<Code> code_;
  std::string source_position_output_;
  RuntimeCallStats* runtime_call_stats_ = nullptr;
  // Components
  std::optional<BuiltinComponent> builtin_component_;
  std::optional<GraphComponent> graph_component_;
  std::optional<CodegenComponent> codegen_component_;
  std::optional<InstructionComponent> instruction_component_;
  std::optional<RegisterComponent> register_component_;

#if V8_ENABLE_WEBASSEMBLY
  // TODO(14108): Consider splitting wasm members into its own WasmPipelineData
  // if we need many of them.
  const wasm::FunctionSig* wasm_sig_ = nullptr;
  const wasm::WasmModule* wasm_module_ = nullptr;
  bool wasm_shared_ = false;
#ifdef V8_ENABLE_WASM_SIMD256_REVEC

  WasmRevecAnalyzer* wasm_revec_analyzer_ = nullptr;
#endif  // V8_ENABLE_WASM_SIMD256_REVEC
#endif  // V8_ENABLE_WEBASSEMBLY
};

void PrintTurboshaftGraph(PipelineData* data, Zone* temp_zone,
                          CodeTracer* code_tracer, const char* phase_name);
void PrintTurboshaftGraphForTurbolizer(std::ofstream& stream,
                                       const Graph& graph,
                                       const char* phase_name,
                                       NodeOriginTable* node_origins,
                                       Zone* temp_zone);

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_PHASE_H_
                                                                                                                                                                                                                        node-23.7.0/deps/v8/src/compiler/turboshaft/pipelines.cc                                            0000664 0000000 0000000 00000005140 14746647661 0023126 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2024 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/pipelines.h"

#include "src/compiler/pipeline-data-inl.h"
#include "src/compiler/turboshaft/csa-optimize-phase.h"
#include "src/compiler/turboshaft/debug-feature-lowering-phase.h"
#include "src/compiler/turboshaft/recreate-schedule-phase.h"

namespace v8::internal::compiler::turboshaft {

void Pipeline::RecreateTurbofanGraph(compiler::TFPipelineData* turbofan_data,
                                     Linkage* linkage) {
  Run<turboshaft::DecompressionOptimizationPhase>();

  Run<turboshaft::RecreateSchedulePhase>(turbofan_data, linkage);
  TraceSchedule(turbofan_data->info(), turbofan_data, turbofan_data->schedule(),
                turboshaft::RecreateSchedulePhase::phase_name());
}

MaybeHandle<Code> Pipeline::GenerateCode(
    Linkage* linkage, std::shared_ptr<OsrHelper> osr_helper,
    JumpOptimizationInfo* jump_optimization_info,
    const ProfileDataFromFile* profile, int initial_graph_hash) {
  // Run code generation. If we optimize jumps, we repeat this a second time.
  data()->InitializeCodegenComponent(osr_helper, jump_optimization_info);

  // Perform instruction selection and register allocation.
  PrepareForInstructionSelection(profile);
  CHECK(SelectInstructions(linkage));
  CHECK(AllocateRegisters(linkage->GetIncomingDescriptor()));
  AssembleCode(linkage);

  if (v8_flags.turbo_profiling) {
    info()->profiler_data()->SetHash(initial_graph_hash);
  }

  if (jump_optimization_info && jump_optimization_info->is_optimizable()) {
    // Reset data for a second run of instruction selection.
    data()->ClearCodegenComponent();
    jump_optimization_info->set_optimizing();

    // Perform instruction selection and register allocation.
    data()->InitializeCodegenComponent(osr_helper, jump_optimization_info);
    if (!SelectInstructions(linkage)) {
      return MaybeHandle<Code>{};
    }
    AllocateRegisters(linkage->GetIncomingDescriptor());
    // Generate the final machine code.
    AssembleCode(linkage);
  }

  return FinalizeCode();
}

void BuiltinPipeline::OptimizeBuiltin() {
  Tracing::Scope tracing_scope(data()->info());

  Run<CsaEarlyMachineOptimizationPhase>();
  Run<CsaLoadEliminationPhase>();
  Run<CsaLateEscapeAnalysisPhase>();
  Run<CsaBranchEliminationPhase>();
  Run<CsaOptimizePhase>();

  if (v8_flags.turboshaft_enable_debug_features) {
    Run<DebugFeatureLoweringPhase>();
  }

  Run<CodeEliminationAndSimplificationPhase>();
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/pipelines.h                                             0000664 0000000 0000000 00000052703 14746647661 0022777 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2024 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_PIPELINES_H_
#define V8_COMPILER_TURBOSHAFT_PIPELINES_H_

#include <optional>

#include "src/codegen/optimized-compilation-info.h"
#include "src/compiler/backend/register-allocator-verifier.h"
#include "src/compiler/basic-block-instrumentor.h"
#include "src/compiler/graph-visualizer.h"
#include "src/compiler/pipeline-statistics.h"
#include "src/compiler/turboshaft/block-instrumentation-phase.h"
#include "src/compiler/turboshaft/build-graph-phase.h"
#include "src/compiler/turboshaft/code-elimination-and-simplification-phase.h"
#include "src/compiler/turboshaft/debug-feature-lowering-phase.h"
#include "src/compiler/turboshaft/decompression-optimization-phase.h"
#include "src/compiler/turboshaft/instruction-selection-phase.h"
#include "src/compiler/turboshaft/loop-peeling-phase.h"
#include "src/compiler/turboshaft/loop-unrolling-phase.h"
#include "src/compiler/turboshaft/machine-lowering-phase.h"
#include "src/compiler/turboshaft/maglev-graph-building-phase.h"
#include "src/compiler/turboshaft/optimize-phase.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/register-allocation-phase.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/simplified-lowering-phase.h"
#include "src/compiler/turboshaft/store-store-elimination-phase.h"
#include "src/compiler/turboshaft/tracing.h"
#include "src/compiler/turboshaft/type-assertions-phase.h"
#include "src/compiler/turboshaft/typed-optimizations-phase.h"

namespace v8::internal::compiler::turboshaft {

inline constexpr char kTempZoneName[] = "temp-zone";

class Pipeline {
 public:
  explicit Pipeline(PipelineData* data) : data_(data) {}

  PipelineData* data() const { return data_; }
  void BeginPhaseKind(const char* phase_kind_name) {
    if (auto statistics = data()->pipeline_statistics()) {
      statistics->BeginPhaseKind(phase_kind_name);
    }
  }
  void EndPhaseKind() {
    if (auto statistics = data()->pipeline_statistics()) {
      statistics->EndPhaseKind();
    }
  }

  template <CONCEPT(TurboshaftPhase) Phase, typename... Args>
  auto Run(Args&&... args) {
    // Setup run scope.
    PhaseScope phase_scope(data_->pipeline_statistics(), Phase::phase_name());
    ZoneWithName<Phase::kPhaseName> temp_zone(data_->zone_stats(),
                                              Phase::phase_name());
    NodeOriginTable::PhaseScope origin_scope(data_->node_origins(),
                                             Phase::phase_name());
#ifdef V8_RUNTIME_CALL_STATS
    RuntimeCallTimerScope runtime_call_timer_scope(data_->runtime_call_stats(),
                                                   Phase::kRuntimeCallCounterId,
                                                   Phase::kCounterMode);
#endif

    Phase phase;
    using result_t =
        decltype(phase.Run(data_, temp_zone, std::forward<Args>(args)...));
    if constexpr (std::is_same_v<result_t, void>) {
      phase.Run(data_, temp_zone, std::forward<Args>(args)...);
      if constexpr (produces_printable_graph<Phase>::value) {
        PrintGraph(temp_zone, Phase::phase_name());
      }
      return;
    } else {
      auto result = phase.Run(data_, temp_zone, std::forward<Args>(args)...);
      if constexpr (produces_printable_graph<Phase>::value) {
        PrintGraph(temp_zone, Phase::phase_name());
      }
      return result;
    }
    UNREACHABLE();
  }

  void PrintGraph(Zone* zone, const char* phase_name) {
    CodeTracer* code_tracer = nullptr;
    if (data_->info()->trace_turbo_graph()) {
      // NOTE: We must not call `GetCodeTracer` if tracing is not enabled,
      // because it may not yet be initialized then and doing so from the
      // background thread is not threadsafe.
      code_tracer = data_->GetCodeTracer();
      DCHECK_NOT_NULL(code_tracer);
    }
    PrintTurboshaftGraph(data_, zone, code_tracer, phase_name);
  }

  void TraceSequence(const char* phase_name) {
    if (info()->trace_turbo_json()) {
      UnparkedScopeIfNeeded scope(data()->broker());
      AllowHandleDereference allow_deref;
      TurboJsonFile json_of(info(), std::ios_base::app);
      json_of
          << "{\"name\":\"" << phase_name << "\",\"type\":\"sequence\""
          << ",\"blocks\":" << InstructionSequenceAsJSON{data()->sequence()}
          << ",\"register_allocation\":{"
          << RegisterAllocationDataAsJSON{*(data()->register_allocation_data()),
                                          *(data()->sequence())}
          << "}},\n";
    }
    if (info()->trace_turbo_graph()) {
      UnparkedScopeIfNeeded scope(data()->broker());
      AllowHandleDereference allow_deref;
      CodeTracer::StreamScope tracing_scope(data()->GetCodeTracer());
      tracing_scope.stream()
          << "----- Instruction sequence " << phase_name << " -----\n"
          << *data()->sequence();
    }
  }

  bool CreateGraphWithMaglev() {
    UnparkedScopeIfNeeded unparked_scope(data_->broker());

    BeginPhaseKind("V8.TFGraphCreation");
    turboshaft::Tracing::Scope tracing_scope(data_->info());
    std::optional<BailoutReason> bailout =
        Run<turboshaft::MaglevGraphBuildingPhase>();
    EndPhaseKind();

    if (bailout.has_value()) {
      data_->info()->AbortOptimization(bailout.value());
      return false;
    }

    return true;
  }

  bool CreateGraphFromTurbofan(compiler::TFPipelineData* turbofan_data,
                               Linkage* linkage) {
    CHECK_IMPLIES(!v8_flags.disable_optimizing_compilers, v8_flags.turboshaft);

    UnparkedScopeIfNeeded scope(data_->broker(),
                                v8_flags.turboshaft_trace_reduction ||
                                    v8_flags.turboshaft_trace_emitted);

    turboshaft::Tracing::Scope tracing_scope(data_->info());

    DCHECK(!v8_flags.turboshaft_from_maglev);
    if (std::optional<BailoutReason> bailout =
            Run<turboshaft::BuildGraphPhase>(turbofan_data, linkage)) {
      info()->AbortOptimization(*bailout);
      return false;
    }

    return true;
  }

  bool OptimizeTurboshaftGraph(Linkage* linkage) {
    UnparkedScopeIfNeeded scope(data_->broker(),
                                v8_flags.turboshaft_trace_reduction ||
                                    v8_flags.turboshaft_trace_emitted);

    turboshaft::Tracing::Scope tracing_scope(data_->info());

    if (v8_flags.turboshaft_frontend) {
      Run<turboshaft::SimplifiedLoweringPhase>();
    }

    Run<turboshaft::MachineLoweringPhase>();

    // TODO(dmercadier): find a way to merge LoopPeeling and LoopUnrolling. It's
    // not currently possible for 2 reasons. First, LoopPeeling reduces the
    // number of iteration of a loop, thus invalidating LoopUnrolling's
    // analysis. This could probably be worked around fairly easily though.
    // Second, LoopPeeling has to emit the non-peeled header of peeled loops, in
    // order to fix their loop phis (because their 1st input should be replace
    // by their 2nd input coming from the peeled iteration), but LoopUnrolling
    // has to be triggered before emitting the loop header. This could be fixed
    // by changing LoopUnrolling start unrolling after the 1st header has been
    // emitted, but this would also require updating CloneSubgraph.
    if (v8_flags.turboshaft_loop_peeling) {
      Run<turboshaft::LoopPeelingPhase>();
    }

    if (v8_flags.turboshaft_loop_unrolling) {
      Run<turboshaft::LoopUnrollingPhase>();
    }

    if (v8_flags.turbo_store_elimination) {
      Run<turboshaft::StoreStoreEliminationPhase>();
    }

    Run<turboshaft::OptimizePhase>();

    if (v8_flags.turboshaft_typed_optimizations) {
      Run<turboshaft::TypedOptimizationsPhase>();
    }

    if (v8_flags.turboshaft_assert_types) {
      Run<turboshaft::TypeAssertionsPhase>();
    }

    // Perform dead code elimination, reduce stack checks, simplify loads on
    // platforms where required, ...
    Run<turboshaft::CodeEliminationAndSimplificationPhase>();

#ifdef V8_ENABLE_DEBUG_CODE
    if (V8_UNLIKELY(v8_flags.turboshaft_enable_debug_features)) {
      // This phase has to run very late to allow all previous phases to use
      // debug features.
      Run<turboshaft::DebugFeatureLoweringPhase>();
    }
#endif  // V8_ENABLE_DEBUG_CODE

    return true;
  }

  void PrepareForInstructionSelection(
      const ProfileDataFromFile* profile = nullptr) {
    if (V8_UNLIKELY(data()->pipeline_kind() == TurboshaftPipelineKind::kCSA ||
                    data()->pipeline_kind() ==
                        TurboshaftPipelineKind::kTSABuiltin)) {
      if (profile) {
        Run<ProfileApplicationPhase>(profile);
      }

      if (v8_flags.reorder_builtins &&
          Builtins::IsBuiltinId(info()->builtin())) {
        UnparkedScopeIfNeeded unparked_scope(data()->broker());
        BasicBlockCallGraphProfiler::StoreCallGraph(info(), data()->graph());
      }

      if (v8_flags.turbo_profiling) {
        UnparkedScopeIfNeeded unparked_scope(data()->broker());

        // Basic block profiling disables concurrent compilation, so handle
        // deref is fine.
        AllowHandleDereference allow_handle_dereference;
        const size_t block_count = data()->graph().block_count();
        BasicBlockProfilerData* profiler_data =
            BasicBlockProfiler::Get()->NewData(block_count);

        // Set the function name.
        profiler_data->SetFunctionName(info()->GetDebugName());
        // Capture the schedule string before instrumentation.
        if (v8_flags.turbo_profiling_verbose) {
          std::ostringstream os;
          os << data()->graph();
          profiler_data->SetSchedule(os);
        }

        info()->set_profiler_data(profiler_data);

        Run<BlockInstrumentationPhase>();
      } else {
        // We run an empty copying phase to make sure that we have the same
        // control flow as when taking the profile.
        ZoneWithName<kTempZoneName> temp_zone(data()->zone_stats(),
                                              kTempZoneName);
        CopyingPhase<>::Run(data(), temp_zone);
      }
    }

    // DecompressionOptimization has to run as the last phase because it
    // constructs an (slightly) invalid graph that mixes Tagged and Compressed
    // representations.
    Run<DecompressionOptimizationPhase>();

    Run<SpecialRPOSchedulingPhase>();
  }

  [[nodiscard]] bool SelectInstructions(Linkage* linkage) {
    auto call_descriptor = linkage->GetIncomingDescriptor();

    // Depending on which code path led us to this function, the frame may or
    // may not have been initialized. If it hasn't yet, initialize it now.
    if (!data_->frame()) {
      data_->InitializeFrameData(call_descriptor);
    }

    // Select and schedule instructions covering the scheduled graph.
    CodeTracer* code_tracer = nullptr;
    if (info()->trace_turbo_graph()) {
      // NOTE: We must not call `GetCodeTracer` if tracing is not enabled,
      // because it may not yet be initialized then and doing so from the
      // background thread is not threadsafe.
      code_tracer = data_->GetCodeTracer();
    }

    if (std::optional<BailoutReason> bailout = Run<InstructionSelectionPhase>(
            call_descriptor, linkage, code_tracer)) {
      data_->info()->AbortOptimization(*bailout);
      EndPhaseKind();
      return false;
    }

    return true;

    // TODO(nicohartmann@): We might need to provide this.
    // if (info()->trace_turbo_json()) {
    //   UnparkedScopeIfNeeded scope(turbofan_data->broker());
    //   AllowHandleDereference allow_deref;
    //   TurboCfgFile tcf(isolate());
    //   tcf << AsC1V("CodeGen", turbofan_data->schedule(),
    //                turbofan_data->source_positions(),
    //                turbofan_data->sequence());

    //   std::ostringstream source_position_output;
    //   // Output source position information before the graph is deleted.
    //   if (data_->source_positions() != nullptr) {
    //     data_->source_positions()->PrintJson(source_position_output);
    //   } else {
    //     source_position_output << "{}";
    //   }
    //   source_position_output << ",\n\"nodeOrigins\" : ";
    //   data_->node_origins()->PrintJson(source_position_output);
    //   data_->set_source_position_output(source_position_output.str());
    // }
  }

  bool AllocateRegisters(CallDescriptor* call_descriptor) {
    BeginPhaseKind("V8.TFRegisterAllocation");

    bool run_verifier = v8_flags.turbo_verify_allocation;

    // Allocate registers.
    const RegisterConfiguration* config = RegisterConfiguration::Default();
    std::unique_ptr<const RegisterConfiguration> restricted_config;
    if (call_descriptor->HasRestrictedAllocatableRegisters()) {
      RegList registers = call_descriptor->AllocatableRegisters();
      DCHECK_LT(0, registers.Count());
      restricted_config.reset(
          RegisterConfiguration::RestrictGeneralRegisters(registers));
      config = restricted_config.get();
    }
    AllocateRegisters(config, call_descriptor, run_verifier);

    // Verify the instruction sequence has the same hash in two stages.
    VerifyGeneratedCodeIsIdempotent();

    Run<FrameElisionPhase>();

    // TODO(mtrofin): move this off to the register allocator.
    bool generate_frame_at_start =
        data_->sequence()->instruction_blocks().front()->must_construct_frame();
    // Optimimize jumps.
    if (v8_flags.turbo_jt) {
      Run<JumpThreadingPhase>(generate_frame_at_start);
    }

    EndPhaseKind();

    return true;
  }

  bool MayHaveUnverifiableGraph() const {
    // TODO(nicohartmann): Are there any graph which are still verifiable?
    return true;
  }

  void VerifyGeneratedCodeIsIdempotent() {
    JumpOptimizationInfo* jump_opt = data()->jump_optimization_info();
    if (jump_opt == nullptr) return;

    InstructionSequence* code = data()->sequence();
    int instruction_blocks = code->InstructionBlockCount();
    int virtual_registers = code->VirtualRegisterCount();
    size_t hash_code =
        base::hash_combine(instruction_blocks, virtual_registers);
    for (Instruction* instr : *code) {
      hash_code = base::hash_combine(hash_code, instr->opcode(),
                                     instr->InputCount(), instr->OutputCount());
    }
    for (int i = 0; i < virtual_registers; i++) {
      hash_code = base::hash_combine(hash_code, code->GetRepresentation(i));
    }
    if (jump_opt->is_collecting()) {
      jump_opt->hash_code = hash_code;
    } else {
      CHECK_EQ(hash_code, jump_opt->hash_code);
    }
  }

  void AllocateRegisters(const RegisterConfiguration* config,
                         CallDescriptor* call_descriptor, bool run_verifier) {
    // Don't track usage for this zone in compiler stats.
    std::unique_ptr<Zone> verifier_zone;
    RegisterAllocatorVerifier* verifier = nullptr;
    if (run_verifier) {
      AccountingAllocator* allocator = data()->allocator();
      DCHECK_NOT_NULL(allocator);
      verifier_zone.reset(
          new Zone(allocator, kRegisterAllocatorVerifierZoneName));
      verifier = verifier_zone->New<RegisterAllocatorVerifier>(
          verifier_zone.get(), config, data()->sequence(), data()->frame());
    }

#ifdef DEBUG
    data_->sequence()->ValidateEdgeSplitForm();
    data_->sequence()->ValidateDeferredBlockEntryPaths();
    data_->sequence()->ValidateDeferredBlockExitPaths();
#endif

    data_->InitializeRegisterComponent(config, call_descriptor);

    Run<MeetRegisterConstraintsPhase>();
    Run<ResolvePhisPhase>();
    Run<BuildLiveRangesPhase>();
    Run<BuildLiveRangeBundlesPhase>();

    TraceSequence("before register allocation");
    if (verifier != nullptr) {
      CHECK(!data_->register_allocation_data()->ExistsUseWithoutDefinition());
      CHECK(data_->register_allocation_data()
                ->RangesDefinedInDeferredStayInDeferred());
    }

    if (data_->info()->trace_turbo_json() && !MayHaveUnverifiableGraph()) {
      TurboCfgFile tcf(data_->isolate());
      tcf << AsC1VRegisterAllocationData("PreAllocation",
                                         data_->register_allocation_data());
    }

    Run<AllocateGeneralRegistersPhase<LinearScanAllocator>>();

    if (data_->sequence()->HasFPVirtualRegisters()) {
      Run<AllocateFPRegistersPhase<LinearScanAllocator>>();
    }

    if (data_->sequence()->HasSimd128VirtualRegisters() &&
        (kFPAliasing == AliasingKind::kIndependent)) {
      Run<AllocateSimd128RegistersPhase<LinearScanAllocator>>();
    }

    Run<DecideSpillingModePhase>();
    Run<AssignSpillSlotsPhase>();
    Run<CommitAssignmentPhase>();

    // TODO(chromium:725559): remove this check once
    // we understand the cause of the bug. We keep just the
    // check at the end of the allocation.
    if (verifier != nullptr) {
      verifier->VerifyAssignment("Immediately after CommitAssignmentPhase.");
    }

    Run<ConnectRangesPhase>();

    Run<ResolveControlFlowPhase>();

    Run<PopulateReferenceMapsPhase>();

    if (v8_flags.turbo_move_optimization) {
      Run<OptimizeMovesPhase>();
    }

    TraceSequence("after register allocation");

    if (verifier != nullptr) {
      verifier->VerifyAssignment("End of regalloc pipeline.");
      verifier->VerifyGapMoves();
    }

    if (data_->info()->trace_turbo_json() && !MayHaveUnverifiableGraph()) {
      TurboCfgFile tcf(data_->isolate());
      tcf << AsC1VRegisterAllocationData("CodeGen",
                                         data_->register_allocation_data());
    }

    data()->ClearRegisterComponent();
  }

  void AssembleCode(Linkage* linkage) {
    BeginPhaseKind("V8.TFCodeGeneration");
    data()->InitializeCodeGenerator(linkage);

    UnparkedScopeIfNeeded unparked_scope(data()->broker());

    Run<AssembleCodePhase>();
    if (info()->trace_turbo_json()) {
      TurboJsonFile json_of(info(), std::ios_base::app);
      json_of
          << "{\"name\":\"code generation\"" << ", \"type\":\"instructions\""
          << InstructionStartsAsJSON{&data()->code_generator()->instr_starts()}
          << TurbolizerCodeOffsetsInfoAsJSON{
                 &data()->code_generator()->offsets_info()};
      json_of << "},\n";
    }

    data()->ClearInstructionComponent();
    EndPhaseKind();
  }

  MaybeHandle<Code> GenerateCode(CallDescriptor* call_descriptor) {
    Linkage linkage(call_descriptor);
    PrepareForInstructionSelection();
    if (!SelectInstructions(&linkage)) {
      return MaybeHandle<Code>();
    }
    AllocateRegisters(linkage.GetIncomingDescriptor());
    AssembleCode(&linkage);
    return FinalizeCode();
  }

  MaybeHandle<Code> GenerateCode(
      Linkage* linkage, std::shared_ptr<OsrHelper> osr_helper = {},
      JumpOptimizationInfo* jump_optimization_info = nullptr,
      const ProfileDataFromFile* profile = nullptr, int initial_graph_hash = 0);

  void RecreateTurbofanGraph(compiler::TFPipelineData* turbofan_data,
                             Linkage* linkage);

  OptimizedCompilationInfo* info() { return data_->info(); }

  MaybeHandle<Code> FinalizeCode(bool retire_broker = true) {
    BeginPhaseKind("V8.TFFinalizeCode");
    if (data_->broker() && retire_broker) {
      data_->broker()->Retire();
    }
    Run<FinalizeCodePhase>();

    MaybeHandle<Code> maybe_code = data_->code();
    Handle<Code> code;
    if (!maybe_code.ToHandle(&code)) {
      return maybe_code;
    }

    data_->info()->SetCode(code);
    PrintCode(data_->isolate(), code, data_->info());

    // Functions with many inline candidates are sensitive to correct call
    // frequency feedback and should therefore not be tiered up early.
    if (v8_flags.profile_guided_optimization &&
        info()->could_not_inline_all_candidates()) {
      info()->shared_info()->set_cached_tiering_decision(
          CachedTieringDecision::kNormal);
    }

    if (info()->trace_turbo_json()) {
      TurboJsonFile json_of(info(), std::ios_base::app);

      json_of << "{\"name\":\"disassembly\",\"type\":\"disassembly\""
              << BlockStartsAsJSON{&data_->code_generator()->block_starts()}
              << "\"data\":\"";
#ifdef ENABLE_DISASSEMBLER
      std::stringstream disassembly_stream;
      code->Disassemble(nullptr, disassembly_stream, data_->isolate());
      std::string disassembly_string(disassembly_stream.str());
      for (const auto& c : disassembly_string) {
        json_of << AsEscapedUC16ForJSON(c);
      }
#endif  // ENABLE_DISASSEMBLER
      json_of << "\"}\n],\n";
      json_of << "\"nodePositions\":";
      // TODO(nicohartmann): We should try to always provide source positions.
      json_of << (data_->source_position_output().empty()
                      ? "{}"
                      : data_->source_position_output())
              << ",\n";
      JsonPrintAllSourceWithPositions(json_of, data_->info(), data_->isolate());
      if (info()->has_bytecode_array()) {
        json_of << ",\n";
        JsonPrintAllBytecodeSources(json_of, info());
      }
      json_of << "\n}";
    }
    if (info()->trace_turbo_json() || info()->trace_turbo_graph()) {
      CodeTracer::StreamScope tracing_scope(data_->GetCodeTracer());
      tracing_scope.stream()
          << "---------------------------------------------------\n"
          << "Finished compiling method " << info()->GetDebugName().get()
          << " using TurboFan" << std::endl;
    }
    EndPhaseKind();
    return code;
  }

  bool CommitDependencies(Handle<Code> code) {
    return data_->depedencies() == nullptr ||
           data_->depedencies()->Commit(code);
  }

 private:
  PipelineData* data_;
};

class BuiltinPipeline : public Pipeline {
 public:
  explicit BuiltinPipeline(PipelineData* data) : Pipeline(data) {}

  void OptimizeBuiltin();
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_PIPELINES_H_
                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/pretenuring-propagation-reducer.cc                      0000664 0000000 0000000 00000011324 14746647661 0027451 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/pretenuring-propagation-reducer.h"

namespace v8::internal::compiler::turboshaft {

namespace {

bool CouldBeAllocate(const Operation& base) {
  return base.Is<PhiOp>() || base.Is<AllocateOp>();
}

}  // namespace

void PretenuringPropagationAnalyzer::ProcessStore(const StoreOp& store) {
  OpIndex base_idx = store.base();
  OpIndex value_idx = store.value();
  const Operation& base = input_graph_.Get(base_idx);
  const Operation& value = input_graph_.Get(value_idx);

  if (!CouldBeAllocate(base) || !CouldBeAllocate(value)) {
    return;
  }

  if (value.Is<AllocateOp>() &&
      value.Cast<AllocateOp>().type == AllocationType::kOld) {
    // {value} is already Old, and we don't care about new-to-old and old-to-old
    // stores.
    return;
  }

  if (value.Is<PhiOp>() && TryFind(value_idx) == nullptr) {
    // {value} is not worth being recorded, as it's not an Allocation (or a Phi
    // of Allocations) that could be promoted to Old.
    return;
  }

  ZoneVector<OpIndex>* stored_in_base = FindOrCreate(base_idx);
  stored_in_base->push_back(value_idx);
}

void PretenuringPropagationAnalyzer::ProcessPhi(const PhiOp& phi) {
  // Phis act as storing all of their inputs. It's not how they work in
  // practice, but if a Phi has a Young input, and is stored in an Old object,
  // it makes sense to Oldify the phi input.

  // For better performance, we only record inputs that could be an allocation:
  // Phis with an entry in {store_graph_} or AllocateOp.
  // Note that this is slightly imprecise for loop Phis (since if the backedge
  // is a Phi itself, it won't have an entry in {store_graph_} yet), but it
  // should still be good enough for most cases.

  base::SmallVector<OpIndex, 16> interesting_inputs;
  for (OpIndex input : phi.inputs()) {
    const Operation& op = input_graph_.Get(input);
    if (op.Is<AllocateOp>()) {
      interesting_inputs.push_back(input);
    } else if (op.Is<PhiOp>() && TryFind(input) != nullptr) {
      interesting_inputs.push_back(input);
    }
  }
  if (interesting_inputs.empty()) return;

  ZoneVector<OpIndex>* stored_in_phi = Create(input_graph_.Index(phi));
  for (OpIndex input : interesting_inputs) {
    stored_in_phi->push_back(input);
  }
}

void PretenuringPropagationAnalyzer::ProcessAllocate(
    const AllocateOp& allocate) {
  if (allocate.type == AllocationType::kOld) {
    // We could be a bit more lazy in storing old AllocateOp into {old_allocs_}
    // (by waiting for a Store or a Phi to use the AllocateOp), but there is
    // usually very few old allocation, so it makes sense to do it eagerly.
    old_allocs_.push_back(input_graph_.Index(allocate));
  }
}

bool PretenuringPropagationAnalyzer::PushContainedValues(OpIndex base) {
  // Push into {queue_} all of the values that are "contained" into {base}:
  // values that are stored to {base} if {base} is an AllocateOp, or Phi inputs
  // if {base} is a Phi.
  ZoneVector<OpIndex>* contained = TryFind(base);
  if (contained == nullptr) return false;
  for (OpIndex index : *contained) {
    queue_.push_back(index);
  }
  return true;
}

// Performs a DFS from {old_alloc} and mark everything it finds as Old. The DFS
// stops on already-Old nodes.
void PretenuringPropagationAnalyzer::OldifySubgraph(OpIndex old_alloc) {
  queue_.clear();
  if (!PushContainedValues(old_alloc)) return;

  while (!queue_.empty()) {
    OpIndex idx = queue_.back();
    queue_.pop_back();
    Operation& op = input_graph_.Get(idx);
    if (AllocateOp* alloc = op.TryCast<AllocateOp>()) {
      if (alloc->type == AllocationType::kOld) continue;
      alloc->type = AllocationType::kOld;
      PushContainedValues(idx);
    } else {
      DCHECK(op.Is<PhiOp>());
      if (old_phis_.find(idx) != old_phis_.end()) continue;
      old_phis_.insert(idx);
      PushContainedValues(idx);
    }
  }
}

void PretenuringPropagationAnalyzer::PropagateAllocationTypes() {
  for (OpIndex old_alloc : old_allocs_) {
    OldifySubgraph(old_alloc);
  }
}

void PretenuringPropagationAnalyzer::BuildStoreInputGraph() {
  for (auto& op : input_graph_.AllOperations()) {
    if (ShouldSkipOperation(op)) {
      continue;
    }
    switch (op.opcode) {
      case Opcode::kStore:
        ProcessStore(op.Cast<StoreOp>());
        break;
      case Opcode::kAllocate:
        ProcessAllocate(op.Cast<AllocateOp>());
        break;
      case Opcode::kPhi:
        ProcessPhi(op.Cast<PhiOp>());
        break;
      default:
        break;
    }
  }
}

void PretenuringPropagationAnalyzer::Run() {
  BuildStoreInputGraph();

  PropagateAllocationTypes();
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/pretenuring-propagation-reducer.h                       0000664 0000000 0000000 00000021313 14746647661 0027312 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_PRETENURING_PROPAGATION_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_PRETENURING_PROPAGATION_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/reducer-traits.h"
#include "src/compiler/turboshaft/utils.h"
#include "src/zone/zone-allocator.h"
#include "src/zone/zone-containers.h"
#include "src/zone/zone.h"

namespace v8::internal::compiler::turboshaft {

// This reducer propagates pretenuring (= allocations of Old objects rather than
// Young objects) throughout the graph: if a young allocation is stored in a old
// allocation, then we'll make it old instead. The idea being that 1) if an
// object is stored in a old object, it makes sense for it for be considered
// old, and 2) this reduces the size of the remembered sets.
// For instance, if we have:
//
//     a = Allocate(old)
//     b = Allocate(young)
//     c = Allocate(young)
//     b.x = c
//     a.x = b
//
// Then we'll make `b` and `c` allocate Old objects directly, because they'll be
// stored to old objects (transitively for `c`, but it still counts).
// On the other hand, if we have:
//
//     a = Allocate(old)
//     b = Allocate(young)
//     c = Allocate(young)
//     a.x = c
//     b.c = a
//
// Then we'll allocate `c` Old as well (because it is stored in an old pointer),
// but `b` will stay young, since it's not stored in an Old object (it contains
// a pointer to an old object, but that's probably not a good reason to make it
// old).
//
//
// Implementation
//
// In a first phase, we iterate the input graph and create a directed graph
// (called `store_graph_`) where each node points to the nodes stored in it (so,
// an edge between `a` and `b` means that `b` is stored in `a`). We also collect
// all of the old allocations in a separate list (`old_allocs_`). The
// `store_graph_` of the first example above will thus be:
//
//    a -----> b -----> c
//
// And the graph of the second example will be:
//
//   a -----> c        b -----> a
//
// (it contains two unconnected subgraphs)
//
// Then, in a second phase, we iterate the old allocations (`old_allocs_`), and
// for each one, we do a DFS in `store_graph_`, marking all of the nodes we
// encounter as old, and stopping on old nodes (which 1- prevents infinite loops
// easily, and 2- is sound because all of the initially-old pointers are roots
// of this phase). On the 2 examples above, `a` will be the old entry in
// `old_allocs_`, so in both cases will do a single DFS starting from `a`. In
// the 1st case, it's easy to see that this DFS will encounter `b` and `c`
// (which will thus both become Old), while in the 2nd case, this DFS will only
// reach `c` (which will thus become Old, while `b` won't be changed).
//
// To be more precise, we also record Phi inputs in `store_graph_`, so that if
// we have something like:
//
//     a = Allocate(old)
//     b = Allocate(young)
//     c = Allocate(young)
//     p = Phi(b, c)
//     a.x = p
//
// Then we oldify both `b` and `c`. In this case, `store_graph_` would be
//
//                --------> b
//               /
//      a ----> p
//               \
//                --------> c
//
// Which means that in the second phase, we'll start a DFS on `a` (the only old
// allocation), move to `p` (the only node reachable from `a`), and the oldify
// `b` and `c` (which are reachable from `p`).
//
//
// Limitation: when a Phi of old allocations is used as the left-hand side of a
// Store where the value being stored is a young allocation, we don't oldify the
// young allocation. For instance, we won't oldify `a` in this example:
//
//     a = Allocate(young)
//     b = Allocate(old)
//     c = Allocate(old)
//     p = Phi(b, c)
//     p.x = a
//
// The reason being that the store_graph sturcture isn't well suited for this,
// since an edge Phi->Node can mean either that Node is stored (via a StoreOp)
// in Phi, or that Node is an input of Phi. The `store_graph_` for the example
// above will thus look like:
//
//      ------> b
//     /
//    p ------> a
//     \
//      ------> c
//
// In order to oldify `a`, we would need to register `p` in `old_allocs_`,
// except that we should only do this when `p` is actually old, and we discover
// that only in the second phase.
// Consider for instance this more complex example:
//
//     a = Allocate(old)
//     b = Allocate(young)
//     c = Allocate(young)
//     d = Allocate(young)
//     a.x = b
//     a.y = c
//     p = Phi(b, c)
//     p.x = d
//
// The graph will be:
//
//      -----> b <-----
//     /               \
//    a                 p -----> d
//     \               /
//      -----> c <-----
//
// And the only entry in `old_allocs_` will be `a`. During the DFS from `a`,
// allocations `b` and `c` will be oldified. At this point, `p` will point to
// edges to 2 old (`b` and `c`) and 1 young (`d`) nodes.
// We could look at all Phis in `store_graph_` and consider one by one for being
// roots of an oldifying DFS: if all of the inputs of a phi `p` (in the sense
// OpIndex inputs in the input_graph) are Old, then start an oldifying DFS from
// `p`. However, the worst case complexity would be something like O(n^2) where
// `n` is the number of Phis in the graph (since we could end up checking all
// Phis but only finding a single one that is old, but the DFS could make a
// single other phi old, thus repeating the process). This complexity could be
// made linear by maintaining additional datastructures on the side, but there
// isn't much evidence that this optimization would be often useful in practice.

class PretenuringPropagationAnalyzer {
 public:
  PretenuringPropagationAnalyzer(Zone* phase_zone, Graph& mutable_input_graph)
      : zone_(phase_zone),
        input_graph_(mutable_input_graph),
        old_allocs_(phase_zone),
        store_graph_(phase_zone),
        old_phis_(phase_zone),
        queue_(phase_zone) {}

  void Run();

 private:
  void ProcessStore(const StoreOp& store);
  void ProcessPhi(const PhiOp& phi);
  void ProcessAllocate(const AllocateOp& allocate);

  bool PushContainedValues(OpIndex base);
  void OldifySubgraph(OpIndex old_alloc);

  void BuildStoreInputGraph();
  void PropagateAllocationTypes();

  ZoneVector<OpIndex>* FindOrCreate(OpIndex idx) {
    auto it = store_graph_.find(idx);
    if (it != store_graph_.end()) return it->second;
    return Create(idx);
  }

  ZoneVector<OpIndex>* Create(OpIndex idx) {
    DCHECK_EQ(store_graph_.count(idx), 0);
    ZoneVector<OpIndex>* stored_items = zone_->New<ZoneVector<OpIndex>>(zone_);
    store_graph_.insert({idx, stored_items});
    return stored_items;
  }

  ZoneVector<OpIndex>* TryFind(OpIndex idx) {
    auto it = store_graph_.find(idx);
    if (it != store_graph_.end()) return it->second;
    return nullptr;
  }

  Zone* zone_;
  Graph& input_graph_;
  ZoneVector<OpIndex> old_allocs_;

  // (see main comment at the begining of this file for the role of
  // `store_graph_`)
  // `store_graph_` contains mapping from OpIndex to vector<OpIndex>. If for an
  // entry `a` it contains a vector `v`, it means that `a` has edges to all of
  // the values in `v`.
  ZoneAbslFlatHashMap<OpIndex, ZoneVector<OpIndex>*> store_graph_;

  // AllocateOp have an AllocationType field, which is set to kOld once they've
  // been visited, thus ensuring that recursion ends. However, PhiOp don't have
  // such a field. Thus, once we've visited a Phi, we store it in {old_phis_} to
  // prevent revisiting it.
  ZoneAbslFlatHashSet<OpIndex> old_phis_;

  // Used in the final phase to do DFS in the graph from each old store. It
  // could be a local variable, but we instead use an instance variable to reuse
  // memory.
  ZoneVector<OpIndex> queue_;
};

// Forward delcaration
template <class Next>
class MemoryOptimizationReducer;

template <class Next>
class PretenuringPropagationReducer : public Next {
#if defined(__clang__)
  // PretenuringPropagationReducer should run before MemoryOptimizationReducer
  // (because once young allocations are marked for folding, they can't be
  // oldified anymore). We enforce this by making PretenuringPropagationReducer
  // run in the same phase as MemoryOptimizationReducer, but before.
  static_assert(next_contains_reducer<Next, MemoryOptimizationReducer>::value);
#endif

 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(PretenuringPropagation)

  void Analyze() {
    PretenuringPropagationAnalyzer analyzer(Asm().phase_zone(),
                                            Asm().modifiable_input_graph());
    analyzer.Run();
    Next::Analyze();
  }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_PRETENURING_PROPAGATION_REDUCER_H_
                                                                                                                                                                                                                                                                                                                     node-23.7.0/deps/v8/src/compiler/turboshaft/recreate-schedule-phase.cc                              0000664 0000000 0000000 00000002035 14746647661 0025620 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/recreate-schedule-phase.h"

#include "src/compiler/pipeline-data-inl.h"

namespace v8::internal::compiler::turboshaft {

RecreateScheduleResult RecreateSchedulePhase::Run(
    PipelineData* data, Zone* temp_zone,
    compiler::TFPipelineData* turbofan_data, Linkage* linkage) {
  const size_t node_count_estimate =
      static_cast<size_t>(1.1 * data->graph().op_id_count());

  turbofan_data->InitializeWithGraphZone(
      std::move(data->graph_zone()), data->source_positions(),
      data->node_origins(), node_count_estimate);

  auto result = RecreateSchedule(data, turbofan_data,
                                 linkage->GetIncomingDescriptor(), temp_zone);

  // Delete GraphComponent because its content is now owned by {turbofan_data}.
  data->ClearGraphComponent();

  return result;
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/recreate-schedule-phase.h                               0000664 0000000 0000000 00000001713 14746647661 0025464 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_PHASE_H_

#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/recreate-schedule.h"

namespace v8::internal::compiler {
class TFPipelineData;
}  // namespace v8::internal::compiler

namespace v8::internal::compiler::turboshaft {

struct RecreateSchedulePhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(RecreateSchedule)
  static constexpr bool kOutputIsTraceableGraph = false;

  RecreateScheduleResult Run(PipelineData* data, Zone* temp_zone,
                             compiler::TFPipelineData* turbofan_data,
                             Linkage* linkage);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_PHASE_H_
                                                     node-23.7.0/deps/v8/src/compiler/turboshaft/recreate-schedule.cc                                    0000664 0000000 0000000 00000216204 14746647661 0024527 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/recreate-schedule.h"

#include "src/base/logging.h"
#include "src/base/safe_conversions.h"
#include "src/base/small-vector.h"
#include "src/base/template-utils.h"
#include "src/base/vector.h"
#include "src/codegen/callable.h"
#include "src/codegen/machine-type.h"
#include "src/common/globals.h"
#include "src/compiler/backend/instruction-selector.h"
#include "src/compiler/common-operator.h"
#include "src/compiler/compiler-source-position-table.h"
#include "src/compiler/feedback-source.h"
#include "src/compiler/graph.h"
#include "src/compiler/js-heap-broker.h"
#include "src/compiler/linkage.h"
#include "src/compiler/machine-operator.h"
#include "src/compiler/node-origin-table.h"
#include "src/compiler/phase.h"
#include "src/compiler/pipeline-data-inl.h"
#include "src/compiler/schedule.h"
#include "src/compiler/scheduler.h"
#include "src/compiler/turboshaft/deopt-data.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/opmasks.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/write-barrier-kind.h"
#include "src/utils/utils.h"
#include "src/zone/zone-containers.h"

namespace v8::internal::compiler::turboshaft {

namespace {

struct ScheduleBuilder {
  PipelineData* data;
  CallDescriptor* call_descriptor;
  Zone* phase_zone;
  compiler::TFPipelineData* turbofan_data;

  const Graph& input_graph = data->graph();
  JSHeapBroker* broker = data->broker();
  Zone* graph_zone = turbofan_data->graph_zone();
  SourcePositionTable* source_positions = turbofan_data->source_positions();
  NodeOriginTable* origins = turbofan_data->node_origins();

  Schedule* const schedule = turbofan_data->schedule();
  compiler::Graph* const tf_graph = turbofan_data->graph();
  compiler::MachineOperatorBuilder& machine = *turbofan_data->machine();
  compiler::CommonOperatorBuilder& common = *turbofan_data->common();

  compiler::BasicBlock* current_block = schedule->start();
  const Block* current_input_block = nullptr;
  ZoneAbslFlatHashMap<int, Node*> parameters{phase_zone};
  ZoneAbslFlatHashMap<int, Node*> osr_values{phase_zone};
  std::vector<BasicBlock*> blocks = {};
  std::vector<Node*> nodes{input_graph.op_id_count()};
  std::vector<std::pair<Node*, OpIndex>> loop_phis = {};

  RecreateScheduleResult Run();
  Node* MakeNode(const Operator* op, base::Vector<Node* const> inputs);
  Node* MakeNode(const Operator* op, std::initializer_list<Node*> inputs) {
    return MakeNode(op, base::VectorOf(inputs));
  }
  Node* AddNode(const Operator* op, base::Vector<Node* const> inputs);
  Node* AddNode(const Operator* op, std::initializer_list<Node*> inputs) {
    return AddNode(op, base::VectorOf(inputs));
  }
  Node* GetNode(OpIndex i) { return nodes[i.id()]; }
  BasicBlock* GetBlock(const Block& block) {
    return blocks[block.index().id()];
  }
  Node* IntPtrConstant(intptr_t value) {
    return AddNode(machine.Is64() ? common.Int64Constant(value)
                                  : common.Int32Constant(
                                        base::checked_cast<int32_t>(value)),
                   {});
  }
  Node* IntPtrAdd(Node* a, Node* b) {
    return AddNode(machine.Is64() ? machine.Int64Add() : machine.Int32Add(),
                   {a, b});
  }
  Node* IntPtrShl(Node* a, Node* b) {
    return AddNode(machine.Is64() ? machine.Word64Shl() : machine.Word32Shl(),
                   {a, b});
  }
  Node* RelocatableIntPtrConstant(intptr_t value, RelocInfo::Mode mode) {
    return AddNode(machine.Is64()
                       ? common.RelocatableInt64Constant(value, mode)
                       : common.RelocatableInt32Constant(
                             base::checked_cast<int32_t>(value), mode),
                   {});
  }
  void ProcessOperation(const Operation& op);
#define DECL_PROCESS_OPERATION(Name) Node* ProcessOperation(const Name##Op& op);
  TURBOSHAFT_OPERATION_LIST(DECL_PROCESS_OPERATION)
#undef DECL_PROCESS_OPERATION

  std::pair<Node*, MachineType> BuildDeoptInput(FrameStateData::Iterator* it);
  Node* BuildStateValues(FrameStateData::Iterator* it, int32_t size);
  Node* BuildTaggedInput(FrameStateData::Iterator* it);
};

Node* ScheduleBuilder::MakeNode(const Operator* op,
                                base::Vector<Node* const> inputs) {
  Node* node = tf_graph->NewNodeUnchecked(op, static_cast<int>(inputs.size()),
                                          inputs.data());
  return node;
}
Node* ScheduleBuilder::AddNode(const Operator* op,
                               base::Vector<Node* const> inputs) {
  DCHECK_NOT_NULL(current_block);
  Node* node = MakeNode(op, inputs);
  schedule->AddNode(current_block, node);
  return node;
}

RecreateScheduleResult ScheduleBuilder::Run() {
  DCHECK_GE(input_graph.block_count(), 1);
  blocks.reserve(input_graph.block_count());
  blocks.push_back(current_block);
  for (size_t i = 1; i < input_graph.block_count(); ++i) {
    blocks.push_back(schedule->NewBasicBlock());
  }
  // The value output count of the start node does not actually matter.
  tf_graph->SetStart(tf_graph->NewNode(common.Start(0)));
  tf_graph->SetEnd(tf_graph->NewNode(common.End(0)));

  for (const Block& block : input_graph.blocks()) {
    current_input_block = &block;
    current_block = GetBlock(block);
    for (OpIndex op : input_graph.OperationIndices(block)) {
      DCHECK_NOT_NULL(current_block);
      ProcessOperation(input_graph.Get(op));
    }
  }

  for (auto& p : loop_phis) {
    p.first->ReplaceInput(1, GetNode(p.second));
  }

  DCHECK(schedule->rpo_order()->empty());
  Scheduler::ComputeSpecialRPO(phase_zone, schedule);
  // Note that Scheduler::GenerateDominatorTree also infers which blocks are
  // deferred, so we only need to set branch targets as deferred based on the
  // hints, and we let Scheduler::GenerateDominatorTree propagate this
  // information to other blocks.
  Scheduler::GenerateDominatorTree(schedule);
  return {tf_graph, schedule};
}

void ScheduleBuilder::ProcessOperation(const Operation& op) {
  if (!turboshaft::ShouldSkipOptimizationStep() && ShouldSkipOperation(op)) {
    return;
  }
  Node* node;
  switch (op.opcode) {
#define SWITCH_CASE(Name)                         \
  case Opcode::k##Name:                           \
    node = ProcessOperation(op.Cast<Name##Op>()); \
    break;
    TURBOSHAFT_OPERATION_LIST(SWITCH_CASE)
#undef SWITCH_CASE
  }
  OpIndex index = input_graph.Index(op);
  DCHECK_LT(index.id(), nodes.size());
  nodes[index.id()] = node;
  if (source_positions && source_positions->IsEnabled() && node) {
    source_positions->SetSourcePosition(node,
                                        input_graph.source_positions()[index]);
  }
  if (origins && node) {
    origins->SetNodeOrigin(node->id(), index.id());
  }
}

#define SHOULD_HAVE_BEEN_LOWERED(op) \
  Node* ScheduleBuilder::ProcessOperation(const op##Op&) { UNREACHABLE(); }
// These operations should have been lowered in previous reducers already.
TURBOSHAFT_JS_OPERATION_LIST(SHOULD_HAVE_BEEN_LOWERED)
TURBOSHAFT_SIMPLIFIED_OPERATION_LIST(SHOULD_HAVE_BEEN_LOWERED)
TURBOSHAFT_OTHER_OPERATION_LIST(SHOULD_HAVE_BEEN_LOWERED)
TURBOSHAFT_WASM_OPERATION_LIST(SHOULD_HAVE_BEEN_LOWERED)
SHOULD_HAVE_BEEN_LOWERED(Dead)
// {AbortCSADcheck} is not emitted in pipelines that still use
// {RecreateSchedule}.
SHOULD_HAVE_BEEN_LOWERED(AbortCSADcheck)
#undef SHOULD_HAVE_BEEN_LOWERED

Node* ScheduleBuilder::ProcessOperation(const WordBinopOp& op) {
  using Kind = WordBinopOp::Kind;
  const Operator* o;
  switch (op.rep.value()) {
    case WordRepresentation::Word32():
      switch (op.kind) {
        case Kind::kAdd:
          o = machine.Int32Add();
          break;
        case Kind::kSub:
          o = machine.Int32Sub();
          break;
        case Kind::kMul:
          o = machine.Int32Mul();
          break;
        case Kind::kSignedMulOverflownBits:
          o = machine.Int32MulHigh();
          break;
        case Kind::kUnsignedMulOverflownBits:
          o = machine.Uint32MulHigh();
          break;
        case Kind::kSignedDiv:
          o = machine.Int32Div();
          break;
        case Kind::kUnsignedDiv:
          o = machine.Uint32Div();
          break;
        case Kind::kSignedMod:
          o = machine.Int32Mod();
          break;
        case Kind::kUnsignedMod:
          o = machine.Uint32Mod();
          break;
        case Kind::kBitwiseAnd:
          o = machine.Word32And();
          break;
        case Kind::kBitwiseOr:
          o = machine.Word32Or();
          break;
        case Kind::kBitwiseXor:
          o = machine.Word32Xor();
          break;
      }
      break;
    case WordRepresentation::Word64():
      switch (op.kind) {
        case Kind::kAdd:
          o = machine.Int64Add();
          break;
        case Kind::kSub:
          o = machine.Int64Sub();
          break;
        case Kind::kMul:
          o = machine.Int64Mul();
          break;
        case Kind::kSignedDiv:
          o = machine.Int64Div();
          break;
        case Kind::kUnsignedDiv:
          o = machine.Uint64Div();
          break;
        case Kind::kSignedMod:
          o = machine.Int64Mod();
          break;
        case Kind::kUnsignedMod:
          o = machine.Uint64Mod();
          break;
        case Kind::kBitwiseAnd:
          o = machine.Word64And();
          break;
        case Kind::kBitwiseOr:
          o = machine.Word64Or();
          break;
        case Kind::kBitwiseXor:
          o = machine.Word64Xor();
          break;
        case Kind::kSignedMulOverflownBits:
          o = machine.Int64MulHigh();
          break;
        case Kind::kUnsignedMulOverflownBits:
          o = machine.Uint64MulHigh();
          break;
      }
      break;
    default:
      UNREACHABLE();
  }
  return AddNode(o, {GetNode(op.left()), GetNode(op.right())});
}
Node* ScheduleBuilder::ProcessOperation(const FloatBinopOp& op) {
  using Kind = FloatBinopOp::Kind;
  const Operator* o;
  switch (op.rep.value()) {
    case FloatRepresentation::Float32():
      switch (op.kind) {
        case Kind::kAdd:
          o = machine.Float32Add();
          break;
        case Kind::kSub:
          o = machine.Float32Sub();
          break;
        case Kind::kMul:
          o = machine.Float32Mul();
          break;
        case Kind::kDiv:
          o = machine.Float32Div();
          break;
        case Kind::kMin:
          o = machine.Float32Min();
          break;
        case Kind::kMax:
          o = machine.Float32Max();
          break;
        case Kind::kPower:
        case Kind::kAtan2:
        case Kind::kMod:
          UNREACHABLE();
      }
      break;
    case FloatRepresentation::Float64():
      switch (op.kind) {
        case Kind::kAdd:
          o = machine.Float64Add();
          break;
        case Kind::kSub:
          o = machine.Float64Sub();
          break;
        case Kind::kMul:
          o = machine.Float64Mul();
          break;
        case Kind::kDiv:
          o = machine.Float64Div();
          break;
        case Kind::kMod:
          o = machine.Float64Mod();
          break;
        case Kind::kMin:
          o = machine.Float64Min();
          break;
        case Kind::kMax:
          o = machine.Float64Max();
          break;
        case Kind::kPower:
          o = machine.Float64Pow();
          break;
        case Kind::kAtan2:
          o = machine.Float64Atan2();
          break;
      }
      break;
    default:
      UNREACHABLE();
  }
  return AddNode(o, {GetNode(op.left()), GetNode(op.right())});
}

Node* ScheduleBuilder::ProcessOperation(const OverflowCheckedBinopOp& op) {
  const Operator* o;
  switch (op.rep.value()) {
    case WordRepresentation::Word32():
      switch (op.kind) {
        case OverflowCheckedBinopOp::Kind::kSignedAdd:
          o = machine.Int32AddWithOverflow();
          break;
        case OverflowCheckedBinopOp::Kind::kSignedSub:
          o = machine.Int32SubWithOverflow();
          break;
        case OverflowCheckedBinopOp::Kind::kSignedMul:
          o = machine.Int32MulWithOverflow();
          break;
      }
      break;
    case WordRepresentation::Word64():
      switch (op.kind) {
        case OverflowCheckedBinopOp::Kind::kSignedAdd:
          o = machine.Int64AddWithOverflow();
          break;
        case OverflowCheckedBinopOp::Kind::kSignedSub:
          o = machine.Int64SubWithOverflow();
          break;
        case OverflowCheckedBinopOp::Kind::kSignedMul:
          o = machine.Int64MulWithOverflow();
          break;
      }
      break;
    default:
      UNREACHABLE();
  }
  return AddNode(o, {GetNode(op.left()), GetNode(op.right())});
}
Node* ScheduleBuilder::ProcessOperation(const WordUnaryOp& op) {
  bool word64 = op.rep == WordRepresentation::Word64();
  const Operator* o;
  switch (op.kind) {
    case WordUnaryOp::Kind::kReverseBytes:
      o = word64 ? machine.Word64ReverseBytes() : machine.Word32ReverseBytes();
      break;
    case WordUnaryOp::Kind::kCountLeadingZeros:
      o = word64 ? machine.Word64Clz() : machine.Word32Clz();
      break;
    case WordUnaryOp::Kind::kCountTrailingZeros:
      o = word64 ? machine.Word64Ctz().op() : machine.Word32Ctz().op();
      break;
    case WordUnaryOp::Kind::kPopCount:
      o = word64 ? machine.Word64Popcnt().op() : machine.Word32Popcnt().op();
      break;
    case WordUnaryOp::Kind::kSignExtend8:
      o = word64 ? machine.SignExtendWord8ToInt64()
                 : machine.SignExtendWord8ToInt32();
      break;
    case WordUnaryOp::Kind::kSignExtend16:
      o = word64 ? machine.SignExtendWord16ToInt64()
                 : machine.SignExtendWord16ToInt32();
      break;
  }
  return AddNode(o, {GetNode(op.input())});
}

Node* ScheduleBuilder::ProcessOperation(const OverflowCheckedUnaryOp& op) {
  bool word64 = op.rep == WordRepresentation::Word64();
  const Operator* o;
  switch (op.kind) {
    case OverflowCheckedUnaryOp::Kind::kAbs:
      o = word64 ? machine.Int64AbsWithOverflow().op()
                 : machine.Int32AbsWithOverflow().op();
  }
  return AddNode(o, {GetNode(op.input())});
}

Node* ScheduleBuilder::ProcessOperation(const FloatUnaryOp& op) {
  DCHECK(FloatUnaryOp::IsSupported(op.kind, op.rep));
  bool float64 = op.rep == FloatRepresentation::Float64();
  const Operator* o;
  switch (op.kind) {
    case FloatUnaryOp::Kind::kAbs:
      o = float64 ? machine.Float64Abs() : machine.Float32Abs();
      break;
    case FloatUnaryOp::Kind::kNegate:
      o = float64 ? machine.Float64Neg() : machine.Float32Neg();
      break;
    case FloatUnaryOp::Kind::kRoundDown:
      o = float64 ? machine.Float64RoundDown().op()
                  : machine.Float32RoundDown().op();
      break;
    case FloatUnaryOp::Kind::kRoundUp:
      o = float64 ? machine.Float64RoundUp().op()
                  : machine.Float32RoundUp().op();
      break;
    case FloatUnaryOp::Kind::kRoundToZero:
      o = float64 ? machine.Float64RoundTruncate().op()
                  : machine.Float32RoundTruncate().op();
      break;
    case FloatUnaryOp::Kind::kRoundTiesEven:
      o = float64 ? machine.Float64RoundTiesEven().op()
                  : machine.Float32RoundTiesEven().op();
      break;
    case FloatUnaryOp::Kind::kSqrt:
      o = float64 ? machine.Float64Sqrt() : machine.Float32Sqrt();
      break;
    case FloatUnaryOp::Kind::kSilenceNaN:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64SilenceNaN();
      break;
    case FloatUnaryOp::Kind::kLog:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Log();
      break;
    case FloatUnaryOp::Kind::kExp:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Exp();
      break;
    case FloatUnaryOp::Kind::kExpm1:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Expm1();
      break;
    case FloatUnaryOp::Kind::kSin:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Sin();
      break;
    case FloatUnaryOp::Kind::kCos:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Cos();
      break;
    case FloatUnaryOp::Kind::kAsin:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Asin();
      break;
    case FloatUnaryOp::Kind::kAcos:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Acos();
      break;
    case FloatUnaryOp::Kind::kSinh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Sinh();
      break;
    case FloatUnaryOp::Kind::kCosh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Cosh();
      break;
    case FloatUnaryOp::Kind::kAsinh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Asinh();
      break;
    case FloatUnaryOp::Kind::kAcosh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Acosh();
      break;
    case FloatUnaryOp::Kind::kTan:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Tan();
      break;
    case FloatUnaryOp::Kind::kTanh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Tanh();
      break;
    case FloatUnaryOp::Kind::kLog2:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Log2();
      break;
    case FloatUnaryOp::Kind::kLog10:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Log10();
      break;
    case FloatUnaryOp::Kind::kLog1p:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Log1p();
      break;
    case FloatUnaryOp::Kind::kAtan:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Atan();
      break;
    case FloatUnaryOp::Kind::kAtanh:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Atanh();
      break;
    case FloatUnaryOp::Kind::kCbrt:
      DCHECK_EQ(op.rep, FloatRepresentation::Float64());
      o = machine.Float64Cbrt();
      break;
  }
  return AddNode(o, {GetNode(op.input())});
}
Node* ScheduleBuilder::ProcessOperation(const ShiftOp& op) {
  DCHECK(op.rep == WordRepresentation::Word32() ||
         op.rep == WordRepresentation::Word64());
  bool word64 = op.rep == WordRepresentation::Word64();
  Node* right = GetNode(op.right());
  if (word64) {
    // In Turboshaft's ShiftOp, the right hand side always has Word32
    // representation, so for 64 bit shifts, we have to zero-extend when
    // constructing Turbofan.
    if (const ConstantOp* constant =
            input_graph.Get(op.right()).TryCast<Opmask::kWord32Constant>()) {
      int64_t value = static_cast<int64_t>(constant->word32());
      right = AddNode(common.Int64Constant(value), {});
    } else {
      right = AddNode(machine.ChangeUint32ToUint64(), {right});
    }
  }
  const Operator* o;
  switch (op.kind) {
    case ShiftOp::Kind::kShiftRightArithmeticShiftOutZeros:
      o = word64 ? machine.Word64SarShiftOutZeros()
                 : machine.Word32SarShiftOutZeros();
      break;
    case ShiftOp::Kind::kShiftRightArithmetic:
      o = word64 ? machine.Word64Sar() : machine.Word32Sar();
      break;
    case ShiftOp::Kind::kShiftRightLogical:
      o = word64 ? machine.Word64Shr() : machine.Word32Shr();
      break;
    case ShiftOp::Kind::kShiftLeft:
      o = word64 ? machine.Word64Shl() : machine.Word32Shl();
      break;
    case ShiftOp::Kind::kRotateLeft:
      o = word64 ? machine.Word64Rol().op() : machine.Word32Rol().op();
      break;
    case ShiftOp::Kind::kRotateRight:
      o = word64 ? machine.Word64Ror() : machine.Word32Ror();
      break;
  }
  return AddNode(o, {GetNode(op.left()), right});
}
Node* ScheduleBuilder::ProcessOperation(const ComparisonOp& op) {
  const Operator* o;
  switch (op.rep.value()) {
    case RegisterRepresentation::Word32():
      switch (op.kind) {
        case ComparisonOp::Kind::kEqual:
          o = machine.Word32Equal();
          break;
        case ComparisonOp::Kind::kSignedLessThan:
          o = machine.Int32LessThan();
          break;
        case ComparisonOp::Kind::kSignedLessThanOrEqual:
          o = machine.Int32LessThanOrEqual();
          break;
        case ComparisonOp::Kind::kUnsignedLessThan:
          o = machine.Uint32LessThan();
          break;
        case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
          o = machine.Uint32LessThanOrEqual();
          break;
      }
      break;
    case RegisterRepresentation::Word64():
      switch (op.kind) {
        case ComparisonOp::Kind::kEqual:
          o = machine.Word64Equal();
          break;
        case ComparisonOp::Kind::kSignedLessThan:
          o = machine.Int64LessThan();
          break;
        case ComparisonOp::Kind::kSignedLessThanOrEqual:
          o = machine.Int64LessThanOrEqual();
          break;
        case ComparisonOp::Kind::kUnsignedLessThan:
          o = machine.Uint64LessThan();
          break;
        case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
          o = machine.Uint64LessThanOrEqual();
          break;
      }
      break;
    case RegisterRepresentation::Float32():
      switch (op.kind) {
        case ComparisonOp::Kind::kEqual:
          o = machine.Float32Equal();
          break;
        case ComparisonOp::Kind::kSignedLessThan:
          o = machine.Float32LessThan();
          break;
        case ComparisonOp::Kind::kSignedLessThanOrEqual:
          o = machine.Float32LessThanOrEqual();
          break;
        case ComparisonOp::Kind::kUnsignedLessThan:
        case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
          UNREACHABLE();
      }
      break;
    case RegisterRepresentation::Float64():
      switch (op.kind) {
        case ComparisonOp::Kind::kEqual:
          o = machine.Float64Equal();
          break;
        case ComparisonOp::Kind::kSignedLessThan:
          o = machine.Float64LessThan();
          break;
        case ComparisonOp::Kind::kSignedLessThanOrEqual:
          o = machine.Float64LessThanOrEqual();
          break;
        case ComparisonOp::Kind::kUnsignedLessThan:
        case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
          UNREACHABLE();
      }
      break;
    case RegisterRepresentation::Tagged():
      switch (op.kind) {
        case ComparisonOp::Kind::kEqual:
          o = machine.TaggedEqual();
          break;
        case ComparisonOp::Kind::kSignedLessThan:
        case ComparisonOp::Kind::kSignedLessThanOrEqual:
        case ComparisonOp::Kind::kUnsignedLessThan:
        case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
          UNREACHABLE();
      }
      break;
    default:
      UNREACHABLE();
  }
  return AddNode(o, {GetNode(op.left()), GetNode(op.right())});
}
Node* ScheduleBuilder::ProcessOperation(const ChangeOp& op) {
  const Operator* o;
  switch (op.kind) {
    using Kind = ChangeOp::Kind;
    using Assumption = ChangeOp::Assumption;
    case Kind::kFloatConversion:
      if (op.from == FloatRepresentation::Float64() &&
          op.to == FloatRepresentation::Float32()) {
        o = machine.TruncateFloat64ToFloat32();
      } else if (op.from == FloatRepresentation::Float32() &&
                 op.to == FloatRepresentation::Float64()) {
        o = machine.ChangeFloat32ToFloat64();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kSignedFloatTruncateOverflowToMin:
    case Kind::kUnsignedFloatTruncateOverflowToMin: {
      bool is_signed = op.kind == Kind::kSignedFloatTruncateOverflowToMin;
      if (op.assumption == Assumption::kReversible) {
        if (op.from == FloatRepresentation::Float64() &&
            op.to == WordRepresentation::Word64()) {
          o = is_signed ? machine.ChangeFloat64ToInt64()
                        : machine.ChangeFloat64ToUint64();
        } else if (op.from == FloatRepresentation::Float64() &&
                   op.to == WordRepresentation::Word32()) {
          o = is_signed ? machine.ChangeFloat64ToInt32()
                        : machine.ChangeFloat64ToUint32();
        } else {
          UNIMPLEMENTED();
        }
        break;
      }
      TruncateKind truncate_kind;
      switch (op.assumption) {
        case ChangeOp::Assumption::kReversible:
          UNREACHABLE();
        case ChangeOp::Assumption::kNoAssumption:
          truncate_kind = TruncateKind::kSetOverflowToMin;
          break;
        case ChangeOp::Assumption::kNoOverflow:
          truncate_kind = TruncateKind::kArchitectureDefault;
          break;
      }
      if (op.from == FloatRepresentation::Float64() &&
          op.to == WordRepresentation::Word64()) {
        DCHECK(is_signed);
        o = machine.TruncateFloat64ToInt64(truncate_kind);
      } else if (op.from == FloatRepresentation::Float64() &&
                 op.to == WordRepresentation::Word32()) {
        if (is_signed) {
          DCHECK_EQ(truncate_kind, TruncateKind::kArchitectureDefault);
          o = machine.RoundFloat64ToInt32();
        } else {
          o = machine.TruncateFloat64ToUint32();
        }
      } else if (op.from == FloatRepresentation::Float32() &&
                 op.to == WordRepresentation::Word32()) {
        o = is_signed ? machine.TruncateFloat32ToInt32(truncate_kind)
                      : machine.TruncateFloat32ToUint32(truncate_kind);
      } else {
        UNIMPLEMENTED();
      }
      break;
    }
    case Kind::kJSFloatTruncate:
      if (op.from == FloatRepresentation::Float64() &&
          op.to == WordRepresentation::Word32()) {
        o = machine.TruncateFloat64ToWord32();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kSignedToFloat:
      if (op.from == WordRepresentation::Word32() &&
          op.to == FloatRepresentation::Float64()) {
        DCHECK_EQ(op.assumption, Assumption::kNoAssumption);
        o = machine.ChangeInt32ToFloat64();
      } else if (op.from == WordRepresentation::Word64() &&
                 op.to == FloatRepresentation::Float64()) {
        o = op.assumption == Assumption::kReversible
                ? machine.ChangeInt64ToFloat64()
                : machine.RoundInt64ToFloat64();
      } else if (op.from == WordRepresentation::Word32() &&
                 op.to == FloatRepresentation::Float32()) {
        o = machine.RoundInt32ToFloat32();
      } else if (op.from == WordRepresentation::Word64() &&
                 op.to == FloatRepresentation::Float32()) {
        o = machine.RoundInt64ToFloat32();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kUnsignedToFloat:
      if (op.from == WordRepresentation::Word32() &&
          op.to == FloatRepresentation::Float64()) {
        o = machine.ChangeUint32ToFloat64();
      } else if (op.from == WordRepresentation::Word32() &&
                 op.to == FloatRepresentation::Float32()) {
        o = machine.RoundUint32ToFloat32();
      } else if (op.from == WordRepresentation::Word64() &&
                 op.to == FloatRepresentation::Float32()) {
        o = machine.RoundUint64ToFloat32();
      } else if (op.from == WordRepresentation::Word64() &&
                 op.to == FloatRepresentation::Float64()) {
        o = machine.RoundUint64ToFloat64();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kExtractHighHalf:
      DCHECK_EQ(op.from, FloatRepresentation::Float64());
      DCHECK_EQ(op.to, WordRepresentation::Word32());
      o = machine.Float64ExtractHighWord32();
      break;
    case Kind::kExtractLowHalf:
      DCHECK_EQ(op.from, FloatRepresentation::Float64());
      DCHECK_EQ(op.to, WordRepresentation::Word32());
      o = machine.Float64ExtractLowWord32();
      break;
    case Kind::kBitcast:
      if (op.from == WordRepresentation::Word32() &&
          op.to == WordRepresentation::Word64()) {
        o = machine.BitcastWord32ToWord64();
      } else if (op.from == FloatRepresentation::Float32() &&
                 op.to == WordRepresentation::Word32()) {
        o = machine.BitcastFloat32ToInt32();
      } else if (op.from == WordRepresentation::Word32() &&
                 op.to == FloatRepresentation::Float32()) {
        o = machine.BitcastInt32ToFloat32();
      } else if (op.from == FloatRepresentation::Float64() &&
                 op.to == WordRepresentation::Word64()) {
        o = machine.BitcastFloat64ToInt64();
      } else if (op.from == WordRepresentation::Word64() &&
                 op.to == FloatRepresentation::Float64()) {
        o = machine.BitcastInt64ToFloat64();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kSignExtend:
      if (op.from == WordRepresentation::Word32() &&
          op.to == WordRepresentation::Word64()) {
        o = machine.ChangeInt32ToInt64();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kZeroExtend:
      if (op.from == WordRepresentation::Word32() &&
          op.to == WordRepresentation::Word64()) {
        o = machine.ChangeUint32ToUint64();
      } else {
        UNIMPLEMENTED();
      }
      break;
    case Kind::kTruncate:
      if (op.from == WordRepresentation::Word64() &&
          op.to == WordRepresentation::Word32()) {
        o = machine.TruncateInt64ToInt32();
      } else {
        UNIMPLEMENTED();
      }
  }
  return AddNode(o, {GetNode(op.input())});
}
Node* ScheduleBuilder::ProcessOperation(const TryChangeOp& op) {
  const Operator* o;
  switch (op.kind) {
    using Kind = TryChangeOp::Kind;
    case Kind::kSignedFloatTruncateOverflowUndefined:
      if (op.from == FloatRepresentation::Float64() &&
          op.to == WordRepresentation::Word64()) {
        o = machine.TryTruncateFloat64ToInt64();
      } else if (op.from == FloatRepresentation::Float64() &&
                 op.to == WordRepresentation::Word32()) {
        o = machine.TryTruncateFloat64ToInt32();
      } else if (op.from == FloatRepresentation::Float32() &&
                 op.to == WordRepresentation::Word64()) {
        o = machine.TryTruncateFloat32ToInt64();
      } else {
        UNREACHABLE();
      }
      break;
    case Kind::kUnsignedFloatTruncateOverflowUndefined:
      if (op.from == FloatRepresentation::Float64() &&
          op.to == WordRepresentation::Word64()) {
        o = machine.TryTruncateFloat64ToUint64();
      } else if (op.from == FloatRepresentation::Float64() &&
                 op.to == WordRepresentation::Word32()) {
        o = machine.TryTruncateFloat64ToUint32();
      } else if (op.from == FloatRepresentation::Float32() &&
                 op.to == WordRepresentation::Word64()) {
        o = machine.TryTruncateFloat32ToUint64();
      } else {
        UNREACHABLE();
      }
      break;
  }
  return AddNode(o, {GetNode(op.input())});
}
Node* ScheduleBuilder::ProcessOperation(
    const BitcastWord32PairToFloat64Op& op) {
  Node* temp = AddNode(
      machine.Float64InsertHighWord32(),
      {AddNode(common.Float64Constant(0), {}), GetNode(op.high_word32())});
  return AddNode(machine.Float64InsertLowWord32(),
                 {temp, GetNode(op.low_word32())});
}
Node* ScheduleBuilder::ProcessOperation(const TaggedBitcastOp& op) {
  using Rep = RegisterRepresentation;
  const Operator* o;
  switch (multi(op.from, op.to)) {
    case multi(Rep::Tagged(), Rep::Word32()):
      if constexpr (Is64()) {
        DCHECK_EQ(op.kind, TaggedBitcastOp::Kind::kSmi);
        DCHECK(SmiValuesAre31Bits());
        o = machine.TruncateInt64ToInt32();
      } else {
        o = machine.BitcastTaggedToWord();
      }
      break;
    case multi(Rep::Tagged(), Rep::Word64()):
      o = machine.BitcastTaggedToWord();
      break;
    case multi(Rep::Word32(), Rep::Tagged()):
    case multi(Rep::Word64(), Rep::Tagged()):
      if (op.kind == TaggedBitcastOp::Kind::kSmi) {
        o = machine.BitcastWordToTaggedSigned();
      } else {
        o = machine.BitcastWordToTagged();
      }
      break;
    case multi(Rep::Compressed(), Rep::Word32()):
      o = machine.BitcastTaggedToWord();
      break;
    default:
      UNIMPLEMENTED();
  }
  return AddNode(o, {GetNode(op.input())});
}
Node* ScheduleBuilder::ProcessOperation(const SelectOp& op) {
  // If there is a Select, then it should only be one that is supported by the
  // machine, and it should be meant to be implementation with cmove.
  DCHECK_EQ(op.implem, SelectOp::Implementation::kCMove);
  DCHECK((op.rep == RegisterRepresentation::Word32() &&
          SupportedOperations::word32_select()) ||
         (op.rep == RegisterRepresentation::Word64() &&
          SupportedOperations::word64_select()) ||
         (op.rep == RegisterRepresentation::Float32() &&
          SupportedOperations::float32_select()) ||
         (op.rep == RegisterRepresentation::Float64() &&
          SupportedOperations::float64_select()));
  const Operator* o = nullptr;
  switch (op.rep.value()) {
    case RegisterRepresentation::Enum::kWord32:
      o = machine.Word32Select().op();
      break;
    case RegisterRepresentation::Enum::kWord64:
      o = machine.Word64Select().op();
      break;
    case RegisterRepresentation::Enum::kFloat32:
      o = machine.Float32Select().op();
      break;
    case RegisterRepresentation::Enum::kFloat64:
      o = machine.Float64Select().op();
      break;
    case RegisterRepresentation::Enum::kTagged:
    case RegisterRepresentation::Enum::kCompressed:
    case RegisterRepresentation::Enum::kSimd128:
    case RegisterRepresentation::Enum::kSimd256:
      UNREACHABLE();
  }

  return AddNode(
      o, {GetNode(op.cond()), GetNode(op.vtrue()), GetNode(op.vfalse())});
}
Node* ScheduleBuilder::ProcessOperation(const PendingLoopPhiOp& op) {
  UNREACHABLE();
}

Node* ScheduleBuilder::ProcessOperation(const AtomicWord32PairOp& op) {
  DCHECK(!Is64());
  Node* index;
  if (op.index().valid() && op.offset) {
    index = AddNode(machine.Int32Add(),
                    {GetNode(op.index().value()), IntPtrConstant(op.offset)});
  } else if (op.index().valid()) {
    index = GetNode(op.index().value());
  } else {
    index = IntPtrConstant(op.offset);
  }
#define BINOP_CASE(OP)                                               \
  if (op.kind == AtomicWord32PairOp::Kind::k##OP) {                  \
    return AddNode(                                                  \
        machine.Word32AtomicPair##OP(),                              \
        {GetNode(op.base()), index, GetNode(op.value_low().value()), \
         GetNode(op.value_high().value())});                         \
  }
#define ATOMIC_BINOPS(V) \
  V(Add)                 \
  V(Sub)                 \
  V(And)                 \
  V(Or)                  \
  V(Xor)                 \
  V(Exchange)
  ATOMIC_BINOPS(BINOP_CASE)
#undef ATOMIC_BINOPS
#undef BINOP_CASE

  if (op.kind == AtomicWord32PairOp::Kind::kLoad) {
    return AddNode(machine.Word32AtomicPairLoad(AtomicMemoryOrder::kSeqCst),
                   {GetNode(op.base()), index});
  }
  if (op.kind == AtomicWord32PairOp::Kind::kStore) {
    return AddNode(machine.Word32AtomicPairStore(AtomicMemoryOrder::kSeqCst),
                   {GetNode(op.base()), index, GetNode(op.value_low().value()),
                    GetNode(op.value_high().value())});
  }
  DCHECK_EQ(op.kind, AtomicWord32PairOp::Kind::kCompareExchange);
  return AddNode(
      machine.Word32AtomicPairCompareExchange(),
      {GetNode(op.base()), index, GetNode(op.expected_low().value()),
       GetNode(op.expected_high().value()), GetNode(op.value_low().value()),
       GetNode(op.value_high().value())});
}

Node* ScheduleBuilder::ProcessOperation(const AtomicRMWOp& op) {
#define ATOMIC_BINOPS(V) \
  V(Add)                 \
  V(Sub)                 \
  V(And)                 \
  V(Or)                  \
  V(Xor)                 \
  V(Exchange)            \
  V(CompareExchange)

  AtomicOpParameters param(op.memory_rep.ToMachineType(),
                           op.memory_access_kind);
  const Operator* node_op;
  if (op.in_out_rep == RegisterRepresentation::Word32()) {
    switch (op.bin_op) {
#define CASE(Name)                               \
  case AtomicRMWOp::BinOp::k##Name:              \
    node_op = machine.Word32Atomic##Name(param); \
    break;
      ATOMIC_BINOPS(CASE)
#undef CASE
    }
  } else {
    DCHECK_EQ(op.in_out_rep, RegisterRepresentation::Word64());
    switch (op.bin_op) {
#define CASE(Name)                               \
  case AtomicRMWOp::BinOp::k##Name:              \
    node_op = machine.Word64Atomic##Name(param); \
    break;
      ATOMIC_BINOPS(CASE)
#undef CASE
    }
  }
#undef ATOMIC_BINOPS
  Node* base = GetNode(op.base());
  Node* index = GetNode(op.index());
  Node* value = GetNode(op.value());
  if (op.bin_op == AtomicRMWOp::BinOp::kCompareExchange) {
    Node* expected = GetNode(op.expected().value());
    return AddNode(node_op, {base, index, expected, value});
  } else {
    return AddNode(node_op, {base, index, value});
  }
}

Node* ScheduleBuilder::ProcessOperation(const MemoryBarrierOp& op) {
  return AddNode(machine.MemoryBarrier(op.memory_order), {});
}

Node* ScheduleBuilder::ProcessOperation(const TupleOp& op) {
  // Tuples are only used for lowerings during reduction. Therefore, we can
  // assume that it is unused if it occurs at this point.
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const ConstantOp& op) {
  switch (op.kind) {
    case ConstantOp::Kind::kWord32:
      return AddNode(common.Int32Constant(static_cast<int32_t>(op.word32())),
                     {});
    case ConstantOp::Kind::kWord64:
      return AddNode(common.Int64Constant(static_cast<int64_t>(op.word64())),
                     {});
    case ConstantOp::Kind::kSmi:
      if constexpr (Is64()) {
        return AddNode(
            machine.BitcastWordToTaggedSigned(),
            {AddNode(common.Int64Constant(static_cast<int64_t>(op.smi().ptr())),
                     {})});
      } else {
        return AddNode(
            machine.BitcastWordToTaggedSigned(),
            {AddNode(common.Int32Constant(static_cast<int32_t>(op.smi().ptr())),
                     {})});
      }
    case ConstantOp::Kind::kExternal:
      return AddNode(common.ExternalConstant(op.external_reference()), {});
    case ConstantOp::Kind::kHeapObject:
      return AddNode(common.HeapConstant(op.handle()), {});
    case ConstantOp::Kind::kCompressedHeapObject:
      return AddNode(common.CompressedHeapConstant(op.handle()), {});
    case ConstantOp::Kind::kTrustedHeapObject:
      return AddNode(common.TrustedHeapConstant(op.handle()), {});
    case ConstantOp::Kind::kNumber:
      return AddNode(common.NumberConstant(op.number().get_scalar()), {});
    case ConstantOp::Kind::kTaggedIndex:
      return AddNode(common.TaggedIndexConstant(op.tagged_index()), {});
    case ConstantOp::Kind::kFloat64:
      return AddNode(common.Float64Constant(op.float64().get_scalar()), {});
    case ConstantOp::Kind::kFloat32:
      return AddNode(common.Float32Constant(op.float32().get_scalar()), {});
    case ConstantOp::Kind::kRelocatableWasmCall:
      return RelocatableIntPtrConstant(op.integral(), RelocInfo::WASM_CALL);
    case ConstantOp::Kind::kRelocatableWasmStubCall:
      return RelocatableIntPtrConstant(op.integral(),
                                       RelocInfo::WASM_STUB_CALL);
    case ConstantOp::Kind::kRelocatableWasmCanonicalSignatureId:
      return AddNode(common.RelocatableInt32Constant(
                         base::checked_cast<int32_t>(op.integral()),
                         RelocInfo::WASM_CANONICAL_SIG_ID),
                     {});
  }
}

Node* ScheduleBuilder::ProcessOperation(const LoadOp& op) {
  intptr_t offset = op.offset;
  if (op.kind.tagged_base) {
    CHECK_GE(offset, std::numeric_limits<int32_t>::min() + kHeapObjectTag);
    offset -= kHeapObjectTag;
  }
  Node* base = GetNode(op.base());
  Node* index;
  if (op.index().valid()) {
    index = GetNode(op.index().value());
    if (op.element_size_log2 != 0) {
      index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
    }
    if (offset != 0) {
      index = IntPtrAdd(index, IntPtrConstant(offset));
    }
  } else {
    index = IntPtrConstant(offset);
  }

  MachineType loaded_rep = op.machine_type();
  const Operator* o;
  if (op.kind.maybe_unaligned) {
    DCHECK(!op.kind.with_trap_handler);
    if (loaded_rep.representation() == MachineRepresentation::kWord8 ||
        machine.UnalignedLoadSupported(loaded_rep.representation())) {
      o = machine.Load(loaded_rep);
    } else {
      o = machine.UnalignedLoad(loaded_rep);
    }
  } else if (op.kind.is_atomic) {
    DCHECK(!op.kind.maybe_unaligned);
    AtomicLoadParameters params(loaded_rep, AtomicMemoryOrder::kSeqCst,
                                op.kind.with_trap_handler
                                    ? MemoryAccessKind::kProtected
                                    : MemoryAccessKind::kNormal);
    if (op.result_rep == RegisterRepresentation::Word32()) {
      o = machine.Word32AtomicLoad(params);
    } else {
      DCHECK_EQ(op.result_rep, RegisterRepresentation::Word64());
      o = machine.Word64AtomicLoad(params);
    }
  } else if (op.kind.with_trap_handler) {
    DCHECK(!op.kind.maybe_unaligned);
    if (op.kind.tagged_base) {
      o = machine.LoadTrapOnNull(loaded_rep);
    } else {
      o = machine.ProtectedLoad(loaded_rep);
    }
  } else {
    o = machine.Load(loaded_rep);
  }
  return AddNode(o, {base, index});
}

Node* ScheduleBuilder::ProcessOperation(const StoreOp& op) {
  intptr_t offset = op.offset;
  if (op.kind.tagged_base) {
    CHECK(offset >= std::numeric_limits<int32_t>::min() + kHeapObjectTag);
    offset -= kHeapObjectTag;
  }
  Node* base = GetNode(op.base());
  Node* index;
  if (op.index().valid()) {
    index = GetNode(op.index().value());
    if (op.element_size_log2 != 0) {
      index = IntPtrShl(index, IntPtrConstant(op.element_size_log2));
    }
    if (offset != 0) {
      index = IntPtrAdd(index, IntPtrConstant(offset));
    }
  } else {
    index = IntPtrConstant(offset);
  }
  Node* value = GetNode(op.value());

  const Operator* o;
  if (op.kind.maybe_unaligned) {
    DCHECK(!op.kind.with_trap_handler);
    DCHECK_EQ(op.write_barrier, WriteBarrierKind::kNoWriteBarrier);
    if (op.stored_rep.ToMachineType().representation() ==
            MachineRepresentation::kWord8 ||
        machine.UnalignedStoreSupported(
            op.stored_rep.ToMachineType().representation())) {
      o = machine.Store(StoreRepresentation(
          op.stored_rep.ToMachineType().representation(), op.write_barrier));
    } else {
      o = machine.UnalignedStore(
          op.stored_rep.ToMachineType().representation());
    }
  } else if (op.kind.is_atomic) {
    AtomicStoreParameters params(op.stored_rep.ToMachineType().representation(),
                                 op.write_barrier, AtomicMemoryOrder::kSeqCst,
                                 op.kind.with_trap_handler
                                     ? MemoryAccessKind::kProtected
                                     : MemoryAccessKind::kNormal);
    if (op.stored_rep == MemoryRepresentation::Int64() ||
        op.stored_rep == MemoryRepresentation::Uint64()) {
      o = machine.Word64AtomicStore(params);
    } else {
      o = machine.Word32AtomicStore(params);
    }
  } else if (op.kind.with_trap_handler) {
    DCHECK(!op.kind.maybe_unaligned);
    if (op.kind.tagged_base) {
      o = machine.StoreTrapOnNull(StoreRepresentation(
          op.stored_rep.ToMachineType().representation(), op.write_barrier));
    } else {
      DCHECK_EQ(op.write_barrier, WriteBarrierKind::kNoWriteBarrier);
      o = machine.ProtectedStore(
          op.stored_rep.ToMachineType().representation());
    }
  } else if (op.stored_rep == MemoryRepresentation::IndirectPointer()) {
    o = machine.StoreIndirectPointer(op.write_barrier);
    // In this case we need a fourth input: the indirect pointer tag.
    Node* tag = IntPtrConstant(op.indirect_pointer_tag());
    return AddNode(o, {base, index, value, tag});
  } else {
    o = machine.Store(StoreRepresentation(
        op.stored_rep.ToMachineType().representation(), op.write_barrier));
  }
  return AddNode(o, {base, index, value});
}

Node* ScheduleBuilder::ProcessOperation(const RetainOp& op) {
  return AddNode(common.Retain(), {GetNode(op.retained())});
}
Node* ScheduleBuilder::ProcessOperation(const ParameterOp& op) {
  // Parameters need to be cached because the register allocator assumes that
  // there are no duplicate nodes for the same parameter.
  if (parameters.count(op.parameter_index)) {
    return parameters[op.parameter_index];
  }
  Node* parameter = MakeNode(
      common.Parameter(static_cast<int>(op.parameter_index), op.debug_name),
      {tf_graph->start()});
  schedule->AddNode(schedule->start(), parameter);
  parameters[op.parameter_index] = parameter;
  return parameter;
}
Node* ScheduleBuilder::ProcessOperation(const OsrValueOp& op) {
  // OSR values behave like parameters, so they also need to be cached.
  if (osr_values.count(op.index)) {
    return osr_values[op.index];
  }
  Node* osr_value = MakeNode(common.OsrValue(static_cast<int>(op.index)),
                             {tf_graph->start()});
  schedule->AddNode(schedule->start(), osr_value);
  osr_values[op.index] = osr_value;
  return osr_value;
}
Node* ScheduleBuilder::ProcessOperation(const GotoOp& op) {
  schedule->AddGoto(current_block, blocks[op.destination->index().id()]);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const StackPointerGreaterThanOp& op) {
  return AddNode(machine.StackPointerGreaterThan(op.kind),
                 {GetNode(op.stack_limit())});
}
Node* ScheduleBuilder::ProcessOperation(const StackSlotOp& op) {
  return AddNode(machine.StackSlot(op.size, op.alignment, op.is_tagged), {});
}
Node* ScheduleBuilder::ProcessOperation(const FrameConstantOp& op) {
  switch (op.kind) {
    case FrameConstantOp::Kind::kStackCheckOffset:
      return AddNode(machine.LoadStackCheckOffset(), {});
    case FrameConstantOp::Kind::kFramePointer:
      return AddNode(machine.LoadFramePointer(), {});
    case FrameConstantOp::Kind::kParentFramePointer:
      return AddNode(machine.LoadParentFramePointer(), {});
  }
}
Node* ScheduleBuilder::ProcessOperation(const DeoptimizeIfOp& op) {
  Node* condition = GetNode(op.condition());
  Node* frame_state = GetNode(op.frame_state());
  const Operator* o = op.negated
                          ? common.DeoptimizeUnless(op.parameters->reason(),
                                                    op.parameters->feedback())
                          : common.DeoptimizeIf(op.parameters->reason(),
                                                op.parameters->feedback());
  return AddNode(o, {condition, frame_state});
}

#if V8_ENABLE_WEBASSEMBLY
Node* ScheduleBuilder::ProcessOperation(const TrapIfOp& op) {
  Node* condition = GetNode(op.condition());
  bool has_frame_state = op.frame_state().valid();
  Node* frame_state =
      has_frame_state ? GetNode(op.frame_state().value()) : nullptr;
  const Operator* o = op.negated
                          ? common.TrapUnless(op.trap_id, has_frame_state)
                          : common.TrapIf(op.trap_id, has_frame_state);
  return has_frame_state ? AddNode(o, {condition, frame_state})
                         : AddNode(o, {condition});
}
#endif  // V8_ENABLE_WEBASSEMBLY

Node* ScheduleBuilder::ProcessOperation(const DeoptimizeOp& op) {
  Node* frame_state = GetNode(op.frame_state());
  const Operator* o =
      common.Deoptimize(op.parameters->reason(), op.parameters->feedback());
  Node* node = MakeNode(o, {frame_state});
  schedule->AddDeoptimize(current_block, node);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const PhiOp& op) {
  if (current_input_block->IsLoop()) {
    DCHECK_EQ(op.input_count, 2);
    Node* input = GetNode(op.input(0));
    // The second `input` is a placeholder that is patched when we process the
    // backedge.
    Node* node =
        AddNode(common.Phi(op.rep.machine_representation(), 2), {input, input});
    loop_phis.emplace_back(node, op.input(1));
    return node;
  } else {
    // Predecessors of {current_input_block} and the TF's matching block might
    // not be in the same order, so Phi inputs might need to be reordered to
    // match the new order.
    // This is similar to what AssembleOutputGraphPhi in CopyingPhase does,
    // except that CopyingPhase has a new->old block mapping, which we
    // don't have here in RecreateSchedule, so the implementation is slightly
    // different (relying on std::lower_bound rather than looking up the
    // old->new mapping).
    ZoneVector<BasicBlock*> new_predecessors = current_block->predecessors();
    // Since RecreateSchedule visits the blocks in increasing ID order,
    // predecessors should be sorted (we rely on this property to binary search
    // new predecessors corresponding to old ones).
    auto cmp_basic_block = [](BasicBlock* a, BasicBlock* b) {
      return a->id().ToInt() < b->id().ToInt();
    };
    DCHECK(std::is_sorted(new_predecessors.begin(), new_predecessors.end(),
                          cmp_basic_block));
    size_t predecessor_count = new_predecessors.size();
    base::SmallVector<Node*, 8> inputs(predecessor_count);
#ifdef DEBUG
    std::fill(inputs.begin(), inputs.end(), nullptr);
#endif

    int current_index = 0;
    for (const Block* pred : current_input_block->PredecessorsIterable()) {
      size_t pred_index = predecessor_count - current_index - 1;
      auto lower =
          std::lower_bound(new_predecessors.begin(), new_predecessors.end(),
                           GetBlock(*pred), cmp_basic_block);
      DCHECK_NE(lower, new_predecessors.end());
      size_t new_pred_index = std::distance(new_predecessors.begin(), lower);
      // Block {pred_index} became predecessor {new_pred_index} in the TF graph.
      // We thus put the input {pred_index} in position {new_pred_index}.
      inputs[new_pred_index] = GetNode(op.input(pred_index));
      ++current_index;
    }
    DCHECK(!base::contains(inputs, nullptr));

    return AddNode(common.Phi(op.rep.machine_representation(), op.input_count),
                   base::VectorOf(inputs));
  }
}
Node* ScheduleBuilder::ProcessOperation(const ProjectionOp& op) {
  return AddNode(common.Projection(op.index), {GetNode(op.input())});
}
Node* ScheduleBuilder::ProcessOperation(const AssumeMapOp&) {
  // AssumeMapOp is just a hint that optimization phases can use, but has no
  // Turbofan equivalent and is thus not used past this point.
  return nullptr;
}

std::pair<Node*, MachineType> ScheduleBuilder::BuildDeoptInput(
    FrameStateData::Iterator* it) {
  switch (it->current_instr()) {
    using Instr = FrameStateData::Instr;
    case Instr::kInput: {
      MachineType type;
      OpIndex input;
      it->ConsumeInput(&type, &input);
      const Operation& op = input_graph.Get(input);
      if (op.outputs_rep()[0] == RegisterRepresentation::Word64() &&
          type.representation() == MachineRepresentation::kWord32) {
        // 64 to 32-bit conversion is implicit in turboshaft, but explicit in
        // turbofan, so we insert this conversion.
        Node* conversion =
            AddNode(machine.TruncateInt64ToInt32(), {GetNode(input)});
        return {conversion, type};
      }
      return {GetNode(input), type};
    }
    case Instr::kDematerializedObject: {
      uint32_t obj_id;
      uint32_t field_count;
      it->ConsumeDematerializedObject(&obj_id, &field_count);
      base::SmallVector<Node*, 16> fields;
      ZoneVector<MachineType>& field_types =
          *tf_graph->zone()->New<ZoneVector<MachineType>>(field_count,
                                                          tf_graph->zone());
      for (uint32_t i = 0; i < field_count; ++i) {
        std::pair<Node*, MachineType> p = BuildDeoptInput(it);
        fields.push_back(p.first);
        field_types[i] = p.second;
      }
      return {AddNode(common.TypedObjectState(obj_id, &field_types),
                      base::VectorOf(fields)),
              MachineType::AnyTagged()};
    }
    case Instr::kDematerializedObjectReference: {
      uint32_t obj_id;
      it->ConsumeDematerializedObjectReference(&obj_id);
      return {AddNode(common.ObjectId(obj_id), {}), MachineType::AnyTagged()};
    }
    case Instr::kArgumentsElements: {
      CreateArgumentsType type;
      it->ConsumeArgumentsElements(&type);
      return {AddNode(common.ArgumentsElementsState(type), {}),
              MachineType::AnyTagged()};
    }
    case Instr::kArgumentsLength: {
      it->ConsumeArgumentsLength();
      return {AddNode(common.ArgumentsLengthState(), {}),
              MachineType::AnyTagged()};
    }
    case Instr::kRestLength:
      // For now, kRestLength is only generated when using the Maglev frontend,
      // which doesn't use recreate-schedule.
      [[fallthrough]];
    case Instr::kUnusedRegister:
      UNREACHABLE();
  }
}

// Create a mostly balanced tree of `StateValues` nodes.
Node* ScheduleBuilder::BuildStateValues(FrameStateData::Iterator* it,
                                        int32_t size) {
  constexpr int32_t kMaxStateValueInputCount = 8;

  base::SmallVector<Node*, kMaxStateValueInputCount> inputs;
  base::SmallVector<MachineType, kMaxStateValueInputCount> types;
  SparseInputMask::BitMaskType input_mask = 0;
  int32_t child_size =
      (size + kMaxStateValueInputCount - 1) / kMaxStateValueInputCount;
  // `state_value_inputs` counts the number of inputs used for the current
  // `StateValues` node. It is gradually adjusted as nodes are shifted to lower
  // levels in the tree.
  int32_t state_value_inputs = size;
  int32_t mask_size = 0;
  for (int32_t i = 0; i < state_value_inputs; ++i) {
    DCHECK_LT(i, kMaxStateValueInputCount);
    ++mask_size;
    if (state_value_inputs <= kMaxStateValueInputCount) {
      // All the remaining inputs fit at the current level.
      if (it->current_instr() == FrameStateData::Instr::kUnusedRegister) {
        it->ConsumeUnusedRegister();
      } else {
        std::pair<Node*, MachineType> p = BuildDeoptInput(it);
        input_mask |= SparseInputMask::BitMaskType{1} << i;
        inputs.push_back(p.first);
        types.push_back(p.second);
      }
    } else {
      // We have too many inputs, so recursively create another `StateValues`
      // node.
      input_mask |= SparseInputMask::BitMaskType{1} << i;
      int32_t actual_child_size = std::min(child_size, state_value_inputs - i);
      inputs.push_back(BuildStateValues(it, actual_child_size));
      // This is a dummy type that shouldn't matter.
      types.push_back(MachineType::AnyTagged());
      // `child_size`-many inputs were shifted to the next level, being replaced
      // with 1 `StateValues` node.
      state_value_inputs = state_value_inputs - actual_child_size + 1;
    }
  }
  input_mask |= SparseInputMask::kEndMarker << mask_size;
  return AddNode(
      common.TypedStateValues(graph_zone->New<ZoneVector<MachineType>>(
                                  types.begin(), types.end(), graph_zone),
                              SparseInputMask(input_mask)),
      base::VectorOf(inputs));
}

Node* ScheduleBuilder::BuildTaggedInput(FrameStateData::Iterator* it) {
  std::pair<Node*, MachineType> p = BuildDeoptInput(it);
  DCHECK(p.second.IsTagged());
  return p.first;
}

Node* ScheduleBuilder::ProcessOperation(const FrameStateOp& op) {
  const FrameStateInfo& info = op.data->frame_state_info;
  auto it = op.data->iterator(op.state_values());

  Node* closure = BuildTaggedInput(&it);
  Node* parameter_state_values = BuildStateValues(&it, info.parameter_count());
  Node* context = BuildTaggedInput(&it);
  Node* register_state_values = BuildStateValues(&it, info.local_count());
  Node* accumulator_state_values = BuildStateValues(&it, info.stack_count());
  Node* parent =
      op.inlined ? GetNode(op.parent_frame_state()) : tf_graph->start();

  return AddNode(common.FrameState(info.bailout_id(), info.state_combine(),
                                   info.function_info()),
                 {parameter_state_values, register_state_values,
                  accumulator_state_values, context, closure, parent});
}
Node* ScheduleBuilder::ProcessOperation(const CallOp& op) {
  base::SmallVector<Node*, 16> inputs;
  inputs.push_back(GetNode(op.callee()));
  for (OpIndex i : op.arguments()) {
    inputs.push_back(GetNode(i));
  }
  if (op.HasFrameState()) {
    DCHECK(op.frame_state().valid());
    inputs.push_back(GetNode(op.frame_state().value()));
  }
  return AddNode(common.Call(op.descriptor->descriptor),
                 base::VectorOf(inputs));
}
Node* ScheduleBuilder::ProcessOperation(const CheckExceptionOp& op) {
  Node* call_node = GetNode(op.throwing_operation());
  DCHECK_EQ(call_node->opcode(), IrOpcode::kCall);

  // Re-building the IfSuccess/IfException mechanism.
  BasicBlock* success_block = GetBlock(*op.didnt_throw_block);
  BasicBlock* exception_block = GetBlock(*op.catch_block);
  exception_block->set_deferred(true);
  schedule->AddCall(current_block, call_node, success_block, exception_block);
  // Pass `call` as the control input of `IfSuccess` and as both the effect and
  // control input of `IfException`.
  Node* if_success = MakeNode(common.IfSuccess(), {call_node});
  Node* if_exception = MakeNode(common.IfException(), {call_node, call_node});
  schedule->AddNode(success_block, if_success);
  schedule->AddNode(exception_block, if_exception);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const CatchBlockBeginOp& op) {
  Node* if_exception = current_block->NodeAt(0);
  DCHECK(if_exception != nullptr &&
         if_exception->opcode() == IrOpcode::kIfException);
  return if_exception;
}
Node* ScheduleBuilder::ProcessOperation(const DidntThrowOp& op) {
  return GetNode(op.throwing_operation());
}
Node* ScheduleBuilder::ProcessOperation(const TailCallOp& op) {
  base::SmallVector<Node*, 16> inputs;
  inputs.push_back(GetNode(op.callee()));
  for (OpIndex i : op.arguments()) {
    inputs.push_back(GetNode(i));
  }
  Node* call = MakeNode(common.TailCall(op.descriptor->descriptor),
                        base::VectorOf(inputs));
  schedule->AddTailCall(current_block, call);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const UnreachableOp& op) {
  Node* node = MakeNode(common.Throw(), {});
  schedule->AddNode(current_block, MakeNode(common.Unreachable(), {}));
  schedule->AddThrow(current_block, node);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const ReturnOp& op) {
  base::SmallVector<Node*, 8> inputs = {GetNode(op.pop_count())};
  for (OpIndex i : op.return_values()) {
    inputs.push_back(GetNode(i));
  }
  Node* node =
      MakeNode(common.Return(static_cast<int>(op.return_values().size())),
               base::VectorOf(inputs));
  schedule->AddReturn(current_block, node);
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const BranchOp& op) {
  Node* branch = MakeNode(common.Branch(op.hint), {GetNode(op.condition())});
  BasicBlock* true_block = GetBlock(*op.if_true);
  BasicBlock* false_block = GetBlock(*op.if_false);
  schedule->AddBranch(current_block, branch, true_block, false_block);
  schedule->AddNode(true_block, MakeNode(common.IfTrue(), {branch}));
  schedule->AddNode(false_block, MakeNode(common.IfFalse(), {branch}));
  switch (op.hint) {
    case BranchHint::kNone:
      break;
    case BranchHint::kTrue:
      false_block->set_deferred(true);
      break;
    case BranchHint::kFalse:
      true_block->set_deferred(true);
      break;
  }
  current_block = nullptr;
  return nullptr;
}
Node* ScheduleBuilder::ProcessOperation(const SwitchOp& op) {
  size_t succ_count = op.cases.size() + 1;
  Node* switch_node =
      MakeNode(common.Switch(succ_count), {GetNode(op.input())});

  base::SmallVector<BasicBlock*, 16> successors;
  for (SwitchOp::Case c : op.cases) {
    BasicBlock* case_block = GetBlock(*c.destination);
    successors.push_back(case_block);
    Node* case_node =
        MakeNode(common.IfValue(c.value, 0, c.hint), {switch_node});
    schedule->AddNode(case_block, case_node);
    if (c.hint == BranchHint::kFalse) {
      case_block->set_deferred(true);
    }
  }
  BasicBlock* default_block = GetBlock(*op.default_case);
  successors.push_back(default_block);
  schedule->AddNode(default_block,
                    MakeNode(common.IfDefault(op.default_hint), {switch_node}));
  if (op.default_hint == BranchHint::kFalse) {
    default_block->set_deferred(true);
  }

  schedule->AddSwitch(current_block, switch_node, successors.data(),
                      successors.size());
  current_block = nullptr;
  return nullptr;
}

Node* ScheduleBuilder::ProcessOperation(const DebugBreakOp& op) {
  return AddNode(machine.DebugBreak(), {});
}

Node* ScheduleBuilder::ProcessOperation(const LoadRootRegisterOp& op) {
  return AddNode(machine.LoadRootRegister(), {});
}

Node* ScheduleBuilder::ProcessOperation(const Word32PairBinopOp& op) {
  using Kind = Word32PairBinopOp::Kind;
  const Operator* pair_operator = nullptr;
  switch (op.kind) {
    case Kind::kAdd:
      pair_operator = machine.Int32PairAdd();
      break;
    case Kind::kSub:
      pair_operator = machine.Int32PairSub();
      break;
    case Kind::kMul:
      pair_operator = machine.Int32PairMul();
      break;
    case Kind::kShiftLeft:
      pair_operator = machine.Word32PairShl();
      break;
    case Kind::kShiftRightArithmetic:
      pair_operator = machine.Word32PairSar();
      break;
    case Kind::kShiftRightLogical:
      pair_operator = machine.Word32PairShr();
      break;
  }
  return AddNode(pair_operator,
                 {GetNode(op.left_low()), GetNode(op.left_high()),
                  GetNode(op.right_low()), GetNode(op.right_high())});
}

Node* ScheduleBuilder::ProcessOperation(const CommentOp& op) {
  return AddNode(machine.Comment(op.message), {});
}

#ifdef V8_ENABLE_WEBASSEMBLY
Node* ScheduleBuilder::ProcessOperation(const Simd128ConstantOp& op) {
  return AddNode(machine.S128Const(op.value), {});
}

Node* ScheduleBuilder::ProcessOperation(const Simd128BinopOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd128BinopOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.left()), GetNode(op.right())});
    FOREACH_SIMD_128_BINARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128UnaryOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd128UnaryOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.input())});
    FOREACH_SIMD_128_UNARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128ReduceOp& op) {
  UNIMPLEMENTED();
}

Node* ScheduleBuilder::ProcessOperation(const Simd128ShiftOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd128ShiftOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.input()), GetNode(op.shift())});
    FOREACH_SIMD_128_SHIFT_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128TestOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)            \
  case Simd128TestOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.input())});
    FOREACH_SIMD_128_TEST_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128SplatOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd128SplatOp::Kind::k##kind: \
    return AddNode(machine.kind##Splat(), {GetNode(op.input())});
    FOREACH_SIMD_128_SPLAT_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128TernaryOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)                                                      \
  case Simd128TernaryOp::Kind::k##kind:                                        \
    return AddNode(machine.kind(), {GetNode(op.first()), GetNode(op.second()), \
                                    GetNode(op.third())});
    FOREACH_SIMD_128_TERNARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd128ExtractLaneOp& op) {
  const Operator* o = nullptr;
  switch (op.kind) {
    case Simd128ExtractLaneOp::Kind::kI8x16S:
      o = machine.I8x16ExtractLaneS(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kI8x16U:
      o = machine.I8x16ExtractLaneU(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kI16x8S:
      o = machine.I16x8ExtractLaneS(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kI16x8U:
      o = machine.I16x8ExtractLaneU(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kI32x4:
      o = machine.I32x4ExtractLane(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kI64x2:
      o = machine.I64x2ExtractLane(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kF16x8:
      o = machine.F16x8ExtractLane(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kF32x4:
      o = machine.F32x4ExtractLane(op.lane);
      break;
    case Simd128ExtractLaneOp::Kind::kF64x2:
      o = machine.F64x2ExtractLane(op.lane);
      break;
  }

  return AddNode(o, {GetNode(op.input())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd128ReplaceLaneOp& op) {
  const Operator* o = nullptr;
  switch (op.kind) {
    case Simd128ReplaceLaneOp::Kind::kI8x16:
      o = machine.I8x16ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kI16x8:
      o = machine.I16x8ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kI32x4:
      o = machine.I32x4ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kI64x2:
      o = machine.I64x2ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kF16x8:
      o = machine.F16x8ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kF32x4:
      o = machine.F32x4ReplaceLane(op.lane);
      break;
    case Simd128ReplaceLaneOp::Kind::kF64x2:
      o = machine.F64x2ReplaceLane(op.lane);
      break;
  }

  return AddNode(o, {GetNode(op.into()), GetNode(op.new_lane())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd128LaneMemoryOp& op) {
  DCHECK_EQ(op.offset, 0);
  MemoryAccessKind access =
      op.kind.with_trap_handler ? MemoryAccessKind::kProtected
      : op.kind.maybe_unaligned ? MemoryAccessKind::kUnaligned
                                : MemoryAccessKind::kNormal;

  MachineType type;
  switch (op.lane_kind) {
    case Simd128LaneMemoryOp::LaneKind::k8:
      type = MachineType::Int8();
      break;
    case Simd128LaneMemoryOp::LaneKind::k16:
      type = MachineType::Int16();
      break;
    case Simd128LaneMemoryOp::LaneKind::k32:
      type = MachineType::Int32();
      break;
    case Simd128LaneMemoryOp::LaneKind::k64:
      type = MachineType::Int64();
      break;
  }

  const Operator* o = nullptr;
  if (op.mode == Simd128LaneMemoryOp::Mode::kLoad) {
    o = machine.LoadLane(access, type, op.lane);
  } else {
    o = machine.StoreLane(access, type.representation(), op.lane);
  }

  return AddNode(
      o, {GetNode(op.base()), GetNode(op.index()), GetNode(op.value())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd128LoadTransformOp& op) {
  DCHECK_EQ(op.offset, 0);
  MemoryAccessKind access =
      op.load_kind.with_trap_handler ? MemoryAccessKind::kProtected
      : op.load_kind.maybe_unaligned ? MemoryAccessKind::kUnaligned
                                     : MemoryAccessKind::kNormal;
  LoadTransformation transformation;
  switch (op.transform_kind) {
#define HANDLE_KIND(kind)                                 \
  case Simd128LoadTransformOp::TransformKind::k##kind:    \
    transformation = LoadTransformation::kS128Load##kind; \
    break;
    FOREACH_SIMD_128_LOAD_TRANSFORM_OPCODE(HANDLE_KIND)
#undef HANDLE_KIND
  }

  const Operator* o = machine.LoadTransform(access, transformation);

  return AddNode(o, {GetNode(op.base()), GetNode(op.index())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd128ShuffleOp& op) {
  return AddNode(machine.I8x16Shuffle(op.shuffle),
                 {GetNode(op.left()), GetNode(op.right())});
}

#if V8_ENABLE_WASM_SIMD256_REVEC
Node* ScheduleBuilder::ProcessOperation(const Simd256ConstantOp& op) {
  return AddNode(machine.S256Const(op.value), {});
}

Node* ScheduleBuilder::ProcessOperation(const Simd256Extract128LaneOp& op) {
  const Operator* o = machine.ExtractF128(op.lane);
  return AddNode(o, {GetNode(op.input())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd256LoadTransformOp& op) {
  DCHECK_EQ(op.offset, 0);
  MemoryAccessKind access =
      op.load_kind.with_trap_handler ? MemoryAccessKind::kProtected
      : op.load_kind.maybe_unaligned ? MemoryAccessKind::kUnaligned
                                     : MemoryAccessKind::kNormal;
  LoadTransformation transformation;
  switch (op.transform_kind) {
#define HANDLE_KIND(kind)                                 \
  case Simd256LoadTransformOp::TransformKind::k##kind:    \
    transformation = LoadTransformation::kS256Load##kind; \
    break;
    FOREACH_SIMD_256_LOAD_TRANSFORM_OPCODE(HANDLE_KIND)
#undef HANDLE_KIND
  }

  const Operator* o = machine.LoadTransform(access, transformation);

  return AddNode(o, {GetNode(op.base()), GetNode(op.index())});
}

Node* ScheduleBuilder::ProcessOperation(const Simd256UnaryOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd256UnaryOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.input())});
    FOREACH_SIMD_256_UNARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd256BinopOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd256BinopOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.left()), GetNode(op.right())});
    FOREACH_SIMD_256_BINARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd256ShiftOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd256ShiftOp::Kind::k##kind: \
    return AddNode(machine.kind(), {GetNode(op.input()), GetNode(op.shift())});
    FOREACH_SIMD_256_SHIFT_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd256TernaryOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)                                                      \
  case Simd256TernaryOp::Kind::k##kind:                                        \
    return AddNode(machine.kind(), {GetNode(op.first()), GetNode(op.second()), \
                                    GetNode(op.third())});
    FOREACH_SIMD_256_TERNARY_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const Simd256SplatOp& op) {
  switch (op.kind) {
#define HANDLE_KIND(kind)             \
  case Simd256SplatOp::Kind::k##kind: \
    return AddNode(machine.kind##Splat(), {GetNode(op.input())});
    FOREACH_SIMD_256_SPLAT_OPCODE(HANDLE_KIND);
#undef HANDLE_KIND
  }
}

Node* ScheduleBuilder::ProcessOperation(const SimdPack128To256Op& op) {
  UNREACHABLE();
}

#ifdef V8_TARGET_ARCH_X64
Node* ScheduleBuilder::ProcessOperation(const Simd256ShufdOp& op) {
  UNIMPLEMENTED();
}
Node* ScheduleBuilder::ProcessOperation(const Simd256ShufpsOp& op) {
  UNIMPLEMENTED();
}
Node* ScheduleBuilder::ProcessOperation(const Simd256UnpackOp& op) {
  UNIMPLEMENTED();
}
#endif  // V8_TARGET_ARCH_X64
#endif  // V8_ENABLE_WASM_SIMD256_REVEC

Node* ScheduleBuilder::ProcessOperation(const LoadStackPointerOp& op) {
  return AddNode(machine.LoadStackPointer(), {});
}

Node* ScheduleBuilder::ProcessOperation(const SetStackPointerOp& op) {
  return AddNode(machine.SetStackPointer(), {GetNode(op.value())});
}

#endif  // V8_ENABLE_WEBASSEMBLY

}  // namespace

RecreateScheduleResult RecreateSchedule(PipelineData* data,
                                        compiler::TFPipelineData* turbofan_data,
                                        CallDescriptor* call_descriptor,
                                        Zone* phase_zone) {
  ScheduleBuilder builder{data, call_descriptor, phase_zone, turbofan_data};
  return builder.Run();
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/recreate-schedule.h                                     0000664 0000000 0000000 00000002174 14746647661 0024370 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_H_
#define V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_H_

#include "src/compiler/compiler-source-position-table.h"
#include "src/compiler/js-heap-broker.h"
#include "src/compiler/node-origin-table.h"

namespace v8::internal {
class Zone;
}
namespace v8::internal::compiler {
class Schedule;
class Graph;
class CallDescriptor;
class TFPipelineData;
}  // namespace v8::internal::compiler
namespace v8::internal::compiler::turboshaft {
class Graph;
class PipelineData;

struct RecreateScheduleResult {
  compiler::Graph* graph;
  Schedule* schedule;
};

RecreateScheduleResult RecreateSchedule(PipelineData* data,
                                        compiler::TFPipelineData* turbofan_data,
                                        CallDescriptor* call_descriptor,
                                        Zone* phase_zone);

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_RECREATE_SCHEDULE_H_
                                                                                                                                                                                                                                                                                                                                                                                                    node-23.7.0/deps/v8/src/compiler/turboshaft/reducer-traits.h                                        0000664 0000000 0000000 00000006715 14746647661 0023746 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_REDUCER_TRAITS_H_
#define V8_COMPILER_TURBOSHAFT_REDUCER_TRAITS_H_

#include <type_traits>

namespace v8::internal::compiler::turboshaft {

template <class Assembler, bool has_gvn, template <class> class... Reducers>
class ReducerStack;

template <typename Next>
class GenericReducerBase;
template <typename Next>
class EmitProjectionReducer;
template <typename Next>
class TSReducerBase;

// is_same_reducer compares two reducers.
template <template <typename> typename T, template <typename> typename U>
struct is_same_reducer : public std::bool_constant<false> {};

template <template <typename> typename Reducer>
struct is_same_reducer<Reducer, Reducer> : public std::bool_constant<true> {};

template <template <typename> typename...>
struct reducer_list {};
// Converts a ReducerStack {Next} to a reducer_list<>;
template <typename Next>
struct reducer_stack_to_list;
template <typename A, bool has_gvn, template <typename> typename... Reducers>
struct reducer_stack_to_list<ReducerStack<A, has_gvn, Reducers...>> {
  using type = reducer_list<Reducers...>;
};

// Checks if a reducer_list<> {RL} contains reducer {R}.
template <typename RL, template <typename> typename R>
struct reducer_list_contains;
template <template <typename> typename R, template <typename> typename Head,
          template <typename> typename... Tail>
struct reducer_list_contains<reducer_list<Head, Tail...>, R> {
  static constexpr bool value =
      is_same_reducer<Head, R>::value ||
      reducer_list_contains<reducer_list<Tail...>, R>::value;
};
template <template <typename> typename R>
struct reducer_list_contains<reducer_list<>, R>
    : public std::bool_constant<false> {};

// Checks if a reducer_list<> {RL} starts with reducer {R}.
template <typename RL, template <typename> typename R>
struct reducer_list_starts_with;
template <template <typename> typename R, template <typename> typename Head,
          template <typename> typename... Tail>
struct reducer_list_starts_with<reducer_list<Head, Tail...>, R>
    : public std::bool_constant<is_same_reducer<Head, R>::value> {};
template <template <typename> typename R>
struct reducer_list_starts_with<reducer_list<>, R>
    : public std::bool_constant<false> {};

// Check if the {Next} ReducerStack contains {Reducer}.
template <typename Next, template <typename> typename Reducer>
struct next_contains_reducer {
  using list = typename reducer_stack_to_list<Next>::type;
  static constexpr bool value = reducer_list_contains<list, Reducer>::value;
};

// Check if in the {Next} ReducerStack, any of {Reducer} comes next.
template <typename Next, template <typename> typename... Reducer>
struct next_reducer_is {
  using list = typename reducer_stack_to_list<Next>::type;
  static constexpr bool value =
      (reducer_list_starts_with<list, Reducer>::value || ...);
};

// TODO(dmercadier): EmitProjectionReducer is not always the bottom of the stack
// because it could be succeeded by a ValueNumberingReducer. We should take this
// into account in next_is_bottom_of_assembler_stack.
template <typename Next>
struct next_is_bottom_of_assembler_stack
    : public next_reducer_is<Next, GenericReducerBase, EmitProjectionReducer,
                             TSReducerBase> {};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_REDUCER_TRAITS_H_
                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/register-allocation-phase.h                             0000664 0000000 0000000 00000016310 14746647661 0026046 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2024 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_REGISTER_ALLOCATION_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_REGISTER_ALLOCATION_PHASE_H_

#include "src/compiler/backend/frame-elider.h"
#include "src/compiler/backend/jump-threading.h"
#include "src/compiler/backend/move-optimizer.h"
#include "src/compiler/backend/register-allocator.h"
#include "src/compiler/turboshaft/block-instrumentation-reducer.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/value-numbering-reducer.h"

namespace v8::internal::compiler::turboshaft {

struct MeetRegisterConstraintsPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(MeetRegisterConstraints)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    ConstraintBuilder builder(data->register_allocation_data());
    builder.MeetRegisterConstraints();
  }
};

struct ResolvePhisPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(ResolvePhis)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    ConstraintBuilder builder(data->register_allocation_data());
    builder.ResolvePhis();
  }
};

struct BuildLiveRangesPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(BuildLiveRanges)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    LiveRangeBuilder builder(data->register_allocation_data(), temp_zone);
    builder.BuildLiveRanges();
  }
};

struct BuildLiveRangeBundlesPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(BuildLiveRangeBundles)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    BundleBuilder builder(data->register_allocation_data());
    builder.BuildBundles();
  }
};

template <typename RegAllocator>
struct AllocateGeneralRegistersPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(AllocateGeneralRegisters)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    RegAllocator allocator(data->register_allocation_data(),
                           RegisterKind::kGeneral, temp_zone);
    allocator.AllocateRegisters();
  }
};

template <typename RegAllocator>
struct AllocateFPRegistersPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(AllocateFPRegisters)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    RegAllocator allocator(data->register_allocation_data(),
                           RegisterKind::kDouble, temp_zone);
    allocator.AllocateRegisters();
  }
};

template <typename RegAllocator>
struct AllocateSimd128RegistersPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(AllocateSimd128Registers)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    RegAllocator allocator(data->register_allocation_data(),
                           RegisterKind::kSimd128, temp_zone);
    allocator.AllocateRegisters();
  }
};

struct DecideSpillingModePhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(DecideSpillingMode)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    OperandAssigner assigner(data->register_allocation_data());
    assigner.DecideSpillingMode();
  }
};

struct AssignSpillSlotsPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(AssignSpillSlots)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    OperandAssigner assigner(data->register_allocation_data());
    assigner.AssignSpillSlots();
  }
};

struct CommitAssignmentPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(CommitAssignment)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    OperandAssigner assigner(data->register_allocation_data());
    assigner.CommitAssignment();
  }
};

struct PopulateReferenceMapsPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(PopulateReferenceMaps)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    ReferenceMapPopulator populator(data->register_allocation_data());
    populator.PopulateReferenceMaps();
  }
};

struct ConnectRangesPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(ConnectRanges)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    LiveRangeConnector connector(data->register_allocation_data());
    connector.ConnectRanges(temp_zone);
  }
};

struct ResolveControlFlowPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(ResolveControlFlow)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    LiveRangeConnector connector(data->register_allocation_data());
    connector.ResolveControlFlow(temp_zone);
  }
};

struct OptimizeMovesPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(OptimizeMoves)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    MoveOptimizer move_optimizer(temp_zone, data->sequence());
    move_optimizer.Run();
  }
};

struct FrameElisionPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(FrameElision)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
#if V8_ENABLE_WEBASSEMBLY
    const bool is_wasm_to_js =
        data->info()->code_kind() == CodeKind::WASM_TO_JS_FUNCTION ||
        data->info()->builtin() == Builtin::kWasmToJsWrapperCSA;
#else
    const bool is_wasm_to_js = false;
#endif
    FrameElider(data->sequence(), false, is_wasm_to_js).Run();
  }
};

struct JumpThreadingPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(JumpThreading)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone, bool frame_at_start) {
    ZoneVector<RpoNumber> result(temp_zone);
    if (JumpThreading::ComputeForwarding(temp_zone, &result, data->sequence(),
                                         frame_at_start)) {
      JumpThreading::ApplyForwarding(temp_zone, result, data->sequence());
    }
  }
};

struct AssembleCodePhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS_WITH_LEGACY_NAME(AssembleCode)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    CodeGenerator* code_generator = data->code_generator();
    DCHECK_NOT_NULL(code_generator);
    code_generator->AssembleCode();
  }
};

struct FinalizeCodePhase {
  DECL_TURBOSHAFT_MAIN_THREAD_PIPELINE_PHASE_CONSTANTS_WITH_LEGACY_NAME(
      FinalizeCode)
  static constexpr bool kOutputIsTraceableGraph = false;

  void Run(PipelineData* data, Zone* temp_zone) {
    CodeGenerator* code_generator = data->code_generator();
    DCHECK_NOT_NULL(code_generator);
    data->set_code(code_generator->FinalizeCode());
  }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_REGISTER_ALLOCATION_PHASE_H_
                                                                                                                                                                                                                                                                                                                        node-23.7.0/deps/v8/src/compiler/turboshaft/representations.cc                                      0000664 0000000 0000000 00000005612 14746647661 0024367 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/representations.h"

namespace v8::internal::compiler::turboshaft {

std::ostream& operator<<(std::ostream& os, MaybeRegisterRepresentation rep) {
  switch (rep) {
    case MaybeRegisterRepresentation::Word32():
      return os << "Word32";
    case MaybeRegisterRepresentation::Word64():
      return os << "Word64";
    case MaybeRegisterRepresentation::Float32():
      return os << "Float32";
    case MaybeRegisterRepresentation::Float64():
      return os << "Float64";
    case MaybeRegisterRepresentation::Tagged():
      return os << "Tagged";
    case MaybeRegisterRepresentation::Compressed():
      return os << "Compressed";
    case MaybeRegisterRepresentation::Simd128():
      return os << "Simd128";
    case MaybeRegisterRepresentation::Simd256():
      return os << "Simd256";
    case MaybeRegisterRepresentation::None():
      return os << "None";
  }
}

std::ostream& operator<<(std::ostream& os, MemoryRepresentation rep) {
  switch (rep) {
    case MemoryRepresentation::Int8():
      return os << "Int8";
    case MemoryRepresentation::Uint8():
      return os << "Uint8";
    case MemoryRepresentation::Int16():
      return os << "Int16";
    case MemoryRepresentation::Uint16():
      return os << "Uint16";
    case MemoryRepresentation::Int32():
      return os << "Int32";
    case MemoryRepresentation::Uint32():
      return os << "Uint32";
    case MemoryRepresentation::Int64():
      return os << "Int64";
    case MemoryRepresentation::Uint64():
      return os << "Uint64";
    case MemoryRepresentation::Float16():
      return os << "Float16";
    case MemoryRepresentation::Float32():
      return os << "Float32";
    case MemoryRepresentation::Float64():
      return os << "Float64";
    case MemoryRepresentation::AnyTagged():
      return os << "AnyTagged";
    case MemoryRepresentation::TaggedPointer():
      return os << "TaggedPointer";
    case MemoryRepresentation::TaggedSigned():
      return os << "TaggedSigned";
    case MemoryRepresentation::AnyUncompressedTagged():
      return os << "AnyUncompressedTagged";
    case MemoryRepresentation::UncompressedTaggedPointer():
      return os << "UncompressedTaggedPointer";
    case MemoryRepresentation::UncompressedTaggedSigned():
      return os << "UncompressedTaggedSigned";
    case MemoryRepresentation::ProtectedPointer():
      return os << "ProtectedPointer";
    case MemoryRepresentation::IndirectPointer():
      return os << "IndirectPointer";
    case MemoryRepresentation::SandboxedPointer():
      return os << "SandboxedPointer";
    case MemoryRepresentation::Simd128():
      return os << "Simd128";
    case MemoryRepresentation::Simd256():
      return os << "Simd256";
  }
}
}  // namespace v8::internal::compiler::turboshaft
                                                                                                                      node-23.7.0/deps/v8/src/compiler/turboshaft/representations.h                                       0000664 0000000 0000000 00000070715 14746647661 0024237 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_REPRESENTATIONS_H_
#define V8_COMPILER_TURBOSHAFT_REPRESENTATIONS_H_

#include <cstdint>

#include "include/v8-internal.h"
#include "src/base/functional.h"
#include "src/base/logging.h"
#include "src/codegen/machine-type.h"
#include "src/compiler/turboshaft/utils.h"

namespace v8::internal::compiler::turboshaft {

class WordRepresentation;
class FloatRepresentation;

// Optional register representation.
class MaybeRegisterRepresentation {
 public:
  enum class Enum : uint8_t {
    kWord32,
    kWord64,
    kFloat32,
    kFloat64,
    kTagged,
    kCompressed,
    kSimd128,
    kSimd256,
    kNone,  // No register representation.
  };

  explicit constexpr MaybeRegisterRepresentation(Enum value) : value_(value) {}
  constexpr MaybeRegisterRepresentation() : value_(kInvalid) {}

  constexpr bool is_valid() const { return value_ != kInvalid; }

  constexpr Enum value() const {
    DCHECK(is_valid());
    return value_;
  }

  constexpr operator Enum() const { return value(); }

  static constexpr MaybeRegisterRepresentation Word32() {
    return MaybeRegisterRepresentation(Enum::kWord32);
  }

  static constexpr MaybeRegisterRepresentation Word64() {
    return MaybeRegisterRepresentation(Enum::kWord64);
  }

  static constexpr MaybeRegisterRepresentation WordPtr() {
    if constexpr (kSystemPointerSize == 4) {
      return Word32();
    } else {
      DCHECK_EQ(kSystemPointerSize, 8);
      return Word64();
    }
  }

  static constexpr MaybeRegisterRepresentation Float32() {
    return MaybeRegisterRepresentation(Enum::kFloat32);
  }

  static constexpr MaybeRegisterRepresentation Float64() {
    return MaybeRegisterRepresentation(Enum::kFloat64);
  }

  static constexpr MaybeRegisterRepresentation Tagged() {
    return MaybeRegisterRepresentation(Enum::kTagged);
  }

  static constexpr MaybeRegisterRepresentation Compressed() {
    return MaybeRegisterRepresentation(Enum::kCompressed);
  }

  static constexpr MaybeRegisterRepresentation Simd128() {
    return MaybeRegisterRepresentation(Enum::kSimd128);
  }

  static constexpr MaybeRegisterRepresentation Simd256() {
    return MaybeRegisterRepresentation(Enum::kSimd256);
  }

  static constexpr MaybeRegisterRepresentation None() {
    return MaybeRegisterRepresentation(Enum::kNone);
  }

  constexpr bool IsWord() const {
    switch (*this) {
      case Enum::kWord32:
      case Enum::kWord64:
        return true;
      case Enum::kFloat32:
      case Enum::kFloat64:
      case Enum::kTagged:
      case Enum::kCompressed:
      case Enum::kSimd128:
      case Enum::kSimd256:
      case Enum::kNone:
        return false;
    }
  }

  constexpr bool IsFloat() const {
    switch (*this) {
      case Enum::kFloat32:
      case Enum::kFloat64:
        return true;
      case Enum::kWord32:
      case Enum::kWord64:
      case Enum::kTagged:
      case Enum::kCompressed:
      case Enum::kSimd128:
      case Enum::kSimd256:
      case Enum::kNone:
        return false;
    }
  }

  constexpr bool IsTaggedOrCompressed() const {
    switch (*this) {
      case Enum::kTagged:
      case Enum::kCompressed:
        return true;
      case Enum::kWord32:
      case Enum::kWord64:
      case Enum::kFloat32:
      case Enum::kFloat64:
      case Enum::kSimd128:
      case Enum::kSimd256:
      case Enum::kNone:
        return false;
    }
  }

  uint64_t MaxUnsignedValue() const {
    switch (this->value()) {
      case Word32():
        return std::numeric_limits<uint32_t>::max();
      case Word64():
        return std::numeric_limits<uint64_t>::max();
      case Enum::kFloat32:
      case Enum::kFloat64:
      case Enum::kTagged:
      case Enum::kCompressed:
      case Enum::kSimd128:
      case Enum::kSimd256:
      case Enum::kNone:
        UNREACHABLE();
    }
  }

  MachineRepresentation machine_representation() const {
    switch (this->value()) {
      case Word32():
        return MachineRepresentation::kWord32;
      case Word64():
        return MachineRepresentation::kWord64;
      case Float32():
        return MachineRepresentation::kFloat32;
      case Float64():
        return MachineRepresentation::kFloat64;
      case Tagged():
        return MachineRepresentation::kTagged;
      case Compressed():
        return MachineRepresentation::kCompressed;
      case Simd128():
        return MachineRepresentation::kSimd128;
      case Simd256():
        return MachineRepresentation::kSimd256;
      case None():
        UNREACHABLE();
    }
  }

  constexpr uint16_t bit_width() const {
    switch (this->value()) {
      case Word32():
        return 32;
      case Word64():
        return 64;
      case Float32():
        return 32;
      case Float64():
        return 64;
      case Tagged():
        return kSystemPointerSize;
      case Compressed():
        return kSystemPointerSize;
      case Simd128():
        return 128;
      case Simd256():
        return 256;
      case None():
        UNREACHABLE();
    }
  }

 private:
  Enum value_;

  static constexpr Enum kInvalid = static_cast<Enum>(-1);
};

class RegisterRepresentation : public MaybeRegisterRepresentation {
 public:
  enum class Enum : uint8_t {
    kWord32 = static_cast<int>(MaybeRegisterRepresentation::Enum::kWord32),
    kWord64 = static_cast<int>(MaybeRegisterRepresentation::Enum::kWord64),
    kFloat32 = static_cast<int>(MaybeRegisterRepresentation::Enum::kFloat32),
    kFloat64 = static_cast<int>(MaybeRegisterRepresentation::Enum::kFloat64),
    kTagged = static_cast<int>(MaybeRegisterRepresentation::Enum::kTagged),
    kCompressed =
        static_cast<int>(MaybeRegisterRepresentation::Enum::kCompressed),
    kSimd128 = static_cast<int>(MaybeRegisterRepresentation::Enum::kSimd128),
    kSimd256 = static_cast<int>(MaybeRegisterRepresentation::Enum::kSimd256),
  };

  explicit constexpr RegisterRepresentation(Enum value)
      : MaybeRegisterRepresentation(
            static_cast<MaybeRegisterRepresentation::Enum>(value)) {}
  RegisterRepresentation() = default;

  explicit constexpr RegisterRepresentation(MaybeRegisterRepresentation rep)
      : RegisterRepresentation(static_cast<Enum>(rep.value())) {}

  constexpr operator Enum() const { return value(); }

  constexpr Enum value() const {
    return static_cast<Enum>(MaybeRegisterRepresentation::value());
  }

  static constexpr RegisterRepresentation Word32() {
    return RegisterRepresentation(Enum::kWord32);
  }
  static constexpr RegisterRepresentation Word64() {
    return RegisterRepresentation(Enum::kWord64);
  }
  // The equivalent of intptr_t/uintptr_t: An integral type with the same size
  // as machine pointers.
  static constexpr RegisterRepresentation WordPtr() {
    return RegisterRepresentation(MaybeRegisterRepresentation::WordPtr());
  }
  static constexpr RegisterRepresentation Float32() {
    return RegisterRepresentation(Enum::kFloat32);
  }
  static constexpr RegisterRepresentation Float64() {
    return RegisterRepresentation(Enum::kFloat64);
  }
  // A tagged pointer stored in a register, in the case of pointer compression
  // it is an uncompressed pointer or a Smi.
  static constexpr RegisterRepresentation Tagged() {
    return RegisterRepresentation(Enum::kTagged);
  }
  // A compressed tagged pointer stored in a register, the upper 32bit are
  // unspecified.
  static constexpr RegisterRepresentation Compressed() {
    return RegisterRepresentation(Enum::kCompressed);
  }
  static constexpr RegisterRepresentation Simd128() {
    return RegisterRepresentation(Enum::kSimd128);
  }
  static constexpr RegisterRepresentation Simd256() {
    return RegisterRepresentation(Enum::kSimd256);
  }

  static constexpr RegisterRepresentation FromMachineRepresentation(
      MachineRepresentation rep) {
    switch (rep) {
      case MachineRepresentation::kBit:
      case MachineRepresentation::kWord8:
      case MachineRepresentation::kWord16:
      case MachineRepresentation::kWord32:
        return Word32();
      case MachineRepresentation::kWord64:
        return Word64();
      case MachineRepresentation::kTaggedSigned:
      case MachineRepresentation::kTaggedPointer:
      case MachineRepresentation::kTagged:
      case MachineRepresentation::kProtectedPointer:
        return Tagged();
      case MachineRepresentation::kCompressedPointer:
      case MachineRepresentation::kCompressed:
        return Compressed();
      case MachineRepresentation::kFloat16:
      case MachineRepresentation::kFloat32:
        return Float32();
      case MachineRepresentation::kFloat64:
        return Float64();
      case MachineRepresentation::kSimd128:
        return Simd128();
      case MachineRepresentation::kSimd256:
        return Simd256();
      case MachineRepresentation::kMapWord:
        // Turboshaft does not support map packing.
        DCHECK(!V8_MAP_PACKING_BOOL);
        return RegisterRepresentation::Tagged();
      case MachineRepresentation::kIndirectPointer:
      case MachineRepresentation::kSandboxedPointer:
        // TODO(saelo/jkummerow): This is suspicious: after resolving the
        // indirection, we have a Tagged pointer.
        return WordPtr();
      case MachineRepresentation::kNone:
        UNREACHABLE();
    }
  }

  static constexpr RegisterRepresentation FromMachineType(MachineType type) {
    return FromMachineRepresentation(type.representation());
  }

  constexpr bool AllowImplicitRepresentationChangeTo(
      RegisterRepresentation dst_rep, bool graph_created_from_turbofan) const;

  constexpr RegisterRepresentation MapTaggedToWord() const {
    if (this->value() == RegisterRepresentation::Tagged()) {
      return COMPRESS_POINTERS_BOOL ? RegisterRepresentation::Word32()
                                    : RegisterRepresentation::WordPtr();
    }
    return *this;
  }
};

V8_INLINE constexpr bool operator==(MaybeRegisterRepresentation a,
                                    MaybeRegisterRepresentation b) {
  return a.value() == b.value();
}
V8_INLINE constexpr bool operator!=(MaybeRegisterRepresentation a,
                                    MaybeRegisterRepresentation b) {
  return a.value() != b.value();
}

V8_INLINE size_t hash_value(MaybeRegisterRepresentation rep) {
  return static_cast<size_t>(rep.value());
}

constexpr bool RegisterRepresentation::AllowImplicitRepresentationChangeTo(
    RegisterRepresentation dst_rep, bool graph_created_from_turbofan) const {
  if (*this == dst_rep) {
    return true;
  }
  switch (dst_rep.value()) {
    case RegisterRepresentation::Word32():
      // We allow implicit tagged -> untagged conversions.
      // Even without pointer compression, we use `Word32And` for Smi-checks on
      // tagged values.
      if (*this == any_of(RegisterRepresentation::Tagged(),
                          RegisterRepresentation::Compressed())) {
        return true;
      }
      if (graph_created_from_turbofan &&
          *this == RegisterRepresentation::Word64()) {
        // TODO(12783): Remove this once Turboshaft graphs are not constructed
        // via Turbofan any more. Unfortunately Turbofan has many implicit
        // truncations which are hard to fix. Still, for wasm it is required
        // that truncations in Turboshaft are explicit.
        return true;
      }
      break;
    case RegisterRepresentation::Word64():
      // We allow implicit tagged -> untagged conversions.
      if (kTaggedSize == kInt64Size &&
          *this == RegisterRepresentation::Tagged()) {
        return true;
      }
      break;
    case RegisterRepresentation::Tagged():
      // We allow implicit untagged -> tagged conversions. This is only safe for
      // Smi values.
      if (*this == RegisterRepresentation::WordPtr()) {
        return true;
      }
      break;
    case RegisterRepresentation::Compressed():
      // Compression is a no-op.
      if (*this == any_of(RegisterRepresentation::Tagged(),
                          RegisterRepresentation::WordPtr(),
                          RegisterRepresentation::Word32())) {
        return true;
      }
      break;
    default:
      break;
  }
  return false;
}

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           MaybeRegisterRepresentation rep);

template <typename T>
struct MultiSwitch<
    T, std::enable_if_t<std::is_base_of_v<MaybeRegisterRepresentation, T>>> {
  static constexpr uint64_t max_value = 8;
  static constexpr uint64_t encode(T rep) {
    const uint64_t value = static_cast<uint64_t>(rep.value());
    DCHECK_LT(value, max_value);
    return value;
  }
};

class WordRepresentation : public RegisterRepresentation {
 public:
  enum class Enum : uint8_t {
    kWord32 = static_cast<int>(RegisterRepresentation::Enum::kWord32),
    kWord64 = static_cast<int>(RegisterRepresentation::Enum::kWord64)
  };
  explicit constexpr WordRepresentation(Enum value)
      : RegisterRepresentation(
            static_cast<RegisterRepresentation::Enum>(value)) {}
  WordRepresentation() = default;
  explicit constexpr WordRepresentation(RegisterRepresentation rep)
      : WordRepresentation(static_cast<Enum>(rep.value())) {
    DCHECK(rep.IsWord());
  }

  static constexpr WordRepresentation Word32() {
    return WordRepresentation(Enum::kWord32);
  }
  static constexpr WordRepresentation Word64() {
    return WordRepresentation(Enum::kWord64);
  }

  static constexpr WordRepresentation WordPtr() {
    return WordRepresentation(RegisterRepresentation::WordPtr());
  }

  constexpr Enum value() const {
    return static_cast<Enum>(RegisterRepresentation::value());
  }
  constexpr operator Enum() const { return value(); }

  constexpr uint64_t MaxUnsignedValue() const {
    switch (this->value()) {
      case Word32():
        return std::numeric_limits<uint32_t>::max();
      case Word64():
        return std::numeric_limits<uint64_t>::max();
    }
  }
  constexpr int64_t MinSignedValue() const {
    switch (this->value()) {
      case Word32():
        return std::numeric_limits<int32_t>::min();
      case Word64():
        return std::numeric_limits<int64_t>::min();
    }
  }
  constexpr int64_t MaxSignedValue() const {
    switch (this->value()) {
      case Word32():
        return std::numeric_limits<int32_t>::max();
      case Word64():
        return std::numeric_limits<int64_t>::max();
    }
  }
};

class FloatRepresentation : public RegisterRepresentation {
 public:
  enum class Enum : uint8_t {
    kFloat32 = static_cast<int>(RegisterRepresentation::Enum::kFloat32),
    kFloat64 = static_cast<int>(RegisterRepresentation::Enum::kFloat64)
  };

  static constexpr FloatRepresentation Float32() {
    return FloatRepresentation(Enum::kFloat32);
  }
  static constexpr FloatRepresentation Float64() {
    return FloatRepresentation(Enum::kFloat64);
  }

  explicit constexpr FloatRepresentation(Enum value)
      : RegisterRepresentation(
            static_cast<RegisterRepresentation::Enum>(value)) {}
  explicit constexpr FloatRepresentation(RegisterRepresentation rep)
      : FloatRepresentation(static_cast<Enum>(rep.value())) {
    DCHECK(rep.IsFloat());
  }
  FloatRepresentation() = default;

  constexpr Enum value() const {
    return static_cast<Enum>(RegisterRepresentation::value());
  }
  constexpr operator Enum() const { return value(); }
};

class MemoryRepresentation {
 public:
  enum class Enum : uint8_t {
    kInt8,
    kUint8,
    kInt16,
    kUint16,
    kInt32,
    kUint32,
    kInt64,
    kUint64,
    kFloat16,
    kFloat32,
    kFloat64,
    kAnyTagged,
    kTaggedPointer,
    kTaggedSigned,
    kAnyUncompressedTagged,
    kUncompressedTaggedPointer,
    kUncompressedTaggedSigned,
    kProtectedPointer,
    kIndirectPointer,
    kSandboxedPointer,
    kSimd128,
    kSimd256
  };

  explicit constexpr MemoryRepresentation(Enum value) : value_(value) {}
  constexpr MemoryRepresentation() : value_(kInvalid) {}

  constexpr bool is_valid() const { return value_ != kInvalid; }

  constexpr Enum value() const {
    DCHECK(is_valid());
    return value_;
  }
  constexpr operator Enum() const { return value(); }

  static constexpr MemoryRepresentation Int8() {
    return MemoryRepresentation(Enum::kInt8);
  }
  static constexpr MemoryRepresentation Uint8() {
    return MemoryRepresentation(Enum::kUint8);
  }
  static constexpr MemoryRepresentation Int16() {
    return MemoryRepresentation(Enum::kInt16);
  }
  static constexpr MemoryRepresentation Uint16() {
    return MemoryRepresentation(Enum::kUint16);
  }
  static constexpr MemoryRepresentation Int32() {
    return MemoryRepresentation(Enum::kInt32);
  }
  static constexpr MemoryRepresentation Uint32() {
    return MemoryRepresentation(Enum::kUint32);
  }
  static constexpr MemoryRepresentation Int64() {
    return MemoryRepresentation(Enum::kInt64);
  }
  static constexpr MemoryRepresentation Uint64() {
    return MemoryRepresentation(Enum::kUint64);
  }
  static constexpr MemoryRepresentation UintPtr() {
    if constexpr (Is64()) {
      return Uint64();
    } else {
      return Uint32();
    }
  }
  static constexpr MemoryRepresentation Float16() {
    return MemoryRepresentation(Enum::kFloat16);
  }
  static constexpr MemoryRepresentation Float32() {
    return MemoryRepresentation(Enum::kFloat32);
  }
  static constexpr MemoryRepresentation Float64() {
    return MemoryRepresentation(Enum::kFloat64);
  }
  static constexpr MemoryRepresentation AnyTagged() {
    return MemoryRepresentation(Enum::kAnyTagged);
  }
  static constexpr MemoryRepresentation TaggedPointer() {
    return MemoryRepresentation(Enum::kTaggedPointer);
  }
  static constexpr MemoryRepresentation TaggedSigned() {
    return MemoryRepresentation(Enum::kTaggedSigned);
  }
  static constexpr MemoryRepresentation AnyUncompressedTagged() {
    return MemoryRepresentation(Enum::kAnyUncompressedTagged);
  }
  static constexpr MemoryRepresentation UncompressedTaggedPointer() {
    return MemoryRepresentation(Enum::kUncompressedTaggedPointer);
  }
  static constexpr MemoryRepresentation UncompressedTaggedSigned() {
    return MemoryRepresentation(Enum::kUncompressedTaggedSigned);
  }
  static constexpr MemoryRepresentation ProtectedPointer() {
    return MemoryRepresentation(Enum::kProtectedPointer);
  }
  static constexpr MemoryRepresentation IndirectPointer() {
    return MemoryRepresentation(Enum::kIndirectPointer);
  }
  static constexpr MemoryRepresentation SandboxedPointer() {
    return MemoryRepresentation(Enum::kSandboxedPointer);
  }
  static constexpr MemoryRepresentation Simd128() {
    return MemoryRepresentation(Enum::kSimd128);
  }
  static constexpr MemoryRepresentation Simd256() {
    return MemoryRepresentation(Enum::kSimd256);
  }

  bool IsSigned() const {
    switch (*this) {
      case Int8():
      case Int16():
      case Int32():
      case Int64():
        return true;
      case Uint8():
      case Uint16():
      case Uint32():
      case Uint64():
        return false;
      case Float16():
      case Float32():
      case Float64():
      case AnyTagged():
      case TaggedPointer():
      case TaggedSigned():
      case AnyUncompressedTagged():
      case UncompressedTaggedPointer():
      case UncompressedTaggedSigned():
      case ProtectedPointer():
      case IndirectPointer():
      case SandboxedPointer():
      case Simd128():
      case Simd256():
        UNREACHABLE();
    }
  }

  // This predicate is used in particular to decide which load/store ops
  // have to deal with pointer compression. Indirect/sandboxed pointers,
  // while they resolve to tagged pointers, return {false} because they
  // use incompatible compression schemes.
  bool IsCompressibleTagged() const {
    switch (*this) {
      case AnyTagged():
      case TaggedPointer():
      case TaggedSigned():
        return true;
      case Int8():
      case Int16():
      case Int32():
      case Int64():
      case Uint8():
      case Uint16():
      case Uint32():
      case Uint64():
      case Float16():
      case Float32():
      case Float64():
      case AnyUncompressedTagged():
      case UncompressedTaggedPointer():
      case UncompressedTaggedSigned():
      case IndirectPointer():
      case ProtectedPointer():
      case SandboxedPointer():
      case Simd128():
      case Simd256():
        return false;
    }
  }

  RegisterRepresentation ToRegisterRepresentation() const {
    switch (*this) {
      case Int8():
      case Uint8():
      case Int16():
      case Uint16():
      case Int32():
      case Uint32():
        return RegisterRepresentation::Word32();
      case Int64():
      case Uint64():
        return RegisterRepresentation::Word64();
      case Float16():
      case Float32():
        return RegisterRepresentation::Float32();
      case Float64():
        return RegisterRepresentation::Float64();
      case AnyTagged():
      case TaggedPointer():
      case TaggedSigned():
      case AnyUncompressedTagged():
      case UncompressedTaggedPointer():
      case UncompressedTaggedSigned():
      case IndirectPointer():
      case ProtectedPointer():
        return RegisterRepresentation::Tagged();
      case SandboxedPointer():
        return RegisterRepresentation::Word64();
      case Simd128():
        return RegisterRepresentation::Simd128();
      case Simd256():
        return RegisterRepresentation::Simd256();
    }
  }

  static MemoryRepresentation FromRegisterRepresentation(
      RegisterRepresentation repr, bool is_signed) {
    switch (repr.value()) {
      case RegisterRepresentation::Word32():
        return is_signed ? Int32() : Uint32();
      case RegisterRepresentation::Word64():
        return is_signed ? Int64() : Uint64();
      case RegisterRepresentation::Float32():
        return Float32();
      case RegisterRepresentation::Float64():
        return Float64();
      case RegisterRepresentation::Tagged():
        return AnyTagged();
      case RegisterRepresentation::Simd128():
        return Simd128();
      case RegisterRepresentation::Simd256():
        return Simd256();
      case RegisterRepresentation::Compressed():
        UNREACHABLE();
    }
  }

  // The required register representation for storing a value. When pointer
  // compression is enabled, we only store the lower 32bit of a tagged value,
  // which we indicate as `RegisterRepresentation::Compressed()` here.
  RegisterRepresentation ToRegisterRepresentationForStore() const {
    RegisterRepresentation result = ToRegisterRepresentation();
#ifdef V8_COMPRESS_POINTERS
    if (result == RegisterRepresentation::Tagged()) {
      result = RegisterRepresentation::Compressed();
    }
#endif
    return result;
  }

  MachineType ToMachineType() const {
    switch (*this) {
      case Int8():
        return MachineType::Int8();
      case Uint8():
        return MachineType::Uint8();
      case Int16():
        return MachineType::Int16();
      case Uint16():
        return MachineType::Uint16();
      case Int32():
        return MachineType::Int32();
      case Uint32():
        return MachineType::Uint32();
      case Int64():
        return MachineType::Int64();
      case Uint64():
        return MachineType::Uint64();
      case Float16():
        return MachineType::Float16();
      case Float32():
        return MachineType::Float32();
      case Float64():
        return MachineType::Float64();
      case AnyTagged():
        return MachineType::AnyTagged();
      case TaggedPointer():
        return MachineType::TaggedPointer();
      case TaggedSigned():
        return MachineType::TaggedSigned();
      case AnyUncompressedTagged():
        return MachineType::AnyTagged();
      case UncompressedTaggedPointer():
        return MachineType::TaggedPointer();
      case UncompressedTaggedSigned():
        return MachineType::TaggedSigned();
      case ProtectedPointer():
        return MachineType::ProtectedPointer();
      case IndirectPointer():
        return MachineType::IndirectPointer();
      case SandboxedPointer():
        return MachineType::SandboxedPointer();
      case Simd128():
        return MachineType::Simd128();
      case Simd256():
        return MachineType::Simd256();
    }
  }

  static MemoryRepresentation FromMachineType(MachineType type) {
    switch (type.representation()) {
      case MachineRepresentation::kWord8:
        return type.IsSigned() ? Int8() : Uint8();
      case MachineRepresentation::kWord16:
        return type.IsSigned() ? Int16() : Uint16();
      case MachineRepresentation::kWord32:
        return type.IsSigned() ? Int32() : Uint32();
      case MachineRepresentation::kWord64:
        return type.IsSigned() ? Int64() : Uint64();
      case MachineRepresentation::kTaggedSigned:
        return TaggedSigned();
      case MachineRepresentation::kTaggedPointer:
        return TaggedPointer();
      case MachineRepresentation::kMapWord:
        // Turboshaft does not support map packing.
        DCHECK(!V8_MAP_PACKING_BOOL);
        return TaggedPointer();
      case MachineRepresentation::kProtectedPointer:
        return ProtectedPointer();
      case MachineRepresentation::kIndirectPointer:
        return IndirectPointer();
      case MachineRepresentation::kTagged:
        return AnyTagged();
      case MachineRepresentation::kFloat16:
        return Float16();
      case MachineRepresentation::kFloat32:
        return Float32();
      case MachineRepresentation::kFloat64:
        return Float64();
      case MachineRepresentation::kSandboxedPointer:
        return SandboxedPointer();
      case MachineRepresentation::kSimd128:
        return Simd128();
      case MachineRepresentation::kSimd256:
        return Simd256();
      case MachineRepresentation::kNone:
      case MachineRepresentation::kBit:
      case MachineRepresentation::kCompressedPointer:
      case MachineRepresentation::kCompressed:
        UNREACHABLE();
    }
  }

  static constexpr MemoryRepresentation FromMachineRepresentation(
      MachineRepresentation rep) {
    switch (rep) {
      case MachineRepresentation::kWord8:
        return Uint8();
      case MachineRepresentation::kWord16:
        return Uint16();
      case MachineRepresentation::kWord32:
        return Uint32();
      case MachineRepresentation::kWord64:
        return Uint64();
      case MachineRepresentation::kTaggedSigned:
        return TaggedSigned();
      case MachineRepresentation::kTaggedPointer:
        return TaggedPointer();
      case MachineRepresentation::kTagged:
        return AnyTagged();
      case MachineRepresentation::kFloat16:
        return Float16();
      case MachineRepresentation::kFloat32:
        return Float32();
      case MachineRepresentation::kFloat64:
        return Float64();
      case MachineRepresentation::kSandboxedPointer:
        return SandboxedPointer();
      case MachineRepresentation::kSimd128:
        return Simd128();
      case MachineRepresentation::kSimd256:
        return Simd256();
      case MachineRepresentation::kNone:
      case MachineRepresentation::kMapWord:
      case MachineRepresentation::kBit:
      case MachineRepresentation::kCompressedPointer:
      case MachineRepresentation::kCompressed:
      case MachineRepresentation::kProtectedPointer:
      case MachineRepresentation::kIndirectPointer:
        UNREACHABLE();
    }
  }

  constexpr uint8_t SizeInBytes() const {
    return uint8_t{1} << SizeInBytesLog2();
  }

  constexpr uint8_t SizeInBytesLog2() const {
    switch (*this) {
      case Int8():
      case Uint8():
        return 0;
      case Int16():
      case Uint16():
      case Float16():
        return 1;
      case Int32():
      case Uint32():
      case Float32():
      case IndirectPointer():
        return 2;
      case Int64():
      case Uint64():
      case Float64():
      case SandboxedPointer():
        return 3;
      case AnyTagged():
      case TaggedPointer():
      case TaggedSigned():
      case ProtectedPointer():
        return kTaggedSizeLog2;
      case AnyUncompressedTagged():
      case UncompressedTaggedPointer():
      case UncompressedTaggedSigned():
        return kSystemPointerSizeLog2;
      case Simd128():
        return 4;
      case Simd256():
        return 5;
    }
  }

 private:
  Enum value_;

  static constexpr Enum kInvalid = static_cast<Enum>(-1);
};

V8_INLINE constexpr bool operator==(MemoryRepresentation a,
                                    MemoryRepresentation b) {
  return a.value() == b.value();
}
V8_INLINE constexpr bool operator!=(MemoryRepresentation a,
                                    MemoryRepresentation b) {
  return a.value() != b.value();
}

V8_INLINE size_t hash_value(MemoryRepresentation rep) {
  return static_cast<size_t>(rep.value());
}

V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                           MemoryRepresentation rep);

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_REPRESENTATIONS_H_
                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/required-optimization-reducer.h                         0000664 0000000 0000000 00000007740 14746647661 0027003 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_REQUIRED_OPTIMIZATION_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_REQUIRED_OPTIMIZATION_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/operations.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

// The RequiredOptimizationReducer performs reductions that might be needed for
// correctness, because instruction selection or other reducers rely on it. In
// particular, we have the following dependencies:
//   - VariableReducer can introduce phi nodes for call target constants, which
//     have to be reduced in order for instruction selection to detect the call
//     target. So we have to run RequiredOptimizationReducer at least once after
//     every occurence of VariableReducer.
//   - Loop peeling/unrolling can introduce phi nodes for RttCanons, which have
//     to be reduced to aid `WasmGCTypedOptimizationReducer` resolve type
//     indices corresponding to RttCanons.
template <class Next>
class RequiredOptimizationReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(RequiredOptimization)

  OpIndex REDUCE(Phi)(base::Vector<const OpIndex> inputs,
                      RegisterRepresentation rep) {
    LABEL_BLOCK(no_change) { return Next::ReducePhi(inputs, rep); }
    if (inputs.size() == 0) goto no_change;
    OpIndex first = inputs.first();
    bool same_inputs = true;
    for (const OpIndex& input : inputs.SubVectorFrom(1)) {
      if (input != first) {
        same_inputs = false;
        break;
      }
    }
    if (same_inputs) {
      return first;
    }
    if (const ConstantOp* first_constant =
            __ Get(first).template TryCast<ConstantOp>()) {
      for (const OpIndex& input : inputs.SubVectorFrom(1)) {
        const ConstantOp* maybe_constant =
            __ Get(input).template TryCast<ConstantOp>();
        if (!(maybe_constant && *maybe_constant == *first_constant)) {
          goto no_change;
        }
      }
      // If all of the predecessors are the same Constant, then we re-emit
      // this Constant rather than emitting a Phi. This is a good idea in
      // general, but is in particular needed for Constant that are used as
      // call target: if they were merged into a Phi, this would result in an
      // indirect call rather than a direct one, which:
      //   - is probably slower than a direct call in general
      //   - is probably not supported for builtins on 32-bit architectures.
      return __ ReduceConstant(first_constant->kind, first_constant->storage);
    }
#if V8_ENABLE_WEBASSEMBLY
    if (const RttCanonOp* first_rtt =
            __ Get(first).template TryCast<RttCanonOp>()) {
      for (const OpIndex& input : inputs.SubVectorFrom(1)) {
        const RttCanonOp* maybe_rtt =
            __ Get(input).template TryCast<RttCanonOp>();
        if (!(maybe_rtt && maybe_rtt->rtts() == first_rtt->rtts() &&
              maybe_rtt->type_index == first_rtt->type_index)) {
          goto no_change;
        }
      }
      // If all of the predecessors are the same RttCanon, then we re-emit this
      // RttCanon rather than emitting a Phi. This helps the subsequent
      // phases (in particular, `WasmGCTypedOptimizationReducer`) to resolve the
      // type index corresponding to an RttCanon.
      // Note: this relies on all RttCanons having the same `rtts()` input,
      // which is the case due to instance field caching during graph
      // generation.
      // TODO(manoskouk): Can we generalize these two (and possibly more) cases?
      return __ ReduceRttCanon(first_rtt->rtts(), first_rtt->type_index);
    }
#endif
    goto no_change;
  }
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_REQUIRED_OPTIMIZATION_REDUCER_H_
                                node-23.7.0/deps/v8/src/compiler/turboshaft/runtime-call-descriptors.h                              0000664 0000000 0000000 00000032074 14746647661 0025741 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_RUNTIME_CALL_DESCRIPTORS_H_
#define V8_COMPILER_TURBOSHAFT_RUNTIME_CALL_DESCRIPTORS_H_

#include "src/compiler/globals.h"
#include "src/compiler/operator.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/runtime/runtime.h"

namespace v8::internal::compiler::turboshaft {

struct RuntimeCallDescriptor {
 private:
  template <typename Derived>
  struct Descriptor {
    static const TSCallDescriptor* Create(
        Zone* zone, LazyDeoptOnThrow lazy_deopt_on_throw) {
      DCHECK_IMPLIES(lazy_deopt_on_throw == LazyDeoptOnThrow::kYes,
                     Derived::kNeedsFrameState);
      auto descriptor = Linkage::GetRuntimeCallDescriptor(
          zone, Derived::kFunction,
          std::tuple_size_v<typename Derived::arguments_t>,
          Derived::kProperties,
          Derived::kNeedsFrameState ? CallDescriptor::kNeedsFrameState
                                    : CallDescriptor::kNoFlags);
#ifdef DEBUG
      Derived::Verify(descriptor);
#endif  // DEBUG
      CanThrow can_throw = (Derived::kProperties & Operator::kNoThrow)
                               ? CanThrow::kNo
                               : CanThrow::kYes;
      return TSCallDescriptor::Create(descriptor, can_throw,
                                      lazy_deopt_on_throw, zone);
    }

#ifdef DEBUG
    static void Verify(const CallDescriptor* desc) {
      using result_t = typename Derived::result_t;
      using arguments_t = typename Derived::arguments_t;
      if constexpr (std::is_same_v<result_t, void>) {
        DCHECK_EQ(desc->ReturnCount(), 0);
      } else {
        DCHECK_EQ(desc->ReturnCount(), 1);
        DCHECK(result_t::allows_representation(
            RegisterRepresentation::FromMachineRepresentation(
                desc->GetReturnType(0).representation())));
      }
      DCHECK_EQ(desc->NeedsFrameState(), Derived::kNeedsFrameState);
      DCHECK_EQ(desc->properties(), Derived::kProperties);
      constexpr int additional_stub_arguments =
          3;  // function id, argument count, context (or NoContextConstant)
      DCHECK_EQ(desc->ParameterCount(),
                std::tuple_size_v<arguments_t> + additional_stub_arguments);
      DCHECK(VerifyArguments<arguments_t>(desc));
    }

    template <typename Arguments>
    static bool VerifyArguments(const CallDescriptor* desc) {
      return VerifyArgumentsImpl<Arguments>(
          desc, std::make_index_sequence<std::tuple_size_v<Arguments>>());
    }

   private:
    template <typename Arguments, size_t... Indices>
    static bool VerifyArgumentsImpl(const CallDescriptor* desc,
                                    std::index_sequence<Indices...>) {
      return (std::tuple_element_t<Indices, Arguments>::allows_representation(
                  RegisterRepresentation::FromMachineRepresentation(
                      desc->GetParameterType(Indices).representation())) &&
              ...);
    }
#endif  // DEBUG
  };

  // TODO(nicohartmann@): Unfortunately, we cannot define builtins with
  // void/never return types properly (e.g. in Torque), but they typically have
  // a JSAny dummy return type. Use Void/Never sentinels to express that in
  // Turboshaft's descriptors. We should find a better way to model this.
  using Void = V<Any>;
  using Never = V<Any>;

 public:
  struct Abort : public Descriptor<Abort> {
    static constexpr auto kFunction = Runtime::kAbort;
    using arguments_t = std::tuple<V<Smi>>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

  struct DateCurrentTime : public Descriptor<DateCurrentTime> {
    static constexpr auto kFunction = Runtime::kDateCurrentTime;
    using arguments_t = std::tuple<>;
    using result_t = V<Number>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

  struct DebugPrint : public Descriptor<DebugPrint> {
    static constexpr auto kFunction = Runtime::kDebugPrint;
    using arguments_t = std::tuple<V<Object>>;
    using result_t = Void;  // No actual result

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

  struct StackGuard : public Descriptor<StackGuard> {
    static constexpr auto kFunction = Runtime::kStackGuard;
    using arguments_t = std::tuple<>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = false;
    // TODO(nicohartmann@): Verify this.
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct StackGuardWithGap : public Descriptor<StackGuardWithGap> {
    static constexpr auto kFunction = Runtime::kStackGuardWithGap;
    using arguments_t = std::tuple<V<Smi>>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = true;
    // TODO(nicohartmann@): Verify this.
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct HandleNoHeapWritesInterrupts
      : public Descriptor<HandleNoHeapWritesInterrupts> {
    static constexpr auto kFunction = Runtime::kHandleNoHeapWritesInterrupts;
    using arguments_t = std::tuple<>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoWrite;
  };

  struct PropagateException : public Descriptor<PropagateException> {
    static constexpr auto kFunction = Runtime::kPropagateException;
    using arguments_t = std::tuple<>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct StringCharCodeAt : public Descriptor<StringCharCodeAt> {
    static constexpr auto kFunction = Runtime::kStringCharCodeAt;
    using arguments_t = std::tuple<V<String>, V<Number>>;
    using result_t = V<Smi>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

#ifdef V8_INTL_SUPPORT
  struct StringToUpperCaseIntl : public Descriptor<StringToUpperCaseIntl> {
    static constexpr auto kFunction = Runtime::kStringToUpperCaseIntl;
    using arguments_t = std::tuple<V<String>>;
    using result_t = V<String>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };
#endif  // V8_INTL_SUPPORT

  struct SymbolDescriptiveString : public Descriptor<SymbolDescriptiveString> {
    static constexpr auto kFunction = Runtime::kSymbolDescriptiveString;
    using arguments_t = std::tuple<V<Symbol>>;
    using result_t = V<String>;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoDeopt;
  };

  struct TerminateExecution : public Descriptor<TerminateExecution> {
    static constexpr auto kFunction = Runtime::kTerminateExecution;
    using arguments_t = std::tuple<>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoDeopt;
  };

  struct TransitionElementsKind : public Descriptor<TransitionElementsKind> {
    static constexpr auto kFunction = Runtime::kTransitionElementsKind;
    using arguments_t = std::tuple<V<HeapObject>, V<Map>>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

  struct TryMigrateInstance : public Descriptor<TryMigrateInstance> {
    static constexpr auto kFunction = Runtime::kTryMigrateInstance;
    using arguments_t = std::tuple<V<HeapObject>>;
    using result_t = V<Object>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties =
        Operator::kNoDeopt | Operator::kNoThrow;
  };

  struct ThrowAccessedUninitializedVariable
      : public Descriptor<ThrowAccessedUninitializedVariable> {
    static constexpr auto kFunction =
        Runtime::kThrowAccessedUninitializedVariable;
    using arguments_t = std::tuple<V<Object>>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowConstructorReturnedNonObject
      : public Descriptor<ThrowConstructorReturnedNonObject> {
    static constexpr auto kFunction =
        Runtime::kThrowConstructorReturnedNonObject;
    using arguments_t = std::tuple<>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowNotSuperConstructor
      : public Descriptor<ThrowNotSuperConstructor> {
    static constexpr auto kFunction = Runtime::kThrowNotSuperConstructor;
    using arguments_t = std::tuple<V<Object>, V<Object>>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowSuperAlreadyCalledError
      : public Descriptor<ThrowSuperAlreadyCalledError> {
    static constexpr auto kFunction = Runtime::kThrowSuperAlreadyCalledError;
    using arguments_t = std::tuple<>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowSuperNotCalled : public Descriptor<ThrowSuperNotCalled> {
    static constexpr auto kFunction = Runtime::kThrowSuperNotCalled;
    using arguments_t = std::tuple<>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowCalledNonCallable : public Descriptor<ThrowCalledNonCallable> {
    static constexpr auto kFunction = Runtime::kThrowCalledNonCallable;
    using arguments_t = std::tuple<V<Object>>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct ThrowInvalidStringLength
      : public Descriptor<ThrowInvalidStringLength> {
    static constexpr auto kFunction = Runtime::kThrowInvalidStringLength;
    using arguments_t = std::tuple<>;
    // Doesn't actually return something, but the actual runtime call descriptor
    // (returned by Linkage::GetRuntimeCallDescriptor) returns 1 instead of 0.
    using result_t = Never;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };

  struct NewClosure : public Descriptor<NewClosure> {
    static constexpr auto kFunction = Runtime::kNewClosure;
    using arguments_t = std::tuple<V<SharedFunctionInfo>, V<FeedbackCell>>;
    using result_t = V<JSFunction>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties = Operator::kNoThrow;
  };

  struct NewClosure_Tenured : public Descriptor<NewClosure_Tenured> {
    static constexpr auto kFunction = Runtime::kNewClosure_Tenured;
    using arguments_t = std::tuple<V<SharedFunctionInfo>, V<FeedbackCell>>;
    using result_t = V<JSFunction>;

    static constexpr bool kNeedsFrameState = false;
    static constexpr Operator::Properties kProperties = Operator::kNoThrow;
  };

  struct HasInPrototypeChain : public Descriptor<HasInPrototypeChain> {
    static constexpr auto kFunction = Runtime::kHasInPrototypeChain;
    using arguments_t = std::tuple<V<Object>, V<HeapObject>>;
    using result_t = V<Boolean>;

    static constexpr bool kNeedsFrameState = true;
    static constexpr Operator::Properties kProperties = Operator::kNoProperties;
  };
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_RUNTIME_CALL_DESCRIPTORS_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                    node-23.7.0/deps/v8/src/compiler/turboshaft/select-lowering-reducer.h                               0000664 0000000 0000000 00000003520 14746647661 0025532 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SELECT_LOWERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_SELECT_LOWERING_REDUCER_H_

#include "src/base/vector.h"
#include "src/compiler/common-operator.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/operations.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

// Lowers Select operations to diamonds.
//
// A Select is conceptually somewhat similar to a ternary if:
//
//       res = Select(cond, val_true, val_false)
//
// means:
//
//       res = cond ? val_true : val_false
//
// SelectLoweringReducer lowers such operations into:
//
//     if (cond) {
//         res = val_true
//     } else {
//         res = val_false
//     }

template <class Next>
class SelectLoweringReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(SelectLowering)

  V<Any> REDUCE(Select)(V<Word32> cond, V<Any> vtrue, V<Any> vfalse,
                        RegisterRepresentation rep, BranchHint hint,
                        SelectOp::Implementation implem) {
    if (implem == SelectOp::Implementation::kCMove) {
      // We do not lower Select operations that should be implemented with
      // CMove.
      return Next::ReduceSelect(cond, vtrue, vfalse, rep, hint, implem);
    }

    Variable result = __ NewLoopInvariantVariable(rep);
    IF (cond) {
      __ SetVariable(result, vtrue);
    } ELSE {
      __ SetVariable(result, vfalse);
    }

    return __ GetVariable(result);
  }
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SELECT_LOWERING_REDUCER_H_
                                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/sidetable.cc                                            0000664 0000000 0000000 00000001031 14746647661 0023065 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/sidetable.h"

#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/index.h"

namespace v8::internal::compiler::turboshaft {

#ifdef DEBUG
bool OpIndexBelongsToTableGraph(const Graph* graph, OpIndex index) {
  return graph->BelongsToThisGraph(index);
}
#endif

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/v8/src/compiler/turboshaft/sidetable.h                                             0000664 0000000 0000000 00000014000 14746647661 0022727 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SIDETABLE_H_
#define V8_COMPILER_TURBOSHAFT_SIDETABLE_H_

#include <algorithm>
#include <iterator>
#include <limits>
#include <memory>
#include <type_traits>

#include "src/base/iterator.h"
#include "src/base/small-vector.h"
#include "src/base/vector.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/zone/zone-containers.h"

namespace v8::internal::compiler::turboshaft {

#ifdef DEBUG
V8_EXPORT_PRIVATE bool OpIndexBelongsToTableGraph(const Graph* graph,
                                                  OpIndex index);
#endif

namespace detail {

// This sidetable is a conceptually infinite mapping from Turboshaft operation
// indices to values. It grows automatically and default-initializes the table
// when accessed out-of-bounds.
template <class T, class Key>
class GrowingSidetable {
 public:
  static_assert(std::is_same_v<Key, OpIndex> ||
                std::is_same_v<Key, BlockIndex>);

  T& operator[](Key index) {
    DCHECK(index.valid());
    size_t i = index.id();
    if (V8_UNLIKELY(i >= table_.size())) {
      table_.resize(NextSize(i));
      // Make sure we also get access to potential over-allocation by
      // `resize()`.
      table_.resize(table_.capacity());
    }
    return table_[i];
  }

  const T& operator[](Key index) const {
    DCHECK(index.valid());
    size_t i = index.id();
    if (V8_UNLIKELY(i >= table_.size())) {
      table_.resize(NextSize(i));
      // Make sure we also get access to potential over-allocation by
      // `resize()`.
      table_.resize(table_.capacity());
    }
    return table_[i];
  }

  // Reset by filling the table with the default value instead of shrinking to
  // keep the memory for later phases.
  void Reset() { std::fill(table_.begin(), table_.end(), T{}); }

  // Returns `true` if the table never contained any values, even before
  // `Reset()`.
  bool empty() const { return table_.empty(); }

 protected:
  // Constructors are protected: use GrowingBlockSidetable or
  // GrowingOpIndexSidetable instead.
  explicit GrowingSidetable(Zone* zone) : table_(zone) {}
  GrowingSidetable(size_t size, const T& initial_value, Zone* zone)
      : table_(size, initial_value, zone) {}

  mutable ZoneVector<T> table_;

  size_t NextSize(size_t out_of_bounds_index) const {
    DCHECK_GE(out_of_bounds_index, table_.size());
    return out_of_bounds_index + out_of_bounds_index / 2 + 32;
  }
};

// A fixed-size sidetable mapping from `Key` to `T`.
// Elements are default-initialized.
template <class T, class Key>
class FixedSidetable {
 public:
  static_assert(std::is_same_v<Key, OpIndex> ||
                std::is_same_v<Key, BlockIndex>);

  T& operator[](Key op) {
    DCHECK_LT(op.id(), table_.size());
    return table_[op.id()];
  }

  const T& operator[](Key op) const {
    DCHECK_LT(op.id(), table_.size());
    return table_[op.id()];
  }

 protected:
  // Constructors are protected: use FixedBlockSidetable or
  // FixedOpIndexSidetable instead.
  explicit FixedSidetable(size_t size, Zone* zone) : table_(size, zone) {}
  FixedSidetable(size_t size, const T& default_value, Zone* zone)
      : table_(size, default_value, zone) {}

  ZoneVector<T> table_;
};

}  // namespace detail

template <typename T>
class GrowingBlockSidetable : public detail::GrowingSidetable<T, BlockIndex> {
  using Base = detail::GrowingSidetable<T, BlockIndex>;

 public:
  explicit GrowingBlockSidetable(Zone* zone) : Base(zone) {}

  GrowingBlockSidetable(size_t size, const T& initial_value, Zone* zone)
      : Base(size, initial_value, zone) {}
};

template <typename T>
class FixedBlockSidetable : public detail::FixedSidetable<T, BlockIndex> {
  using Base = detail::FixedSidetable<T, BlockIndex>;

 public:
  explicit FixedBlockSidetable(size_t size, Zone* zone) : Base(size, zone) {}

  FixedBlockSidetable(size_t size, const T& initial_value, Zone* zone)
      : Base(size, initial_value, zone) {}
};

template <class T>
class GrowingOpIndexSidetable : public detail::GrowingSidetable<T, OpIndex> {
  using Base = detail::GrowingSidetable<T, OpIndex>;

 public:
  explicit GrowingOpIndexSidetable(Zone* zone, const Graph* graph)
      : Base(zone)
#ifdef DEBUG
        ,
        graph_(graph)
#endif
  {
    USE(graph);
  }

  GrowingOpIndexSidetable(size_t size, const T& initial_value, Zone* zone,
                          const Graph* graph)
      : Base(size, initial_value, zone)
#ifdef DEBUG
        ,
        graph_(graph)
#endif
  {
    USE(graph);
  }

  T& operator[](OpIndex index) {
    DCHECK(OpIndexBelongsToTableGraph(graph_, index));
    return Base::operator[](index);
  }

  const T& operator[](OpIndex index) const {
    DCHECK(OpIndexBelongsToTableGraph(graph_, index));
    return Base::operator[](index);
  }

  void SwapData(GrowingOpIndexSidetable<T>& other) {
    std::swap(Base::table_, other.table_);
  }

 public:
#ifdef DEBUG
  const Graph* graph_;
#endif
};

template <class T>
class FixedOpIndexSidetable : public detail::FixedSidetable<T, OpIndex> {
  using Base = detail::FixedSidetable<T, OpIndex>;

 public:
  FixedOpIndexSidetable(size_t size, Zone* zone, const Graph* graph)
      : Base(size, zone)
#ifdef DEBUG
        ,
        graph_(graph)
#endif
  {
  }
  FixedOpIndexSidetable(size_t size, const T& default_value, Zone* zone,
                        const Graph* graph)
      : Base(size, default_value, zone)
#ifdef DEBUG
        ,
        graph_(graph)
#endif
  {
  }

  T& operator[](OpIndex index) {
    DCHECK(OpIndexBelongsToTableGraph(graph_, index));
    return Base::operator[](index);
  }

  const T& operator[](OpIndex index) const {
    DCHECK(OpIndexBelongsToTableGraph(graph_, index));
    return Base::operator[](index);
  }

  void SwapData(FixedOpIndexSidetable<T>& other) {
    std::swap(Base::table_, other.table_);
  }

 public:
#ifdef DEBUG
  const Graph* graph_;
#endif
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SIDETABLE_H_
node-23.7.0/deps/v8/src/compiler/turboshaft/simplified-lowering-phase.cc                            0000664 0000000 0000000 00000001115 14746647661 0026203 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/simplified-lowering-phase.h"

#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/simplified-lowering-reducer.h"

namespace v8::internal::compiler::turboshaft {

void SimplifiedLoweringPhase::Run(PipelineData* data, Zone* temp_zone) {
  CopyingPhase<SimplifiedLoweringReducer>::Run(data, temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/simplified-lowering-phase.h                             0000664 0000000 0000000 00000001175 14746647661 0026053 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct SimplifiedLoweringPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(SimplifiedLowering)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_PHASE_H_
                                                                                                                                                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/simplified-lowering-reducer.h                           0000664 0000000 0000000 00000007337 14746647661 0026412 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/utils.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

template <typename Next>
class SimplifiedLoweringReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(SimplifiedLowering)

  using Rep = RegisterRepresentation;

  enum class CheckKind {
    kNone,
    kSigned32,
  };

  OpIndex REDUCE_INPUT_GRAPH(SpeculativeNumberBinop)(
      OpIndex ig_index, const SpeculativeNumberBinopOp& op) {
    DCHECK_EQ(op.kind, SpeculativeNumberBinopOp::Kind::kSafeIntegerAdd);

    OpIndex frame_state = Map(op.frame_state());
    V<Word32> left = ProcessInput(Map(op.left()), Rep::Word32(),
                                  CheckKind::kSigned32, frame_state);
    V<Word32> right = ProcessInput(Map(op.right()), Rep::Word32(),
                                   CheckKind::kSigned32, frame_state);

    V<Tuple<Word32, Word32>> result = __ Int32AddCheckOverflow(left, right);

    V<Word32> overflow = __ template Projection<1>(result);
    __ DeoptimizeIf(overflow, Map(op.frame_state()),
                    DeoptimizeReason::kOverflow, FeedbackSource{});
    return __ template Projection<0>(result);
  }

  OpIndex REDUCE_INPUT_GRAPH(Return)(OpIndex ig_index, const ReturnOp& ret) {
    base::SmallVector<OpIndex, 8> return_values;
    for (OpIndex input : ret.return_values()) {
      return_values.push_back(
          ProcessInput(Map(input), Rep::Tagged(), CheckKind::kNone, {}));
    }

    __ Return(Map(ret.pop_count()), base::VectorOf(return_values));
    return OpIndex::Invalid();
  }

  OpIndex ProcessInput(OpIndex input_index, RegisterRepresentation target_rep,
                       CheckKind type_check, OptionalOpIndex frame_state) {
    const Operation& input_op = __ output_graph().Get(input_index);
    DCHECK_EQ(input_op.outputs_rep().size(), 1);

    RegisterRepresentation output_rep = input_op.outputs_rep()[0];
    if (output_rep == target_rep && type_check == CheckKind::kNone) {
      return input_index;
    }

    switch (multi(output_rep, target_rep)) {
      case multi(Rep::Tagged(), Rep::Word32()): {
        // TODO(nicohartmann@): More cases.
        DCHECK_EQ(type_check, CheckKind::kSigned32);
        return __ ConvertJSPrimitiveToUntaggedOrDeopt(
            input_index, frame_state.value(),
            ConvertJSPrimitiveToUntaggedOrDeoptOp::JSPrimitiveKind::kNumber,
            ConvertJSPrimitiveToUntaggedOrDeoptOp::UntaggedKind::kInt32,
            CheckForMinusZeroMode::kCheckForMinusZero, {});
      }
      case multi(Rep::Word32(), Rep::Tagged()): {
        // TODO(nicohartmann@): More cases.
        return __ ConvertUntaggedToJSPrimitive(
            input_index,
            ConvertUntaggedToJSPrimitiveOp::JSPrimitiveKind::kNumber,
            Rep::Word32(),
            ConvertUntaggedToJSPrimitiveOp::InputInterpretation::kSigned,
            CheckForMinusZeroMode::kDontCheckForMinusZero);
      }

      default:
        UNIMPLEMENTED();
    }
  }

  inline OpIndex Map(OpIndex ig_index) { return __ MapToNewGraph(ig_index); }
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SIMPLIFIED_LOWERING_REDUCER_H_
                                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/simplify-tf-loops.cc                                    0000664 0000000 0000000 00000003655 14746647661 0024544 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/simplify-tf-loops.h"

#include "src/base/small-vector.h"
#include "src/compiler/machine-graph.h"
#include "src/compiler/node-properties.h"

namespace v8::internal::compiler {

Reduction SimplifyTFLoops::Reduce(Node* node) {
  if (node->opcode() != IrOpcode::kLoop) return NoChange();
  if (node->InputCount() <= 2) return NoChange();

  Node* new_loop = mcgraph_->graph()->NewNode(mcgraph_->common()->Loop(2),
                                              node->InputAt(0), node);
  node->RemoveInput(0);
  NodeProperties::ChangeOp(node, mcgraph_->common()->Merge(node->InputCount()));

  base::SmallVector<Edge, 4> control_uses;

  for (Edge edge : node->use_edges()) {
    Node* use = edge.from();
    if (!NodeProperties::IsPhi(use)) {
      control_uses.emplace_back(edge);
      continue;
    }
    Node* dominating_input = use->InputAt(0);
    use->RemoveInput(0);
    NodeProperties::ChangeOp(
        use, use->opcode() == IrOpcode::kPhi
                 ? mcgraph_->common()->Phi(PhiRepresentationOf(use->op()),
                                           use->InputCount() - 1)
                 : mcgraph_->common()->EffectPhi(use->InputCount() - 1));

    Node* new_phi = mcgraph_->graph()->NewNode(
        use->opcode() == IrOpcode::kPhi
            ? mcgraph_->common()->Phi(PhiRepresentationOf(use->op()), 2)
            : mcgraph_->common()->EffectPhi(2),
        dominating_input, use, new_loop);

    ReplaceWithValue(use, new_phi, new_phi, new_phi);
    // Restore the use <- new_phi edge we just broke.
    new_phi->ReplaceInput(1, use);
  }

  for (Edge edge : control_uses) {
    if (edge.from() != new_loop) {
      edge.from()->ReplaceInput(edge.index(), new_loop);
    }
  }

  return NoChange();
}

}  // namespace v8::internal::compiler
                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/simplify-tf-loops.h                                     0000664 0000000 0000000 00000001605 14746647661 0024377 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SIMPLIFY_TF_LOOPS_H_
#define V8_COMPILER_TURBOSHAFT_SIMPLIFY_TF_LOOPS_H_

#include "src/compiler/graph-reducer.h"

namespace v8::internal::compiler {

class MachineGraph;

// Constrain loop nodes to have at most two inputs, by introducing additional
// merges as needed.
class SimplifyTFLoops final : public AdvancedReducer {
 public:
  SimplifyTFLoops(Editor* editor, MachineGraph* mcgraph)
      : AdvancedReducer(editor), mcgraph_(mcgraph) {}

  const char* reducer_name() const override { return "SimplifyTFLoops"; }

  Reduction Reduce(Node* node) final;

 private:
  MachineGraph* const mcgraph_;
};

}  // namespace v8::internal::compiler

#endif  // V8_COMPILER_TURBOSHAFT_SIMPLIFY_TF_LOOPS_H_
                                                                                                                           node-23.7.0/deps/v8/src/compiler/turboshaft/snapshot-table-opindex.h                                0000664 0000000 0000000 00000005011 14746647661 0025365 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_OPINDEX_H_
#define V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_OPINDEX_H_

#include <optional>

#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/snapshot-table.h"

namespace v8::internal::compiler::turboshaft {
// A Wrapper around a SnapshotTable, which takes care of mapping OpIndex to Key.
// It uses a ZoneAbslFlatHashMap to store this mapping, and is thus more
// appropriate for cases where not many OpIndex have a corresponding key.
template <class Value, class KeyData = NoKeyData>
class SparseOpIndexSnapshotTable : public SnapshotTable<Value, KeyData> {
 public:
  using Base = SnapshotTable<Value, KeyData>;
  using Key = typename SnapshotTable<Value, KeyData>::Key;

  explicit SparseOpIndexSnapshotTable(Zone* zone)
      : Base(zone), indices_to_keys_(zone) {}

  using Base::Get;
  Value Get(OpIndex idx) const {
    auto it = indices_to_keys_.find(idx);
    if (it == indices_to_keys_.end()) return Value{};
    return Base::Get(it->second);
  }

  Value GetPredecessorValue(OpIndex idx, int predecessor_index) {
    auto it = indices_to_keys_.find(idx);
    if (it == indices_to_keys_.end()) return Value{};
    return Base::GetPredecessorValue(it->second, predecessor_index);
  }

  using Base::Set;
  bool Set(OpIndex idx, Value new_value) {
    Key key = GetOrCreateKey(idx);
    return Base::Set(key, new_value);
  }

  void NewKey(OpIndex idx, KeyData data, Value initial_value = Value{}) {
    DCHECK(!indices_to_keys_[idx].has_value());
    indices_to_keys_[idx] = Base::NewKey(data, initial_value);
  }
  void NewKey(OpIndex idx, Value initial_value = Value{}) {
    NewKey(idx, KeyData{}, initial_value);
  }

  bool HasKeyFor(OpIndex idx) const {
    return indices_to_keys_.find(idx) != indices_to_keys_.end();
  }

  std::optional<Key> TryGetKeyFor(OpIndex idx) const {
    auto it = indices_to_keys_.find(idx);
    if (it != indices_to_keys_.end()) return it->second;
    return std::nullopt;
  }

 private:
  Key GetOrCreateKey(OpIndex idx) {
    auto it = indices_to_keys_.find(idx);
    if (it != indices_to_keys_.end()) return it->second;
    Key key = Base::NewKey();
    indices_to_keys_.insert({idx, key});
    return key;
  }
  ZoneAbslFlatHashMap<OpIndex, Key> indices_to_keys_;
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_OPINDEX_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/v8/src/compiler/turboshaft/snapshot-table.h                                        0000664 0000000 0000000 00000055712 14746647661 0023736 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_H_
#define V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_H_

#include <iostream>
#include <limits>

#include "src/base/iterator.h"
#include "src/base/small-vector.h"
#include "src/compiler/turboshaft/fast-hash.h"
#include "src/zone/zone-containers.h"

// A `SnapshotTable` stores a mapping from keys to values and creates snapshots,
// which capture the current state efficiently and allow us to return to a
// previous snapshot later. It is optimized for the case where we switch between
// similar snapshots with a closeby common ancestor.
//
// Complexity:
//   creating a snapshot   linear in the number of `Set` operations between the
//                         current state and the common ancestor of all
//                         predecessors and the current state, plus the `Set`
//                         operations from the common ancestor to all
//                         predecessors.
//   Get()                      O(1)
//   Set()                      O(1) + operator== for Value
//   Seal()                     O(1)
//   NewKey()                   O(1)
//   GetPredecessorValue()      O(1)
namespace v8::internal::compiler::turboshaft {

struct NoKeyData {};

struct NoChangeCallback {
  template <class Key, class Value>
  void operator()(Key key, const Value& old_value,
                  const Value& new_value) const {}
};

template <class Value, class KeyData>
class SnapshotTable;

// Place `KeyData` in a superclass to benefit from empty-base optimization.
template <class Value, class KeyData>
struct SnapshotTableEntry : KeyData {
  Value value;
  // `merge_offset` is the offset in `merge_values_` where we store the
  // merged values. It is used during merging (to know what to merge) and when
  // calling GetPredecessorValue.
  uint32_t merge_offset = kNoMergeOffset;
  // Used during merging: the index of the predecessor for which we last
  // recorded a value. This allows us to only use the last value for a given
  // predecessor and skip over all earlier ones.
  uint32_t last_merged_predecessor = kNoMergedPredecessor;

  explicit SnapshotTableEntry(Value value, KeyData data)
      : KeyData(std::move(data)), value(std::move(value)) {}

  static constexpr uint32_t kNoMergeOffset =
      std::numeric_limits<uint32_t>::max();
  static constexpr uint32_t kNoMergedPredecessor =
      std::numeric_limits<uint32_t>::max();
};

// A `SnapshotTableKey` identifies an entry in the `SnapshotTable`. For better
// performance, keys always have identity. The template parameter `KeyData` can
// be used to embed additional data in the keys. A Key is implemented as a
// pointer into the table, which also contains the `KeyData`. Therefore, keys
// have pointer-size and are cheap to copy.
template <class Value, class KeyData>
class SnapshotTableKey {
 public:
  bool operator==(SnapshotTableKey other) const {
    return entry_ == other.entry_;
  }
  const KeyData& data() const { return *entry_; }
  KeyData& data() { return *entry_; }
  SnapshotTableKey() : entry_(nullptr) {}

  bool valid() const { return entry_ != nullptr; }

 private:
  friend class SnapshotTable<Value, KeyData>;
  SnapshotTableEntry<Value, KeyData>* entry_;
  explicit SnapshotTableKey(SnapshotTableEntry<Value, KeyData>& entry)
      : entry_(&entry) {}
};

template <class Value, class KeyData = NoKeyData>
class SnapshotTable {
 private:
  struct LogEntry;
  struct SnapshotData;

 public:
  using TableEntry = SnapshotTableEntry<Value, KeyData>;
  using Key = SnapshotTableKey<Value, KeyData>;

  // A `Snapshot` captures the state of the `SnapshotTable`.
  // A `Snapshot` is implemented as a pointer to internal data and is therefore
  // cheap to copy.
  class MaybeSnapshot;
  class Snapshot {
   public:
    bool operator==(Snapshot other) const { return data_ == other.data_; }

   private:
    friend SnapshotTable;
    friend MaybeSnapshot;

    SnapshotData* data_;
    explicit Snapshot(SnapshotData& data) : data_(&data) {}
    explicit Snapshot(SnapshotData* data) : data_(data) {}
  };

  class MaybeSnapshot {
   public:
    bool has_value() const { return data_ != nullptr; }
    Snapshot value() const {
      DCHECK(has_value());
      return Snapshot{data_};
    }

    void Set(Snapshot snapshot) { data_ = snapshot.data_; }

    MaybeSnapshot() = default;
    explicit MaybeSnapshot(Snapshot snapshot) : data_(snapshot.data_) {}

   private:
    SnapshotData* data_ = nullptr;
  };

  // A new Snapshot is based on a list of predecessor Snapshots. If no
  // predecessor is given, the new Snapshot is based on the initial state of the
  // table. A single predecessor Snapshot resets the table to exactly this
  // Snapshot. In the case of multiple Snapshots, a merge function is used to
  // unify values that were set since the last common ancestor snapshot.
  // The previous Snapshot needs to be closed using Seal() before another one
  // can be created.
  // The function `change_callback` is invoked for every atomic update to a
  // table entry as part of switching to the new snapshot and merging.
  // Note that the callback might be invoked multiple times for the same key,
  // because we first roll-back changes to the common ancestor and then apply
  // the merge function. The second update will have the new value of the first
  // update as old value. We should not rely on the exact sequence of updates,
  // only on the fact that the updates collectively transform the table into the
  // new state. The motivation for this feature are secondary indices that need
  // to be kept in sync with the main table.
  template <class ChangeCallback = NoChangeCallback,
            std::enable_if_t<std::is_invocable_v<ChangeCallback, Key, Value,
                                                 Value>>* = nullptr>
  void StartNewSnapshot(base::Vector<const Snapshot> predecessors,
                        const ChangeCallback& change_callback = {}) {
    DCHECK(current_snapshot_->IsSealed());
    MoveToNewSnapshot(predecessors, change_callback);
#ifdef DEBUG
    snapshot_was_created_with_merge = false;
#endif
  }
  template <class ChangeCallback = NoChangeCallback,
            std::enable_if_t<std::is_invocable_v<ChangeCallback, Key, Value,
                                                 Value>>* = nullptr>
  void StartNewSnapshot(std::initializer_list<Snapshot> predecessors = {},
                        const ChangeCallback& change_callback = {}) {
    StartNewSnapshot(base::VectorOf(predecessors), change_callback);
  }
  template <class ChangeCallback = NoChangeCallback,
            std::enable_if_t<std::is_invocable_v<ChangeCallback, Key, Value,
                                                 Value>>* = nullptr>
  void StartNewSnapshot(Snapshot parent,
                        const ChangeCallback& change_callback = {}) {
    StartNewSnapshot({parent}, change_callback);
  }
  template <
      class MergeFun, class ChangeCallback = NoChangeCallback,
      std::enable_if_t<
          std::is_invocable_v<MergeFun, Key, base::Vector<const Value>> &&
          std::is_invocable_v<ChangeCallback, Key, Value, Value>>* = nullptr>
  void StartNewSnapshot(base::Vector<const Snapshot> predecessors,
                        const MergeFun& merge_fun,
                        const ChangeCallback& change_callback = {}) {
    StartNewSnapshot(predecessors, change_callback);
    MergePredecessors(predecessors, merge_fun, change_callback);
#ifdef DEBUG
    snapshot_was_created_with_merge = true;
#endif
  }
  template <
      class MergeFun, class ChangeCallback = NoChangeCallback,
      std::enable_if_t<
          std::is_invocable_v<MergeFun, Key, base::Vector<const Value>> &&
          std::is_invocable_v<ChangeCallback, Key, Value, Value>>* = nullptr>
  void StartNewSnapshot(std::initializer_list<Snapshot> predecessors,
                        const MergeFun& merge_fun,
                        const ChangeCallback& change_callback = {}) {
    StartNewSnapshot(base::VectorOf(predecessors), merge_fun, change_callback);
  }

  Snapshot Seal() {
    current_snapshot_->Seal(log_.size());
    // Reseting the entries' `merge_offset` and `last_merged_predecessor`
    // fields, so that they are cleared for the next Merge.
    for (TableEntry* entry : merging_entries_) {
      entry->last_merged_predecessor = kNoMergedPredecessor;
      entry->merge_offset = kNoMergeOffset;
    }
    merge_values_.clear();
    merging_entries_.clear();

    // Optimization: If nothing changed in the new snapshot, we discard it and
    // use its parent instead.
    if (current_snapshot_->log_begin == current_snapshot_->log_end) {
      SnapshotData* parent = current_snapshot_->parent;
      DCHECK_EQ(current_snapshot_, &snapshots_.back());
      snapshots_.pop_back();
      current_snapshot_ = parent;
      return Snapshot{*parent};
    }
    return Snapshot{*current_snapshot_};
  }

  const Value& Get(Key key) const { return key.entry_->value; }

  // Returns the value associated to {key} in its {predecessor_index}th
  // predecessor (where "predecessor" refers to the predecessors that were
  // passed to StartNewSnapshot when creating the current snapshot).
  // This function should only be used if the snapshot was started with a merge
  // function.
  // If {key} wasn't merged but was Set in the current snapshot, then
  // the newly set value will be returned rather than the predecessor value.
  const Value& GetPredecessorValue(Key key, int predecessor_index) {
    DCHECK(!current_snapshot_->IsSealed());
    DCHECK(snapshot_was_created_with_merge);
    if (key.entry_->merge_offset == kNoMergeOffset) return Get(key);
    return merge_values_[key.entry_->merge_offset + predecessor_index];
  }

  // {Set} returns whether the {new_value} is different from the previous value.
  bool Set(Key key, Value new_value) {
    DCHECK(!current_snapshot_->IsSealed());
    if (key.entry_->value == new_value) return false;
    log_.push_back(LogEntry{*key.entry_, key.entry_->value, new_value});
    key.entry_->value = new_value;
    return true;
  }

  explicit SnapshotTable(Zone* zone) : zone_(zone) {
    root_snapshot_ = &NewSnapshot(nullptr);
    root_snapshot_->Seal(0);
    current_snapshot_ = root_snapshot_;
  }

  // The initial value is independent of the snapshot mechanism. Creating a key
  // with a certain initial value later has the same effect as creating the key
  // before all modifications to the table.
  // Keys have identity, and the data embedded in the key is mutable.
  Key NewKey(KeyData data, Value initial_value = Value{}) {
    return Key{table_.emplace_back(
        TableEntry{std::move(initial_value), std::move(data)})};
  }
  Key NewKey(Value initial_value = Value{}) {
    return NewKey(KeyData{}, initial_value);
  }

  // Returns true if {current_snapshot_} is sealed.
  bool IsSealed() { return current_snapshot_->IsSealed(); }

 private:
  Zone* zone_;
  ZoneDeque<TableEntry> table_{zone_};
  ZoneDeque<SnapshotData> snapshots_{zone_};
  // While logically each snapshot has its own log, we allocate the memory as a
  // single global log with each snapshot pointing to a section of it to reduce
  // the number of allocations.
  ZoneVector<LogEntry> log_{zone_};
  SnapshotData* root_snapshot_;
  SnapshotData* current_snapshot_;

  // The following members are only used temporarily during a merge operation
  // or when creating a new snapshot.
  // They are declared here to recycle the memory, avoiding repeated
  // Zone-allocation.
  ZoneVector<TableEntry*> merging_entries_{zone_};
  ZoneVector<Value> merge_values_{zone_};
  ZoneVector<SnapshotData*> path_{zone_};

#ifdef DEBUG
  bool snapshot_was_created_with_merge = false;
#endif

  SnapshotData& NewSnapshot(SnapshotData* parent) {
    return snapshots_.emplace_back(parent, log_.size());
  }

  base::Vector<LogEntry> LogEntries(SnapshotData* s) {
    return base::VectorOf(&log_[s->log_begin], s->log_end - s->log_begin);
  }

  template <class ChangeCallback = NoChangeCallback>
  void RevertCurrentSnapshot(ChangeCallback& change_callback) {
    DCHECK(current_snapshot_->IsSealed());
    base::Vector<LogEntry> log_entries = LogEntries(current_snapshot_);
    for (const LogEntry& entry : base::Reversed(log_entries)) {
      DCHECK_EQ(entry.table_entry.value, entry.new_value);
      DCHECK_NE(entry.new_value, entry.old_value);
      change_callback(Key{entry.table_entry}, entry.new_value, entry.old_value);
      entry.table_entry.value = entry.old_value;
    }
    current_snapshot_ = current_snapshot_->parent;
    DCHECK_NOT_NULL(current_snapshot_);
  }

  template <class ChangeCallback = NoChangeCallback>
  void ReplaySnapshot(SnapshotData* snapshot, ChangeCallback& change_callback) {
    DCHECK_EQ(snapshot->parent, current_snapshot_);
    for (const LogEntry& entry : LogEntries(snapshot)) {
      DCHECK_EQ(entry.table_entry.value, entry.old_value);
      DCHECK_NE(entry.new_value, entry.old_value);
      change_callback(Key{entry.table_entry}, entry.old_value, entry.new_value);
      entry.table_entry.value = entry.new_value;
    }
    current_snapshot_ = snapshot;
  }

  void RecordMergeValue(TableEntry& entry, const Value& value,
                        uint32_t predecessor_index, uint32_t predecessor_count);
  template <class ChangeCallback>
  SnapshotData& MoveToNewSnapshot(base::Vector<const Snapshot> predecessors,
                                  const ChangeCallback& change_callback);
  template <class MergeFun, class ChangeCallback>
  void MergePredecessors(base::Vector<const Snapshot> predecessors,
                         const MergeFun& merge_fun,
                         const ChangeCallback& change_callback);

  static constexpr uint32_t kNoMergeOffset =
      std::numeric_limits<uint32_t>::max();
  static constexpr uint32_t kNoMergedPredecessor =
      std::numeric_limits<uint32_t>::max();
};

template <class Value, class KeyData>
struct SnapshotTable<Value, KeyData>::LogEntry {
  TableEntry& table_entry;
  Value old_value;
  Value new_value;
};

template <class Value, class KeyData>
struct SnapshotTable<Value, KeyData>::SnapshotData {
  SnapshotData* parent;

  const uint32_t depth = parent ? parent->depth + 1 : 0;
  size_t log_begin;
  size_t log_end = kInvalidOffset;

  static constexpr size_t kInvalidOffset = std::numeric_limits<size_t>::max();

  SnapshotData(SnapshotData* parent, size_t log_begin)
      : parent(parent), log_begin(log_begin) {}

  SnapshotData* CommonAncestor(SnapshotData* other) {
    SnapshotData* self = this;
    while (other->depth > self->depth) other = other->parent;
    while (self->depth > other->depth) self = self->parent;
    while (other != self) {
      self = self->parent;
      other = other->parent;
    }
    return self;
  }
  void Seal(size_t log_end) {
    DCHECK_WITH_MSG(!IsSealed(), "A Snapshot can only be sealed once");
    this->log_end = log_end;
  }

  bool IsSealed() const { return log_end != kInvalidOffset; }
};

template <class Value, class KeyData>
void SnapshotTable<Value, KeyData>::RecordMergeValue(
    TableEntry& entry, const Value& value, uint32_t predecessor_index,
    uint32_t predecessor_count) {
  if (predecessor_index == entry.last_merged_predecessor) {
    DCHECK_NE(entry.merge_offset, kNoMergeOffset);
    // We already recorded a later value for this predecessor, so we should skip
    // earlier values.
    return;
  }
  if (entry.merge_offset == kNoMergeOffset) {
    // Allocate space for the merge values. All the merge values are initialized
    // to the value from the parent snapshot. This way, we get the right value
    // for predecessors that did not change the value.
    DCHECK_EQ(entry.last_merged_predecessor, kNoMergedPredecessor);
    CHECK_LE(merge_values_.size() + predecessor_count,
             std::numeric_limits<uint32_t>::max());
    entry.merge_offset = static_cast<uint32_t>(merge_values_.size());
    merging_entries_.push_back(&entry);
    merge_values_.insert(merge_values_.end(), predecessor_count, entry.value);
  }
  merge_values_[entry.merge_offset + predecessor_index] = value;
  entry.last_merged_predecessor = predecessor_index;
}

// This function prepares the SnapshotTable to start a new snapshot whose
// predecessors are `predecessors`. To do this, it resets and replay snapshots
// in between the `current_snapshot_` and the position of the new snapshot. For
// instance:
//
//        S0
//      /    \
//     S1      S3
//     |         \
//     S2         S4
//               /  \
//              S5   S6
// If `predecessors` are S5 and S6, and `current_snapshot_` is S2, we:
//
// - First find the common ancestor of S5 and S6 (it's S4). This will be the
//   parent snapshot of the new snapshot.
// - Find the common ancestor of S4 and the current snapshot S2 (it's S0).
// - Roll back S2 and S1 to reach S0
// - Replay S3 and S4 go be in the state of S4 (the common ancestor of
//   `predecessors`).
// - Start creating a new snapshot with parent S4.
template <class Value, class KeyData>
template <class ChangeCallback>
typename SnapshotTable<Value, KeyData>::SnapshotData&
SnapshotTable<Value, KeyData>::MoveToNewSnapshot(
    base::Vector<const Snapshot> predecessors,
    const ChangeCallback& change_callback) {
  DCHECK_WITH_MSG(
      current_snapshot_->IsSealed(),
      "A new Snapshot was opened before the previous Snapshot was sealed");

  SnapshotData* common_ancestor;
  if (predecessors.empty()) {
    common_ancestor = root_snapshot_;
  } else {
    common_ancestor = predecessors.first().data_;
    for (Snapshot s : predecessors.SubVectorFrom(1)) {
      common_ancestor = common_ancestor->CommonAncestor(s.data_);
    }
  }
  SnapshotData* go_back_to = common_ancestor->CommonAncestor(current_snapshot_);
  while (current_snapshot_ != go_back_to) {
    RevertCurrentSnapshot(change_callback);
  }
  {
    // Replay to common_ancestor.
    path_.clear();
    for (SnapshotData* s = common_ancestor; s != go_back_to; s = s->parent) {
      path_.push_back(s);
    }
    for (SnapshotData* s : base::Reversed(path_)) {
      ReplaySnapshot(s, change_callback);
    }
  }

  DCHECK_EQ(current_snapshot_, common_ancestor);
  SnapshotData& new_snapshot = NewSnapshot(common_ancestor);
  current_snapshot_ = &new_snapshot;
  return new_snapshot;
}

// Merges all entries modified in `predecessors` since the last common ancestor
// by adding them to the current snapshot.
template <class Value, class KeyData>
template <class MergeFun, class ChangeCallback>
void SnapshotTable<Value, KeyData>::MergePredecessors(
    base::Vector<const Snapshot> predecessors, const MergeFun& merge_fun,
    const ChangeCallback& change_callback) {
  CHECK_LE(predecessors.size(), std::numeric_limits<uint32_t>::max());
  uint32_t predecessor_count = static_cast<uint32_t>(predecessors.size());
  if (predecessor_count < 1) return;

  // The merging works by reserving `predecessor_count` many slots in
  // `merge_values_` for every key that we find while going through the
  // predecessor logs. There, we place the values of the corresponding
  // predecessors, so that we can finally call the `merge_fun` by creating a
  // `base::Vector` pointing to the collected values inside of `merge_values_`.
  DCHECK(merge_values_.empty());
  DCHECK(merging_entries_.empty());
  SnapshotData* common_ancestor = current_snapshot_->parent;

  // Collect all the entries that require merging. For this, we walk the logs of
  // the predecessors backwards until reaching the common ancestor.
  for (uint32_t i = 0; i < predecessor_count; ++i) {
    for (SnapshotData* predecessor = predecessors[i].data_;
         predecessor != common_ancestor; predecessor = predecessor->parent) {
      base::Vector<LogEntry> log_entries = LogEntries(predecessor);
      for (const LogEntry& entry : base::Reversed(log_entries)) {
        RecordMergeValue(entry.table_entry, entry.new_value, i,
                         predecessor_count);
      }
    }
  }
  // Actually perform the merging by calling the merge function and modifying
  // the table.
  for (TableEntry* entry : merging_entries_) {
    Key key{*entry};
    Value value = merge_fun(
        key, base::VectorOf<const Value>(&merge_values_[entry->merge_offset],
                                         predecessor_count));
    Value old_value = entry->value;
    if (Set(key, std::move(value))) {
      change_callback(key, old_value, entry->value);
    }
  }
}

// ChangeTrackingSnapshotTable extends SnapshotTable by automatically invoking
// OnNewKey and OnValueChange on the subclass whenever the table state changes.
// This makes it easy to maintain consistent additional tables for faster lookup
// of the state of the snapshot table, similar to how secondary indices can
// speed-up lookups in database tables.
// For example usage, see TEST_F(SnapshotTableTest, ChangeTrackingSnapshotTable)
// in test/unittests/compiler/turboshaft/snapshot-table-unittest.cc.
template <class Derived, class Value, class KeyData = NoKeyData>
class ChangeTrackingSnapshotTable : public SnapshotTable<Value, KeyData> {
 public:
  using Super = SnapshotTable<Value, KeyData>;
  using Super::Super;
  using typename Super::Key;
  using typename Super::Snapshot;

  void StartNewSnapshot(base::Vector<const Snapshot> predecessors) {
    Super::StartNewSnapshot(
        predecessors,
        [this](Key key, const Value& old_value, const Value& new_value) {
          static_cast<Derived*>(this)->OnValueChange(key, old_value, new_value);
        });
  }
  void StartNewSnapshot(std::initializer_list<Snapshot> predecessors = {}) {
    StartNewSnapshot(base::VectorOf(predecessors));
  }
  void StartNewSnapshot(Snapshot parent) { StartNewSnapshot({parent}); }
  template <class MergeFun,
            std::enable_if_t<std::is_invocable_v<
                MergeFun, Key, base::Vector<const Value>>>* = nullptr>
  void StartNewSnapshot(base::Vector<const Snapshot> predecessors,
                        const MergeFun& merge_fun) {
    Super::StartNewSnapshot(
        predecessors, merge_fun,
        [this](Key key, const Value& old_value, const Value& new_value) {
          static_cast<Derived*>(this)->OnValueChange(key, old_value, new_value);
        });
  }
  template <class MergeFun,
            std::enable_if_t<std::is_invocable_v<
                MergeFun, Key, base::Vector<const Value>>>* = nullptr>
  void StartNewSnapshot(std::initializer_list<Snapshot> predecessors,
                        const MergeFun& merge_fun) {
    StartNewSnapshot(base::VectorOf(predecessors), merge_fun);
  }

  void Set(Key key, Value new_value) {
    Value old_value = Super::Get(key);
    if (Super::Set(key, std::move(new_value))) {
      static_cast<Derived*>(this)->OnValueChange(key, old_value,
                                                 Super::Get(key));
    }
  }

  void SetNoNotify(Key key, Value new_value) {
    Super::Set(key, std::move(new_value));
  }

  Key NewKey(KeyData data, Value initial_value = Value{}) {
    Key key = Super::NewKey(std::move(data), std::move(initial_value));
    static_cast<Derived*>(this)->OnNewKey(key, Super::Get(key));
    return key;
  }
  Key NewKey(Value initial_value = Value{}) {
    return NewKey(KeyData{}, initial_value);
  }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_SNAPSHOT_TABLE_H_
                                                      node-23.7.0/deps/v8/src/compiler/turboshaft/stack-check-lowering-reducer.h                          0000664 0000000 0000000 00000011470 14746647661 0026436 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_STACK_CHECK_LOWERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_STACK_CHECK_LOWERING_REDUCER_H_

#include "src/compiler/globals.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/uniform-reducer-adapter.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

template <class Next>
class StackCheckLoweringReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(StackCheckLowering)

  V<None> REDUCE(JSStackCheck)(V<Context> context,
                               OptionalV<FrameState> frame_state,
                               JSStackCheckOp::Kind kind) {
    switch (kind) {
      case JSStackCheckOp::Kind::kFunctionEntry: {
        // Loads of the stack limit should not be load-eliminated as it can be
        // modified by another thread.
        V<WordPtr> limit =
            __ Load(__ ExternalConstant(
                        ExternalReference::address_of_jslimit(isolate())),
                    LoadOp::Kind::RawAligned().NotLoadEliminable(),
                    MemoryRepresentation::UintPtr());

        IF_NOT (LIKELY(__ StackPointerGreaterThan(
                    limit, StackCheckKind::kJSFunctionEntry))) {
          __ CallRuntime_StackGuardWithGap(isolate(), frame_state.value(),
                                           context, __ StackCheckOffset());
        }
        break;
      }
      case JSStackCheckOp::Kind::kBuiltinEntry: {
        V<WordPtr> stack_limit = __ LoadOffHeap(
            __ ExternalConstant(
                ExternalReference::address_of_jslimit(isolate())),
            MemoryRepresentation::UintPtr());
        IF_NOT (LIKELY(__ StackPointerGreaterThan(
                    stack_limit, StackCheckKind::kCodeStubAssembler))) {
          __ CallRuntime_StackGuard(isolate(), context);
        }
        break;
      }
      case JSStackCheckOp::Kind::kLoop: {
        V<Word32> limit = __ Load(
            __ ExternalConstant(
                ExternalReference::address_of_no_heap_write_interrupt_request(
                    isolate())),
            LoadOp::Kind::RawAligned().NotLoadEliminable(),
            MemoryRepresentation::Uint8());

        IF_NOT (LIKELY(__ Word32Equal(limit, 0))) {
          __ CallRuntime_HandleNoHeapWritesInterrupts(
              isolate(), frame_state.value(), context);
        }
        break;
      }
    }

    return V<None>::Invalid();
  }

#ifdef V8_ENABLE_WEBASSEMBLY
  V<None> REDUCE(WasmStackCheck)(WasmStackCheckOp::Kind kind) {
    if (kind == WasmStackCheckOp::Kind::kFunctionEntry && __ IsLeafFunction()) {
      return V<None>::Invalid();
    }

    // Loads of the stack limit should not be load-eliminated as it can be
    // modified by another thread.
    V<WordPtr> limit = __ Load(
        __ LoadRootRegister(), LoadOp::Kind::RawAligned().NotLoadEliminable(),
        MemoryRepresentation::UintPtr(), IsolateData::jslimit_offset());

    IF_NOT (LIKELY(__ StackPointerGreaterThan(limit, StackCheckKind::kWasm))) {
      // TODO(14108): Cache descriptor.
      V<WordPtr> builtin =
          __ RelocatableWasmBuiltinCallTarget(Builtin::kWasmStackGuard);
      const CallDescriptor* call_descriptor =
          compiler::Linkage::GetStubCallDescriptor(
              __ graph_zone(),                      // zone
              NoContextDescriptor{},                // descriptor
              0,                                    // stack parameter count
              CallDescriptor::kNoFlags,             // flags
              Operator::kNoProperties,              // properties
              StubCallMode::kCallWasmRuntimeStub);  // stub call mode
      const TSCallDescriptor* ts_call_descriptor =
          TSCallDescriptor::Create(call_descriptor, compiler::CanThrow::kNo,
                                   LazyDeoptOnThrow::kNo, __ graph_zone());
      // Pass custom effects to the `Call` node to mark it as non-writing.
      __ Call(
          builtin, {}, ts_call_descriptor,
          OpEffects().CanReadMemory().RequiredWhenUnused().CanCreateIdentity());
    }

    return V<None>::Invalid();
  }
#endif  // V8_ENABLE_WEBASSEMBLY

 private:
  Isolate* isolate() {
    if (!isolate_) isolate_ = __ data() -> isolate();
    return isolate_;
  }

  Isolate* isolate_ = nullptr;
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_STACK_CHECK_LOWERING_REDUCER_H_
                                                                                                                                                                                                        node-23.7.0/deps/v8/src/compiler/turboshaft/store-store-elimination-phase.cc                        0000664 0000000 0000000 00000002345 14746647661 0027034 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/store-store-elimination-phase.h"

#include "src/compiler/turboshaft/branch-elimination-reducer.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/late-load-elimination-reducer.h"
#include "src/compiler/turboshaft/loop-unrolling-reducer.h"
#include "src/compiler/turboshaft/machine-optimization-reducer.h"
#include "src/compiler/turboshaft/required-optimization-reducer.h"
#include "src/compiler/turboshaft/store-store-elimination-reducer-inl.h"
#include "src/compiler/turboshaft/value-numbering-reducer.h"
#include "src/compiler/turboshaft/variable-reducer.h"
#include "src/numbers/conversions-inl.h"

namespace v8::internal::compiler::turboshaft {

void StoreStoreEliminationPhase::Run(PipelineData* data, Zone* temp_zone) {
  turboshaft::CopyingPhase<
      LoopStackCheckElisionReducer, StoreStoreEliminationReducer,
      LateLoadEliminationReducer, MachineOptimizationReducer,
      BranchEliminationReducer, ValueNumberingReducer>::Run(data, temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/compiler/turboshaft/store-store-elimination-phase.h                         0000664 0000000 0000000 00000001217 14746647661 0026673 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct StoreStoreEliminationPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(StoreStoreElimination)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_PHASE_H_
                                                                                                                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/store-store-elimination-reducer-inl.h                   0000664 0000000 0000000 00000045310 14746647661 0030006 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_REDUCER_INL_H_
#define V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_REDUCER_INL_H_

#include <optional>

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/snapshot-table.h"
#include "src/compiler/turboshaft/uniform-reducer-adapter.h"
#include "src/objects/heap-object-inl.h"

namespace v8::internal::compiler::turboshaft {

// 1. StoreStoreEliminationReducer tries to identify and remove redundant
// stores. E.g. for an input like
//
//   let o = {};
//   o.x = 2;
//   o.y = 3;
//   o.x = 4;
//   use(o.x);
//
// we don't need the first store to `o.x` because the value is overwritten
// before it can be observed.
//
// The analysis considers loads and stores as corresponding pairs of `base` (the
// OpIndex that defines `o` in the above example) and `offset` (that is the
// static offset at which the field is stored inside the object). We run the
// analysis backwards and track potentially redundant stores in a
// `MaybeRedundantStoresTable` roughly as follows:
//
//   1. When we see a `base`+`offset` store and
//     a. `base`+`offset` is observable in the table, we keep the store, but
//        track following `base`+`offset` stores as unobservable in the table.
//     b. `base`+`offset` is unobservable in the table, we can eliminate the
//        store and keep the table unchanged.
//     c. `base`+`offset` is gc-observable in the table, we eliminate it only
//        if it is not an initializing store, otherwise, we keep it and update
//        the table accordingly.
//   2. When we see a `base`+`offset` load, we mark all stores to this `offset`
//      as observable in the table. Notice that we do this regardless of the
//      `base`, because they might alias potentially.
//   3. When we see an allocation, we mark all stores that are currently
//      unobservable, as gc-observable. The idea behind gc-observability is
//      that we do not observe the actual value stored, but we need to make
//      sure that the fields are written at least once, so that the GC does not
//      see uninitialized fields.
//   4. When we see another operation that can observe memory, we mark all
//      stores as observable.
//
// Notice that the table also tracks the `size` of the store, such that if
// fields are partially written, we don't incorrectly treat them as redundant to
// the full store (this can happen in strings for example).
//
// When the analysis reaches a branch, we combine values using traditional least
// upper bound operation on the `StoreObservability` lattice. After processing a
// loop header, we revisit the loop if the resulting state has changed until we
// reach a fixpoint.
//
//
// 2. StoreStoreEliminationReducer tries to merge 2 continuous 32-bits stores
// into a 64-bits one.
// When v8 create a new js object, it will initialize it's in object fields to
// some constant value after allocation, like `undefined`. When pointer
// compression is enabled, they are continuous 32-bits stores, and the store
// values are usually constants (heap object). This reducer will try to merge 2
// continuous 32-bits stores into a 64-bits one.

#include "src/compiler/turboshaft/define-assembler-macros.inc"

enum class StoreObservability {
  kUnobservable = 0,
  kGCObservable = 1,
  kObservable = 2,
};

inline std::ostream& operator<<(std::ostream& os,
                                StoreObservability observability) {
  switch (observability) {
    case StoreObservability::kUnobservable:
      return os << "Unobservable";
    case StoreObservability::kGCObservable:
      return os << "GCObservable";
    case StoreObservability::kObservable:
      return os << "Observable";
  }
}

struct MaybeRedundantStoresKeyData {
  OpIndex base;
  int32_t offset;
  uint8_t size;
  IntrusiveSetIndex active_keys_index = {};
};

class MaybeRedundantStoresTable
    : public ChangeTrackingSnapshotTable<MaybeRedundantStoresTable,
                                         StoreObservability,
                                         MaybeRedundantStoresKeyData> {
  using super =
      ChangeTrackingSnapshotTable<MaybeRedundantStoresTable, StoreObservability,
                                  MaybeRedundantStoresKeyData>;

 public:
  explicit MaybeRedundantStoresTable(const Graph& graph, Zone* zone)
      : ChangeTrackingSnapshotTable(zone),
        graph_(graph),
        block_to_snapshot_mapping_(zone),
        key_mapping_(zone),
        active_keys_(zone),
        successor_snapshots_(zone) {}

  void OnNewKey(Key key, StoreObservability value) {
    DCHECK_EQ(value, StoreObservability::kObservable);
    DCHECK(!active_keys_.Contains(key));
  }

  void OnValueChange(Key key, StoreObservability old_value,
                     StoreObservability new_value) {
    DCHECK_NE(old_value, new_value);
    if (new_value == StoreObservability::kObservable) {
      active_keys_.Remove(key);
    } else if (old_value == StoreObservability::kObservable) {
      active_keys_.Add(key);
    }
  }

  void BeginBlock(const Block* block) {
    // Seal the current block first.
    if (IsSealed()) {
      DCHECK_NULL(current_block_);
    } else {
      // If we bind a new block while the previous one is still unsealed, we
      // finalize it.
      Seal();
    }

    // Collect the snapshots of all successors.
    {
      auto successors = SuccessorBlocks(block->LastOperation(graph_));
      successor_snapshots_.clear();
      for (const Block* s : successors) {
        std::optional<Snapshot> s_snapshot =
            block_to_snapshot_mapping_[s->index()];
        // When we visit the loop for the first time, the loop header hasn't
        // been visited yet, so we ignore it.
        DCHECK_IMPLIES(!s_snapshot.has_value(), s->IsLoop());
        if (!s_snapshot.has_value()) continue;
        successor_snapshots_.push_back(*s_snapshot);
      }
    }

    // Start a new snapshot for this block by merging information from
    // successors.
    StartNewSnapshot(
        base::VectorOf(successor_snapshots_),
        [](Key, base::Vector<const StoreObservability> successors) {
          return static_cast<StoreObservability>(
              *std::max_element(successors.begin(), successors.end()));
        });

    current_block_ = block;
  }

  StoreObservability GetObservability(OpIndex base, int32_t offset,
                                      uint8_t size) {
    Key key = map_to_key(base, offset, size);
    if (key.data().size < size) return StoreObservability::kObservable;
    return Get(key);
  }

  void MarkStoreAsUnobservable(OpIndex base, int32_t offset, uint8_t size) {
    // We can only shadow stores to the exact same `base`+`offset` and keep
    // everything else because they might or might not alias.
    Key key = map_to_key(base, offset, size);
    // If the `size` we want to mark unobservable here is less than the size we
    // have seen for this key before, we do not overwrite the entire field, so
    // preceeding stores are not (fully) unobservable.
    if (size < key.data().size) return;
    Set(key, StoreObservability::kUnobservable);
  }

  void MarkPotentiallyAliasingStoresAsObservable(OpIndex base, int32_t offset) {
    // For now, we consider all stores to the same offset as potentially
    // aliasing. We might improve this to eliminate more precisely, if we have
    // some sort of aliasing information.
    for (Key key : active_keys_) {
      if (key.data().offset == offset)
        Set(key, StoreObservability::kObservable);
    }
  }

  void MarkAllStoresAsObservable() {
    for (Key key : active_keys_) {
      Set(key, StoreObservability::kObservable);
    }
  }

  void MarkAllStoresAsGCObservable() {
    for (Key key : active_keys_) {
      auto current = Get(key);
      DCHECK_NE(current, StoreObservability::kObservable);
      if (current == StoreObservability::kUnobservable) {
        Set(key, StoreObservability::kGCObservable);
      }
    }
  }

  void Seal(bool* snapshot_has_changed = nullptr) {
    DCHECK(!IsSealed());
    DCHECK_NOT_NULL(current_block_);
    DCHECK(current_block_->index().valid());
    auto& snapshot = block_to_snapshot_mapping_[current_block_->index()];
    if (!snapshot_has_changed) {
      snapshot = super::Seal();
    } else if (!snapshot.has_value()) {
      *snapshot_has_changed = true;
      snapshot = super::Seal();
    } else {
      auto new_snapshot = super::Seal();
      *snapshot_has_changed = false;
      StartNewSnapshot(
          base::VectorOf({snapshot.value(), new_snapshot}),
          [&](Key key, base::Vector<const StoreObservability> successors) {
            DCHECK_LE(successors[0], successors[1]);
            if (successors[0] != successors[1]) *snapshot_has_changed = true;
            return static_cast<StoreObservability>(
                *std::max_element(successors.begin(), successors.end()));
          });
      snapshot = super::Seal();
    }
    current_block_ = nullptr;
  }

  void Print(std::ostream& os, const char* sep = "\n") const {
    bool first = true;
    for (Key key : active_keys_) {
      os << (first ? "" : sep) << key.data().base.id() << "@"
         << key.data().offset << ": " << Get(key);
      first = false;
    }
  }

 private:
  Key map_to_key(OpIndex base, int32_t offset, uint8_t size) {
    std::pair p{base, offset};
    auto it = key_mapping_.find(p);
    if (it != key_mapping_.end()) return it->second;
    Key new_key = NewKey(MaybeRedundantStoresKeyData{base, offset, size},
                         StoreObservability::kObservable);
    key_mapping_.emplace(p, new_key);
    return new_key;
  }
  struct GetActiveKeysIndex {
    IntrusiveSetIndex& operator()(Key key) const {
      return key.data().active_keys_index;
    }
  };

  const Graph& graph_;
  GrowingBlockSidetable<std::optional<Snapshot>> block_to_snapshot_mapping_;
  ZoneAbslFlatHashMap<std::pair<OpIndex, int32_t>, Key> key_mapping_;
  // In `active_keys_`, we track the keys of all stores that arge gc-observable
  // or unobservable. Keys that are mapped to the default value (observable) are
  // removed from the `active_keys_`.
  ZoneIntrusiveSet<Key, GetActiveKeysIndex> active_keys_;
  const Block* current_block_ = nullptr;
  // {successor_snapshots_} and {temp_key_vector_} are used as temporary vectors
  // inside functions. We store them as members to avoid reallocation.
  ZoneVector<Snapshot> successor_snapshots_;
};

class RedundantStoreAnalysis {
 public:
  RedundantStoreAnalysis(const Graph& graph, Zone* phase_zone)
      : graph_(graph), table_(graph, phase_zone) {}

  void Run(ZoneSet<OpIndex>& eliminable_stores,
           ZoneMap<OpIndex, uint64_t>& mergeable_store_pairs) {
    eliminable_stores_ = &eliminable_stores;
    mergeable_store_pairs_ = &mergeable_store_pairs;
    for (uint32_t processed = graph_.block_count(); processed > 0;
         --processed) {
      BlockIndex block_index = static_cast<BlockIndex>(processed - 1);

      const Block& block = graph_.Get(block_index);
      ProcessBlock(block);

      // If this block is a loop header, check if this loop needs to be
      // revisited.
      if (block.IsLoop()) {
        DCHECK(!table_.IsSealed());
        bool needs_revisit = false;
        table_.Seal(&needs_revisit);
        if (needs_revisit) {
          Block* back_edge = block.LastPredecessor();
          DCHECK_GE(back_edge->index(), block_index);
          processed = back_edge->index().id() + 1;
        }
      }
    }
    eliminable_stores_ = nullptr;
    mergeable_store_pairs_ = nullptr;
  }

  void ProcessBlock(const Block& block) {
    table_.BeginBlock(&block);

    auto op_range = graph_.OperationIndices(block);
    for (auto it = op_range.end(); it != op_range.begin();) {
      --it;
      OpIndex index = *it;
      const Operation& op = graph_.Get(index);

      switch (op.opcode) {
        case Opcode::kStore: {
          const StoreOp& store = op.Cast<StoreOp>();
          // TODO(nicohartmann@): Use the new effect flags to distinguish heap
          // access once available.
          const bool is_on_heap_store = store.kind.tagged_base;
          const bool is_field_store = !store.index().valid();
          const uint8_t size = store.stored_rep.SizeInBytes();
          // For now we consider only stores of fields of objects on the heap.
          if (is_on_heap_store && is_field_store) {
            bool is_eliminable_store = false;
            switch (table_.GetObservability(store.base(), store.offset, size)) {
              case StoreObservability::kUnobservable:
                eliminable_stores_->insert(index);
                last_field_initialization_store_ = OpIndex::Invalid();
                is_eliminable_store = true;
                break;
              case StoreObservability::kGCObservable:
                if (store.maybe_initializing_or_transitioning) {
                  // We cannot eliminate this store, but we mark all following
                  // stores to the same `base+offset` as unobservable.
                  table_.MarkStoreAsUnobservable(store.base(), store.offset,
                                                 size);
                } else {
                  eliminable_stores_->insert(index);
                  last_field_initialization_store_ = OpIndex::Invalid();
                  is_eliminable_store = true;
                }
                break;
              case StoreObservability::kObservable:
                // We cannot eliminate this store, but we mark all following
                // stores to the same `base+offset` as unobservable.
                table_.MarkStoreAsUnobservable(store.base(), store.offset,
                                               size);
                break;
            }

            // Try to merge 2 consecutive 32-bit stores into a single 64-bit
            // one.
            if (COMPRESS_POINTERS_BOOL && !is_eliminable_store &&
                store.maybe_initializing_or_transitioning &&
                store.kind == StoreOp::Kind::TaggedBase() &&
                store.write_barrier == WriteBarrierKind::kNoWriteBarrier &&
                store.stored_rep.IsCompressibleTagged()) {
              if (last_field_initialization_store_.valid() &&
                  graph_.NextIndex(index) == last_field_initialization_store_) {
                const StoreOp& store0 = store;
                const StoreOp& store1 =
                    graph_.Get(last_field_initialization_store_)
                        .Cast<StoreOp>();

                DCHECK(!store0.index().valid());
                DCHECK(!store1.index().valid());

                const ConstantOp* c0 =
                    graph_.Get(store0.value()).TryCast<ConstantOp>();
                const ConstantOp* c1 =
                    graph_.Get(store1.value()).TryCast<ConstantOp>();

                // TODO(dmercadier): for now, we only apply this optimization
                // when storing read-only values, because otherwise the GC will
                // lose track of Handles when we convert them to a raw Word64.
                // However, if we were to keep the reloc info up-to-date, then
                // this might work for any object. To do this, we might need to
                // delay this optimization to later (instruction selector for
                // instance).
                if (c0 && c1 && c0->kind == ConstantOp::Kind::kHeapObject &&
                    c1->kind == ConstantOp::Kind::kHeapObject &&
                    store1.offset - store0.offset == 4 &&
                    InReadOnlySpace(*c0->handle()) &&
                    InReadOnlySpace(*c1->handle())) {
                  uint32_t high = static_cast<uint32_t>(c1->handle()->ptr());
                  uint32_t low = static_cast<uint32_t>(c0->handle()->ptr());
#if V8_TARGET_BIG_ENDIAN
                  uint64_t merged = make_uint64(low, high);
#else
                  uint64_t merged = make_uint64(high, low);
#endif
                  mergeable_store_pairs_->insert({index, merged});

                  eliminable_stores_->insert(last_field_initialization_store_);
                  last_field_initialization_store_ = OpIndex::Invalid();
                }

              } else {
                last_field_initialization_store_ = index;
              }
            }
          }
          break;
        }
        case Opcode::kLoad: {
          const LoadOp& load = op.Cast<LoadOp>();
          // TODO(nicohartmann@): Use the new effect flags to distinguish heap
          // access once available.
          const bool is_on_heap_load = load.kind.tagged_base;
          const bool is_field_load = !load.index().valid();
          // For now we consider only loads of fields of objects on the heap.
          if (is_on_heap_load && is_field_load) {
            table_.MarkPotentiallyAliasingStoresAsObservable(load.base(),
                                                             load.offset);
          }
          break;
        }
        default: {
          OpEffects effects = op.Effects();
          if (effects.can_read_mutable_memory()) {
            table_.MarkAllStoresAsObservable();
          } else if (effects.requires_consistent_heap()) {
            table_.MarkAllStoresAsGCObservable();
          }
        } break;
      }
    }
  }

 private:
  const Graph& graph_;
  MaybeRedundantStoresTable table_;
  ZoneSet<OpIndex>* eliminable_stores_ = nullptr;

  ZoneMap<OpIndex, uint64_t>* mergeable_store_pairs_ = nullptr;
  OpIndex last_field_initialization_store_ = OpIndex::Invalid();
};

template <class Next>
class StoreStoreEliminationReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(StoreStoreElimination)

  void Analyze() {
    analysis_.Run(eliminable_stores_, mergeable_store_pairs_);
    Next::Analyze();
  }

  OpIndex REDUCE_INPUT_GRAPH(Store)(OpIndex ig_index, const StoreOp& store) {
    if (eliminable_stores_.count(ig_index) > 0) {
      return OpIndex::Invalid();
    } else if (mergeable_store_pairs_.count(ig_index) > 0) {
      DCHECK(COMPRESS_POINTERS_BOOL);
      OpIndex value = __ Word64Constant(mergeable_store_pairs_[ig_index]);
      __ Store(__ MapToNewGraph(store.base()), value,
               StoreOp::Kind::TaggedBase(), MemoryRepresentation::Uint64(),
               WriteBarrierKind::kNoWriteBarrier, store.offset);
      return OpIndex::Invalid();
    }
    return Next::ReduceInputGraphStore(ig_index, store);
  }

 private:
  RedundantStoreAnalysis analysis_{Asm().input_graph(), Asm().phase_zone()};
  ZoneSet<OpIndex> eliminable_stores_{Asm().phase_zone()};
  ZoneMap<OpIndex, uint64_t> mergeable_store_pairs_{Asm().phase_zone()};
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_STORE_STORE_ELIMINATION_REDUCER_INL_H_
                                                                                                                                                                                                                                                                                                                        node-23.7.0/deps/v8/src/compiler/turboshaft/string-view.h                                           0000664 0000000 0000000 00000007312 14746647661 0023261 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2024 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_STRING_VIEW_H_
#define V8_COMPILER_TURBOSHAFT_STRING_VIEW_H_

#include "src/compiler/turboshaft/operations.h"

namespace v8::internal::compiler::turboshaft {

// `StringView` implements the `ForeachIterable` concept for iterating the
// characters of a string.
class StringView {
 public:
  using value_type = V<Word32>;
  using iterator_type = V<WordPtr>;

  StringView(const DisallowGarbageCollection& can_rely_on_no_gc,
             V<String> string, String::Encoding encoding,
             ConstOrV<WordPtr> start_index = 0,
             ConstOrV<WordPtr> character_count = V<WordPtr>::Invalid())
      : string_(string),
        encoding_(encoding),
        start_index_(start_index),
        character_count_(character_count),
        can_rely_on_no_gc_(&can_rely_on_no_gc) {}

  StringView(V<String> string, String::Encoding encoding,
             ConstOrV<WordPtr> start_index = 0,
             ConstOrV<WordPtr> character_count = V<WordPtr>::Invalid())
      : string_(string),
        encoding_(encoding),
        start_index_(start_index),
        character_count_(character_count),
        can_rely_on_no_gc_(nullptr) {}

  template <typename A>
  iterator_type Begin(A& assembler) {
    static_assert(OFFSET_OF_DATA_START(SeqOneByteString) ==
                  OFFSET_OF_DATA_START(SeqTwoByteString));
    const size_t data_offset = OFFSET_OF_DATA_START(SeqOneByteString);
    const int stride = (encoding_ == String::ONE_BYTE_ENCODING ? 1 : 2);
    if (can_rely_on_no_gc_ == nullptr) {
      // TODO(nicohartmann): If we cannot rely on no GC happening during
      // iteration, we cannot operate on raw inner pointers but have to
      // recompute the character address from the base on each dereferencing.
      UNIMPLEMENTED();
    }
    V<WordPtr> begin_offset = assembler.WordPtrAdd(
        assembler.BitcastTaggedToWordPtr(string_),
        assembler.WordPtrAdd(
            data_offset - kHeapObjectTag,
            assembler.WordPtrMul(assembler.resolve(start_index_), stride)));
    V<WordPtr> count;
    if (character_count_.is_constant()) {
      count = assembler.resolve(character_count_);
    } else if (character_count_.value().valid()) {
      count = character_count_.value();
    } else {
      // TODO(nicohartmann): Load from string.
      UNIMPLEMENTED();
    }
    end_offset_ =
        assembler.WordPtrAdd(begin_offset, assembler.WordPtrMul(count, stride));
    return begin_offset;
  }

  template <typename A>
  OptionalV<Word32> IsEnd(A& assembler, iterator_type current_iterator) const {
    return assembler.UintPtrLessThanOrEqual(end_offset_, current_iterator);
  }

  template <typename A>
  iterator_type Advance(A& assembler, iterator_type current_iterator) const {
    const int stride = (encoding_ == String::ONE_BYTE_ENCODING ? 1 : 2);
    return assembler.WordPtrAdd(current_iterator, stride);
  }

  template <typename A>
  value_type Dereference(A& assembler, iterator_type current_iterator) const {
    const auto loaded_rep = encoding_ == String::ONE_BYTE_ENCODING
                                ? MemoryRepresentation::Uint8()
                                : MemoryRepresentation::Uint16();
    return assembler.Load(current_iterator, LoadOp::Kind::RawAligned(),
                          loaded_rep);
  }

 private:
  V<String> string_;
  String::Encoding encoding_;
  ConstOrV<WordPtr> start_index_;
  ConstOrV<WordPtr> character_count_;
  V<WordPtr> end_offset_;
  const DisallowGarbageCollection* can_rely_on_no_gc_;
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_STRING_VIEW_H_
                                                                                                                                                                                                                                                                                                                      node-23.7.0/deps/v8/src/compiler/turboshaft/structural-optimization-reducer.h                       0000664 0000000 0000000 00000025701 14746647661 0027370 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_STRUCTURAL_OPTIMIZATION_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_STRUCTURAL_OPTIMIZATION_REDUCER_H_

#include <cstdio>

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/opmasks.h"
#include "src/zone/zone.h"

// The StructuralOptimizationReducer reducer is suitable for changing the
// graph in a way that doesn't reduce individual operations, rather changes
// the structure of the graph.
//
// We currently support a reduction which transforms if-else cascades
// that check if a given value is equal to a 32-bit constant from a given set
// into a switch with cases corresponding to the constants in the set.
//
// So for example code like:
//    [only pure ops 1]
//    if (x == 3) {
//      B1;
//    } else {
//      [only pure ops 2]
//      if (x == 5) {
//        B2;
//      } else {
//        B3;
//      }
//    }
//
// will be transformed to:
//    [only pure ops 1]
//    [only pure ops 2]
//    switch (x) {
//      case 3:
//        B1;
//      case 5:
//        B2;
//      default:
//        B3;
//    }
//
// Or represented graphically:
//                                                 [only pure ops 1]
//       [only pure ops 1]                         [only pure ops 2]
//           x == 3                                    Switch(x)
//           Branch                                    |    |   |
//           |    |                                -----    |   ------
//       -----    ------                    case 3 |        |        | default
//       |             |                           |        |        |
//     T |             | F                         v        |        |
//       v             v                           B1       |        v
//       B1      [only pure ops 2]    becomes               |        B3
//                   x == 5           ======>        case 5 |
//                   Branch                                 v
//                   |    |                                 B2
//               -----    ------
//               |             |
//             T |             | F
//               v             v
//              B2            B3
//

// TODO(mslekova): Introduce a flag and move to a common graph place.
// #define TRACE_REDUCTIONS
#ifdef TRACE_REDUCTIONS
#define TRACE(str, ...) \
  { PrintF(str, ##__VA_ARGS__); }
#else  // TRACE_REDUCTIONS
#define TRACE(str, ...)

#endif  // TRACE_REDUCTIONS

namespace v8::internal::compiler::turboshaft {

template <class Next>
class StructuralOptimizationReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(StructuralOptimization)

  OpIndex ReduceInputGraphBranch(OpIndex input_index, const BranchOp& branch) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphBranch(input_index, branch);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    TRACE("[structural] Calling ReduceInputGraphBranch for index: %u\n",
          static_cast<unsigned int>(input_index.id()));

    base::SmallVector<SwitchOp::Case, 16> cases;
    base::SmallVector<const Block*, 16> false_blocks;

    Block* current_if_true;
    Block* current_if_false;
    const BranchOp* current_branch = &branch;
    BranchHint current_branch_hint;
    BranchHint next_hint = BranchHint::kNone;

    OpIndex switch_var = OpIndex::Invalid();
    uint32_t value;
    while (true) {
      // If we encounter a condition that is not equality, we can't turn it
      // into a switch case.
      const Operation& cond =
          Asm().input_graph().Get(current_branch->condition());

      if (!cond.template Is<ComparisonOp>()) {
        // 'if(x==0)' may be optimized to 'if(x)', we should take this into
        // consideration.

        // The "false" destination will be inlined before the switch is emitted,
        // so it should only contain pure operations.
        if (!ContainsOnlyPureOps(current_branch->if_true,
                                 Asm().input_graph())) {
          TRACE("\t [break] End of only-pure-ops cascade reached.\n");
          break;
        }

        OpIndex current_var = current_branch->condition();
        if (!switch_var.valid()) {
          switch_var = current_var;
        } else if (switch_var != current_var) {
          TRACE("\t [bailout] Not all branches compare the same variable.\n");
          break;
        }
        value = 0;
        // The true/false of 'if(x)' is reversed from 'if(x==0)'
        current_if_true = current_branch->if_false;
        current_if_false = current_branch->if_true;
        const BranchHint hint = current_branch->hint;
        current_branch_hint = hint == BranchHint::kNone   ? BranchHint::kNone
                              : hint == BranchHint::kTrue ? BranchHint::kFalse
                                                          : BranchHint::kTrue;
      } else {
        const ComparisonOp* equal =
            cond.template TryCast<Opmask::kWord32Equal>();
        if (!equal) {
          TRACE(
              "\t [bailout] Branch with different condition than Word32 "
              "Equal.\n");
          break;
        }
        // MachineOptimizationReducer should normalize equality to put constants
        // right.
        const Operation& right_op = Asm().input_graph().Get(equal->right());
        if (!right_op.Is<Opmask::kWord32Constant>()) {
          TRACE(
              "\t [bailout] No Word32 constant on the right side of Equal.\n");
          break;
        }

        // The "false" destination will be inlined before the switch is emitted,
        // so it should only contain pure operations.
        if (!ContainsOnlyPureOps(current_branch->if_false,
                                 Asm().input_graph())) {
          TRACE("\t [break] End of only-pure-ops cascade reached.\n");
          break;
        }
        const ConstantOp& const_op = right_op.Cast<ConstantOp>();
        value = const_op.word32();

        // If we encounter equal to a different value, we can't introduce
        // a switch.
        OpIndex current_var = equal->left();
        if (!switch_var.valid()) {
          switch_var = current_var;
        } else if (switch_var != current_var) {
          TRACE("\t [bailout] Not all branches compare the same variable.\n");
          break;
        }

        current_if_true = current_branch->if_true;
        current_if_false = current_branch->if_false;
        current_branch_hint = current_branch->hint;
      }

      DCHECK(current_if_true && current_if_false);

      // We can't just use `current_branch->hint` for every case. Consider:
      //
      //     if (a) { }
      //     else if (b) { }
      //     else if (likely(c)) { }
      //     else if (d) { }
      //     else { }
      //
      // The fact that `c` is Likely doesn't tell anything about the likelyness
      // of `a` and `b` compared to `c`, which means that `c` shouldn't have the
      // Likely hint in the switch. However, since `c` is likely here, it means
      // that `d` and "default" are both unlikely, even in the switch.
      //
      // So, for the 1st case, we use `current_branch->hint`.
      // Then, when we encounter a Likely hint, we mark all of the subsequent
      // cases are Unlikely, but don't mark the current one as Likely. This is
      // done with the `next_hint` variable, which is initially kNone, but
      // because kFalse when we encounter a Likely branch.
      // We never set `next_hint` as kTrue as it would only apply to subsequent
      // cases and not to already-emitted cases. The only case that could thus
      // have a kTrue annotation is the 1st one.
      DCHECK_NE(next_hint, BranchHint::kTrue);
      BranchHint hint = next_hint;
      if (cases.size() == 0) {
        // The 1st case gets its original hint.
        hint = current_branch_hint;
      } else if (current_branch_hint == BranchHint::kFalse) {
        // For other cases, if the branch has a kFalse hint, we do use it,
        // regardless of `next_hint`.
        hint = BranchHint::kNone;
      }
      if (current_branch_hint == BranchHint::kTrue) {
        // This branch is likely true, which means that all subsequent cases are
        // unlikely.
        next_hint = BranchHint::kFalse;
      }

      // The current_if_true block becomes the corresponding switch case block.
      cases.emplace_back(value, Asm().MapToNewGraph(current_if_true), hint);

      // All pure ops from the if_false block should be executed before
      // the switch, except the last Branch operation (which we drop).
      false_blocks.push_back(current_if_false);

      // If we encounter a if_false block that doesn't end with a Branch,
      // this means we've reached the end of the cascade.
      const Operation& maybe_branch =
          current_if_false->LastOperation(Asm().input_graph());
      if (!maybe_branch.Is<BranchOp>()) {
        TRACE("\t [break] Reached end of the if-else cascade.\n");
        break;
      }

      // Iterate to the next if_false block in the cascade.
      current_branch = &maybe_branch.template Cast<BranchOp>();
    }

    // Probably better to keep short if-else cascades as they are.
    if (cases.size() <= 2) {
      TRACE("\t [bailout] Cascade with less than 2 levels of nesting.\n");
      goto no_change;
    }
    CHECK_EQ(cases.size(), false_blocks.size());

    // We're skipping the last false block, as it becomes the default block.
    for (size_t i = 0; i < false_blocks.size() - 1; ++i) {
      const Block* block = false_blocks[i];
      InlineAllOperationsWithoutLast(block);
    }

    TRACE("[reduce] Successfully emit a Switch with %zu cases.", cases.size());

    // The last current_if_true block that ends the cascade becomes the default
    // case.
    Block* default_block = current_if_false;
    Asm().Switch(
        Asm().MapToNewGraph(switch_var),
        Asm().output_graph().graph_zone()->CloneVector(base::VectorOf(cases)),
        Asm().MapToNewGraph(default_block), next_hint);
    return OpIndex::Invalid();
  }

 private:
  static bool ContainsOnlyPureOps(const Block* block, const Graph& graph) {
    for (const auto& op : base::IterateWithoutLast(graph.operations(*block))) {
      // We are moving the block content to before the switch, effectively
      // moving it before the previously existing branches.
      if (!op.Effects().hoistable_before_a_branch()) {
        return false;
      }
    }
    return true;
  }

  // Visits and emits {input_block} right now (ie, in the current block)
  // until the one before the last operation is reached.
  void InlineAllOperationsWithoutLast(const Block* input_block) {
    base::iterator_range<Graph::OpIndexIterator> all_ops =
        Asm().input_graph().OperationIndices(*input_block);

    for (OpIndex op : base::IterateWithoutLast(all_ops)) {
      Asm().InlineOp(op, input_block);
    }
  }
};

}  // namespace v8::internal::compiler::turboshaft

#undef TRACE

#endif  // V8_COMPILER_TURBOSHAFT_STRUCTURAL_OPTIMIZATION_REDUCER_H_
                                                               node-23.7.0/deps/v8/src/compiler/turboshaft/tracing.h                                               0000664 0000000 0000000 00000003303 14746647661 0022426 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TRACING_H_
#define V8_COMPILER_TURBOSHAFT_TRACING_H_

#include "src/base/contextual.h"
#include "src/codegen/optimized-compilation-info.h"
#include "src/compiler/graph-visualizer.h"
#include "src/compiler/turboshaft/graph-visualizer.h"
#include "src/compiler/turboshaft/graph.h"

namespace v8::internal::compiler::turboshaft {

class Tracing : public base::ContextualClass<Tracing> {
 public:
  explicit Tracing(OptimizedCompilationInfo* info) : info_(info) {
    DCHECK_NOT_NULL(info_);
  }

  using OperationDataPrinter =
      std::function<bool(std::ostream&, const Graph&, OpIndex)>;
  using BlockDataPrinter =
      std::function<bool(std::ostream&, const Graph&, BlockIndex)>;

  inline bool is_enabled() const { return info_->trace_turbo_json(); }

  void PrintPerOperationData(const char* data_name, const Graph& graph,
                             OperationDataPrinter printer) {
    DCHECK(printer);
    if (!is_enabled()) return;
    TurboJsonFile json_of(info_, std::ios_base::app);
    PrintTurboshaftCustomDataPerOperation(json_of, data_name, graph, printer);
  }
  void PrintPerBlockData(const char* data_name, const Graph& graph,
                         BlockDataPrinter printer) {
    DCHECK(printer);
    if (!is_enabled()) return;
    TurboJsonFile json_of(info_, std::ios_base::app);
    PrintTurboshaftCustomDataPerBlock(json_of, data_name, graph, printer);
  }

 private:
  OptimizedCompilationInfo* info_;
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TRACING_H_
                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/type-assertions-phase.cc                                0000664 0000000 0000000 00000002361 14746647661 0025407 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/type-assertions-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/assert-types-reducer.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/type-inference-reducer.h"
#include "src/compiler/turboshaft/value-numbering-reducer.h"

namespace v8::internal::compiler::turboshaft {

void TypeAssertionsPhase::Run(PipelineData* data, Zone* temp_zone) {
  UnparkedScopeIfNeeded scope(data->broker());

  turboshaft::TypeInferenceReducerArgs::Scope typing_args{
      turboshaft::TypeInferenceReducerArgs::InputGraphTyping::kPrecise,
      turboshaft::TypeInferenceReducerArgs::OutputGraphTyping::
          kPreserveFromInputGraph};

  turboshaft::CopyingPhase<turboshaft::AssertTypesReducer,
                           turboshaft::ValueNumberingReducer,
                           turboshaft::TypeInferenceReducer>::Run(data,
                                                                  temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                               node-23.7.0/deps/v8/src/compiler/turboshaft/type-assertions-phase.h                                 0000664 0000000 0000000 00000001151 14746647661 0025245 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPE_ASSERTIONS_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_TYPE_ASSERTIONS_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct TypeAssertionsPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(TypeAssertions)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPE_ASSERTIONS_PHASE_H_
                                                                                                                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/v8/src/compiler/turboshaft/type-inference-analysis.h                               0000664 0000000 0000000 00000050755 14746647661 0025552 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_ANALYSIS_H_
#define V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_ANALYSIS_H_

#include <limits>
#include <optional>

#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/compiler/common-operator.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/snapshot-table.h"
#include "src/compiler/turboshaft/typer.h"
#include "src/compiler/turboshaft/types.h"

namespace v8::internal::compiler::turboshaft {

// This analysis infers types for all operations. It does so by running a
// fixpoint analysis on the input graph in order to properly type PhiOps. The
// analysis visits blocks in order and computes operation types using
// Turboshaft's Typer. For Goto operations, the analysis checks if this is a
// back edge (the Goto's target is a loop block with an index less than the
// index of the current block). If this is the case, the analysis revisits the
// loop block (this is when ProcessBlock<true> is called). During this revisit,
// two things are different to the normal processing of a block:
//
// 1.) PhiOps are handled specially, which means applying proper
// widening/narrowing mechanics to accelerate termination while still computing
// somewhat precise types for Phis. 2.) If the type of any of the loop's Phis
// grows, we reset the index of unprocessed blocks to the block after the loop
// header, such that the entire loop body is revisited with the new type
// information.
class TypeInferenceAnalysis {
 public:
  explicit TypeInferenceAnalysis(const Graph& graph, Zone* phase_zone)
      : graph_(graph),
        // TODO(nicohartmann@): Might put types back into phase_zone once we
        // don't store them in the graph anymore.
        types_(graph.op_id_count(), Type{}, graph.graph_zone(), &graph),
        table_(phase_zone),
        op_to_key_mapping_(phase_zone, &graph),
        block_to_snapshot_mapping_(graph.block_count(), std::nullopt,
                                   phase_zone),
        predecessors_(phase_zone),
        graph_zone_(graph.graph_zone()) {}

  GrowingOpIndexSidetable<Type> Run(
      GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>*
          block_refinements = nullptr) {
#ifdef DEBUG
    block_refinements_ = block_refinements;
#endif  // DEBUG
    TURBOSHAFT_TRACE_TYPING("=== Running Type Inference Analysis ===\n");
    for (uint32_t unprocessed_index = 0;
         unprocessed_index < graph_.block_count();) {
      BlockIndex block_index = static_cast<BlockIndex>(unprocessed_index);
      ++unprocessed_index;
      const Block& block = graph_.Get(block_index);

#ifdef DEBUG
      if (V8_UNLIKELY(v8_flags.turboshaft_trace_typing)) {
        std::stringstream os;
        os << block.kind() << " " << block.index().id();
        TURBOSHAFT_TRACE_TYPING("=== %s ===\n", os.str().c_str());
      }
#endif  // DEBUG

      ProcessBlock<false>(block, &unprocessed_index);
    }
    TURBOSHAFT_TRACE_TYPING("=== Completed Type Inference Analysis ===\n");

    return std::move(types_);
  }

  template <bool revisit_loop_header>
  void ProcessBlock(const Block& block, uint32_t* unprocessed_index) {
    DCHECK_IMPLIES(revisit_loop_header, block.IsLoop());

    // Seal the current block first.
    if (table_.IsSealed()) {
      DCHECK_NULL(current_block_);
    } else {
      // If we process a new block while the previous one is still unsealed, we
      // finalize it.
      DCHECK_NOT_NULL(current_block_);
      DCHECK(current_block_->index().valid());
      block_to_snapshot_mapping_[current_block_->index()] = table_.Seal();
      current_block_ = nullptr;
    }

    // Collect the snapshots of all predecessors.
    {
      predecessors_.clear();
      for (const Block* pred : block.PredecessorsIterable()) {
        std::optional<table_t::Snapshot> pred_snapshot =
            block_to_snapshot_mapping_[pred->index()];
        if (pred_snapshot.has_value()) {
          predecessors_.push_back(pred_snapshot.value());
        } else {
          // The only case where we might not have a snapshot for the
          // predecessor is when we visit a loop header for the first time.
          DCHECK(block.IsLoop() && pred == block.LastPredecessor() &&
                 !revisit_loop_header);
        }
      }
      std::reverse(predecessors_.begin(), predecessors_.end());
    }

    // Start a new snapshot for this block by merging information from
    // predecessors.
    {
      auto MergeTypes = [&](table_t::Key,
                            base::Vector<const Type> predecessors) -> Type {
        DCHECK_GT(predecessors.size(), 0);
        Type result_type = predecessors[0];
        for (size_t i = 1; i < predecessors.size(); ++i) {
          result_type =
              Type::LeastUpperBound(result_type, predecessors[i], graph_zone_);
        }
        return result_type;
      };

      table_.StartNewSnapshot(base::VectorOf(predecessors_), MergeTypes);
    }

    // Check if the predecessor is a branch that allows us to refine a few
    // types.
    DCHECK_IMPLIES(revisit_loop_header, block.PredecessorCount() == 2);
    if (block.PredecessorCount() == 1) {
      Block* predecessor = block.LastPredecessor();
      const Operation& terminator = predecessor->LastOperation(graph_);
      if (const BranchOp* branch = terminator.TryCast<BranchOp>()) {
        DCHECK(branch->if_true == &block || branch->if_false == &block);
        RefineTypesAfterBranch(branch, &block, branch->if_true == &block);
      }
    }
    current_block_ = &block;

    bool loop_needs_revisit = false;
    auto op_range = graph_.OperationIndices(block);
    for (auto it = op_range.begin(); it != op_range.end(); ++it) {
      OpIndex index = *it;
      const Operation& op = graph_.Get(index);

      switch (op.opcode) {
        case Opcode::kBranch:
        case Opcode::kDeoptimize:
        case Opcode::kDeoptimizeIf:
        case Opcode::kFrameState:
        case Opcode::kReturn:
        case Opcode::kStore:
        case Opcode::kRetain:
        case Opcode::kUnreachable:
        case Opcode::kSwitch:
        case Opcode::kTuple:
        case Opcode::kStaticAssert:
        case Opcode::kDebugBreak:
        case Opcode::kDebugPrint:
#if V8_ENABLE_WEBASSEMBLY
        case Opcode::kGlobalSet:
        case Opcode::kTrapIf:
#endif
        case Opcode::kCheckException:
          // These operations do not produce any output that needs to be typed.
          DCHECK_EQ(0, op.outputs_rep().size());
          break;
        case Opcode::kCheckTurboshaftTypeOf:
          ProcessCheckTurboshaftTypeOf(index,
                                       op.Cast<CheckTurboshaftTypeOfOp>());
          break;
        case Opcode::kComparison:
          ProcessComparison(index, op.Cast<ComparisonOp>());
          break;
        case Opcode::kConstant:
          ProcessConstant(index, op.Cast<ConstantOp>());
          break;
        case Opcode::kFloatBinop:
          ProcessFloatBinop(index, op.Cast<FloatBinopOp>());
          break;
        case Opcode::kOverflowCheckedBinop:
          ProcessOverflowCheckedBinop(index, op.Cast<OverflowCheckedBinopOp>());
          break;
        case Opcode::kProjection:
          ProcessProjection(index, op.Cast<ProjectionOp>());
          break;
        case Opcode::kWordBinop:
          ProcessWordBinop(V<Word>::Cast(index), op.Cast<WordBinopOp>());
          break;
        case Opcode::kWord32PairBinop:
        case Opcode::kAtomicWord32Pair:
        case Opcode::kPendingLoopPhi:
          // Input graph must not contain these op codes.
          UNREACHABLE();
        case Opcode::kPhi:
          if constexpr (revisit_loop_header) {
            loop_needs_revisit =
                ProcessLoopPhi(index, op.Cast<PhiOp>()) || loop_needs_revisit;
          } else {
            ProcessPhi(index, op.Cast<PhiOp>());
          }
          break;
        case Opcode::kGoto: {
          const GotoOp& gto = op.Cast<GotoOp>();
          // Check if this is a backedge.
          if (gto.destination->IsLoop()) {
            if (gto.destination->index() < current_block_->index()) {
              ProcessBlock<true>(*gto.destination, unprocessed_index);
            } else if (gto.destination->index() == current_block_->index()) {
              // This is a single block loop. We must only revisit the current
              // header block if we actually need to, in order to prevent
              // infinite recursion.
              if (!revisit_loop_header || loop_needs_revisit) {
                ProcessBlock<true>(*gto.destination, unprocessed_index);
              }
            }
          }
          break;
        }

        default:
          // TODO(nicohartmann@): Support remaining operations. For now we
          // compute fallback types.
          if (op.outputs_rep().size() > 0) {
            constexpr bool allow_narrowing = false;
            constexpr bool is_fallback_for_unsupported_operation = true;
            SetType(index,
                    Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_),
                    allow_narrowing, is_fallback_for_unsupported_operation);
          }
          break;
        case Opcode::kLoadRootRegister:
          SetType(index,
                  Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_));
          break;
      }
    }

    if constexpr (revisit_loop_header) {
      if (loop_needs_revisit) {
        // This is a loop header and the loop body needs to be revisited. Reset
        // {unprocessed_index} to the loop header's successor.
        *unprocessed_index =
            std::min(*unprocessed_index, block.index().id() + 1);
      }
    }
  }

  void ProcessCheckTurboshaftTypeOf(OpIndex index,
                                    const CheckTurboshaftTypeOfOp& check) {
    Type input_type = GetType(check.input());

    if (input_type.IsSubtypeOf(check.type)) {
      TURBOSHAFT_TRACE_TYPING_OK(
          "CTOF %3d:%-40s\n  P: %3d:%-40s ~~> %s\n", index.id(),
          graph_.Get(index).ToString().substr(0, 40).c_str(),
          check.input().id(),
          graph_.Get(check.input()).ToString().substr(0, 40).c_str(),
          input_type.ToString().c_str());
    } else if (check.successful) {
      FATAL(
          "Checking type %s of operation %d:%s failed after it passed in a "
          "previous phase",
          check.type.ToString().c_str(), check.input().id(),
          graph_.Get(check.input()).ToString().c_str());
    } else {
      TURBOSHAFT_TRACE_TYPING_FAIL(
          "CTOF %3d:%-40s\n  F: %3d:%-40s ~~> %s\n", index.id(),
          graph_.Get(index).ToString().substr(0, 40).c_str(),
          check.input().id(),
          graph_.Get(check.input()).ToString().substr(0, 40).c_str(),
          input_type.ToString().c_str());
    }
  }

  void ProcessComparison(OpIndex index, const ComparisonOp& comparison) {
    Type left_type = GetType(comparison.left());
    Type right_type = GetType(comparison.right());

    Type result_type = Typer::TypeComparison(
        left_type, right_type, comparison.rep, comparison.kind, graph_zone_);
    SetType(index, result_type);
  }

  void ProcessConstant(OpIndex index, const ConstantOp& constant) {
    Type type = Typer::TypeConstant(constant.kind, constant.storage);
    SetType(index, type);
  }

  void ProcessFloatBinop(OpIndex index, const FloatBinopOp& binop) {
    Type left_type = GetType(binop.left());
    Type right_type = GetType(binop.right());

    Type result_type = Typer::TypeFloatBinop(left_type, right_type, binop.kind,
                                             binop.rep, graph_zone_);
    SetType(index, result_type);
  }

  bool ProcessLoopPhi(OpIndex index, const PhiOp& phi) {
    Type old_type = GetTypeAtDefinition(index);
    Type new_type = ComputeTypeForPhi(phi);

    if (old_type.IsInvalid()) {
      SetType(index, new_type);
      return true;
    }

    // If the new type is smaller, we narrow it without revisiting the loop.
    if (new_type.IsSubtypeOf(old_type)) {
      TURBOSHAFT_TRACE_TYPING_OK(
          "LOOP %3d:%-40s (FIXPOINT)\n  N:     %-40s ~~> %-40s\n", index.id(),
          graph_.Get(index).ToString().substr(0, 40).c_str(),
          old_type.ToString().c_str(), new_type.ToString().c_str());

      constexpr bool allow_narrowing = true;
      SetType(index, new_type, allow_narrowing);
      return false;
    }

    // Otherwise, the new type is larger and we widen and revisit the loop.
    TURBOSHAFT_TRACE_TYPING_OK(
        "LOOP %3d:%-40s (REVISIT)\n  W:     %-40s ~~> %-40s\n", index.id(),
        graph_.Get(index).ToString().substr(0, 40).c_str(),
        old_type.ToString().c_str(), new_type.ToString().c_str());

    if (!old_type.IsNone()) {
      new_type = Widen(old_type, new_type);
    }
    SetType(index, new_type);
    return true;
  }

  void ProcessOverflowCheckedBinop(OpIndex index,
                                   const OverflowCheckedBinopOp& binop) {
    Type left_type = GetType(binop.left());
    Type right_type = GetType(binop.right());

    Type result_type = Typer::TypeOverflowCheckedBinop(
        left_type, right_type, binop.kind, binop.rep, graph_zone_);
    SetType(index, result_type);
  }

  void ProcessPhi(OpIndex index, const PhiOp& phi) {
    Type result_type = ComputeTypeForPhi(phi);
    SetType(index, result_type);
  }

  void ProcessProjection(OpIndex index, const ProjectionOp& projection) {
    Type input_type = GetType(projection.input());

    Type result_type;
    if (input_type.IsNone()) {
      result_type = Type::None();
    } else if (input_type.IsTuple()) {
      const TupleType& tuple = input_type.AsTuple();
      DCHECK_LT(projection.index, tuple.size());
      result_type = tuple.element(projection.index);
      DCHECK(result_type.IsSubtypeOf(
          Typer::TypeForRepresentation(projection.rep)));
    } else {
      result_type = Typer::TypeForRepresentation(projection.rep);
    }

    SetType(index, result_type);
  }

  void ProcessWordBinop(V<Word> index, const WordBinopOp& binop) {
    Type left_type = GetType(binop.left());
    Type right_type = GetType(binop.right());

    Type result_type = Typer::TypeWordBinop(left_type, right_type, binop.kind,
                                            binop.rep, graph_zone_);
    SetType(index, result_type);
  }

  Type ComputeTypeForPhi(const PhiOp& phi) {
    // Word64 values are truncated to word32 implicitly, we need to handle this
    // here.
    auto MaybeTruncate = [&](Type t) -> Type {
      if (t.IsNone()) return t;
      if (phi.rep == RegisterRepresentation::Word32()) {
        return Typer::TruncateWord32Input(t, true, graph_zone_);
      }
      return t;
    };

    Type result_type =
        MaybeTruncate(GetTypeOrDefault(phi.inputs()[0], Type::None()));
    for (size_t i = 1; i < phi.inputs().size(); ++i) {
      Type input_type =
          MaybeTruncate(GetTypeOrDefault(phi.inputs()[i], Type::None()));
      result_type = Type::LeastUpperBound(result_type, input_type, graph_zone_);
    }
    return result_type;
  }

  void RefineTypesAfterBranch(const BranchOp* branch, const Block* new_block,
                              bool then_branch) {
    TURBOSHAFT_TRACE_TYPING_OK("Br   %3d:%-40s\n", graph_.Index(*branch).id(),
                               branch->ToString().substr(0, 40).c_str());

    Typer::BranchRefinements refinements(
        [this](OpIndex index) { return GetType(index); },
        [&](OpIndex index, const Type& refined_type) {
          RefineOperationType(new_block, index, refined_type,
                              then_branch ? 'T' : 'F');
        });

    // Inspect branch condition.
    const Operation& condition = graph_.Get(branch->condition());
    refinements.RefineTypes(condition, then_branch, graph_zone_);
  }

  void RefineOperationType(const Block* new_block, OpIndex op, const Type& type,
                           char case_for_tracing) {
    DCHECK(op.valid());
    DCHECK(!type.IsInvalid());

    TURBOSHAFT_TRACE_TYPING_OK("  %c: %3d:%-40s ~~> %s\n", case_for_tracing,
                               op.id(),
                               graph_.Get(op).ToString().substr(0, 40).c_str(),
                               type.ToString().c_str());

    auto key_opt = op_to_key_mapping_[op];
    DCHECK(key_opt.has_value());
    table_.Set(*key_opt, type);

#ifdef DEBUG
    if (block_refinements_) {
      (*block_refinements_)[new_block->index()].emplace_back(op, type);
    }
#endif

    // TODO(nicohartmann@): One could push the refined type deeper into the
    // operations.
  }

  void SetType(OpIndex index, Type result_type, bool allow_narrowing = false,
               bool is_fallback_for_unsupported_operation = false) {
    DCHECK(!result_type.IsInvalid());

    if (auto key_opt = op_to_key_mapping_[index]) {
      table_.Set(*key_opt, result_type);
      types_[index] = result_type;
    } else {
      auto key = table_.NewKey(Type::None());
      op_to_key_mapping_[index] = key;
      table_.Set(key, result_type);
      types_[index] = result_type;
    }

    if (!is_fallback_for_unsupported_operation) {
      TURBOSHAFT_TRACE_TYPING_OK(
          "Type %3d:%-40s ==> %s\n", index.id(),
          graph_.Get(index).ToString().substr(0, 40).c_str(),
          result_type.ToString().c_str());
    } else {
      // TODO(nicohartmann@): Remove the fallback case once all operations are
      // supported.
      TURBOSHAFT_TRACE_TYPING_FAIL(
          "TODO %3d:%-40s ==> %s\n", index.id(),
          graph_.Get(index).ToString().substr(0, 40).c_str(),
          result_type.ToString().c_str());
    }
  }

  Type GetTypeOrInvalid(const OpIndex index) {
    if (auto key = op_to_key_mapping_[index]) return table_.Get(*key);
    return Type::Invalid();
  }

  Type GetTypeOrDefault(OpIndex index, const Type& default_type) {
    Type t = GetTypeOrInvalid(index);
    if (t.IsInvalid()) return default_type;
    return t;
  }

  Type GetType(OpIndex index) {
    Type t = GetTypeOrInvalid(index);
    if (t.IsInvalid()) {
      // TODO(nicohartmann@): This is a fallback mechanism as long as not all
      // operations are properly typed. Remove this once typing is complete.
      const Operation& op = graph_.Get(index);
      return Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_);
    }
    return t;
  }

  Type GetTypeAtDefinition(OpIndex index) const { return types_[index]; }

  Type Widen(const Type& old_type, const Type& new_type) {
    if (new_type.IsAny()) return new_type;
    // We might have to relax this eventually and widen different types.
    DCHECK_EQ(old_type.kind(), new_type.kind());

    switch (old_type.kind()) {
      case Type::Kind::kWord32:
        // TODO(nicohartmann@): Reevaluate whether exponential widening is
        // better here.
        //
        // return WordOperationTyper<32>::WidenExponential(old_type.AsWord32(),
        // new_type.AsWord32(), graph_zone_);
        return WordOperationTyper<32>::WidenMaximal(
            old_type.AsWord32(), new_type.AsWord32(), graph_zone_);
      case Type::Kind::kWord64:
        // TODO(nicohartmann@): Reevaluate whether exponential widening is
        // better here.
        //
        // return WordOperationTyper<64>::WidenExponential(old_type.AsWord64(),
        // new_type.AsWord64(), graph_zone_);
        return WordOperationTyper<64>::WidenMaximal(
            old_type.AsWord64(), new_type.AsWord64(), graph_zone_);
      case Type::Kind::kFloat32:
        // TODO(nicohartmann@): Implement proper widening.
        return Float32Type::Any();
      case Type::Kind::kFloat64:
        // TODO(nicohartmann@): Implement proper widening.
        return Float64Type::Any();
      default:
        // TODO(nicohartmann@): Handle remaining cases.
        UNREACHABLE();
    }
  }

 private:
  const Graph& graph_;
  GrowingOpIndexSidetable<Type> types_;
  using table_t = SnapshotTable<Type>;
  table_t table_;
  const Block* current_block_ = nullptr;
  GrowingOpIndexSidetable<std::optional<table_t::Key>> op_to_key_mapping_;
  GrowingBlockSidetable<std::optional<table_t::Snapshot>>
      block_to_snapshot_mapping_;
  // {predecessors_} is used during merging, but we use an instance variable for
  // it, in order to save memory and not reallocate it for each merge.
  ZoneVector<table_t::Snapshot> predecessors_;
  Zone* graph_zone_;

#ifdef DEBUG
  // {block_refinements_} are only stored for tracing in Debug builds.
  GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>*
      block_refinements_ = nullptr;
#endif
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_ANALYSIS_H_
                   node-23.7.0/deps/v8/src/compiler/turboshaft/type-inference-reducer.h                                0000664 0000000 0000000 00000054117 14746647661 0025354 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_REDUCER_H_

#include <limits>
#include <optional>

#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/compiler/common-operator.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/sidetable.h"
#include "src/compiler/turboshaft/snapshot-table.h"
#include "src/compiler/turboshaft/tracing.h"
#include "src/compiler/turboshaft/type-inference-analysis.h"
#include "src/compiler/turboshaft/typer.h"
#include "src/compiler/turboshaft/types.h"
#include "src/compiler/turboshaft/uniform-reducer-adapter.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

template <typename Op>
V8_INLINE bool CanBeTyped(const Op& operation) {
  return operation.outputs_rep().size() > 0;
}

struct TypeInferenceReducerArgs
    : base::ContextualClass<TypeInferenceReducerArgs> {
  enum class InputGraphTyping {
    kNone,     // Do not compute types for the input graph.
    kPrecise,  // Run a complete fixpoint analysis on the input graph.
  };
  enum class OutputGraphTyping {
    kNone,                    // Do not compute types for the output graph.
    kPreserveFromInputGraph,  // Reuse types of the input graph where
                              // possible.
    kRefineFromInputGraph,  // Reuse types of the input graph and compute types
                            // for new nodes and more precise types where
                            // possible.
  };
  InputGraphTyping input_graph_typing;
  OutputGraphTyping output_graph_typing;

  TypeInferenceReducerArgs(InputGraphTyping input_graph_typing,
                           OutputGraphTyping output_graph_typing)
      : input_graph_typing(input_graph_typing),
        output_graph_typing(output_graph_typing) {}
};

// TypeInferenceReducer is the central component to infer types for Turboshaft
// graphs. It comes with different options for how the input and output graph
// should be typed:
//
// - InputGraphTyping::kNone: No types are computed for the input graph.
// - InputGraphTyping::kPrecise: We run a full fixpoint analysis on the input
// graph to infer the most precise types possible (see TypeInferenceAnalysis).
//
// - OutputGraphTyping::kNone: No types will be set for the output graph.
// - OutputGraphTyping::kPreserveFromInputGraph: Types from the input graph will
// be preserved for the output graph. Where this is not possible (e.g. new
// operations introduced during lowering), the output operation will be untyped.
// - OutputGraphTyping::kRefineFromInputGraph: Types from the input graph will
// be used where they provide additional precision (e.g loop phis). For new
// operations, the reducer reruns the typer to make sure that the output graph
// is fully typed.
//
// NOTE: The TypeInferenceReducer has to be the last reducer in the stack!
template <class Next>
class TypeInferenceReducer
    : public UniformReducerAdapter<TypeInferenceReducer, Next> {
  static_assert(next_is_bottom_of_assembler_stack<Next>::value);
  using table_t = SnapshotTable<Type>;

 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(TypeInference)

  using Adapter = UniformReducerAdapter<TypeInferenceReducer, Next>;
  using Args = TypeInferenceReducerArgs;

  TypeInferenceReducer() {
    // It is not reasonable to try to reuse input graph types if there are none.
    DCHECK_IMPLIES(args_.output_graph_typing ==
                       Args::OutputGraphTyping::kPreserveFromInputGraph,
                   args_.input_graph_typing != Args::InputGraphTyping::kNone);
  }

  void Analyze() {
    if (args_.input_graph_typing == Args::InputGraphTyping::kPrecise) {
#ifdef DEBUG
      GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>
          block_refinements(Asm().input_graph().block_count(), {},
                            Asm().phase_zone());
      input_graph_types_ = analyzer_.Run(&block_refinements);
      Tracing::Get().PrintPerBlockData(
          "Type Refinements", Asm().input_graph(),
          [&](std::ostream& stream, const turboshaft::Graph& graph,
              turboshaft::BlockIndex index) -> bool {
            const std::vector<std::pair<turboshaft::OpIndex, turboshaft::Type>>&
                refinements = block_refinements[index];
            if (refinements.empty()) return false;
            stream << "\\n";
            for (const auto& [op, type] : refinements) {
              stream << op << " : " << type << "\\n";
            }
            return true;
          });
#else
      input_graph_types_ = analyzer_.Run(nullptr);
#endif  // DEBUG
      Tracing::Get().PrintPerOperationData(
          "Types", Asm().input_graph(),
          [&](std::ostream& stream, const turboshaft::Graph& graph,
              turboshaft::OpIndex index) -> bool {
            turboshaft::Type type = input_graph_types_[index];
            if (!type.IsInvalid() && !type.IsNone()) {
              type.PrintTo(stream);
              return true;
            }
            return false;
          });
    }
    Next::Analyze();
  }

  Type GetInputGraphType(OpIndex ig_index) {
    return input_graph_types_[ig_index];
  }

  Type GetOutputGraphType(OpIndex og_index) { return GetType(og_index); }

  template <Opcode opcode, typename Continuation, typename... Ts>
  OpIndex ReduceOperation(Ts... args) {
    OpIndex index = Continuation{this}.Reduce(args...);
    if (!NeedsTyping(index)) return index;

    const Operation& op = Asm().output_graph().Get(index);
    if (CanBeTyped(op)) {
      Type type = Typer::TypeForRepresentation(
          Asm().output_graph().Get(index).outputs_rep(), Asm().graph_zone());
      SetType(index, type, true);
    }
    return index;
  }

  template <typename Op, typename Continuation>
  OpIndex ReduceInputGraphOperation(OpIndex ig_index, const Op& operation) {
    OpIndex og_index = Continuation{this}.ReduceInputGraph(ig_index, operation);
    if (!og_index.valid()) return og_index;
    if (args_.output_graph_typing == Args::OutputGraphTyping::kNone) {
      return og_index;
    }
    if (!CanBeTyped(operation)) return og_index;

    Type ig_type = GetInputGraphType(ig_index);
    DCHECK_IMPLIES(args_.input_graph_typing != Args::InputGraphTyping::kNone,
                   !ig_type.IsInvalid());
    if (!ig_type.IsInvalid()) {
      Type og_type = GetType(og_index);
      // If the type we have from the input graph is more precise, we keep it.
      if (og_type.IsInvalid() ||
          (ig_type.IsSubtypeOf(og_type) && !og_type.IsSubtypeOf(ig_type))) {
        RefineTypeFromInputGraph(og_index, og_type, ig_type);
      }
    }
    return og_index;
  }

  void Bind(Block* new_block) {
    Next::Bind(new_block);

    // Seal the current block first.
    if (table_.IsSealed()) {
      DCHECK_NULL(current_block_);
    } else {
      // If we bind a new block while the previous one is still unsealed, we
      // finalize it.
      DCHECK_NOT_NULL(current_block_);
      DCHECK(current_block_->index().valid());
      block_to_snapshot_mapping_[current_block_->index()] = table_.Seal();
      current_block_ = nullptr;
    }

    // Collect the snapshots of all predecessors.
    {
      predecessors_.clear();
      for (const Block* pred : new_block->PredecessorsIterable()) {
        std::optional<table_t::Snapshot> pred_snapshot =
            block_to_snapshot_mapping_[pred->index()];
        DCHECK(pred_snapshot.has_value());
        predecessors_.push_back(pred_snapshot.value());
      }
      std::reverse(predecessors_.begin(), predecessors_.end());
    }

    // Start a new snapshot for this block by merging information from
    // predecessors.
    {
      auto MergeTypes = [&](table_t::Key,
                            base::Vector<const Type> predecessors) -> Type {
        DCHECK_GT(predecessors.size(), 0);
        Type result_type = predecessors[0];
        for (size_t i = 1; i < predecessors.size(); ++i) {
          result_type = Type::LeastUpperBound(result_type, predecessors[i],
                                              Asm().graph_zone());
        }
        return result_type;
      };

      table_.StartNewSnapshot(base::VectorOf(predecessors_), MergeTypes);
    }

    // Check if the predecessor is a branch that allows us to refine a few
    // types.
    if (args_.output_graph_typing ==
        Args::OutputGraphTyping::kRefineFromInputGraph) {
      if (new_block->PredecessorCount() == 1) {
        Block* predecessor = new_block->LastPredecessor();
        const Operation& terminator =
            predecessor->LastOperation(Asm().output_graph());
        if (const BranchOp* branch = terminator.TryCast<BranchOp>()) {
          DCHECK(branch->if_true == new_block || branch->if_false == new_block);
          RefineTypesAfterBranch(branch, new_block,
                                 branch->if_true == new_block);
        }
      }
    }
    current_block_ = new_block;
  }

  void RefineTypesAfterBranch(const BranchOp* branch, Block* new_block,
                              bool then_branch) {
    const std::string branch_str = branch->ToString().substr(0, 40);
    USE(branch_str);
    TURBOSHAFT_TRACE_TYPING_OK("Br   %3d:%-40s\n",
                               Asm().output_graph().Index(*branch).id(),
                               branch_str.c_str());

    Typer::BranchRefinements refinements(
        [this](OpIndex index) { return GetType(index); },
        [&](OpIndex index, const Type& refined_type) {
          RefineOperationType(new_block, index, refined_type,
                              then_branch ? 'T' : 'F');
        });

    // Inspect branch condition.
    const Operation& condition = Asm().output_graph().Get(branch->condition());
    refinements.RefineTypes(condition, then_branch, Asm().graph_zone());
  }

  void RefineOperationType(Block* new_block, OpIndex op, const Type& type,
                           char case_for_tracing) {
    DCHECK(op.valid());
    DCHECK(!type.IsInvalid());

    TURBOSHAFT_TRACE_TYPING_OK(
        "  %c: %3d:%-40s ~~> %s\n", case_for_tracing, op.id(),
        Asm().output_graph().Get(op).ToString().substr(0, 40).c_str(),
        type.ToString().c_str());

    auto key_opt = op_to_key_mapping_[op];
    // We might not have a key for this value, because we are running in a mode
    // where we don't type all operations.
    if (key_opt.has_value()) {
      table_.Set(*key_opt, type);

#ifdef DEBUG
      std::vector<std::pair<OpIndex, Type>>& refinement =
          Asm().output_graph().block_type_refinement()[new_block->index()];
      refinement.push_back(std::make_pair(op, type));
#endif

      // TODO(nicohartmann@): One could push the refined type deeper into the
      // operations.
    }
  }

  OpIndex REDUCE(PendingLoopPhi)(OpIndex first, RegisterRepresentation rep) {
    OpIndex index = Next::ReducePendingLoopPhi(first, rep);
    if (!NeedsTyping(index)) return index;

    // There is not much we can do for pending loop phis, because we don't know
    // the type of the backedge yet, so we have to assume maximal type. If we
    // run with a typed input graph, we can refine this type using the input
    // graph's type (see ReduceInputGraphOperation).
    SetType(index, Typer::TypeForRepresentation(rep));
    return index;
  }

  OpIndex REDUCE(Phi)(base::Vector<const OpIndex> inputs,
                      RegisterRepresentation rep) {
    OpIndex index = Next::ReducePhi(inputs, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Type::None();
    for (const OpIndex input : inputs) {
      type = Type::LeastUpperBound(type, GetType(input), Asm().graph_zone());
    }
    SetType(index, type);
    return index;
  }

  OpIndex REDUCE(Constant)(ConstantOp::Kind kind, ConstantOp::Storage value) {
    OpIndex index = Next::ReduceConstant(kind, value);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeConstant(kind, value);
    SetType(index, type);
    return index;
  }

  V<Word32> REDUCE(Comparison)(V<Any> left, V<Any> right,
                               ComparisonOp::Kind kind,
                               RegisterRepresentation rep) {
    OpIndex index = Next::ReduceComparison(left, right, kind, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeComparison(GetType(left), GetType(right), rep, kind,
                                      Asm().graph_zone());
    SetType(index, type);
    return index;
  }

  V<Any> REDUCE(Projection)(V<Any> input, uint16_t idx,
                            RegisterRepresentation rep) {
    V<Any> index = Next::ReduceProjection(input, idx, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeProjection(GetType(input), idx);
    SetType(index, type);
    return index;
  }

  V<Word> REDUCE(WordBinop)(V<Word> left, V<Word> right, WordBinopOp::Kind kind,
                            WordRepresentation rep) {
    V<Word> index = Next::ReduceWordBinop(left, right, kind, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeWordBinop(GetType(left), GetType(right), kind, rep,
                                     Asm().graph_zone());
    SetType(index, type);
    return index;
  }

  OpIndex REDUCE(OverflowCheckedBinop)(V<Word> left, V<Word> right,
                                       OverflowCheckedBinopOp::Kind kind,
                                       WordRepresentation rep) {
    OpIndex index = Next::ReduceOverflowCheckedBinop(left, right, kind, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeOverflowCheckedBinop(GetType(left), GetType(right),
                                                kind, rep, Asm().graph_zone());
    SetType(index, type);
    return index;
  }

  V<Float> REDUCE(FloatBinop)(V<Float> left, V<Float> right,
                              FloatBinopOp::Kind kind,
                              FloatRepresentation rep) {
    V<Float> index = Next::ReduceFloatBinop(left, right, kind, rep);
    if (!NeedsTyping(index)) return index;

    Type type = Typer::TypeFloatBinop(GetType(left), GetType(right), kind, rep,
                                      Asm().graph_zone());
    SetType(index, type);
    return index;
  }

  OpIndex REDUCE(CheckTurboshaftTypeOf)(OpIndex input,
                                        RegisterRepresentation rep, Type type,
                                        bool successful) {
    Type input_type = GetType(input);
    if (input_type.IsSubtypeOf(type)) {
      OpIndex index = Next::ReduceCheckTurboshaftTypeOf(input, rep, type, true);
      TURBOSHAFT_TRACE_TYPING_OK(
          "CTOF %3d:%-40s\n  P: %3d:%-40s ~~> %s\n", index.id(),
          Asm().output_graph().Get(index).ToString().substr(0, 40).c_str(),
          input.id(),
          Asm().output_graph().Get(input).ToString().substr(0, 40).c_str(),
          input_type.ToString().c_str());
      return index;
    }
    if (successful) {
      FATAL(
          "Checking type %s of operation %d:%s failed after it passed in a "
          "previous phase",
          type.ToString().c_str(), input.id(),
          Asm().output_graph().Get(input).ToString().c_str());
    }
    OpIndex index =
        Next::ReduceCheckTurboshaftTypeOf(input, rep, type, successful);
    TURBOSHAFT_TRACE_TYPING_FAIL(
        "CTOF %3d:%-40s\n  F: %3d:%-40s ~~> %s\n", index.id(),
        Asm().output_graph().Get(index).ToString().substr(0, 40).c_str(),
        input.id(),
        Asm().output_graph().Get(input).ToString().substr(0, 40).c_str(),
        input_type.ToString().c_str());
    return index;
  }

  void RemoveLast(OpIndex index_of_last_operation) {
    if (op_to_key_mapping_[index_of_last_operation]) {
      op_to_key_mapping_[index_of_last_operation] = std::nullopt;
      TURBOSHAFT_TRACE_TYPING_OK(
          "REM  %3d:%-40s %-40s\n", index_of_last_operation.id(),
          Asm()
              .output_graph()
              .Get(index_of_last_operation)
              .ToString()
              .substr(0, 40)
              .c_str(),
          GetType(index_of_last_operation).ToString().substr(0, 40).c_str());
      output_graph_types_[index_of_last_operation] = Type::Invalid();
    }
    Next::RemoveLast(index_of_last_operation);
  }

 private:
  void RefineTypeFromInputGraph(OpIndex index, const Type& og_type,
                                const Type& ig_type) {
    // Refinement should happen when we just lowered the corresponding
    // operation, so we should be at the point where the operation is defined
    // (e.g. not in a refinement after a branch). So the current block must
    // contain the operation.
    DCHECK(!ig_type.IsInvalid());

    TURBOSHAFT_TRACE_TYPING_OK(
        "Refi %3d:%-40s\n  I:     %-40s ~~> %-40s\n", index.id(),
        Asm().output_graph().Get(index).ToString().substr(0, 40).c_str(),
        (og_type.IsInvalid() ? "invalid" : og_type.ToString().c_str()),
        ig_type.ToString().c_str());

    RefineOperationType(Asm().current_block(), index, ig_type, 'I');
  }

  Type GetTypeOrInvalid(OpIndex index) {
    if (auto key = op_to_key_mapping_[index]) return table_.Get(*key);
    return Type::Invalid();
  }

  Type GetTupleType(const TupleOp& tuple) {
    base::SmallVector<Type, 4> tuple_types;
    for (OpIndex input : tuple.inputs()) {
      tuple_types.push_back(GetType(input));
    }
    return TupleType::Tuple(base::VectorOf(tuple_types), Asm().graph_zone());
  }

  Type GetType(OpIndex index) {
    Type type = GetTypeOrInvalid(index);
    if (type.IsInvalid()) {
      const Operation& op = Asm().output_graph().Get(index);
      if (op.Is<TupleOp>()) {
        return GetTupleType(op.Cast<TupleOp>());
      } else {
        return Typer::TypeForRepresentation(op.outputs_rep(),
                                            Asm().graph_zone());
      }
    }
    return type;
  }

  void SetType(OpIndex index, const Type& result_type,
               bool is_fallback_for_unsupported_operation = false) {
    DCHECK(!result_type.IsInvalid());

    if (auto key_opt = op_to_key_mapping_[index]) {
      table_.Set(*key_opt, result_type);
      DCHECK(result_type.IsSubtypeOf(output_graph_types_[index]));
      output_graph_types_[index] = result_type;
      DCHECK(!output_graph_types_[index].IsInvalid());
    } else {
      auto key = table_.NewKey(Type::None());
      op_to_key_mapping_[index] = key;
      table_.Set(key, result_type);
      output_graph_types_[index] = result_type;
    }

    if (!is_fallback_for_unsupported_operation) {
      TURBOSHAFT_TRACE_TYPING_OK(
          "Type %3d:%-40s ==> %s\n", index.id(),
          Asm().output_graph().Get(index).ToString().substr(0, 40).c_str(),
          result_type.ToString().c_str());
    } else {
      // TODO(nicohartmann@): Remove the fallback case once all operations are
      // supported.
      TURBOSHAFT_TRACE_TYPING_FAIL(
          "TODO %3d:%-40s ==> %s\n", index.id(),
          Asm().output_graph().Get(index).ToString().substr(0, 40).c_str(),
          result_type.ToString().c_str());
    }
  }

// Verification is more difficult, now that the output graph uses types from the
// input graph. It is generally not possible to verify that the output graph's
// type is a subtype of the input graph's type, because the typer might not
// support a precise typing of the operations after the lowering.
// TODO(nicohartmann@): Evaluate new strategies for verification.
#if 0
#ifdef DEBUG
  void Verify(OpIndex input_index, OpIndex output_index) {
    DCHECK(input_index.valid());
    DCHECK(output_index.valid());

    const auto& input_type = Asm().input_graph().operation_types()[input_index];
    const auto& output_type = types_[output_index];

    if (input_type.IsInvalid()) return;
    DCHECK(!output_type.IsInvalid());

    const bool is_okay = output_type.IsSubtypeOf(input_type);

    TURBOSHAFT_TRACE_TYPING(
        "\033[%s %3d:%-40s %-40s\n     %3d:%-40s %-40s\033[0m\n",
        is_okay ? "32mOK  " : "31mFAIL", input_index.id(),
        Asm().input_graph().Get(input_index).ToString().substr(0, 40).c_str(),
        input_type.ToString().substr(0, 40).c_str(), output_index.id(),
        Asm().output_graph().Get(output_index).ToString().substr(0, 40).c_str(),
        output_type.ToString().substr(0, 40).c_str());

    if (V8_UNLIKELY(!is_okay)) {
      FATAL(
          "\033[%s %3d:%-40s %-40s\n     %3d:%-40s %-40s\033[0m\n",
          is_okay ? "32mOK  " : "31mFAIL", input_index.id(),
          Asm().input_graph().Get(input_index).ToString().substr(0, 40).c_str(),
          input_type.ToString().substr(0, 40).c_str(), output_index.id(),
          Asm()
              .output_graph()
              .Get(output_index)
              .ToString()
              .substr(0, 40)
              .c_str(),
          output_type.ToString().substr(0, 40).c_str());
    }
  }
#endif
#endif

  bool NeedsTyping(OpIndex index) const {
    return index.valid() && args_.output_graph_typing ==
                                Args::OutputGraphTyping::kRefineFromInputGraph;
  }

  TypeInferenceReducerArgs args_{TypeInferenceReducerArgs::Get()};
  GrowingOpIndexSidetable<Type> input_graph_types_{Asm().graph_zone(),
                                                   &Asm().input_graph()};
  GrowingOpIndexSidetable<Type>& output_graph_types_{
      Asm().output_graph().operation_types()};
  table_t table_{Asm().phase_zone()};
  const Block* current_block_ = nullptr;
  GrowingOpIndexSidetable<std::optional<table_t::Key>> op_to_key_mapping_{
      Asm().phase_zone(), &Asm().output_graph()};
  GrowingBlockSidetable<std::optional<table_t::Snapshot>>
      block_to_snapshot_mapping_{Asm().input_graph().block_count(),
                                 std::nullopt, Asm().phase_zone()};
  // {predecessors_} is used during merging, but we use an instance variable for
  // it, in order to save memory and not reallocate it for each merge.
  ZoneVector<table_t::Snapshot> predecessors_{Asm().phase_zone()};
  TypeInferenceAnalysis analyzer_{Asm().modifiable_input_graph(),
                                  Asm().phase_zone()};
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPE_INFERENCE_REDUCER_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/type-parser.cc                                          0000664 0000000 0000000 00000002254 14746647661 0023414 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/type-parser.h"

#include <optional>

namespace v8::internal::compiler::turboshaft {

std::optional<Type> TypeParser::ParseType() {
  if (ConsumeIf("Word32")) {
    if (IsNext("{")) return ParseSet<Word32Type>();
    if (IsNext("[")) return ParseRange<Word32Type>();
    return Word32Type::Any();
  } else if (ConsumeIf("Word64")) {
    if (IsNext("{")) return ParseSet<Word64Type>();
    if (IsNext("[")) return ParseRange<Word64Type>();
    return Word64Type::Any();
  } else if (ConsumeIf("Float32")) {
    // TODO(nicohartmann@): Handle NaN.
    if (IsNext("{")) return ParseSet<Float32Type>();
    if (IsNext("[")) return ParseRange<Float32Type>();
    return Float64Type::Any();
  } else if (ConsumeIf("Float64")) {
    // TODO(nicohartmann@): Handle NaN.
    if (IsNext("{")) return ParseSet<Float64Type>();
    if (IsNext("[")) return ParseRange<Float64Type>();
    return Float64Type::Any();
  } else {
    return std::nullopt;
  }
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                    node-23.7.0/deps/v8/src/compiler/turboshaft/type-parser.h                                           0000664 0000000 0000000 00000007717 14746647661 0023267 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPE_PARSER_H_
#define V8_COMPILER_TURBOSHAFT_TYPE_PARSER_H_

#include <optional>

#include "src/compiler/turboshaft/types.h"

namespace v8::internal::compiler::turboshaft {

// TypeParser is used to construct a Type from a string literal.
// It's primary use is the %CheckTurboshaftTypeOf intrinsic, which allows
// mjsunit tests to check the static type of expressions. Typically the string
// has to have the format that Type::ToString() would produce.
//
// Examples: "Word32", "Word64[30, 100]", "Float32{-1.02}", "Float64{3.2, 17.8}"
class TypeParser {
 public:
  explicit TypeParser(const std::string_view& str, Zone* zone)
      : str_(str), zone_(zone) {}

  std::optional<Type> Parse() {
    std::optional<Type> type = ParseType();
    // Skip trailing whitespace.
    while (pos_ < str_.length() && str_[pos_] == ' ') ++pos_;
    if (pos_ < str_.length()) return std::nullopt;
    return type;
  }

 private:
  std::optional<Type> ParseType();

  template <typename T>
  std::optional<T> ParseRange() {
    if (!ConsumeIf("[")) return std::nullopt;
    auto from = ReadValue<typename T::value_type>();
    if (!from) return std::nullopt;
    if (!ConsumeIf(",")) return std::nullopt;
    auto to = ReadValue<typename T::value_type>();
    if (!to) return std::nullopt;
    if (!ConsumeIf("]")) return std::nullopt;
    if constexpr (!std::is_same_v<T, Word32Type> &&
                  !std::is_same_v<T, Word64Type>) {
      CHECK_LE(*from, *to);
    }
    return T::Range(*from, *to, zone_);
  }

  template <typename T>
  std::optional<T> ParseSet() {
    if (!ConsumeIf("{")) return std::nullopt;
    auto elements = ParseSetElements<typename T::value_type>();
    if (!elements) return std::nullopt;
    if (!ConsumeIf("}")) return std::nullopt;
    CHECK_LT(0, elements->size());
    CHECK_LE(elements->size(), T::kMaxSetSize);
    return T::Set(*elements, zone_);
  }

  template <typename T>
  std::optional<std::vector<T>> ParseSetElements() {
    std::vector<T> elements;
    if (IsNext("}")) return elements;
    while (true) {
      auto element_opt = ReadValue<T>();
      if (!element_opt) return std::nullopt;
      elements.push_back(*element_opt);

      if (IsNext("}")) break;
      if (!ConsumeIf(",")) return std::nullopt;
    }
    base::sort(elements);
    elements.erase(std::unique(elements.begin(), elements.end()),
                   elements.end());
    return elements;
  }

  bool ConsumeIf(const std::string_view& prefix) {
    if (IsNext(prefix)) {
      pos_ += prefix.length();
      return true;
    }
    return false;
  }

  bool IsNext(const std::string_view& prefix) {
    // Skip leading whitespace.
    while (pos_ < str_.length() && str_[pos_] == ' ') ++pos_;
    if (pos_ >= str_.length()) return false;
    size_t remaining_length = str_.length() - pos_;
    if (prefix.length() > remaining_length) return false;
    return str_.compare(pos_, prefix.length(), prefix, 0, prefix.length()) == 0;
  }

  template <typename T>
  std::optional<T> ReadValue() {
    T result;
    size_t read = 0;
    // TODO(nicohartmann@): Ideally we want to avoid this string construction
    // (e.g. using std::from_chars).
    std::string s(str_.cbegin() + pos_, str_.cend());
    if constexpr (std::is_same_v<T, uint32_t>) {
      result = static_cast<uint32_t>(std::stoul(s, &read));
    } else if constexpr (std::is_same_v<T, uint64_t>) {
      result = std::stoull(s, &read);
    } else if constexpr (std::is_same_v<T, float>) {
      result = std::stof(s, &read);
    } else if constexpr (std::is_same_v<T, double>) {
      result = std::stod(s, &read);
    }
    if (read == 0) return std::nullopt;
    pos_ += read;
    return result;
  }

  std::string_view str_;
  Zone* zone_;
  size_t pos_ = 0;
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPE_PARSER_H_
                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/typed-optimizations-phase.cc                            0000664 0000000 0000000 00000002245 14746647661 0026273 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/typed-optimizations-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/type-inference-reducer.h"
#include "src/compiler/turboshaft/typed-optimizations-reducer.h"

namespace v8::internal::compiler::turboshaft {

void TypedOptimizationsPhase::Run(PipelineData* data, Zone* temp_zone) {
#ifdef DEBUG
  UnparkedScopeIfNeeded scope(data->broker(), v8_flags.turboshaft_trace_typing);
#endif

  turboshaft::TypeInferenceReducerArgs::Scope typing_args{
      turboshaft::TypeInferenceReducerArgs::InputGraphTyping::kPrecise,
      turboshaft::TypeInferenceReducerArgs::OutputGraphTyping::kNone};

  turboshaft::CopyingPhase<turboshaft::TypedOptimizationsReducer,
                           turboshaft::TypeInferenceReducer>::Run(data,
                                                                  temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                                                           node-23.7.0/deps/v8/src/compiler/turboshaft/typed-optimizations-phase.h                             0000664 0000000 0000000 00000001175 14746647661 0026136 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct TypedOptimizationsPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(TypedOptimizations)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_PHASE_H_
                                                                                                                                                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/typed-optimizations-reducer.h                           0000664 0000000 0000000 00000010521 14746647661 0026462 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/typer.h"
#include "src/compiler/turboshaft/uniform-reducer-adapter.h"

namespace v8::internal::compiler::turboshaft {

template <typename>
class TypeInferenceReducer;

template <typename Next>
class TypedOptimizationsReducer
    : public UniformReducerAdapter<TypedOptimizationsReducer, Next> {
#if defined(__clang__)
  // Typed optimizations require a typed graph.
  static_assert(next_contains_reducer<Next, TypeInferenceReducer>::value);
#endif

 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(TypedOptimizations)

  using Adapter = UniformReducerAdapter<TypedOptimizationsReducer, Next>;

  OpIndex ReduceInputGraphBranch(OpIndex ig_index, const BranchOp& operation) {
    if (!ShouldSkipOptimizationStep()) {
      Type condition_type = GetType(operation.condition());
      if (!condition_type.IsInvalid()) {
        if (condition_type.IsNone()) {
          Asm().Unreachable();
          return OpIndex::Invalid();
        }
        condition_type = Typer::TruncateWord32Input(condition_type, true,
                                                    Asm().graph_zone());
        DCHECK(condition_type.IsWord32());
        if (auto c = condition_type.AsWord32().try_get_constant()) {
          Block* goto_target = *c == 0 ? operation.if_false : operation.if_true;
          Asm().Goto(Asm().MapToNewGraph(goto_target));
          return OpIndex::Invalid();
        }
      }
    }
    return Adapter::ReduceInputGraphBranch(ig_index, operation);
  }

  template <typename Op, typename Continuation>
  OpIndex ReduceInputGraphOperation(OpIndex ig_index, const Op& operation) {
    if (!ShouldSkipOptimizationStep()) {
      Type type = GetType(ig_index);
      if (type.IsNone()) {
        // This operation is dead. Remove it.
        DCHECK(CanBeTyped(operation));
        Asm().Unreachable();
        return OpIndex::Invalid();
      } else if (!type.IsInvalid()) {
        // See if we can replace the operation by a constant.
        if (OpIndex constant = TryAssembleConstantForType(type);
            constant.valid()) {
          return constant;
        }
      }
    }

    // Otherwise just continue with reduction.
    return Continuation{this}.ReduceInputGraph(ig_index, operation);
  }

 private:
  // If {type} is a single value that can be respresented by a constant, this
  // function returns the index for a corresponding ConstantOp. It returns
  // OpIndex::Invalid otherwise.
  OpIndex TryAssembleConstantForType(const Type& type) {
    switch (type.kind()) {
      case Type::Kind::kWord32: {
        auto w32 = type.AsWord32();
        if (auto c = w32.try_get_constant()) {
          return Asm().Word32Constant(*c);
        }
        break;
      }
      case Type::Kind::kWord64: {
        auto w64 = type.AsWord64();
        if (auto c = w64.try_get_constant()) {
          return Asm().Word64Constant(*c);
        }
        break;
      }
      case Type::Kind::kFloat32: {
        auto f32 = type.AsFloat32();
        if (f32.is_only_nan()) {
          return Asm().Float32Constant(nan_v<32>);
        } else if (f32.is_only_minus_zero()) {
          return Asm().Float32Constant(-0.0f);
        } else if (auto c = f32.try_get_constant()) {
          return Asm().Float32Constant(*c);
        }
        break;
      }
      case Type::Kind::kFloat64: {
        auto f64 = type.AsFloat64();
        if (f64.is_only_nan()) {
          return Asm().Float64Constant(nan_v<64>);
        } else if (f64.is_only_minus_zero()) {
          return Asm().Float64Constant(-0.0);
        } else if (auto c = f64.try_get_constant()) {
          return Asm().Float64Constant(*c);
        }
        break;
      }
      default:
        break;
    }
    return OpIndex::Invalid();
  }

  Type GetType(const OpIndex index) {
    // Typed optimizations use the types of the input graph.
    return Asm().GetInputGraphType(index);
  }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPED_OPTIMIZATIONS_REDUCER_H_
                                                                                                                                                                               node-23.7.0/deps/v8/src/compiler/turboshaft/typer.cc                                                0000664 0000000 0000000 00000010761 14746647661 0022306 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/typer.h"

namespace v8::internal::compiler::turboshaft {

void Typer::BranchRefinements::RefineTypes(const Operation& condition,
                                           bool then_branch, Zone* zone) {
  if (const ComparisonOp* comparison = condition.TryCast<ComparisonOp>()) {
    Type lhs = type_getter_(comparison->left());
    Type rhs = type_getter_(comparison->right());

    bool is_signed, is_less_than;
    switch (comparison->kind) {
      case ComparisonOp::Kind::kEqual:
        // TODO(nicohartmann@): Add support for equality.
        return;
      case ComparisonOp::Kind::kSignedLessThan:
        is_signed = true;
        is_less_than = true;
        break;
      case ComparisonOp::Kind::kSignedLessThanOrEqual:
        is_signed = true;
        is_less_than = false;
        break;
      case ComparisonOp::Kind::kUnsignedLessThan:
        is_signed = false;
        is_less_than = true;
        break;
      case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
        is_signed = false;
        is_less_than = false;
        break;
    }

    Type l_refined;
    Type r_refined;

    if (lhs.IsNone() || rhs.IsNone()) {
      type_refiner_(comparison->left(), Type::None());
      type_refiner_(comparison->right(), Type::None());
      return;
    } else if (lhs.IsAny() || rhs.IsAny()) {
      // If either side has any type, there is not much we can do.
      return;
    }

    switch (comparison->rep.value()) {
      case RegisterRepresentation::Word32(): {
        if (is_signed) {
          // TODO(nicohartmann@): Support signed comparison.
          return;
        }
        Word32Type l = Typer::TruncateWord32Input(lhs, true, zone).AsWord32();
        Word32Type r = Typer::TruncateWord32Input(rhs, true, zone).AsWord32();
        Type l_restrict, r_restrict;
        using OpTyper = WordOperationTyper<32>;
        if (is_less_than) {
          std::tie(l_restrict, r_restrict) =
              then_branch
                  ? OpTyper::RestrictionForUnsignedLessThan_True(l, r, zone)
                  : OpTyper::RestrictionForUnsignedLessThan_False(l, r, zone);
        } else {
          std::tie(l_restrict, r_restrict) =
              then_branch
                  ? OpTyper::RestrictionForUnsignedLessThanOrEqual_True(l, r,
                                                                        zone)
                  : OpTyper::RestrictionForUnsignedLessThanOrEqual_False(l, r,
                                                                         zone);
        }

        // Special handling for word32 restriction, because the inputs might
        // have been truncated from word64 implicitly.
        l_refined = RefineWord32Type<true>(lhs, l_restrict, zone);
        r_refined = RefineWord32Type<true>(rhs, r_restrict, zone);
        break;
      }
      case RegisterRepresentation::Float64(): {
        Float64Type l = lhs.AsFloat64();
        Float64Type r = rhs.AsFloat64();
        Type l_restrict, r_restrict;
        using OpTyper = FloatOperationTyper<64>;
        if (is_less_than) {
          std::tie(l_restrict, r_restrict) =
              then_branch ? OpTyper::RestrictionForLessThan_True(l, r, zone)
                          : OpTyper::RestrictionForLessThan_False(l, r, zone);
        } else {
          std::tie(l_restrict, r_restrict) =
              then_branch
                  ? OpTyper::RestrictionForLessThanOrEqual_True(l, r, zone)
                  : OpTyper::RestrictionForLessThanOrEqual_False(l, r, zone);
        }

        l_refined = l_restrict.IsNone() ? Type::None()
                                        : Float64Type::Intersect(
                                              l, l_restrict.AsFloat64(), zone);
        r_refined = r_restrict.IsNone() ? Type::None()
                                        : Float64Type::Intersect(
                                              r, r_restrict.AsFloat64(), zone);
        break;
      }
      default:
        return;
    }

    // In some cases, the refined type is not a subtype of the old type,
    // because it cannot be represented precisely. In this case we keep the
    // old type to be stable.
    if (l_refined.IsSubtypeOf(lhs)) {
      type_refiner_(comparison->left(), l_refined);
    }
    if (r_refined.IsSubtypeOf(rhs)) {
      type_refiner_(comparison->right(), r_refined);
    }
  }
}

}  // namespace v8::internal::compiler::turboshaft
               node-23.7.0/deps/v8/src/compiler/turboshaft/typer.h                                                 0000664 0000000 0000000 00000173126 14746647661 0022155 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPER_H_
#define V8_COMPILER_TURBOSHAFT_TYPER_H_

#include <limits>

#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/types.h"

namespace v8::internal::compiler::turboshaft {

// Returns the array's least element, ignoring NaN.
// There must be at least one non-NaN element.
// Any -0 is converted to 0.
template <typename T, size_t N>
T array_min(const std::array<T, N>& a) {
  DCHECK_NE(0, N);
  T x = +std::numeric_limits<T>::infinity();
  for (size_t i = 0; i < N; ++i) {
    if (!std::isnan(a[i])) {
      x = std::min(a[i], x);
    }
  }
  DCHECK(!std::isnan(x));
  return x == T{0} ? T{0} : x;  // -0 -> 0
}

// Returns the array's greatest element, ignoring NaN.
// There must be at least one non-NaN element.
// Any -0 is converted to 0.
template <typename T, size_t N>
T array_max(const std::array<T, N>& a) {
  DCHECK_NE(0, N);
  T x = -std::numeric_limits<T>::infinity();
  for (size_t i = 0; i < N; ++i) {
    if (!std::isnan(a[i])) {
      x = std::max(a[i], x);
    }
  }
  DCHECK(!std::isnan(x));
  return x == T{0} ? T{0} : x;  // -0 -> 0
}

template <size_t Bits>
struct WordOperationTyper {
  static_assert(Bits == 32 || Bits == 64);
  using word_t = uint_type<Bits>;
  using type_t = WordType<Bits>;
  using ElementsVector = base::SmallVector<word_t, type_t::kMaxSetSize * 2>;
  static constexpr word_t max = std::numeric_limits<word_t>::max();

  static type_t FromElements(ElementsVector elements, Zone* zone) {
    base::sort(elements);
    auto it = std::unique(elements.begin(), elements.end());
    elements.pop_back(std::distance(it, elements.end()));
    DCHECK(!elements.empty());
    if (elements.size() <= type_t::kMaxSetSize) {
      return type_t::Set(elements, zone);
    }

    auto range =
        MakeRange(base::Vector<const word_t>{elements.data(), elements.size()});
    auto result = type_t::Range(range.first, range.second, zone);
    DCHECK(
        base::all_of(elements, [&](word_t e) { return result.Contains(e); }));
    return result;
  }

  static std::pair<word_t, word_t> MakeRange(const type_t& t) {
    if (t.is_range()) return t.range();
    DCHECK(t.is_set());
    return MakeRange(t.set_elements());
  }

  // This function tries to find a somewhat reasonable range for a given set of
  // values. If the elements span no more than half of the range, we just
  // construct the range from min(elements) to max(elements) Otherwise, we
  // consider a wrapping range because it is likely that there is a larger gap
  // in the middle of the elements. For that, we start with a wrapping range
  // from max(elements) to min(elements) and then incrementally add another
  // element either by increasing the 'to' or decreasing the 'from' of the
  // range, whichever leads to a smaller range.
  static std::pair<word_t, word_t> MakeRange(
      base::Vector<const word_t> elements) {
    DCHECK(!elements.empty());
    DCHECK(detail::is_unique_and_sorted(elements));
    if (elements[elements.size() - 1] - elements[0] <= max / 2) {
      // Construct a non-wrapping range.
      return {elements[0], elements[elements.size() - 1]};
    }
    // Construct a wrapping range.
    size_t from_index = elements.size() - 1;
    size_t to_index = 0;
    while (to_index + 1 < from_index) {
      if ((elements[to_index + 1] - elements[to_index]) <
          (elements[from_index] - elements[from_index - 1])) {
        ++to_index;
      } else {
        --from_index;
      }
    }
    return {elements[from_index], elements[to_index]};
  }

  static word_t distance(const std::pair<word_t, word_t>& range) {
    return distance(range.first, range.second);
  }
  static word_t distance(word_t from, word_t to) {
    return is_wrapping(from, to) ? (max - from + to) : to - from;
  }

  static bool is_wrapping(const std::pair<word_t, word_t>& range) {
    return is_wrapping(range.first, range.second);
  }
  static bool is_wrapping(word_t from, word_t to) { return from > to; }

  static type_t Add(const type_t& lhs, const type_t& rhs, Zone* zone) {
    if (lhs.is_any() || rhs.is_any()) return type_t::Any();

    // If both sides are decently small sets, we produce the product set (which
    // we convert to a range if it exceeds the set limit).
    if (lhs.is_set() && rhs.is_set()) {
      ElementsVector result_elements;
      for (int i = 0; i < lhs.set_size(); ++i) {
        for (int j = 0; j < rhs.set_size(); ++j) {
          result_elements.push_back(lhs.set_element(i) + rhs.set_element(j));
        }
      }
      return FromElements(std::move(result_elements), zone);
    }

    // Otherwise just construct a range.
    std::pair<word_t, word_t> x = MakeRange(lhs);
    std::pair<word_t, word_t> y = MakeRange(rhs);

    // If the result would not be a complete range, we compute it.
    // Check: (lhs.to - lhs.from + 1) + rhs.to - rhs.from < max
    // =====> (lhs.to - lhs.from + 1) < max - rhs.to + rhs.from
    // =====> (lhs.to - lhs.from + 1) < max - (rhs.to - rhs.from)
    if (distance(x) + 1 < max - distance(y)) {
      return type_t::Range(x.first + y.first, x.second + y.second, zone);
    }

    return type_t::Any();
  }

  static type_t Subtract(const type_t& lhs, const type_t& rhs, Zone* zone) {
    if (lhs.is_any() || rhs.is_any()) return type_t::Any();

    // If both sides are decently small sets, we produce the product set (which
    // we convert to a range if it exceeds the set limit).
    if (lhs.is_set() && rhs.is_set()) {
      ElementsVector result_elements;
      for (int i = 0; i < lhs.set_size(); ++i) {
        for (int j = 0; j < rhs.set_size(); ++j) {
          result_elements.push_back(lhs.set_element(i) - rhs.set_element(j));
        }
      }
      return FromElements(std::move(result_elements), zone);
    }

    // Otherwise just construct a range.
    std::pair<word_t, word_t> x = MakeRange(lhs);
    std::pair<word_t, word_t> y = MakeRange(rhs);

    if (!is_wrapping(x) && !is_wrapping(y)) {
      // If the result would not be a complete range, we compute it.
      // Check: (lhs.to - lhs.from + 1) + rhs.to - rhs.from < max
      // =====> (lhs.to - lhs.from + 1) < max - rhs.to + rhs.from
      // =====> (lhs.to - lhs.from + 1) < max - (rhs.to - rhs.from)
      if (distance(x) + 1 < max - distance(y)) {
        return type_t::Range(x.first - y.second, x.second - y.first, zone);
      }
    }

    // TODO(nicohartmann@): Improve the wrapping cases.
    return type_t::Any();
  }

  static Word32Type UnsignedLessThan(const type_t& lhs, const type_t& rhs,
                                     Zone* zone) {
    bool can_be_true = lhs.unsigned_min() < rhs.unsigned_max();
    bool can_be_false = lhs.unsigned_max() >= rhs.unsigned_min();

    if (!can_be_true) return Word32Type::Constant(0);
    if (!can_be_false) return Word32Type::Constant(1);
    return Word32Type::Set({0, 1}, zone);
  }

  static Word32Type UnsignedLessThanOrEqual(const type_t& lhs,
                                            const type_t& rhs, Zone* zone) {
    bool can_be_true = lhs.unsigned_min() <= rhs.unsigned_max();
    bool can_be_false = lhs.unsigned_max() > rhs.unsigned_min();

    if (!can_be_true) return Word32Type::Constant(0);
    if (!can_be_false) return Word32Type::Constant(1);
    return Word32Type::Set({0, 1}, zone);
  }

  // Computes the ranges to which the sides of the unsigned comparison (lhs <
  // rhs) can be restricted when the comparison is true. When the comparison is
  // true, we learn: lhs cannot be >= rhs.max and rhs cannot be <= lhs.min.
  static std::pair<Type, Type> RestrictionForUnsignedLessThan_True(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    Type restrict_lhs;
    if (rhs.unsigned_max() == 0) {
      // There is no value for lhs that could make (lhs < 0) true.
      restrict_lhs = Type::None();
    } else {
      restrict_lhs = type_t::Range(0, next_smaller(rhs.unsigned_max()), zone);
    }

    Type restrict_rhs;
    if (lhs.unsigned_min() == max) {
      // There is no value for rhs that could make (max < rhs) true.
      restrict_rhs = Type::None();
    } else {
      restrict_rhs = type_t::Range(next_larger(lhs.unsigned_min()), max, zone);
    }

    return {restrict_lhs, restrict_rhs};
  }

  // Computes the ranges to which the sides of the unsigned comparison (lhs <
  // rhs) can be restricted when the comparison is false. When the comparison is
  // false, we learn: lhs cannot be < rhs.min and rhs cannot be > lhs.max.
  static std::pair<Type, Type> RestrictionForUnsignedLessThan_False(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    return {type_t::Range(rhs.unsigned_min(), max, zone),
            type_t::Range(0, lhs.unsigned_max(), zone)};
  }

  // Computes the ranges to which the sides of the unsigned comparison (lhs <=
  // rhs) can be restricted when the comparison is true. When the comparison is
  // true, we learn: lhs cannot be > rhs.max and rhs cannot be < lhs.min.
  static std::pair<Type, Type> RestrictionForUnsignedLessThanOrEqual_True(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    return {type_t::Range(0, rhs.unsigned_max(), zone),
            type_t::Range(lhs.unsigned_min(), max, zone)};
  }

  // Computes the ranges to which the sides of the unsigned comparison (lhs <=
  // rhs) can be restricted when the comparison is false. When the comparison is
  // false, we learn: lhs cannot be <= rhs.min and rhs cannot be >= lhs.max.
  static std::pair<Type, Type> RestrictionForUnsignedLessThanOrEqual_False(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    Type restrict_lhs;
    if (rhs.unsigned_min() == max) {
      // There is no value for lhs that could make (lhs <= max) false.
      restrict_lhs = Type::None();
    } else {
      restrict_lhs = type_t::Range(next_larger(rhs.unsigned_min()), max, zone);
    }

    Type restrict_rhs;
    if (lhs.unsigned_max() == 0) {
      // There is no value for rhs that could make (0 <= rhs) false.
      restrict_rhs = Type::None();
    } else {
      restrict_rhs = type_t::Range(0, next_smaller(lhs.unsigned_max()), zone);
    }

    return {restrict_lhs, restrict_rhs};
  }

  // WidenMaximal widens one of the boundary to the extreme immediately.
  static type_t WidenMaximal(const type_t& old_type, const type_t& new_type,
                             Zone* zone) {
    if (new_type.is_any()) return new_type;
    if (old_type.is_wrapping() || new_type.is_wrapping()) return type_t::Any();

    word_t result_from = new_type.unsigned_min();
    if (result_from < old_type.unsigned_min()) result_from = 0;
    word_t result_to = new_type.unsigned_max();
    if (result_to > old_type.unsigned_max()) {
      result_to = std::numeric_limits<word_t>::max();
    }
    return type_t::Range(result_from, result_to, zone);
  }

  // Performs exponential widening, which means that the number of values
  // described by the resulting type is at least doubled with respect to the
  // {old_type}. If {new_type} is already twice the size of {old_type},
  // {new_type} may be returned directly.
  static type_t WidenExponential(const type_t& old_type, type_t new_type,
                                 Zone* zone) {
    if (new_type.is_any()) return new_type;
    word_t old_from, old_to, new_from, new_to;
    if (old_type.is_set()) {
      const word_t old_size = old_type.set_size();
      if (new_type.is_set()) {
        const word_t new_size = new_type.set_size();
        if (new_size >= 2 * old_size) return new_type;
        std::tie(new_from, new_to) = MakeRange(new_type);
      } else {
        DCHECK(new_type.is_range());
        std::tie(new_from, new_to) = new_type.range();
      }
      if (distance(new_from, new_to) >= 2 * old_size) {
        return type_t::Range(new_from, new_to, zone);
      }
      std::tie(old_from, old_to) = MakeRange(old_type);
    } else {
      DCHECK(old_type.is_range());
      std::tie(old_from, old_to) = old_type.range();
      if (new_type.is_set()) {
        std::tie(new_from, new_to) = MakeRange(new_type);
      } else {
        DCHECK(new_type.is_range());
        std::tie(new_from, new_to) = new_type.range();
      }
    }

    // If the old type is already quite large, we go to full range.
    if (distance(old_from, old_to) >= std::numeric_limits<word_t>::max() / 4) {
      return type_t::Any();
    }

    const word_t min_size = 2 * (distance(old_from, old_to) + 1);
    if (distance(new_from, new_to) >= min_size) {
      return type_t::Range(new_from, new_to, zone);
    }

    // If old is wrapping (and so is new).
    if (is_wrapping(old_from, old_to)) {
      DCHECK(is_wrapping(new_from, new_to));
      if (new_from < old_from) {
        DCHECK_LE(old_to, new_to);
        // We widen the `from` (although `to` might have grown, too).
        DCHECK_LT(new_to, min_size);
        word_t result_from =
            std::numeric_limits<word_t>::max() - (min_size - new_to);
        DCHECK_LT(result_from, new_from);
        DCHECK_LE(min_size, distance(result_from, new_to));
        return type_t::Range(result_from, new_to, zone);
      } else {
        DCHECK_EQ(old_from, new_from);
        // We widen the `to`.
        DCHECK_LT(std::numeric_limits<word_t>::max() - new_from, min_size);
        word_t result_to =
            min_size - (std::numeric_limits<word_t>::max() - new_from);
        DCHECK_GT(result_to, new_to);
        DCHECK_LE(min_size, distance(new_from, result_to));
        return type_t::Range(new_from, result_to, zone);
      }
    }

    // If old is not wrapping, but new is.
    if (is_wrapping(new_from, new_to)) {
      if (new_to < old_to) {
        // If wrapping was caused by to growing over max, grow `to` further
        // (although `from` might have grown, too).
        DCHECK_LT(std::numeric_limits<word_t>::max() - new_from, min_size);
        word_t result_to =
            min_size - (std::numeric_limits<word_t>::max() - new_from);
        DCHECK_LT(new_to, result_to);
        return type_t::Range(new_from, result_to, zone);
      } else {
        DCHECK_LT(old_from, new_from);
        // If wrapping was caused by `from` growing below 0, grow `from`
        // further.
        DCHECK_LT(new_to, min_size);
        word_t result_from =
            std::numeric_limits<word_t>::max() - (min_size - new_to);
        DCHECK_LT(result_from, new_from);
        return type_t::Range(result_from, new_to, zone);
      }
    }

    // Neither old nor new is wrapping.
    if (new_from < old_from) {
      DCHECK_LE(old_to, new_to);
      // Check if we can widen the `from`.
      if (new_to >= min_size) {
        // We can decrease `from` without going below 0.
        word_t result_from = new_to - min_size;
        DCHECK_LT(result_from, new_from);
        return type_t::Range(result_from, new_to, zone);
      } else {
        // We cannot grow `from` enough, so we also have to grow `to`.
        return type_t::Range(0, min_size, zone);
      }
    } else {
      DCHECK_EQ(old_from, new_from);
      // Check if we can widen the `to`.
      if (new_from <= std::numeric_limits<word_t>::max() - min_size) {
        // We can increase `to` without going above max.
        word_t result_to = new_from + min_size;
        DCHECK_GT(result_to, new_to);
        return type_t::Range(new_from, result_to, zone);
      } else {
        // We cannot grow `to` enough, so we also have to grow `from`.
        return type_t::Range(std::numeric_limits<word_t>::max() - min_size,
                             std::numeric_limits<word_t>::max(), zone);
      }
    }
  }
};

template <size_t Bits>
struct FloatOperationTyper {
  static_assert(Bits == 32 || Bits == 64);
  using float_t = std::conditional_t<Bits == 32, float, double>;
  using type_t = FloatType<Bits>;
  static constexpr float_t inf = std::numeric_limits<float_t>::infinity();
  static constexpr int kSetThreshold = type_t::kMaxSetSize;

  static type_t Range(float_t min, float_t max, uint32_t special_values,
                      Zone* zone) {
    DCHECK_LE(min, max);
    DCHECK_IMPLIES(detail::is_minus_zero(min),
                   (special_values & type_t::kMinusZero));
    DCHECK_IMPLIES(detail::is_minus_zero(max),
                   (special_values & type_t::kMinusZero));
    if (min == max) return Set({min + float_t{0}}, special_values, zone);
    return type_t::Range(min, max, special_values, zone);
  }

  static type_t Set(std::vector<float_t> elements, uint32_t special_values,
                    Zone* zone) {
    base::sort(elements);
    elements.erase(std::unique(elements.begin(), elements.end()),
                   elements.end());
    if (base::erase_if(elements, [](float_t v) { return std::isnan(v); }) > 0) {
      special_values |= type_t::kNaN;
    }
    if (base::erase_if(elements, [](float_t v) { return IsMinusZero(v); }) >
        0) {
      special_values |= type_t::kMinusZero;
    }
    if (elements.empty()) {
      DCHECK_NE(0, special_values);
      return type_t::OnlySpecialValues(special_values);
    }
    return type_t::Set(elements, special_values, zone);
  }

  // Check if the elements in the set are all integers. This ignores special
  // values (NaN, -0)!
  static bool IsIntegerSet(const type_t& t) {
    if (!t.is_set()) return false;
    int size = t.set_size();
    DCHECK_LT(0, size);

    float_t unused_ipart;
    float_t min = t.set_element(0);
    if (std::modf(min, &unused_ipart) != 0.0) return false;
    if (min == -inf) return false;
    float_t max = t.set_element(size - 1);
    if (std::modf(max, &unused_ipart) != 0.0) return false;
    if (max == inf) return false;

    for (int i = 1; i < size - 1; ++i) {
      if (std::modf(t.set_element(i), &unused_ipart) != 0.0) return false;
    }
    return true;
  }

  static bool IsZeroish(const type_t& l) {
    return l.has_nan() || l.has_minus_zero() || l.Contains(0);
  }

  // Tries to construct the product of two sets where values are generated using
  // {combine}. Returns Type::Invalid() if a set cannot be constructed (e.g.
  // because the result exceeds the maximal number of set elements).
  static Type ProductSet(const type_t& l, const type_t& r,
                         uint32_t special_values, Zone* zone,
                         std::function<float_t(float_t, float_t)> combine) {
    DCHECK(l.is_set());
    DCHECK(r.is_set());

    std::vector<float_t> results;
    auto CombineWithLeft = [&](float_t left) {
      for (int j = 0; j < r.set_size(); ++j) {
        results.push_back(combine(left, r.set_element(j)));
      }
      if (r.has_minus_zero()) results.push_back(combine(left, -0.0));
      if (r.has_nan()) results.push_back(combine(left, nan_v<Bits>));
    };

    for (int i = 0; i < l.set_size(); ++i) {
      CombineWithLeft(l.set_element(i));
    }
    if (l.has_minus_zero()) CombineWithLeft(-0.0);
    if (l.has_nan()) CombineWithLeft(nan_v<Bits>);

    if (base::erase_if(results, [](float_t v) { return std::isnan(v); }) > 0) {
      special_values |= type_t::kNaN;
    }
    if (base::erase_if(results, [](float_t v) { return IsMinusZero(v); }) > 0) {
      special_values |= type_t::kMinusZero;
    }
    base::sort(results);
    auto it = std::unique(results.begin(), results.end());
    if (std::distance(results.begin(), it) > kSetThreshold)
      return Type::Invalid();
    results.erase(it, results.end());
    if (results.empty()) return type_t::OnlySpecialValues(special_values);
    return Set(std::move(results), special_values, zone);
  }

  static Type Add(type_t l, type_t r, Zone* zone) {
    // Addition can return NaN if either input can be NaN or we try to compute
    // the sum of two infinities of opposite sign.
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan = l.has_nan() || r.has_nan();

    // Addition can yield minus zero only if both inputs can be minus zero.
    bool maybe_minuszero = true;
    if (l.has_minus_zero()) {
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
    } else {
      maybe_minuszero = false;
    }
    if (r.has_minus_zero()) {
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    } else {
      maybe_minuszero = false;
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return a + b; };
    if (l.is_set() && r.is_set()) {
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // Otherwise just construct a range.
    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    std::array<float_t, 4> results;
    results[0] = l_min + r_min;
    results[1] = l_min + r_max;
    results[2] = l_max + r_min;
    results[3] = l_max + r_max;

    int nans = 0;
    for (int i = 0; i < 4; ++i) {
      if (std::isnan(results[i])) ++nans;
    }
    if (nans > 0) {
      special_values |= type_t::kNaN;
      if (nans >= 4) {
        // All combinations of inputs produce NaN.
        return type_t::OnlySpecialValues(special_values);
      }
    }
    const float_t result_min = array_min(results);
    const float_t result_max = array_max(results);
    return Range(result_min, result_max, special_values, zone);
  }

  static Type Subtract(type_t l, type_t r, Zone* zone) {
    // Subtraction can return NaN if either input can be NaN or we try to
    // compute the sum of two infinities of opposite sign.
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan = l.has_nan() || r.has_nan();

    // Subtraction can yield minus zero if {lhs} can be minus zero and {rhs}
    // can be zero.
    bool maybe_minuszero = false;
    if (l.has_minus_zero()) {
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
      maybe_minuszero = r.Contains(0);
    }
    if (r.has_minus_zero()) {
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return a - b; };
    if (l.is_set() && r.is_set()) {
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // Otherwise just construct a range.
    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    std::array<float_t, 4> results;
    results[0] = l_min - r_min;
    results[1] = l_min - r_max;
    results[2] = l_max - r_min;
    results[3] = l_max - r_max;

    int nans = 0;
    for (int i = 0; i < 4; ++i) {
      if (std::isnan(results[i])) ++nans;
    }
    if (nans > 0) {
      special_values |= type_t::kNaN;
      if (nans >= 4) {
        // All combinations of inputs produce NaN.
        return type_t::NaN();
      }
    }
    const float_t result_min = array_min(results);
    const float_t result_max = array_max(results);
    return Range(result_min, result_max, special_values, zone);
  }

  static Type Multiply(type_t l, type_t r, Zone* zone) {
    // Multiplication propagates NaN:
    //   NaN * x = NaN         (regardless of sign of x)
    //   0 * Infinity = NaN    (regardless of signs)
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan = l.has_nan() || r.has_nan() ||
                     (IsZeroish(l) && (r.min() == -inf || r.max() == inf)) ||
                     (IsZeroish(r) && (l.min() == -inf || r.max() == inf));

    // Try to rule out -0.
    bool maybe_minuszero = l.has_minus_zero() || r.has_minus_zero() ||
                           (IsZeroish(l) && r.min() < 0.0) ||
                           (IsZeroish(r) && l.min() < 0.0);
    if (l.has_minus_zero()) {
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
    }
    if (r.has_minus_zero()) {
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return a * b; };
    if (l.is_set() && r.is_set()) {
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // Otherwise just construct a range.
    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    std::array<float_t, 4> results;
    results[0] = l_min * r_min;
    results[1] = l_min * r_max;
    results[2] = l_max * r_min;
    results[3] = l_max * r_max;

    for (int i = 0; i < 4; ++i) {
      if (std::isnan(results[i])) {
        return type_t::Any();
      }
    }

    float_t result_min = array_min(results);
    float_t result_max = array_max(results);
    if (result_min <= 0.0 && 0.0 <= result_max &&
        (l_min < 0.0 || r_min < 0.0)) {
      special_values |= type_t::kMinusZero;
      // Remove -0.
      result_min += 0.0;
      result_max += 0.0;
    }
    // 0 * V8_INFINITY is NaN, regardless of sign
    if (((l_min == -inf || l_max == inf) && (r_min <= 0.0 && 0.0 <= r_max)) ||
        ((r_min == -inf || r_max == inf) && (l_min <= 0.0 && 0.0 <= l_max))) {
      special_values |= type_t::kNaN;
    }

    type_t type = Range(result_min, result_max, special_values, zone);
    return type;
  }

  static Type Divide(const type_t& l, const type_t& r, Zone* zone) {
    // Division is tricky, so all we do is try ruling out -0 and NaN.
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();

    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) {
      if V8_UNLIKELY (!std::isfinite(a) && !std::isfinite(b)) {
        return nan_v<Bits>;
      }
      if V8_UNLIKELY (IsMinusZero(b)) {
        // +-0 / -0 ==> NaN
        if (a == 0 || std::isnan(a)) return nan_v<Bits>;
        return a > 0 ? -inf : inf;
      }
      if V8_UNLIKELY (b == 0) {
        // +-0 / 0 ==> NaN
        if (a == 0 || std::isnan(a)) return nan_v<Bits>;
        return a > 0 ? inf : -inf;
      }
      return a / b;
    };
    if (l.is_set() && r.is_set()) {
      auto result = ProductSet(l, r, 0, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    bool maybe_nan =
        l.has_nan() || IsZeroish(r) ||
        ((l_min == -inf || l_max == inf) && (r_min == -inf || r_max == inf));

    // Try to rule out -0.
    bool maybe_minuszero =
        // -0 / r (r > 0)
        (l.has_minus_zero() && r_max > 0)
        // 0 / r (r < 0)
        || (l.Contains(0) && r_min < 0)
        // -0.0..01 / r (r > 1)
        || (l.Contains(0) && l_min < 0 && r_max > 1)
        // 0.0..01 / r (r < -1)
        || (l.Contains(0) && l_max >= 0 && r_min < -1)
        // l / large (l < 0)
        || (l_max < 0 && detail::is_minus_zero(l_max / r_max))
        // l / -large (l > 0)
        || (l_min > 0 && detail::is_minus_zero(l_min / r_min));

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);

    const bool r_all_positive = r_min >= 0 && !r.has_minus_zero();
    const bool r_all_negative = r_max < 0;

    // If r doesn't span 0, we can try to compute a more precise type.
    if (r_all_positive || r_all_negative) {
      // If r does not contain 0 or -0, we can compute a range.
      if (r_min > 0 && !r.has_minus_zero()) {
        std::array<float_t, 4> results;
        results[0] = l_min / r_min;
        results[1] = l_min / r_max;
        results[2] = l_max / r_min;
        results[3] = l_max / r_max;

        for (float_t r : results) {
          if (std::isnan(r)) return type_t::Any();
        }

        const float_t result_min = array_min(results);
        const float_t result_max = array_max(results);
        return Range(result_min, result_max, special_values, zone);
      }

      // Otherwise we try to check for the sign of the result.
      if (l_max < 0) {
        if (r_all_positive) {
          // All values are negative.
          return Range(-inf, next_smaller(float_t{0}), special_values, zone);
        } else {
          DCHECK(r_all_negative);
          // All values are positive.
          return Range(0, inf, special_values, zone);
        }
      } else if (l_min >= 0 && !l.has_minus_zero()) {
        if (r_all_positive) {
          // All values are positive.
          DCHECK_EQ(special_values & type_t::kMinusZero, 0);
          return Range(0, inf, special_values, zone);
        } else {
          DCHECK(r_all_negative);
          // All values are negative.
          return Range(-inf, next_smaller(float_t{0}), special_values, zone);
        }
      }
    }

    // Otherwise we give up on a precise type.
    return type_t::Any(special_values);
  }

  static Type Modulus(type_t l, type_t r, Zone* zone) {
    // Modulus can yield NaN if either {lhs} or {rhs} are NaN, or
    // {lhs} is not finite, or the {rhs} is a zero value.
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan =
        l.has_nan() || IsZeroish(r) || l.min() == -inf || l.max() == inf;

    // Deal with -0 inputs, only the signbit of {lhs} matters for the result.
    bool maybe_minuszero = l.min() < 0;
    if (l.has_minus_zero()) {
      maybe_minuszero = true;
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
    }
    if (r.has_minus_zero()) {
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // For integer inputs {l} and {r} we can infer a precise type.
    if (IsIntegerSet(l) && IsIntegerSet(r)) {
      auto [l_min, l_max] = l.minmax();
      auto [r_min, r_max] = r.minmax();
      // l % r is:
      // - never greater than abs(l)
      // - never greater than abs(r) - 1
      auto l_abs = std::max(std::abs(l_min), std::abs(l_max));
      auto r_abs = std::max(std::abs(r_min), std::abs(r_max));
      // If rhs is 0, we can only produce NaN.
      if (r_abs == 0) return type_t::NaN();
      r_abs -= 1;
      auto abs = std::min(l_abs, r_abs);
      float_t min = 0.0, max = 0.0;
      if (l_min >= 0.0) {
        // {l} positive.
        max = abs;
      } else if (l_max <= 0.0) {
        // {l} negative.
        min = 0.0 - abs;
      } else {
        // {l} positive or negative.
        min = 0.0 - abs;
        max = abs;
      }
      if (min == max) return Set({min}, special_values, zone);
      return Range(min, max, special_values, zone);
    }

    // Otherwise, we give up.
    return type_t::Any(special_values);
  }

  static Type Min(type_t l, type_t r, Zone* zone) {
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan = l.has_nan() || r.has_nan();

    // In order to ensure monotonicity of the computation below, we additionally
    // pretend +0 is present (for simplicity on both sides).
    bool maybe_minuszero = false;
    if (l.has_minus_zero() && !(r.max() < 0.0)) {
      maybe_minuszero = true;
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
    }
    if (r.has_minus_zero() && !(l.max() < 0.0)) {
      maybe_minuszero = true;
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return std::min(a, b); };
    if (l.is_set() && r.is_set()) {
      // TODO(nicohartmann@): There is a faster way to compute this set.
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // Otherwise just construct a range.
    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    auto min = std::min(l_min, r_min);
    auto max = std::min(l_max, r_max);
    return Range(min, max, special_values, zone);
  }

  static Type Max(type_t l, type_t r, Zone* zone) {
    if (l.is_only_nan() || r.is_only_nan()) return type_t::NaN();
    bool maybe_nan = l.has_nan() || r.has_nan();

    // In order to ensure monotonicity of the computation below, we additionally
    // pretend +0 is present (for simplicity on both sides).
    bool maybe_minuszero = false;
    if (l.has_minus_zero() && !(r.min() > 0.0)) {
      maybe_minuszero = true;
      l = type_t::LeastUpperBound(l, type_t::Constant(0), zone);
    }
    if (r.has_minus_zero() && !(l.min() > 0.0)) {
      maybe_minuszero = true;
      r = type_t::LeastUpperBound(r, type_t::Constant(0), zone);
    }

    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minuszero ? type_t::kMinusZero : 0);
    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return std::max(a, b); };
    if (l.is_set() && r.is_set()) {
      // TODO(nicohartmann@): There is a faster way to compute this set.
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // Otherwise just construct a range.
    auto [l_min, l_max] = l.minmax();
    auto [r_min, r_max] = r.minmax();

    auto min = std::max(l_min, r_min);
    auto max = std::max(l_max, r_max);
    return Range(min, max, special_values, zone);
  }

  static Type Power(const type_t& l, const type_t& r, Zone* zone) {
    // x ** NaN => Nan.
    if (r.is_only_nan()) return type_t::NaN();
    // x ** +-0 => 1.
    if (r.is_constant(0) || r.is_only_minus_zero()) return type_t::Constant(1);
    if (l.is_only_nan()) {
      // NaN ** 0 => 1.
      if (r.Contains(0) || r.has_minus_zero()) {
        return type_t::Set({1}, type_t::kNaN, zone);
      }
      // NaN ** x => NaN (x != +-0).
      return type_t::NaN();
    }
    bool maybe_nan = l.has_nan() || r.has_nan();
    // +-1 ** +-Infinity => NaN.
    if (r.Contains(-inf) || r.Contains(inf)) {
      if (l.Contains(1) || l.Contains(-1)) maybe_nan = true;
    }

    // a ** b produces NaN if a < 0 && b is fraction.
    if (l.min() < 0.0 && !IsIntegerSet(r)) maybe_nan = true;

    // Precise checks for when the result can be -0 is difficult, because of
    // large (negative) exponents. To be safe we add -0 whenever the left hand
    // side can be negative. We might refine this when necessary.
    bool maybe_minus_zero = l.min() < 0.0 || l.has_minus_zero();
    uint32_t special_values = (maybe_nan ? type_t::kNaN : 0) |
                              (maybe_minus_zero ? type_t::kMinusZero : 0) |
                              l.special_values();

    // If both sides are decently small sets, we produce the product set.
    auto combine = [](float_t a, float_t b) { return std::pow(a, b); };
    if (l.is_set() && r.is_set()) {
      auto result = ProductSet(l, r, special_values, zone, combine);
      if (!result.IsInvalid()) return result;
    }

    // TODO(nicohartmann@): Maybe we can produce a more precise range here.
    return type_t::Any(special_values);
  }

  static Type Atan2(const type_t& l, const type_t& r, Zone* zone) {
    // TODO(nicohartmann@): Maybe we can produce a more precise range here.
    return type_t::Any();
  }

  static Type LessThan(const type_t& lhs, const type_t& rhs, Zone* zone) {
    bool can_be_true = false;
    bool can_be_false = false;
    if (lhs.is_only_special_values()) {
      if (lhs.has_minus_zero()) {
        can_be_true = !rhs.is_only_special_values() && rhs.max() > 0.0;
        can_be_false = rhs.min() <= 0.0;
      } else {
        DCHECK(lhs.is_only_nan());
      }
    } else if (rhs.is_only_special_values()) {
      if (rhs.has_minus_zero()) {
        can_be_true = lhs.min() < 0.0;
        can_be_false = lhs.max() >= 0.0;
      } else {
        DCHECK(rhs.is_only_nan());
      }
    } else {
      // Both sides have at least one non-special value. We don't have to treat
      // special values here, because nan has been taken care of already and
      // -0.0 is included in min/max.
      can_be_true = lhs.min() < rhs.max();
      can_be_false = lhs.max() >= rhs.min();
    }

    // Consider NaN.
    can_be_false = can_be_false || lhs.has_nan() || rhs.has_nan();

    if (!can_be_true) return Word32Type::Constant(0);
    if (!can_be_false) return Word32Type::Constant(1);
    return Word32Type::Set({0, 1}, zone);
  }

  static Type LessThanOrEqual(const type_t& lhs, const type_t& rhs,
                              Zone* zone) {
    bool can_be_true = false;
    bool can_be_false = false;
    if (lhs.is_only_special_values()) {
      if (lhs.has_minus_zero()) {
        can_be_true = (!rhs.is_only_special_values() && rhs.max() >= 0.0) ||
                      rhs.has_minus_zero();
        can_be_false = rhs.min() < 0.0;
      } else {
        DCHECK(lhs.is_only_nan());
      }
    } else if (rhs.is_only_special_values()) {
      if (rhs.has_minus_zero()) {
        can_be_true = (!lhs.is_only_special_values() && lhs.min() <= 0.0) ||
                      lhs.has_minus_zero();
        can_be_false = lhs.max() > 0.0;
      } else {
        DCHECK(rhs.is_only_nan());
      }
    } else {
      // Both sides have at least one non-special value. We don't have to treat
      // special values here, because nan has been taken care of already and
      // -0.0 is included in min/max.
      can_be_true = can_be_true || lhs.min() <= rhs.max();
      can_be_false = can_be_false || lhs.max() > rhs.min();
    }

    // Consider NaN.
    can_be_false = can_be_false || lhs.has_nan() || rhs.has_nan();

    if (!can_be_true) return Word32Type::Constant(0);
    if (!can_be_false) return Word32Type::Constant(1);
    return Word32Type::Set({0, 1}, zone);
  }

  static Word32Type UnsignedLessThanOrEqual(const type_t& lhs,
                                            const type_t& rhs, Zone* zone) {
    bool can_be_true = lhs.unsigned_min() <= rhs.unsigned_max();
    bool can_be_false = lhs.unsigned_max() > rhs.unsigned_min();

    if (!can_be_true) return Word32Type::Constant(0);
    if (!can_be_false) return Word32Type::Constant(1);
    return Word32Type::Set({0, 1}, zone);
  }

  // Computes the ranges to which the sides of the comparison (lhs < rhs) can be
  // restricted when the comparison is true. When the comparison is true, we
  // learn: lhs cannot be >= rhs.max and rhs cannot be <= lhs.min and neither
  // can be NaN.
  static std::pair<Type, Type> RestrictionForLessThan_True(const type_t& lhs,
                                                           const type_t& rhs,
                                                           Zone* zone) {
    // If either side is only NaN, this comparison can never be true.
    if (lhs.is_only_nan() || rhs.is_only_nan()) {
      return {Type::None(), Type::None()};
    }

    Type restrict_lhs;
    if (rhs.max() == -inf) {
      // There is no value for lhs that could make (lhs < -inf) true.
      restrict_lhs = Type::None();
    } else {
      const auto max = next_smaller(rhs.max());
      uint32_t sv = max >= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues;
      restrict_lhs = type_t::Range(-inf, max, sv, zone);
    }

    Type restrict_rhs;
    if (lhs.min() == inf) {
      // There is no value for rhs that could make (inf < rhs) true.
      restrict_rhs = Type::None();
    } else {
      const auto min = next_larger(lhs.min());
      uint32_t sv = min <= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues;
      restrict_rhs = type_t::Range(min, inf, sv, zone);
    }

    return {restrict_lhs, restrict_rhs};
  }

  // Computes the ranges to which the sides of the comparison (lhs < rhs) can be
  // restricted when the comparison is false. When the comparison is false, we
  // learn: lhs cannot be < rhs.min and rhs cannot be > lhs.max.
  static std::pair<Type, Type> RestrictionForLessThan_False(const type_t& lhs,
                                                            const type_t& rhs,
                                                            Zone* zone) {
    Type restrict_lhs;
    if (rhs.has_nan()) {
      restrict_lhs = type_t::Any();
    } else {
      uint32_t lhs_sv =
          type_t::kNaN |
          (rhs.min() <= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues);
      restrict_lhs = type_t::Range(rhs.min(), inf, lhs_sv, zone);
    }

    Type restrict_rhs;
    if (lhs.has_nan()) {
      restrict_rhs = type_t::Any();
    } else {
      uint32_t rhs_sv =
          type_t::kNaN |
          (lhs.max() >= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues);
      restrict_rhs = type_t::Range(-inf, lhs.max(), rhs_sv, zone);
    }

    return {restrict_lhs, restrict_rhs};
  }

  // Computes the ranges to which the sides of the comparison (lhs <= rhs) can
  // be restricted when the comparison is true. When the comparison is true, we
  // learn: lhs cannot be > rhs.max and rhs cannot be < lhs.min and neither can
  // be NaN.
  static std::pair<Type, Type> RestrictionForLessThanOrEqual_True(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    // If either side is only NaN, this comparison can never be true.
    if (lhs.is_only_nan() || rhs.is_only_nan()) {
      return {Type::None(), Type::None()};
    }

    uint32_t lhs_sv =
        rhs.max() >= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues;
    uint32_t rhs_sv =
        lhs.min() <= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues;
    return {type_t::Range(-inf, rhs.max(), lhs_sv, zone),
            type_t::Range(lhs.min(), inf, rhs_sv, zone)};
  }

  // Computes the ranges to which the sides of the comparison (lhs <= rhs) can
  // be restricted when the comparison is false. When the comparison is false,
  // we learn: lhs cannot be <= rhs.min and rhs cannot be >= lhs.max.
  static std::pair<Type, Type> RestrictionForLessThanOrEqual_False(
      const type_t& lhs, const type_t& rhs, Zone* zone) {
    Type restrict_lhs;
    if (rhs.has_nan()) {
      restrict_lhs = type_t::Any();
    } else if (rhs.min() == inf) {
      // The only value for lhs that could make (lhs <= inf) false is NaN.
      restrict_lhs = type_t::NaN();
    } else {
      const auto min = next_larger(rhs.min());
      uint32_t sv = type_t::kNaN |
                    (min <= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues);
      restrict_lhs = type_t::Range(min, inf, sv, zone);
    }

    Type restrict_rhs;
    if (lhs.has_nan()) {
      restrict_rhs = type_t::Any();
    } else if (lhs.max() == -inf) {
      // The only value for rhs that could make (-inf <= rhs) false is NaN.
      restrict_rhs = type_t::NaN();
    } else {
      const auto max = next_smaller(lhs.max());
      uint32_t sv = type_t::kNaN |
                    (max >= 0 ? type_t::kMinusZero : type_t::kNoSpecialValues);
      restrict_rhs = type_t::Range(-inf, max, sv, zone);
    }

    return {restrict_lhs, restrict_rhs};
  }
};

class Typer {
 public:
  static Type TypeForRepresentation(RegisterRepresentation rep) {
    switch (rep.value()) {
      case RegisterRepresentation::Word32():
        return Word32Type::Any();
      case RegisterRepresentation::Word64():
        return Word64Type::Any();
      case RegisterRepresentation::Float32():
        return Float32Type::Any();
      case RegisterRepresentation::Float64():
        return Float64Type::Any();

      case RegisterRepresentation::Tagged():
      case RegisterRepresentation::Compressed():
      case RegisterRepresentation::Simd128():
      case RegisterRepresentation::Simd256():
        // TODO(nicohartmann@): Support these representations.
        return Type::Any();
    }
  }

  static Type TypeForRepresentation(
      base::Vector<const RegisterRepresentation> reps, Zone* zone) {
    DCHECK_LT(0, reps.size());
    if (reps.size() == 1) return TypeForRepresentation(reps[0]);
    base::SmallVector<Type, 4> tuple_types;
    for (auto rep : reps) tuple_types.push_back(TypeForRepresentation(rep));
    return TupleType::Tuple(base::VectorOf(tuple_types), zone);
  }

  static Type TypeConstant(ConstantOp::Kind kind, ConstantOp::Storage value) {
    switch (kind) {
      case ConstantOp::Kind::kFloat32:
        if (value.float32.is_nan()) return Float32Type::NaN();
        if (IsMinusZero(value.float32.get_scalar()))
          return Float32Type::MinusZero();
        return Float32Type::Constant(value.float32.get_scalar());
      case ConstantOp::Kind::kFloat64:
        if (value.float64.is_nan()) return Float64Type::NaN();
        if (IsMinusZero(value.float64.get_scalar()))
          return Float64Type::MinusZero();
        return Float64Type::Constant(value.float64.get_scalar());
      case ConstantOp::Kind::kWord32:
        return Word32Type::Constant(static_cast<uint32_t>(value.integral));
      case ConstantOp::Kind::kWord64:
        return Word64Type::Constant(static_cast<uint64_t>(value.integral));
      default:
        // TODO(nicohartmann@): Support remaining {kind}s.
        return Type::Any();
    }
  }

  static Type TypeProjection(const Type& input, uint16_t idx) {
    if (input.IsNone()) return Type::None();
    if (!input.IsTuple()) return Type::Any();
    const TupleType& tuple = input.AsTuple();
    DCHECK_LT(idx, tuple.size());
    return tuple.element(idx);
  }

  static Type TypeWordBinop(Type left_type, Type right_type,
                            WordBinopOp::Kind kind, WordRepresentation rep,
                            Zone* zone) {
    DCHECK(!left_type.IsInvalid());
    DCHECK(!right_type.IsInvalid());

    if (rep == WordRepresentation::Word32()) {
      switch (kind) {
        case WordBinopOp::Kind::kAdd:
          return TypeWord32Add(left_type, right_type, zone);
        case WordBinopOp::Kind::kSub:
          return TypeWord32Sub(left_type, right_type, zone);
        default:
          // TODO(nicohartmann@): Support remaining {kind}s.
          return Word32Type::Any();
      }
    } else {
      DCHECK_EQ(rep, WordRepresentation::Word64());
      switch (kind) {
        case WordBinopOp::Kind::kAdd:
          return TypeWord64Add(left_type, right_type, zone);
        case WordBinopOp::Kind::kSub:
          return TypeWord64Sub(left_type, right_type, zone);
        default:
          // TODO(nicohartmann@): Support remaining {kind}s.
          return Word64Type::Any();
      }
    }
  }

  static Type TypeWord32Add(const Type& lhs, const Type& rhs, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    auto l = TruncateWord32Input(lhs, true, zone);
    auto r = TruncateWord32Input(rhs, true, zone);
    return WordOperationTyper<32>::Add(l, r, zone);
  }

  static Type TypeWord32Sub(const Type& lhs, const Type& rhs, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    auto l = TruncateWord32Input(lhs, true, zone);
    auto r = TruncateWord32Input(rhs, true, zone);
    return WordOperationTyper<32>::Subtract(l, r, zone);
  }

  static Type TypeWord64Add(const Type& lhs, const Type& rhs, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    if (!InputIs(lhs, Type::Kind::kWord64) ||
        !InputIs(rhs, Type::Kind::kWord64)) {
      return Word64Type::Any();
    }
    const auto& l = lhs.AsWord64();
    const auto& r = rhs.AsWord64();

    return WordOperationTyper<64>::Add(l, r, zone);
  }

  static Type TypeWord64Sub(const Type& lhs, const Type& rhs, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    if (!InputIs(lhs, Type::Kind::kWord64) ||
        !InputIs(rhs, Type::Kind::kWord64)) {
      return Word64Type::Any();
    }

    const auto& l = lhs.AsWord64();
    const auto& r = rhs.AsWord64();

    return WordOperationTyper<64>::Subtract(l, r, zone);
  }

  static Type TypeFloatBinop(Type left_type, Type right_type,
                             FloatBinopOp::Kind kind, FloatRepresentation rep,
                             Zone* zone) {
    DCHECK(!left_type.IsInvalid());
    DCHECK(!right_type.IsInvalid());

#define FLOAT_BINOP(op, bits)     \
  case FloatBinopOp::Kind::k##op: \
    return TypeFloat##bits##op(left_type, right_type, zone);

    if (rep == FloatRepresentation::Float32()) {
      switch (kind) {
        FLOAT_BINOP(Add, 32)
        FLOAT_BINOP(Sub, 32)
        FLOAT_BINOP(Mul, 32)
        FLOAT_BINOP(Div, 32)
        FLOAT_BINOP(Mod, 32)
        FLOAT_BINOP(Min, 32)
        FLOAT_BINOP(Max, 32)
        FLOAT_BINOP(Power, 32)
        FLOAT_BINOP(Atan2, 32)
      }
    } else {
      DCHECK_EQ(rep, FloatRepresentation::Float64());
      switch (kind) {
        FLOAT_BINOP(Add, 64)
        FLOAT_BINOP(Sub, 64)
        FLOAT_BINOP(Mul, 64)
        FLOAT_BINOP(Div, 64)
        FLOAT_BINOP(Mod, 64)
        FLOAT_BINOP(Min, 64)
        FLOAT_BINOP(Max, 64)
        FLOAT_BINOP(Power, 64)
        FLOAT_BINOP(Atan2, 64)
      }
    }

#undef FLOAT_BINOP
  }

#define FLOAT_BINOP(op, bits, float_typer_handler)                     \
  static Type TypeFloat##bits##op(const Type& lhs, const Type& rhs,    \
                                  Zone* zone) {                        \
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();             \
    if (!InputIs(lhs, Type::Kind::kFloat##bits) ||                     \
        !InputIs(rhs, Type::Kind::kFloat##bits)) {                     \
      return Float##bits##Type::Any();                                 \
    }                                                                  \
    const auto& l = lhs.AsFloat##bits();                               \
    const auto& r = rhs.AsFloat##bits();                               \
    return FloatOperationTyper<bits>::float_typer_handler(l, r, zone); \
  }

  // Float32 operations
  FLOAT_BINOP(Add, 32, Add)
  FLOAT_BINOP(Sub, 32, Subtract)
  FLOAT_BINOP(Mul, 32, Multiply)
  FLOAT_BINOP(Div, 32, Divide)
  FLOAT_BINOP(Mod, 32, Modulus)
  FLOAT_BINOP(Min, 32, Min)
  FLOAT_BINOP(Max, 32, Max)
  FLOAT_BINOP(Power, 32, Power)
  FLOAT_BINOP(Atan2, 32, Atan2)
  // Float64 operations
  FLOAT_BINOP(Add, 64, Add)
  FLOAT_BINOP(Sub, 64, Subtract)
  FLOAT_BINOP(Mul, 64, Multiply)
  FLOAT_BINOP(Div, 64, Divide)
  FLOAT_BINOP(Mod, 64, Modulus)
  FLOAT_BINOP(Min, 64, Min)
  FLOAT_BINOP(Max, 64, Max)
  FLOAT_BINOP(Power, 64, Power)
  FLOAT_BINOP(Atan2, 64, Atan2)
#undef FLOAT_BINOP

  static Type TypeOverflowCheckedBinop(const Type& left_type,
                                       const Type& right_type,
                                       OverflowCheckedBinopOp::Kind kind,
                                       WordRepresentation rep, Zone* zone) {
    DCHECK(!left_type.IsInvalid());
    DCHECK(!right_type.IsInvalid());

    if (rep == WordRepresentation::Word32()) {
      switch (kind) {
        case OverflowCheckedBinopOp::Kind::kSignedAdd:
          return TypeWord32OverflowCheckedAdd(left_type, right_type, zone);
        case OverflowCheckedBinopOp::Kind::kSignedSub:
        case OverflowCheckedBinopOp::Kind::kSignedMul:
          // TODO(nicohartmann@): Support these.
          return TupleType::Tuple(Word32Type::Any(),
                                  Word32Type::Set({0, 1}, zone), zone);
      }
    } else {
      DCHECK_EQ(rep, WordRepresentation::Word64());
      switch (kind) {
        case OverflowCheckedBinopOp::Kind::kSignedAdd:
        case OverflowCheckedBinopOp::Kind::kSignedSub:
        case OverflowCheckedBinopOp::Kind::kSignedMul:
          // TODO(nicohartmann@): Support these.
          return TupleType::Tuple(Word64Type::Any(),
                                  Word32Type::Set({0, 1}, zone), zone);
      }
    }
  }

  static Type TypeWord32OverflowCheckedAdd(const Type& lhs, const Type& rhs,
                                           Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    auto l = TruncateWord32Input(lhs, true, zone);
    auto r = TruncateWord32Input(rhs, true, zone);

    auto value = WordOperationTyper<32>::Add(l, r, zone);
    // We check for signed overflow and if the topmost bits of both opperands
    // are 0, we know that the result cannot overflow.
    if ((0xC0000000 & l.unsigned_max()) == 0 &&
        (0xC0000000 & r.unsigned_max()) == 0) {
      // Cannot overflow.
      return TupleType::Tuple(value, Word32Type::Constant(0), zone);
    }
    // Special case for two constant inputs to figure out the overflow.
    if (l.is_constant() && r.is_constant()) {
      constexpr uint32_t msb_mask = 0x80000000;
      DCHECK(value.is_constant());
      uint32_t l_msb = (*l.try_get_constant()) & msb_mask;
      uint32_t r_msb = (*r.try_get_constant()) & msb_mask;
      if (l_msb != r_msb) {
        // Different sign bits can never lead to an overflow.
        return TupleType::Tuple(value, Word32Type::Constant(0), zone);
      }
      uint32_t value_msb = (*value.try_get_constant()) & msb_mask;
      const uint32_t overflow = value_msb == l_msb ? 0 : 1;
      return TupleType::Tuple(value, Word32Type::Constant(overflow), zone);
    }
    // Otherwise we accept some imprecision.
    return TupleType::Tuple(value, Word32Type::Set({0, 1}, zone), zone);
  }

  static Type TypeComparison(const Type& lhs, const Type& rhs,
                             RegisterRepresentation rep,
                             ComparisonOp::Kind kind, Zone* zone) {
    switch (rep.value()) {
      case RegisterRepresentation::Word32():
        return TypeWord32Comparison(lhs, rhs, kind, zone);
      case RegisterRepresentation::Word64():
        return TypeWord64Comparison(lhs, rhs, kind, zone);
      case RegisterRepresentation::Float32():
        return TypeFloat32Comparison(lhs, rhs, kind, zone);
      case RegisterRepresentation::Float64():
        return TypeFloat64Comparison(lhs, rhs, kind, zone);
      case RegisterRepresentation::Tagged():
      case RegisterRepresentation::Compressed():
      case RegisterRepresentation::Simd128():
      case RegisterRepresentation::Simd256():
        if (lhs.IsNone() || rhs.IsNone()) return Type::None();
        // TODO(nicohartmann@): Support those cases.
        return Word32Type::Set({0, 1}, zone);
    }
  }

  static Type TypeWord32Comparison(const Type& lhs, const Type& rhs,
                                   ComparisonOp::Kind kind, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    auto l = TruncateWord32Input(lhs, true, zone);
    auto r = TruncateWord32Input(rhs, true, zone);
    switch (kind) {
      case ComparisonOp::Kind::kEqual:
      case ComparisonOp::Kind::kSignedLessThan:
      case ComparisonOp::Kind::kSignedLessThanOrEqual:
        // TODO(nicohartmann@): Support this.
        return Word32Type::Set({0, 1}, zone);
      case ComparisonOp::Kind::kUnsignedLessThan:
        return WordOperationTyper<32>::UnsignedLessThan(l, r, zone);
      case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
        return WordOperationTyper<32>::UnsignedLessThanOrEqual(l, r, zone);
    }
    UNREACHABLE();
  }

  static Type TypeWord64Comparison(const Type& lhs, const Type& rhs,
                                   ComparisonOp::Kind kind, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    switch (kind) {
      case ComparisonOp::Kind::kEqual:
      case ComparisonOp::Kind::kSignedLessThan:
      case ComparisonOp::Kind::kSignedLessThanOrEqual:
        // TODO(nicohartmann@): Support this.
        return Word32Type::Set({0, 1}, zone);
      case ComparisonOp::Kind::kUnsignedLessThan:
        return WordOperationTyper<64>::UnsignedLessThan(lhs.AsWord64(),
                                                        rhs.AsWord64(), zone);
      case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
        return WordOperationTyper<64>::UnsignedLessThanOrEqual(
            lhs.AsWord64(), rhs.AsWord64(), zone);
    }
    UNREACHABLE();
  }

  static Type TypeFloat32Comparison(const Type& lhs, const Type& rhs,
                                    ComparisonOp::Kind kind, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    switch (kind) {
      case ComparisonOp::Kind::kEqual:
        // TODO(nicohartmann@): Support this.
        return Word32Type::Set({0, 1}, zone);
      case ComparisonOp::Kind::kSignedLessThan:
        return FloatOperationTyper<32>::LessThan(lhs.AsFloat32(),
                                                 rhs.AsFloat32(), zone);
      case ComparisonOp::Kind::kSignedLessThanOrEqual:
        return FloatOperationTyper<32>::LessThanOrEqual(lhs.AsFloat32(),
                                                        rhs.AsFloat32(), zone);
      case ComparisonOp::Kind::kUnsignedLessThan:
      case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
        UNREACHABLE();
    }
  }

  static Type TypeFloat64Comparison(const Type& lhs, const Type& rhs,
                                    ComparisonOp::Kind kind, Zone* zone) {
    if (lhs.IsNone() || rhs.IsNone()) return Type::None();
    switch (kind) {
      case ComparisonOp::Kind::kEqual:
        // TODO(nicohartmann@): Support this.
        return Word32Type::Set({0, 1}, zone);
      case ComparisonOp::Kind::kSignedLessThan:
        return FloatOperationTyper<64>::LessThan(lhs.AsFloat64(),
                                                 rhs.AsFloat64(), zone);
      case ComparisonOp::Kind::kSignedLessThanOrEqual:
        return FloatOperationTyper<64>::LessThanOrEqual(lhs.AsFloat64(),
                                                        rhs.AsFloat64(), zone);
      case ComparisonOp::Kind::kUnsignedLessThan:
      case ComparisonOp::Kind::kUnsignedLessThanOrEqual:
        UNREACHABLE();
    }
  }

  static Word64Type ExtendWord32ToWord64(const Word32Type& t, Zone* zone) {
    // We cannot infer much, but the lower bound of the word32 is also the lower
    // bound of the word64 type.
    if (t.is_wrapping()) return Word64Type::Any();
    return Word64Type::Range(static_cast<uint64_t>(t.unsigned_min()),
                             std::numeric_limits<uint64_t>::max(), zone);
  }

  static Word32Type TruncateWord32Input(const Type& input,
                                        bool implicit_word64_narrowing,
                                        Zone* zone) {
    DCHECK(!input.IsInvalid());
    DCHECK(!input.IsNone());

    if (input.IsAny()) {
      if (allow_invalid_inputs()) return Word32Type::Any();
    } else if (input.IsWord32()) {
      return input.AsWord32();
    } else if (input.IsWord64() && implicit_word64_narrowing) {
      // The input is implicitly converted to word32.
      const auto& w64 = input.AsWord64();
      if (w64.is_set()) {
        WordOperationTyper<32>::ElementsVector elements;
        for (uint64_t e : w64.set_elements()) {
          elements.push_back(static_cast<uint32_t>(e));
        }
        return WordOperationTyper<32>::FromElements(std::move(elements), zone);
      }

      if (w64.is_any() || w64.is_wrapping()) return Word32Type::Any();

      if (w64.range_to() <= std::numeric_limits<uint32_t>::max()) {
        DCHECK_LE(w64.range_from(), std::numeric_limits<uint32_t>::max());
        return Word32Type::Range(static_cast<uint32_t>(w64.range_from()),
                                 static_cast<uint32_t>(w64.range_to()), zone);
      }

      // TODO(nicohartmann@): Might compute a more precise range here.
      return Word32Type::Any();
    }

    FATAL("Missing proper type for TruncateWord32Input. Type is: %s",
          input.ToString().c_str());
  }

  class BranchRefinements {
   public:
    // type_getter_t has to provide the type for a given input index.
    using type_getter_t = std::function<Type(OpIndex)>;
    // type_refiner_t is called with those arguments:
    //  - OpIndex: index of the operation whose type is refined by the branch.
    //  - Type: the refined type of the operation (after refinement, guaranteed
    //  to be a subtype of the original type).
    using type_refiner_t = std::function<void(OpIndex, const Type&)>;

    BranchRefinements(type_getter_t type_getter, type_refiner_t type_refiner)
        : type_getter_(type_getter), type_refiner_(type_refiner) {
      DCHECK(type_getter_);
      DCHECK(type_refiner_);
    }

    void RefineTypes(const Operation& condition, bool then_branch, Zone* zone);

   private:
    template <bool allow_implicit_word64_truncation>
    Type RefineWord32Type(const Type& type, const Type& refinement,
                          Zone* zone) {
      // If refinement is Type::None(), the operation/branch is unreachable.
      if (refinement.IsNone()) return Type::None();
      DCHECK(refinement.IsWord32());
      if constexpr (allow_implicit_word64_truncation) {
        // Turboshaft allows implicit trunction of Word64 values to Word32. When
        // an operation on Word32 representation computes a refinement type,
        // this is going to be a Type::Word32() even if the actual {type} was
        // Word64 before truncation. To correctly refine this type, we need to
        // extend the {refinement} to Word64 such that it reflects the
        // corresponding values in the original type (before truncation) before
        // we intersect.
        if (type.IsWord64()) {
          return Word64Type::Intersect(
              type.AsWord64(),
              Typer::ExtendWord32ToWord64(refinement.AsWord32(), zone),
              Type::ResolutionMode::kOverApproximate, zone);
        }
      }
      // We limit the values of {type} to those in {refinement}.
      return Word32Type::Intersect(type.AsWord32(), refinement.AsWord32(),
                                   Type::ResolutionMode::kOverApproximate,
                                   zone);
    }

    type_getter_t type_getter_;
    type_refiner_t type_refiner_;
  };

  static bool InputIs(const Type& input, Type::Kind expected) {
    if (input.IsInvalid()) {
      if (allow_invalid_inputs()) return false;
    } else if (input.kind() == expected) {
      return true;
    } else if (input.IsAny()) {
      if (allow_invalid_inputs()) return false;
    }

    std::stringstream s;
    s << expected;
    FATAL("Missing proper type (%s). Type is: %s", s.str().c_str(),
          input.ToString().c_str());
  }

  // For now we allow invalid inputs (which will then just lead to very generic
  // typing). Once all operations are implemented, we are going to disable this.
  static bool allow_invalid_inputs() { return true; }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPER_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/compiler/turboshaft/types.cc                                                0000664 0000000 0000000 00000057577 14746647661 0022327 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/types.h"

#include <optional>
#include <sstream>
#include <string_view>

#include "src/base/logging.h"
#include "src/compiler/turboshaft/type-parser.h"
#include "src/heap/factory.h"
#include "src/objects/turboshaft-types-inl.h"

namespace v8::internal::compiler::turboshaft {

namespace {

std::pair<uint32_t, uint32_t> uint64_to_high_low(uint64_t value) {
  return {static_cast<uint32_t>(value >> 32), static_cast<uint32_t>(value)};
}

}  // namespace

bool Type::Equals(const Type& other) const {
  DCHECK(!IsInvalid());
  DCHECK(!other.IsInvalid());

  if (kind_ != other.kind_) return false;
  switch (kind_) {
    case Kind::kInvalid:
      UNREACHABLE();
    case Kind::kNone:
      return true;
    case Kind::kWord32:
      return AsWord32().Equals(other.AsWord32());
    case Kind::kWord64:
      return AsWord64().Equals(other.AsWord64());
    case Kind::kFloat32:
      return AsFloat32().Equals(other.AsFloat32());
    case Kind::kFloat64:
      return AsFloat64().Equals(other.AsFloat64());
    case Kind::kTuple:
      return AsTuple().Equals(other.AsTuple());
    case Kind::kAny:
      return true;
  }
}

bool Type::IsSubtypeOf(const Type& other) const {
  DCHECK(!IsInvalid());
  DCHECK(!other.IsInvalid());

  if (other.IsAny() || IsNone()) return true;
  if (kind_ != other.kind_) return false;

  switch (kind_) {
    case Kind::kInvalid:
    case Kind::kNone:
      UNREACHABLE();
    case Kind::kWord32:
      return AsWord32().IsSubtypeOf(other.AsWord32());
    case Kind::kWord64:
      return AsWord64().IsSubtypeOf(other.AsWord64());
    case Kind::kFloat32:
      return AsFloat32().IsSubtypeOf(other.AsFloat32());
    case Kind::kFloat64:
      return AsFloat64().IsSubtypeOf(other.AsFloat64());
    case Kind::kTuple:
      return AsTuple().IsSubtypeOf(other.AsTuple());
    case Kind::kAny:
      UNREACHABLE();
  }
}

void Type::PrintTo(std::ostream& stream) const {
  switch (kind_) {
    case Kind::kInvalid:
      UNREACHABLE();
    case Kind::kNone:
      stream << "None";
      break;
    case Kind::kWord32: {
      AsWord32().PrintTo(stream);
      break;
    }
    case Kind::kWord64: {
      AsWord64().PrintTo(stream);
      break;
    }
    case Kind::kFloat32: {
      AsFloat32().PrintTo(stream);
      break;
    }
    case Kind::kFloat64: {
      AsFloat64().PrintTo(stream);
      break;
    }
    case Kind::kTuple: {
      AsTuple().PrintTo(stream);
      break;
    }
    case Kind::kAny: {
      stream << "Any";
      break;
    }
  }
}

void Type::Print() const {
  StdoutStream os;
  PrintTo(os);
  os << '\n';
}

// static
Type Type::LeastUpperBound(const Type& lhs, const Type& rhs, Zone* zone) {
  if (lhs.IsAny() || rhs.IsAny()) return Type::Any();
  if (lhs.IsNone()) return rhs;
  if (rhs.IsNone()) return lhs;

  // TODO(nicohartmann@): We might use more precise types here but currently
  // there is not much benefit in that.
  if (lhs.kind() != rhs.kind()) return Type::Any();

  switch (lhs.kind()) {
    case Type::Kind::kInvalid:
    case Type::Kind::kNone:
    case Type::Kind::kAny:
      UNREACHABLE();
    case Type::Kind::kWord32:
      return Word32Type::LeastUpperBound(lhs.AsWord32(), rhs.AsWord32(), zone);
    case Type::Kind::kWord64:
      return Word64Type::LeastUpperBound(lhs.AsWord64(), rhs.AsWord64(), zone);
    case Type::Kind::kFloat32:
      return Float32Type::LeastUpperBound(lhs.AsFloat32(), rhs.AsFloat32(),
                                          zone);
    case Type::Kind::kFloat64:
      return Float64Type::LeastUpperBound(lhs.AsFloat64(), rhs.AsFloat64(),
                                          zone);
    case Type::Kind::kTuple:
      return TupleType::LeastUpperBound(lhs.AsTuple(), rhs.AsTuple(), zone);
  }
}

std::optional<Type> Type::ParseFromString(const std::string_view& str,
                                          Zone* zone) {
  TypeParser parser(str, zone);
  return parser.Parse();
}

Handle<TurboshaftType> Type::AllocateOnHeap(Factory* factory) const {
  DCHECK_NOT_NULL(factory);
  switch (kind_) {
    case Kind::kInvalid:
      UNREACHABLE();
    case Kind::kNone:
      UNIMPLEMENTED();
    case Kind::kWord32:
      return AsWord32().AllocateOnHeap(factory);
    case Kind::kWord64:
      return AsWord64().AllocateOnHeap(factory);
    case Kind::kFloat32:
      return AsFloat32().AllocateOnHeap(factory);
    case Kind::kFloat64:
      return AsFloat64().AllocateOnHeap(factory);
    case Kind::kTuple:
      UNIMPLEMENTED();
    case Kind::kAny:
      UNIMPLEMENTED();
  }
}

template <size_t Bits>
bool WordType<Bits>::Contains(word_t value) const {
  switch (sub_kind()) {
    case SubKind::kRange: {
      if (is_wrapping()) return range_to() >= value || range_from() <= value;
      return range_from() <= value && value <= range_to();
    }
    case SubKind::kSet: {
      for (int i = 0; i < set_size(); ++i) {
        if (set_element(i) == value) return true;
      }
      return false;
    }
  }
}

template <size_t Bits>
bool WordType<Bits>::Equals(const WordType<Bits>& other) const {
  if (sub_kind() != other.sub_kind()) return false;
  switch (sub_kind()) {
    case SubKind::kRange:
      return (range_from() == other.range_from() &&
              range_to() == other.range_to()) ||
             (is_any() && other.is_any());
    case SubKind::kSet: {
      if (set_size() != other.set_size()) return false;
      for (int i = 0; i < set_size(); ++i) {
        if (set_element(i) != other.set_element(i)) return false;
      }
      return true;
    }
  }
}

template <size_t Bits>
bool WordType<Bits>::IsSubtypeOf(const WordType<Bits>& other) const {
  if (other.is_any()) return true;
  switch (sub_kind()) {
    case SubKind::kRange: {
      if (other.is_set()) return false;
      DCHECK(other.is_range());
      if (is_wrapping() == other.is_wrapping()) {
        return range_from() >= other.range_from() &&
               range_to() <= other.range_to();
      }
      return !is_wrapping() && (range_to() <= other.range_to() ||
                                range_from() >= other.range_from());
    }
    case SubKind::kSet: {
      if (other.is_set() && set_size() > other.set_size()) return false;
      for (int i = 0; i < set_size(); ++i) {
        if (!other.Contains(set_element(i))) return false;
      }
      return true;
    }
  }
}

template <size_t Bits, typename word_t = typename WordType<Bits>::word_t>
WordType<Bits> LeastUpperBoundFromRanges(word_t l_from, word_t l_to,
                                         word_t r_from, word_t r_to,
                                         Zone* zone) {
  const bool lhs_wrapping = l_to < l_from;
  const bool rhs_wrapping = r_to < r_from;
  // Case 1: Both ranges non-wrapping
  // lhs ---|XXX|--  --|XXX|---  -|XXXXXX|-  ---|XX|--- -|XX|------
  // rhs -|XXX|----  ----|XXX|-  ---|XX|---  -|XXXXXX|- ------|XX|-
  // ==> -|XXXXX|--  --|XXXXX|-  -|XXXXXX|-  -|XXXXXX|- -|XXXXXXX|-
  if (!lhs_wrapping && !rhs_wrapping) {
    return WordType<Bits>::Range(std::min(l_from, r_from), std::max(l_to, r_to),
                                 zone);
  }
  // Case 2: Both ranges wrapping
  // lhs XXX|----|XXX   X|---|XXXXXX   XXXXXX|---|X   XX|--|XXXXXX
  // rhs X|---|XXXXXX   XXX|----|XXX   XX|--|XXXXXX   XXXXXX|--|XX
  // ==> XXX|-|XXXXXX   XXX|-|XXXXXX   XXXXXXXXXXXX   XXXXXXXXXXXX
  if (lhs_wrapping && rhs_wrapping) {
    const auto from = std::min(l_from, r_from);
    const auto to = std::max(l_to, r_to);
    if (to >= from) return WordType<Bits>::Any();
    auto result = WordType<Bits>::Range(from, to, zone);
    DCHECK(result.is_wrapping());
    return result;
  }

  if (rhs_wrapping)
    return LeastUpperBoundFromRanges<Bits>(r_from, r_to, l_from, l_to, zone);
  DCHECK(lhs_wrapping);
  DCHECK(!rhs_wrapping);
  // Case 3 & 4: lhs is wrapping, rhs is not
  // lhs XXX|----|XXX   XXX|----|XXX   XXXXX|--|XXX   X|-------|XX
  // rhs -------|XX|-   -|XX|-------   ----|XXXXX|-   ---|XX|-----
  // ==> XXX|---|XXXX   XXXX|---|XXX   XXXXXXXXXXXX   XXXXXX|--|XX
  if (r_from <= l_to) {
    if (r_to <= l_to)
      return WordType<Bits>::Range(l_from, l_to, zone);       // y covered by x
    if (r_to >= l_from) return WordType<Bits>::Any();         // ex3
    auto result = WordType<Bits>::Range(l_from, r_to, zone);  // ex 1
    DCHECK(result.is_wrapping());
    return result;
  } else if (r_to >= l_from) {
    if (r_from >= l_from)
      return WordType<Bits>::Range(l_from, l_to, zone);       // y covered by x
    DCHECK_GT(r_from, l_to);                                  // handled above
    auto result = WordType<Bits>::Range(r_from, l_to, zone);  // ex 2
    DCHECK(result.is_wrapping());
    return result;
  } else {
    const auto df = r_from - l_to;
    const auto dt = l_from - r_to;
    WordType<Bits> result =
        df > dt ? WordType<Bits>::Range(r_from, l_to, zone)  // ex 4
                : WordType<Bits>::Range(l_from, r_to, zone);
    DCHECK(result.is_wrapping());
    return result;
  }
}

template <size_t Bits>
// static
WordType<Bits> WordType<Bits>::LeastUpperBound(const WordType<Bits>& lhs,
                                               const WordType<Bits>& rhs,
                                               Zone* zone) {
  if (lhs.is_set()) {
    if (!rhs.is_set()) {
      if (lhs.set_size() == 1) {
        word_t e = lhs.set_element(0);
        if (rhs.is_wrapping()) {
          // If {rhs} already contains e, {rhs} is the upper bound.
          if (e <= rhs.range_to() || rhs.range_from() <= e) return rhs;
          return (e - rhs.range_to() < rhs.range_from() - e)
                     ? Range(rhs.range_from(), e, zone)
                     : Range(e, rhs.range_to(), zone);
        }
        return Range(std::min(e, rhs.range_from()), std::max(e, rhs.range_to()),
                     zone);
      }

      // TODO(nicohartmann@): A wrapping range may be a better fit in some
      // cases.
      return LeastUpperBoundFromRanges<Bits>(
          lhs.unsigned_min(), lhs.unsigned_max(), rhs.range_from(),
          rhs.range_to(), zone);
    }

    // Both sides are sets. We try to construct the combined set.
    base::SmallVector<word_t, kMaxSetSize * 2> result_elements;
    base::vector_append(result_elements, lhs.set_elements());
    base::vector_append(result_elements, rhs.set_elements());
    DCHECK(!result_elements.empty());
    base::sort(result_elements);
    auto it = std::unique(result_elements.begin(), result_elements.end());
    result_elements.pop_back(std::distance(it, result_elements.end()));
    if (result_elements.size() <= kMaxSetSize) {
      return Set(result_elements, zone);
    }
    // We have to construct a range instead.
    // TODO(nicohartmann@): A wrapping range may be a better fit in some cases.
    return Range(result_elements.front(), result_elements.back(), zone);
  } else if (rhs.is_set()) {
    return LeastUpperBound(rhs, lhs, zone);
  }

  // Both sides are ranges.
  return LeastUpperBoundFromRanges<Bits>(
      lhs.range_from(), lhs.range_to(), rhs.range_from(), rhs.range_to(), zone);
}

template <size_t Bits>
Type WordType<Bits>::Intersect(const WordType<Bits>& lhs,
                               const WordType<Bits>& rhs,
                               ResolutionMode resolution_mode, Zone* zone) {
  if (lhs.is_any()) return rhs;
  if (rhs.is_any()) return lhs;

  if (lhs.is_set() || rhs.is_set()) {
    const auto& x = lhs.is_set() ? lhs : rhs;
    const auto& y = lhs.is_set() ? rhs : lhs;
    base::SmallVector<word_t, kMaxSetSize * 2> result_elements;
    for (int i = 0; i < x.set_size(); ++i) {
      const word_t element = x.set_element(i);
      if (y.Contains(element)) result_elements.push_back(element);
    }
    if (result_elements.empty()) return Type::None();
    DCHECK(detail::is_unique_and_sorted(result_elements));
    return Set(result_elements, zone);
  }

  DCHECK(lhs.is_range() && rhs.is_range());
  const bool lhs_wrapping = lhs.is_wrapping();
  if (!lhs_wrapping && !rhs.is_wrapping()) {
    const auto result_from = std::max(lhs.range_from(), rhs.range_from());
    const auto result_to = std::min(lhs.range_to(), rhs.range_to());
    return result_to < result_from
               ? Type::None()
               : WordType::Range(result_from, result_to, zone);
  }

  if (lhs_wrapping && rhs.is_wrapping()) {
    const auto result_from = std::max(lhs.range_from(), rhs.range_from());
    const auto result_to = std::min(lhs.range_to(), rhs.range_to());
    auto result = WordType::Range(result_from, result_to, zone);
    DCHECK(result.is_wrapping());
    return result;
  }

  const auto& x = lhs_wrapping ? lhs : rhs;
  const auto& y = lhs_wrapping ? rhs : lhs;
  DCHECK(x.is_wrapping());
  DCHECK(!y.is_wrapping());
  auto subrange_low = Intersect(y, Range(0, x.range_to(), zone),
                                ResolutionMode::kPreciseOrInvalid, zone);
  DCHECK(!subrange_low.IsInvalid());
  auto subrange_high = Intersect(
      y, Range(x.range_from(), std::numeric_limits<word_t>::max(), zone),
      ResolutionMode::kPreciseOrInvalid, zone);
  DCHECK(!subrange_high.IsInvalid());

  if (subrange_low.IsNone()) return subrange_high;
  if (subrange_high.IsNone()) return subrange_low;
  auto s_l = subrange_low.template AsWord<Bits>();
  auto s_h = subrange_high.template AsWord<Bits>();

  switch (resolution_mode) {
    case ResolutionMode::kPreciseOrInvalid:
      return Type::Invalid();
    case ResolutionMode::kOverApproximate:
      return LeastUpperBound(s_l, s_h, zone);
    case ResolutionMode::kGreatestLowerBound:
      return (s_l.unsigned_max() - s_l.unsigned_min() <
              s_h.unsigned_max() - s_h.unsigned_min())
                 ? s_h
                 : s_l;
  }
}

template <size_t Bits>
void WordType<Bits>::PrintTo(std::ostream& stream) const {
  stream << (Bits == 32 ? "Word32" : "Word64");
  switch (sub_kind()) {
    case SubKind::kRange:
      stream << "[0x" << std::hex << range_from() << ", 0x" << range_to()
             << std::dec << "]";
      break;
    case SubKind::kSet:
      stream << "{" << std::hex;
      for (int i = 0; i < set_size(); ++i) {
        stream << (i == 0 ? "0x" : ", 0x");
        stream << set_element(i);
      }
      stream << std::dec << "}";
      break;
  }
}

template <size_t Bits>
Handle<TurboshaftType> WordType<Bits>::AllocateOnHeap(Factory* factory) const {
  if constexpr (Bits == 32) {
    if (is_range()) {
      return factory->NewTurboshaftWord32RangeType(range_from(), range_to(),
                                                   AllocationType::kYoung);
    } else {
      DCHECK(is_set());
      auto result = factory->NewTurboshaftWord32SetType(set_size(),
                                                        AllocationType::kYoung);
      for (int i = 0; i < set_size(); ++i) {
        result->set_elements(i, set_element(i));
      }
      return result;
    }
  } else {
    if (is_range()) {
      const auto [from_high, from_low] = uint64_to_high_low(range_from());
      const auto [to_high, to_low] = uint64_to_high_low(range_to());
      return factory->NewTurboshaftWord64RangeType(
          from_high, from_low, to_high, to_low, AllocationType::kYoung);
    } else {
      DCHECK(is_set());
      auto result = factory->NewTurboshaftWord64SetType(set_size(),
                                                        AllocationType::kYoung);
      for (int i = 0; i < set_size(); ++i) {
        const auto [high, low] = uint64_to_high_low(set_element(i));
        result->set_elements_high(i, high);
        result->set_elements_low(i, low);
      }
      return result;
    }
  }
}

template <size_t Bits>
bool FloatType<Bits>::Contains(float_t value) const {
  if (IsMinusZero(value)) return has_minus_zero();
  if (std::isnan(value)) return has_nan();
  switch (sub_kind()) {
    case SubKind::kOnlySpecialValues:
      return false;
    case SubKind::kRange: {
      return range_min() <= value && value <= range_max();
    }
    case SubKind::kSet: {
      for (int i = 0; i < set_size(); ++i) {
        if (set_element(i) == value) return true;
      }
      return false;
    }
  }
}

template <size_t Bits>
bool FloatType<Bits>::Equals(const FloatType<Bits>& other) const {
  if (sub_kind() != other.sub_kind()) return false;
  if (special_values() != other.special_values()) return false;
  switch (sub_kind()) {
    case SubKind::kOnlySpecialValues:
      return true;
    case SubKind::kRange: {
      return range() == other.range();
    }
    case SubKind::kSet: {
      if (set_size() != other.set_size()) {
        return false;
      }
      for (int i = 0; i < set_size(); ++i) {
        if (set_element(i) != other.set_element(i)) return false;
      }
      return true;
    }
  }
}

template <size_t Bits>
bool FloatType<Bits>::IsSubtypeOf(const FloatType<Bits>& other) const {
  if (special_values() & ~other.special_values()) return false;
  switch (sub_kind()) {
    case SubKind::kOnlySpecialValues:
      return true;
    case SubKind::kRange:
      if (!other.is_range()) {
        // This relies on the fact that we don't have singleton ranges.
        DCHECK_NE(range_min(), range_max());
        return false;
      }
      return other.range_min() <= range_min() &&
             range_max() <= other.range_max();
    case SubKind::kSet: {
      switch (other.sub_kind()) {
        case SubKind::kOnlySpecialValues:
          return false;
        case SubKind::kRange:
          return other.range_min() <= min() && max() <= other.range_max();
        case SubKind::kSet:
          for (int i = 0; i < set_size(); ++i) {
            if (!other.Contains(set_element(i))) return false;
          }
          return true;
      }
    }
  }
}

template <size_t Bits>
// static
FloatType<Bits> FloatType<Bits>::LeastUpperBound(const FloatType<Bits>& lhs,
                                                 const FloatType<Bits>& rhs,
                                                 Zone* zone) {
  uint32_t special_values = lhs.special_values() | rhs.special_values();
  if (lhs.is_any() || rhs.is_any()) {
    return Any(special_values);
  }

  const bool lhs_finite = lhs.is_set() || lhs.is_only_special_values();
  const bool rhs_finite = rhs.is_set() || rhs.is_only_special_values();

  if (lhs_finite && rhs_finite) {
    base::SmallVector<float_t, kMaxSetSize * 2> result_elements;
    if (lhs.is_set()) base::vector_append(result_elements, lhs.set_elements());
    if (rhs.is_set()) base::vector_append(result_elements, rhs.set_elements());
    if (result_elements.empty()) {
      return OnlySpecialValues(special_values);
    }
    base::sort(result_elements);
    auto it = std::unique(result_elements.begin(), result_elements.end());
    result_elements.pop_back(std::distance(it, result_elements.end()));
    if (result_elements.size() <= kMaxSetSize) {
      return Set(result_elements, special_values, zone);
    }
    return Range(result_elements.front(), result_elements.back(),
                 special_values, zone);
  } else if (lhs.is_only_special_values()) {
    return ReplacedSpecialValues(rhs, special_values).template AsFloat<Bits>();
  } else if (rhs.is_only_special_values()) {
    return ReplacedSpecialValues(lhs, special_values).template AsFloat<Bits>();
  }

  // We need to construct a range.
  float_t result_min = std::min(lhs.range_or_set_min(), rhs.range_or_set_min());
  float_t result_max = std::max(lhs.range_or_set_max(), rhs.range_or_set_max());
  return Range(result_min, result_max, special_values, zone);
}

template <size_t Bits>
// static
Type FloatType<Bits>::Intersect(const FloatType<Bits>& lhs,
                                const FloatType<Bits>& rhs, Zone* zone) {
  const uint32_t special_values = lhs.special_values() & rhs.special_values();
  if (lhs.is_any()) return ReplacedSpecialValues(rhs, special_values);
  if (rhs.is_any()) return ReplacedSpecialValues(lhs, special_values);
  if (lhs.is_only_special_values() || rhs.is_only_special_values()) {
    return special_values ? OnlySpecialValues(special_values) : Type::None();
  }

  if (lhs.is_set() || rhs.is_set()) {
    const auto& x = lhs.is_set() ? lhs : rhs;
    const auto& y = lhs.is_set() ? rhs : lhs;
    base::SmallVector<float_t, kMaxSetSize * 2> result_elements;
    for (int i = 0; i < x.set_size(); ++i) {
      const float_t element = x.set_element(i);
      if (y.Contains(element)) result_elements.push_back(element);
    }
    if (result_elements.empty()) {
      return special_values ? OnlySpecialValues(special_values) : Type::None();
    }
    return Set(result_elements, special_values, zone);
  }

  DCHECK(lhs.is_range() && rhs.is_range());
  const float_t result_min = std::max(lhs.min(), rhs.min());
  const float_t result_max = std::min(lhs.max(), rhs.max());
  if (result_min < result_max) {
    return Range(result_min, result_max, special_values, zone);
  } else if (result_min == result_max) {
    return Set({result_min}, special_values, zone);
  }
  return special_values ? OnlySpecialValues(special_values) : Type::None();
}

template <size_t Bits>
void FloatType<Bits>::PrintTo(std::ostream& stream) const {
  auto PrintSpecials = [this](auto& stream) {
    if (has_nan()) {
      stream << "NaN" << (has_minus_zero() ? "|MinusZero" : "");
    } else {
      DCHECK(has_minus_zero());
      stream << "MinusZero";
    }
  };
  stream << (Bits == 32 ? "Float32" : "Float64");
  switch (sub_kind()) {
    case SubKind::kOnlySpecialValues:
      PrintSpecials(stream);
      break;
    case SubKind::kRange:
      stream << "[" << range_min() << ", " << range_max() << "]";
      if (has_special_values()) {
        stream << "|";
        PrintSpecials(stream);
      }
      break;
    case SubKind::kSet:
      stream << "{";
      for (int i = 0; i < set_size(); ++i) {
        if (i != 0) stream << ", ";
        stream << set_element(i);
      }
      if (has_special_values()) {
        stream << "}|";
        PrintSpecials(stream);
      } else {
        stream << "}";
      }
      break;
  }
}

template <size_t Bits>
Handle<TurboshaftType> FloatType<Bits>::AllocateOnHeap(Factory* factory) const {
  float_t min = 0.0f, max = 0.0f;
  constexpr uint32_t padding = 0;
  if (is_only_special_values()) {
    min = std::numeric_limits<float_t>::infinity();
    max = -std::numeric_limits<float_t>::infinity();
    return factory->NewTurboshaftFloat64RangeType(
        special_values(), padding, min, max, AllocationType::kYoung);
  } else if (is_range()) {
    std::tie(min, max) = minmax();
    return factory->NewTurboshaftFloat64RangeType(
        special_values(), padding, min, max, AllocationType::kYoung);
  } else {
    DCHECK(is_set());
    auto result = factory->NewTurboshaftFloat64SetType(
        special_values(), set_size(), AllocationType::kYoung);
    for (int i = 0; i < set_size(); ++i) {
      result->set_elements(i, set_element(i));
    }
    return result;
  }
}

bool TupleType::Equals(const TupleType& other) const {
  if (size() != other.size()) return false;
  for (int i = 0; i < size(); ++i) {
    if (!element(i).Equals(other.element(i))) return false;
  }
  return true;
}

bool TupleType::IsSubtypeOf(const TupleType& other) const {
  if (size() != other.size()) return false;
  for (int i = 0; i < size(); ++i) {
    if (!element(i).IsSubtypeOf(other.element(i))) return false;
  }
  return true;
}

// static
Type TupleType::LeastUpperBound(const TupleType& lhs, const TupleType& rhs,
                                Zone* zone) {
  if (lhs.size() != rhs.size()) return Type::Any();
  Payload p;
  p.array = zone->AllocateArray<Type>(lhs.size());
  for (int i = 0; i < lhs.size(); ++i) {
    p.array[i] = Type::LeastUpperBound(lhs.element(i), rhs.element(i), zone);
  }
  return TupleType{static_cast<uint8_t>(lhs.size()), p};
}

void TupleType::PrintTo(std::ostream& stream) const {
  stream << "(";
  for (int i = 0; i < size(); ++i) {
    if (i != 0) stream << ", ";
    element(i).PrintTo(stream);
  }
  stream << ")";
}

template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) WordType<32>;
template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) WordType<64>;
template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) FloatType<32>;
template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) FloatType<64>;

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/types.h                                                 0000664 0000000 0000000 00000074100 14746647661 0022146 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_TYPES_H_
#define V8_COMPILER_TURBOSHAFT_TYPES_H_

#include <cmath>
#include <limits>
#include <optional>

#include "src/base/container-utils.h"
#include "src/base/export-template.h"
#include "src/base/logging.h"
#include "src/base/small-vector.h"
#include "src/common/globals.h"
#include "src/compiler/turboshaft/fast-hash.h"
#include "src/numbers/conversions.h"
#include "src/objects/turboshaft-types.h"
#include "src/utils/ostreams.h"
#include "src/zone/zone-containers.h"

#ifdef DEBUG
#define TURBOSHAFT_TRACE_TYPING(...)                     \
  do {                                                   \
    if (V8_UNLIKELY(v8_flags.turboshaft_trace_typing)) { \
      PrintF(__VA_ARGS__);                               \
    }                                                    \
  } while (false)

#define TURBOSHAFT_TRACE_TYPING_WITH_COLOR(colorcode, str, ...)           \
  TURBOSHAFT_TRACE_TYPING(                                                \
      (v8_flags.log_colour ? ("\033[" colorcode "m" str "\033[m") : str), \
      __VA_ARGS__)
#define TURBOSHAFT_TRACE_TYPING_OK(str, ...) \
  TURBOSHAFT_TRACE_TYPING_WITH_COLOR("32", str, __VA_ARGS__)
#define TURBOSHAFT_TRACE_TYPING_FAIL(str, ...) \
  TURBOSHAFT_TRACE_TYPING_WITH_COLOR("31", str, __VA_ARGS__)
#else
#define TURBOSHAFT_TRACE_TYPING(...) ((void)0)
#define TURBOSHAFT_TRACE_TYPING_WITH_COLOR(colorcode, str, ...) ((void)0)
#define TURBOSHAFT_TRACE_TYPING_OK(str, ...) ((void)0)
#define TURBOSHAFT_TRACE_TYPING_FAIL(str, ...) ((void)0)
#endif  // DEBUG

namespace v8::internal {
class Factory;
}

namespace v8::internal::compiler::turboshaft {

namespace detail {

template <typename T>
inline bool is_unique_and_sorted(const T& container) {
  if (std::size(container) <= 1) return true;
  auto cur = std::begin(container);
  auto next = cur;
  for (++next; next != std::end(container); ++cur, ++next) {
    if (!(*cur < *next)) return false;
  }
  return true;
}

template <typename T>
inline bool is_minus_zero(T value) {
  return IsMinusZero(value);
}

template <typename T>
inline bool is_float_special_value(T value) {
  return std::isnan(value) || is_minus_zero(value);
}

template <size_t Bits>
struct TypeForBits;
template <>
struct TypeForBits<32> {
  using uint_type = uint32_t;
  using float_type = float;
  static constexpr float_type nan =
      std::numeric_limits<float_type>::quiet_NaN();
};
template <>
struct TypeForBits<64> {
  using uint_type = uint64_t;
  using float_type = double;
  static constexpr float_type nan =
      std::numeric_limits<float_type>::quiet_NaN();
};

// gcc versions < 9 may produce the following compilation error:
// > '<anonymous>' is used uninitialized in this function
// if Payload_Empty is initialized without any data, link to a relevant bug:
// https://gcc.gnu.org/bugzilla/show_bug.cgi?id=86465
// A workaround is to add a dummy value which is zero initialized by default.
// More information as well as a sample reproducible code can be found at the
// comment section of this CL crrev.com/c/4057111
// TODO(nicohartmann@): Remove dummy once all platforms are using gcc >= 9.
struct Payload_Empty {
  uint8_t dummy = 0;
};

template <typename T>
struct Payload_Range {
  T min;
  T max;
};

template <typename T>
struct Payload_InlineSet {
  T elements[2];
};

template <typename T>
struct Payload_OutlineSet {
  T* array;
};

}  // namespace detail

template <typename T>
std::enable_if_t<std::is_floating_point<T>::value, T> next_smaller(T v) {
  DCHECK(!std::isnan(v));
  DCHECK_LT(-std::numeric_limits<T>::infinity(), v);
  return std::nextafter(v, -std::numeric_limits<T>::infinity());
}

template <typename T>
std::enable_if_t<std::is_floating_point<T>::value, T> next_larger(T v) {
  DCHECK(!std::isnan(v));
  DCHECK_LT(v, std::numeric_limits<T>::infinity());
  return std::nextafter(v, std::numeric_limits<T>::infinity());
}

template <typename T>
std::enable_if_t<std::is_integral<T>::value, T> next_smaller(T v) {
  DCHECK_LT(std::numeric_limits<T>::min(), v);
  return v - 1;
}

template <typename T>
std::enable_if_t<std::is_integral<T>::value, T> next_larger(T v) {
  DCHECK_LT(v, std::numeric_limits<T>::max());
  return v + 1;
}

template <size_t Bits>
using uint_type = typename detail::TypeForBits<Bits>::uint_type;
template <size_t Bits>
using float_type = typename detail::TypeForBits<Bits>::float_type;
template <size_t Bits>
constexpr float_type<Bits> nan_v = detail::TypeForBits<Bits>::nan;

template <size_t Bits>
class WordType;
template <size_t Bits>
class FloatType;
class TupleType;

using Word32Type = WordType<32>;
using Word64Type = WordType<64>;
using Float32Type = FloatType<32>;
using Float64Type = FloatType<64>;

class V8_EXPORT_PRIVATE Type {
 public:
  enum class Kind : uint8_t {
    kInvalid,
    kNone,
    kWord32,
    kWord64,
    kFloat32,
    kFloat64,
    kTuple,
    kAny,
  };

  // Some operations cannot express the result precisely in a type, e.g. when an
  // intersection with a wrapping range may produce to disconnect subranges,
  // which cannot be represented. {ResolutionMode} allows to specify what the
  // operation should do when the result cannot be represented precisely.
  enum class ResolutionMode {
    // Return Type::Invalid().
    kPreciseOrInvalid,
    // Return a safe over approximation.
    kOverApproximate,
    // Return the greatest lower bound that can be represented.
    kGreatestLowerBound,
  };

  Type() : Type(Kind::kInvalid) {}

  // Type constructors
  static inline Type Invalid() { return Type(); }
  static inline Type None() { return Type(Kind::kNone); }
  static inline Type Any() { return Type(Kind::kAny); }

  // Checks and casts
  inline Kind kind() const { return kind_; }
  inline bool IsInvalid() const { return kind_ == Kind::kInvalid; }
  inline bool IsNone() const { return kind_ == Kind::kNone; }
  inline bool IsWord32() const { return kind_ == Kind::kWord32; }
  inline bool IsWord64() const { return kind_ == Kind::kWord64; }
  inline bool IsFloat32() const { return kind_ == Kind::kFloat32; }
  inline bool IsFloat64() const { return kind_ == Kind::kFloat64; }
  inline bool IsTuple() const { return kind_ == Kind::kTuple; }
  inline bool IsAny() const { return kind_ == Kind::kAny; }
  template <size_t B>
  inline bool IsWord() const {
    static_assert(B == 32 || B == 64);
    if constexpr (B == 32)
      return IsWord32();
    else
      return IsWord64();
  }
  template <size_t B>
  inline bool IsFloat() const {
    static_assert(B == 32 || B == 64);
    if constexpr (B == 32)
      return IsFloat32();
    else
      return IsFloat64();
  }

  // Casts
  inline const Word32Type& AsWord32() const;
  inline const Word64Type& AsWord64() const;
  inline const Float32Type& AsFloat32() const;
  inline const Float64Type& AsFloat64() const;
  inline const TupleType& AsTuple() const;
  template <size_t B>
  inline const auto& AsWord() const {
    static_assert(B == 32 || B == 64);
    if constexpr (B == 32)
      return AsWord32();
    else
      return AsWord64();
  }
  template <size_t B>
  inline const auto& AsFloat() const {
    static_assert(B == 32 || B == 64);
    if constexpr (B == 32)
      return AsFloat32();
    else
      return AsFloat64();
  }

  // Comparison
  bool Equals(const Type& other) const;
  bool IsSubtypeOf(const Type& other) const;

  // Printing
  void PrintTo(std::ostream& stream) const;
  void Print() const;
  std::string ToString() const {
    std::stringstream stream;
    PrintTo(stream);
    return stream.str();
  }

  // Other functions
  static Type LeastUpperBound(const Type& lhs, const Type& rhs, Zone* zone);
  static std::optional<Type> ParseFromString(const std::string_view& str,
                                             Zone* zone);
  Handle<TurboshaftType> AllocateOnHeap(Factory* factory) const;

 protected:
  template <typename Payload>
  Type(Kind kind, uint8_t sub_kind, uint8_t set_size, uint32_t bitfield,
       uint8_t reserved, const Payload& payload)
      : kind_(kind),
        sub_kind_(sub_kind),
        set_size_(set_size),
        reserved_(reserved),
        bitfield_(bitfield) {
    static_assert(sizeof(Payload) <= sizeof(payload_));
    memcpy(&payload_[0], &payload, sizeof(Payload));
    if constexpr (sizeof(Payload) < sizeof(payload_)) {
      memset(reinterpret_cast<uint8_t*>(&payload_[0]) + sizeof(Payload), 0x00,
             sizeof(payload_) - sizeof(Payload));
    }
  }

  template <typename Payload>
  const Payload& get_payload() const {
    static_assert(sizeof(Payload) <= sizeof(payload_));
    return *reinterpret_cast<const Payload*>(&payload_[0]);
  }

  union {
    struct {
      Kind kind_;
      uint8_t sub_kind_;
      uint8_t set_size_;
      uint8_t reserved_;
      uint32_t bitfield_;
    };
    // {header_} can be  used for faster hashing or comparison.
    uint64_t header_;
  };

 private:
  // Access through get_payload<>().
  uint64_t payload_[2];  // Type specific data

  friend struct fast_hash<Type>;
  explicit Type(Kind kind) : Type(kind, 0, 0, 0, 0, detail::Payload_Empty{}) {
    DCHECK(kind == Kind::kInvalid || kind == Kind::kNone || kind == Kind::kAny);
  }
};
static_assert(sizeof(Type) == 24);

template <size_t Bits>
class WordType : public Type {
  static_assert(Bits == 32 || Bits == 64);
  friend class Type;
  static constexpr int kMaxInlineSetSize = 2;

  enum class SubKind : uint8_t {
    kRange,
    kSet,
  };

 public:
  static constexpr int kMaxSetSize = 8;
  using word_t = uint_type<Bits>;
  using value_type = word_t;

  // Constructors
  static WordType Any() {
    return Range(0, std::numeric_limits<word_t>::max(), nullptr);
  }
  static WordType Range(word_t from, word_t to, Zone* zone) {
    // Normalize ranges smaller than {kMaxSetSize} to sets.
    if (to >= from) {
      // (to - from + 1) <= kMaxSetSize
      if (to - from <= kMaxSetSize - 1) {
        // Normalizing non-wrapping ranges to a Set.
        base::SmallVector<word_t, kMaxSetSize> elements;
        for (word_t i = from; i < to; ++i) elements.push_back(i);
        elements.push_back(to);
        return Set(elements, zone);
      }
    } else {
      // (max - from + 1) + (to + 1) <= kMaxSetSize
      if ((std::numeric_limits<word_t>::max() - from + to) <= kMaxSetSize - 2) {
        // Normalizing wrapping ranges to a Set.
        base::SmallVector<word_t, kMaxSetSize> elements;
        for (word_t i = from; i < std::numeric_limits<word_t>::max(); ++i) {
          elements.push_back(i);
        }
        elements.push_back(std::numeric_limits<word_t>::max());
        for (word_t i = 0; i < to; ++i) elements.push_back(i);
        elements.push_back(to);
        base::sort(elements);
        return Set(elements, zone);
      }
    }
    return WordType{SubKind::kRange, 0, Payload_Range{from, to}};
  }
  template <size_t N>
  static WordType Set(const base::SmallVector<word_t, N>& elements,
                      Zone* zone) {
    return Set(base::Vector<const word_t>{elements.data(), elements.size()},
               zone);
  }
  static WordType Set(const std::vector<word_t>& elements, Zone* zone) {
    return Set(base::Vector<const word_t>{elements.data(), elements.size()},
               zone);
  }
  static WordType Set(const std::initializer_list<word_t>& elements,
                      Zone* zone) {
    return Set(base::Vector<const word_t>{elements.begin(), elements.size()},
               zone);
  }
  static WordType Set(base::Vector<const word_t> elements, Zone* zone) {
    DCHECK(detail::is_unique_and_sorted(elements));
    DCHECK_IMPLIES(elements.size() > kMaxInlineSetSize, zone != nullptr);
    DCHECK_GT(elements.size(), 0);
    DCHECK_LE(elements.size(), kMaxSetSize);

    if (elements.size() <= kMaxInlineSetSize) {
      // Use inline storage.
      Payload_InlineSet p;
      DCHECK_LT(0, elements.size());
      p.elements[0] = elements[0];
      if (elements.size() > 1) p.elements[1] = elements[1];
      return WordType{SubKind::kSet, static_cast<uint8_t>(elements.size()), p};
    } else {
      // Allocate storage in the zone.
#if defined(__GNUC__) && !defined(__clang__)
      // Work around a spurious GCC-12 warning. The DCHECK above already
      // checks the right precondition.
      if (zone == nullptr) return WordType::Any();
#endif
      Payload_OutlineSet p;
      p.array = zone->AllocateArray<word_t>(elements.size());
      DCHECK_NOT_NULL(p.array);
      for (size_t i = 0; i < elements.size(); ++i) p.array[i] = elements[i];
      return WordType{SubKind::kSet, static_cast<uint8_t>(elements.size()), p};
    }
  }
  static WordType Constant(word_t constant) { return Set({constant}, nullptr); }

  // Checks
  bool is_range() const { return sub_kind() == SubKind::kRange; }
  bool is_set() const { return sub_kind() == SubKind::kSet; }
  bool is_any() const { return is_range() && range_to() + 1 == range_from(); }
  bool is_constant() const {
    DCHECK_EQ(set_size_ > 0, is_set());
    return set_size_ == 1;
  }
  bool is_wrapping() const { return is_range() && range_from() > range_to(); }

  // Accessors
  word_t range_from() const {
    DCHECK(is_range());
    return get_payload<Payload_Range>().min;
  }
  word_t range_to() const {
    DCHECK(is_range());
    return get_payload<Payload_Range>().max;
  }
  std::pair<word_t, word_t> range() const {
    DCHECK(is_range());
    return {range_from(), range_to()};
  }
  int set_size() const {
    DCHECK(is_set());
    return static_cast<int>(set_size_);
  }
  word_t set_element(int index) const {
    DCHECK(is_set());
    DCHECK_GE(index, 0);
    DCHECK_LT(index, set_size());
    return set_elements()[index];
  }
  base::Vector<const word_t> set_elements() const {
    DCHECK(is_set());
    if (set_size() <= kMaxInlineSetSize) {
      return base::Vector<const word_t>(
          get_payload<Payload_InlineSet>().elements, set_size());
    } else {
      return base::Vector<const word_t>(get_payload<Payload_OutlineSet>().array,
                                        set_size());
    }
  }
  std::optional<word_t> try_get_constant() const {
    if (!is_constant()) return std::nullopt;
    DCHECK(is_set());
    DCHECK_EQ(set_size(), 1);
    return set_element(0);
  }
  bool is_constant(word_t value) const {
    if (auto c = try_get_constant()) return *c == value;
    return false;
  }
  word_t unsigned_min() const {
    switch (sub_kind()) {
      case SubKind::kRange:
        return is_wrapping() ? word_t{0} : range_from();
      case SubKind::kSet:
        return set_element(0);
    }
  }
  word_t unsigned_max() const {
    switch (sub_kind()) {
      case SubKind::kRange:
        return is_wrapping() ? std::numeric_limits<word_t>::max() : range_to();
      case SubKind::kSet:
        DCHECK_GE(set_size(), 1);
        return set_element(set_size() - 1);
    }
  }

  // Misc
  bool Contains(word_t value) const;
  bool Equals(const WordType& other) const;
  bool IsSubtypeOf(const WordType& other) const;
  static WordType LeastUpperBound(const WordType& lhs, const WordType& rhs,
                                  Zone* zone);
  static Type Intersect(const WordType& lhs, const WordType& rhs,
                        ResolutionMode resolution_mode, Zone* zone);
  void PrintTo(std::ostream& stream) const;
  Handle<TurboshaftType> AllocateOnHeap(Factory* factory) const;

 private:
  static constexpr Kind KIND = Bits == 32 ? Kind::kWord32 : Kind::kWord64;
  using Payload_Range = detail::Payload_Range<word_t>;
  using Payload_InlineSet = detail::Payload_InlineSet<word_t>;
  using Payload_OutlineSet = detail::Payload_OutlineSet<word_t>;

  SubKind sub_kind() const { return static_cast<SubKind>(sub_kind_); }
  template <typename Payload>
  WordType(SubKind sub_kind, uint8_t set_size, const Payload& payload)
      : Type(KIND, static_cast<uint8_t>(sub_kind), set_size, 0, 0, payload) {}
};

extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) WordType<32>;
extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) WordType<64>;

template <size_t Bits>
class FloatType : public Type {
  static_assert(Bits == 32 || Bits == 64);
  friend class Type;
  static constexpr int kMaxInlineSetSize = 2;

  enum class SubKind : uint8_t {
    kRange,
    kSet,
    kOnlySpecialValues,
  };

 public:
  static constexpr int kMaxSetSize = 8;
  using float_t = float_type<Bits>;
  using value_type = float_t;

  enum Special : uint32_t {
    kNoSpecialValues = 0x0,
    kNaN = 0x1,
    kMinusZero = 0x2,
  };

  // Constructors
  static FloatType OnlySpecialValues(uint32_t special_values) {
    DCHECK_NE(0, special_values);
    return FloatType{SubKind::kOnlySpecialValues, 0, special_values,
                     Payload_OnlySpecial{}};
  }
  static FloatType NaN() {
    return FloatType{SubKind::kOnlySpecialValues, 0, Special::kNaN,
                     Payload_OnlySpecial{}};
  }
  static FloatType MinusZero() {
    return FloatType{SubKind::kOnlySpecialValues, 0, Special::kMinusZero,
                     Payload_OnlySpecial{}};
  }
  static FloatType Any(uint32_t special_values = Special::kNaN |
                                                 Special::kMinusZero) {
    return FloatType::Range(-std::numeric_limits<float_t>::infinity(),
                            std::numeric_limits<float_t>::infinity(),
                            special_values, nullptr);
  }
  static FloatType Range(float_t min, float_t max, Zone* zone) {
    return Range(min, max, Special::kNoSpecialValues, zone);
  }
  static FloatType Range(float_t min, float_t max, uint32_t special_values,
                         Zone* zone) {
    special_values |= IdentifyMinusZero(min);
    special_values |= IdentifyMinusZero(max);
    DCHECK(!detail::is_float_special_value(min));
    DCHECK(!detail::is_float_special_value(max));
    DCHECK_LE(min, max);
    if (min == max) return Set({min}, special_values, zone);
    return FloatType{SubKind::kRange, 0, special_values,
                     Payload_Range{min, max}};
  }
  template <size_t N>
  static FloatType Set(const base::SmallVector<const float_t, N>& elements,
                       Zone* zone) {
    return Set(elements, Special::kNoSpecialValues, zone);
  }
  template <size_t N>
  static FloatType Set(const base::SmallVector<float_t, N>& elements,
                       uint32_t special_values, Zone* zone) {
    return Set(base::Vector<const float_t>{elements.data(), elements.size()},
               special_values, zone);
  }
  static FloatType Set(const std::initializer_list<float_t>& elements,
                       uint32_t special_values, Zone* zone) {
    return Set(base::Vector<const float_t>{elements.begin(), elements.size()},
               special_values, zone);
  }
  static FloatType Set(const std::vector<float_t>& elements, Zone* zone) {
    return Set(elements, Special::kNoSpecialValues, zone);
  }
  static FloatType Set(const std::vector<float_t>& elements,
                       uint32_t special_values, Zone* zone) {
    return Set(base::Vector<const float_t>{elements.data(), elements.size()},
               special_values, zone);
  }
  static FloatType Set(base::Vector<const float_t> elements,
                       uint32_t special_values, Zone* zone) {
    DCHECK(detail::is_unique_and_sorted(elements));
    // NaN should be passed via {special_values} rather than {elements}.
    DCHECK(base::none_of(elements, [](float_t f) { return std::isnan(f); }));
    DCHECK_IMPLIES(elements.size() > kMaxInlineSetSize, zone != nullptr);
    DCHECK_GT(elements.size(), 0);
    DCHECK_LE(elements.size(), kMaxSetSize);

    if (elements.size() <= kMaxInlineSetSize) {
      // Use inline storage.
      Payload_InlineSet p;
      DCHECK_LT(0, elements.size());
      p.elements[0] = elements[0];
      special_values |= IdentifyMinusZero(p.elements[0]);
      if (elements.size() > 1) {
        p.elements[1] = elements[1];
        special_values |= IdentifyMinusZero(p.elements[1]);
      }
      return FloatType{SubKind::kSet, static_cast<uint8_t>(elements.size()),
                       special_values, p};
    } else {
      // Allocate storage in the zone.
      Payload_OutlineSet p;
      p.array = zone->AllocateArray<float_t>(elements.size());
      DCHECK_NOT_NULL(p.array);
      for (size_t i = 0; i < elements.size(); ++i) {
        p.array[i] = elements[i];
        special_values |= IdentifyMinusZero(p.array[i]);
      }
      return FloatType{SubKind::kSet, static_cast<uint8_t>(elements.size()),
                       special_values, p};
    }
  }
  static FloatType Constant(float_t constant) {
    return Set({constant}, 0, nullptr);
  }

  // Checks
  bool is_only_special_values() const {
    return sub_kind() == SubKind::kOnlySpecialValues;
  }
  bool is_only_nan() const {
    return is_only_special_values() && (special_values() == Special::kNaN);
  }
  bool is_only_minus_zero() const {
    return is_only_special_values() &&
           (special_values() == Special::kMinusZero);
  }
  bool is_range() const { return sub_kind() == SubKind::kRange; }
  bool is_set() const { return sub_kind() == SubKind::kSet; }
  bool is_any() const {
    return is_range() &&
           range_min() == -std::numeric_limits<float_t>::infinity() &&
           range_max() == std::numeric_limits<float_t>::infinity();
  }
  bool is_constant() const {
    DCHECK_EQ(set_size_ > 0, is_set());
    return set_size_ == 1 && !has_special_values();
  }
  uint32_t special_values() const { return bitfield_; }
  bool has_special_values() const { return special_values() != 0; }
  bool has_nan() const { return (special_values() & Special::kNaN) != 0; }
  bool has_minus_zero() const {
    return (special_values() & Special::kMinusZero) != 0;
  }

  // Accessors
  float_t range_min() const {
    DCHECK(is_range());
    return get_payload<Payload_Range>().min;
  }
  float_t range_max() const {
    DCHECK(is_range());
    return get_payload<Payload_Range>().max;
  }
  std::pair<float_t, float_t> range() const {
    DCHECK(is_range());
    return {range_min(), range_max()};
  }
  int set_size() const {
    DCHECK(is_set());
    return static_cast<int>(set_size_);
  }
  float_t set_element(int index) const {
    DCHECK(is_set());
    DCHECK_GE(index, 0);
    DCHECK_LT(index, set_size());
    return set_elements()[index];
  }
  base::Vector<const float_t> set_elements() const {
    DCHECK(is_set());
    if (set_size() <= kMaxInlineSetSize) {
      return base::Vector<const float_t>(
          get_payload<Payload_InlineSet>().elements, set_size());
    } else {
      return base::Vector<const float_t>(
          get_payload<Payload_OutlineSet>().array, set_size());
    }
  }
  float_t min() const {
    switch (sub_kind()) {
      case SubKind::kOnlySpecialValues:
        if (has_minus_zero()) return float_t{-0.0};
        DCHECK(is_only_nan());
        return nan_v<Bits>;
      case SubKind::kRange:
        if (has_minus_zero()) return std::min(float_t{-0.0}, range_min());
        return range_min();
      case SubKind::kSet:
        if (has_minus_zero()) return std::min(float_t{-0.0}, set_element(0));
        return set_element(0);
    }
  }
  float_t max() const {
    switch (sub_kind()) {
      case SubKind::kOnlySpecialValues:
        if (has_minus_zero()) return float_t{-0.0};
        DCHECK(is_only_nan());
        return nan_v<Bits>;
      case SubKind::kRange:
        if (has_minus_zero()) return std::max(float_t{-0.0}, range_max());
        return range_max();
      case SubKind::kSet:
        if (has_minus_zero()) {
          return std::max(float_t{-0.0}, set_element(set_size() - 1));
        }
        return set_element(set_size() - 1);
    }
  }
  std::pair<float_t, float_t> minmax() const { return {min(), max()}; }
  std::optional<float_t> try_get_constant() const {
    if (!is_constant()) return std::nullopt;
    DCHECK(is_set());
    DCHECK_EQ(set_size(), 1);
    return set_element(0);
  }
  bool is_constant(float_t value) const {
    if (V8_UNLIKELY(std::isnan(value))) return is_only_nan();
    if (V8_UNLIKELY(IsMinusZero(value))) return is_only_minus_zero();
    if (auto c = try_get_constant()) return *c == value;
    return false;
  }
  // Returns the minimium value of a range or set, ignoring any special values
  // (in contrast to min() above).
  float_t range_or_set_min() const {
    switch (sub_kind()) {
      case SubKind::kOnlySpecialValues:
        UNREACHABLE();
      case SubKind::kRange:
        return range_min();
      case SubKind::kSet:
        return set_element(0);
    }
  }
  // Returns the maximum value of a range or set, ignoring any special values
  // (in contrast to max() above).
  float_t range_or_set_max() const {
    switch (sub_kind()) {
      case SubKind::kOnlySpecialValues:
        UNREACHABLE();
      case SubKind::kRange:
        return range_max();
      case SubKind::kSet:
        return set_element(set_size() - 1);
    }
  }
  std::pair<float_t, float_t> range_or_set_minmax() const {
    return {range_or_set_min(), range_or_set_max()};
  }

  // Misc
  bool Contains(float_t value) const;
  bool Equals(const FloatType& other) const;
  bool IsSubtypeOf(const FloatType& other) const;
  static FloatType LeastUpperBound(const FloatType& lhs, const FloatType& rhs,
                                   Zone* zone);
  static Type Intersect(const FloatType& lhs, const FloatType& rhs, Zone* zone);
  void PrintTo(std::ostream& stream) const;
  Handle<TurboshaftType> AllocateOnHeap(Factory* factory) const;

 private:
  // This helper turns a -0 into a 0 in {value} and returns the
  // Special::kMinusZero flag in that case. Otherwise the {value} is unchanged
  // and Special::kNoSpecialValues is returned.
  static uint32_t IdentifyMinusZero(float_t& value) {
    if (V8_UNLIKELY(detail::is_minus_zero(value))) {
      value = float_t{0};
      return Special::kMinusZero;
    }
    return Special::kNoSpecialValues;
  }
  static Type ReplacedSpecialValues(const FloatType& t,
                                    uint32_t special_values) {
    if (special_values == 0 && t.is_only_special_values()) {
      return FloatType::None();
    }
    auto result = t;
    result.bitfield_ = special_values;
    DCHECK_EQ(result.bitfield_, result.special_values());
    return result;
  }

  static constexpr Kind KIND = Bits == 32 ? Kind::kFloat32 : Kind::kFloat64;
  SubKind sub_kind() const { return static_cast<SubKind>(sub_kind_); }
  using Payload_Range = detail::Payload_Range<float_t>;
  using Payload_InlineSet = detail::Payload_InlineSet<float_t>;
  using Payload_OutlineSet = detail::Payload_OutlineSet<float_t>;
  using Payload_OnlySpecial = detail::Payload_Empty;

  template <typename Payload>
  FloatType(SubKind sub_kind, uint8_t set_size, uint32_t special_values,
            const Payload& payload)
      : Type(KIND, static_cast<uint8_t>(sub_kind), set_size, special_values, 0,
             payload) {
    DCHECK_EQ(special_values & ~(Special::kNaN | Special::kMinusZero), 0);
  }
};

extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) FloatType<32>;
extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) FloatType<64>;

class TupleType : public Type {
 public:
  static constexpr int kMaxTupleSize = std::numeric_limits<uint8_t>::max();

  // Constructors
  static TupleType Tuple(const Type& element0, const Type& element1,
                         Zone* zone) {
    Payload p;
    p.array = zone->AllocateArray<Type>(2);
    DCHECK_NOT_NULL(p.array);
    p.array[0] = element0;
    p.array[1] = element1;
    return TupleType{2, p};
  }

  static TupleType Tuple(base::Vector<Type> elements, Zone* zone) {
    DCHECK_LE(elements.size(), kMaxTupleSize);
    Payload p;
    p.array = zone->AllocateArray<Type>(elements.size());
    DCHECK_NOT_NULL(p.array);
    for (size_t i = 0; i < elements.size(); ++i) {
      p.array[i] = elements[i];
    }
    return TupleType{static_cast<uint8_t>(elements.size()), p};
  }

  // Accessors
  int size() const { return static_cast<int>(set_size_); }
  const Type& element(int index) const {
    DCHECK_LE(0, index);
    DCHECK_LT(index, size());
    return get_payload<Payload>().array[index];
  }
  base::Vector<Type> elements() const {
    return base::Vector<Type>{get_payload<Payload>().array,
                              static_cast<size_t>(size())};
  }

  // Misc
  bool Equals(const TupleType& other) const;
  bool IsSubtypeOf(const TupleType& other) const;
  static Type LeastUpperBound(const TupleType& lhs, const TupleType& rhs,
                              Zone* zone);
  void PrintTo(std::ostream& stream) const;

 private:
  static constexpr Kind KIND = Kind::kTuple;
  using Payload = detail::Payload_OutlineSet<Type>;

  TupleType(uint8_t tuple_size, const Payload& payload)
      : Type(KIND, 0, tuple_size, 0, 0, payload) {}
};

const Word32Type& Type::AsWord32() const {
  DCHECK(IsWord32());
  return *static_cast<const Word32Type*>(this);
}

const Word64Type& Type::AsWord64() const {
  DCHECK(IsWord64());
  return *static_cast<const Word64Type*>(this);
}

const Float32Type& Type::AsFloat32() const {
  DCHECK(IsFloat32());
  return *static_cast<const Float32Type*>(this);
}

const Float64Type& Type::AsFloat64() const {
  DCHECK(IsFloat64());
  return *static_cast<const Float64Type*>(this);
}

const TupleType& Type::AsTuple() const {
  DCHECK(IsTuple());
  return *static_cast<const TupleType*>(this);
}

inline std::ostream& operator<<(std::ostream& stream, Type::Kind kind) {
  switch (kind) {
    case Type::Kind::kInvalid:
      return stream << "Invalid";
    case Type::Kind::kNone:
      return stream << "None";
    case Type::Kind::kWord32:
      return stream << "Word32";
    case Type::Kind::kWord64:
      return stream << "Word64";
    case Type::Kind::kFloat32:
      return stream << "Float32";
    case Type::Kind::kFloat64:
      return stream << "Float64";
    case Type::Kind::kTuple:
      return stream << "Tuple";
    case Type::Kind::kAny:
      return stream << "Any";
  }
}

inline std::ostream& operator<<(std::ostream& stream, const Type& type) {
  type.PrintTo(stream);
  return stream;
}

inline bool operator==(const Type& lhs, const Type& rhs) {
  return lhs.Equals(rhs);
}

inline bool operator!=(const Type& lhs, const Type& rhs) {
  return !lhs.Equals(rhs);
}

template <>
struct fast_hash<Type> {
  size_t operator()(const Type& v) const {
    // TODO(nicohartmann@): Fix fast_hash for outline payload once this is
    // required.
    UNREACHABLE();
    // return fast_hash_combine(v.header_, v.payload_[0], v.payload_[1]);
  }
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_TYPES_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/undef-assembler-macros.inc                              0000664 0000000 0000000 00000001657 14746647661 0025671 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

// PRESUBMIT_INTENTIONALLY_MISSING_INCLUDE_GUARD

// This file undefines Turboshaft's assembler macros. Include this file after
// your reducer and don't forget to include 'define-assembler-macros.inc' before.

#ifndef V8_COMPILER_TURBOSHAFT_ASSEMBLER_MACROS_DEFINED
#error \
    "Assembler macros not defined. Did you forget to #include \"define-assembler-macros.inc\" in this file?"
#endif

#undef __

#undef Assert

#undef BIND
#undef BIND_LOOP
#undef BREAK
#undef CONTINUE
#undef ELSE
#undef FOREACH
#undef FOREACH_IMPL_2
#undef FOREACH_IMPL_3
#undef GOTO
#undef GOTO_IF
#undef GOTO_IF_NOT
#undef IF
#undef IF_NOT
#undef LIKLEY
#undef REDUCE
#undef REDUCE_INPUT_GRAPH
#undef TSA_DCHECK
#undef UNLIKELY
#undef WHILE

#undef V8_COMPILER_TURBOSHAFT_ASSEMBLER_MACROS_DEFINED
                                                                                 node-23.7.0/deps/v8/src/compiler/turboshaft/uniform-reducer-adapter.h                               0000664 0000000 0000000 00000016664 14746647661 0025541 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_UNIFORM_REDUCER_ADAPTER_H_
#define V8_COMPILER_TURBOSHAFT_UNIFORM_REDUCER_ADAPTER_H_

#include "src/compiler/turboshaft/operations.h"

namespace v8::internal::compiler::turboshaft {

// UniformReducerAdapter allows to handle all operations uniformly during a
// reduction by wiring all ReduceInputGraphXyz and ReduceXyz calls through
// a single ReduceInputGraphOperation and ReduceOperation, respectively.
//
// This is how to use the adapter with your reducer MyReducer, which can then
// be used in a ReducerStack like any other reducer):
//
// template <typename Next>
// class MyReducer : public UniformReducerAdapter<MyReducer, Next> {
//  public:
//   TURBOSHAFT_REDUCER_BOILERPLATE()
//   using Adapter = UniformReducerAdapter<MyReducer, Next>;
//
//   OpIndex ReduceInputGraphConstant(OpIndex ig_index, const ConstantOp& op) {
//     /* Handle ConstantOps separately */
//     /* ... */
//
//     /* Call Adapter::ReduceInputGraphConstant(index, op) to also run */
//     /* through the generic handling in ReduceInputGraphOperation */
//     return Next::ReduceInputGraphConstant(index, op);
//   }
//
//   template <typename Op, typename Continuation>
//   OpIndex ReduceInputGraphOperation(OpIndex ig_index, const Op& op) {
//     /* Handle all (other) operations uniformly */
//     /* ... */
//
//     /* Forward to next reducer using the Continuation object */
//     return Continuation{this}.ReduceInputGraph(ig_index, op);
//   }
//
//   OpIndex ReduceConstant(ConstantOp::Kind kind, ConstantOp::Storage st) {
//     /* Handle Constants separately */
//     /* ... */
//
//     /* Call Adapter::ReduceConstant(kind, st) to also run through the */
//     /* generic handling in ReduceOperation */
//     return Next::ReduceConstant(kind, st);
//   }
//
//   template <Opcode opcode, typename Continuation, typename... Args>
//   OpIndex ReduceOperation(Args... args) {
//     /* Handle all (other) operations uniformly */
//     /* ... */
//
//     /* Forward to next reducer using the Continuation object */
//     return Continuation{this}.Reduce(args...);
//   }
//
//  private:
//   /* ... */
// };
//
// NOTICE: Inside the ReduceXyz and ReduceInputGraphXyz callbacks of MyReducer,
// you need to make a choice:
//
//   A) Call Next::ReduceXyz (or Next::ReduceInputGraphXyz) to forward to the
//      next reducer in the stack. Then the uniform ReduceOperation (and
//      ReduceInputGraphOperation) of the current reducer is not visited for
//      OperationXyz.
//   B) Call Adapter::ReduceXyz (or Adapter::ReduceInputGraphXyz) to forward to
//      the uniform ReduceOperation (and ReduceInputGraphOperation) such that
//      OperationXyz is also processed by those (in addition to the special
//      handling in ReduceXyz and ReduceInputGraphXyz).
//
// For the above MyReducer, consider this CopyingPhase<R1, MyReducer, R2>.
// Then the ReduceInputGraph (RIG) and Reduce (R) implementations are visited as
// follows for Operations OpA and OpB (and all other operations that are not
// ConstantOp), when all reducers just forward to Next. For ConstantOp, the
// reduction is equivalent to any "normal" reducer that does not use a
// UniformReducerAdapter.
//
//
// InputGraph OpA                     OpB     ____________________________
//             |                       |     |  ___                       |
//             |                       |     | |   |                      |
//             v                       v     | |   v                      v
// R1        RIGOpA                  RIGOpB  | |  ROpA                   ROpB
//             |     __          __    |     | |   |    ___        ___    |
//             |    |  |        |  |   |     | |   |   |   |      |   |   |
//             |    |  v        v  |   |     | |   |   |   v      v   |   |
// MyReducer   |    | RIGOperation |   |     | |   |   |  ROperation  |   |
//             v    |      v       |   |     | |   v   |      v       |   v
// (Adapter) RIGOpA | Continuation | RIGOpB  | |  ROpA | Continuation |  ROpB
//             |____|  |        |  |___|     | |   |___|  |        |  |___|
//                     |        |            | |          |        |
//              _______|        |______      | |    ______|        |______
//             |                       |     | |   |                      |
//             |                       |     | |   |                      |
//             v                       v     | |   v                      v
// R2        RIGOpA                  RIGOpB  | |  ROpA                   ROpB
//             |                       |_____| |   |                      |
//             |_______________________________|   |                      |
//                                                 v                      v
// OutputGraph                                    OpA                    OpB
//
//
template <template <typename> typename Reducer, typename Next>
class UniformReducerAdapter : public Next {
 public:
  template <Opcode opcode, typename Continuation, typename... Args>
  auto ReduceOperation(Args... args) {
    return Continuation{this}.Reduce(args...);
  }

  template <typename Op, typename Continuation>
  auto ReduceInputGraphOperation(OpIndex ig_index, const Op& operation) {
    return Continuation{this}.ReduceInputGraph(ig_index, operation);
  }

#define REDUCE(op)                                                           \
  struct Reduce##op##Continuation final {                                    \
    explicit Reduce##op##Continuation(Next* _this) : this_(_this) {}         \
    using Op = op##Op;                                                       \
    auto ReduceInputGraph(OpIndex ig_index, const op##Op& operation) {       \
      return this_->ReduceInputGraph##op(ig_index, operation);               \
    }                                                                        \
    template <typename... Args>                                              \
    auto Reduce(Args... args) const {                                        \
      return this_->Reduce##op(args...);                                     \
    }                                                                        \
    Next* this_;                                                             \
  };                                                                         \
  auto ReduceInputGraph##op(OpIndex ig_index, const op##Op& operation) {     \
    return static_cast<Reducer<Next>*>(this)                                 \
        ->template ReduceInputGraphOperation<op##Op,                         \
                                             Reduce##op##Continuation>(      \
            ig_index, operation);                                            \
  }                                                                          \
  template <typename... Args>                                                \
  auto Reduce##op(Args... args) {                                            \
    return static_cast<Reducer<Next>*>(this)                                 \
        ->template ReduceOperation<Opcode::k##op, Reduce##op##Continuation>( \
            args...);                                                        \
  }
  TURBOSHAFT_OPERATION_LIST(REDUCE)
#undef REDUCE
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_UNIFORM_REDUCER_ADAPTER_H_
                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/use-map.cc                                              0000664 0000000 0000000 00000006375 14746647661 0022520 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/use-map.h"

#include "src/compiler/turboshaft/graph.h"

namespace v8::internal::compiler::turboshaft {

UseMap::UseMap(const Graph& graph, Zone* zone, FunctionType filter)
    : table_(graph.op_id_count(), zone, &graph),
      uses_(zone),
      saturated_uses_(zone) {
  ZoneVector<std::pair<OpIndex, OpIndex>> delayed_phi_uses(zone);

  // We preallocate for 2 uses per operation.
  uses_.reserve(graph.op_id_count() * 2);

  // We skip {offset:0} to use {offset == 0} as uninitialized.
  uint32_t offset = 1;
  for (uint32_t index = 0; index < graph.block_count(); ++index) {
    BlockIndex block_index(index);
    const Block& block = graph.Get(block_index);

    auto block_ops = graph.OperationIndices(block);
    for (OpIndex op_index : block_ops) {
      const Operation& op = graph.Get(op_index);
      // When we see a definition, we allocate space in the {uses_}.
      DCHECK_EQ(table_[op_index].offset, 0);
      DCHECK_EQ(table_[op_index].count, 0);

      if (op.saturated_use_count.IsSaturated()) {
        table_[op_index].offset =
            -static_cast<int32_t>(saturated_uses_.size()) - 1;
        saturated_uses_.emplace_back(zone);
        saturated_uses_.back().reserve(std::numeric_limits<uint8_t>::max());
      } else {
        table_[op_index].offset = offset;
        offset += op.saturated_use_count.Get();
        uses_.resize(offset);
      }

      if (filter(op, zone)) continue;

      if (block.IsLoop()) {
        if (op.Is<PhiOp>()) {
          DCHECK_EQ(op.input_count, 2);
          DCHECK_EQ(PhiOp::kLoopPhiBackEdgeIndex, 1);
          AddUse(&graph, op.input(0), op_index);
          // Delay back edge of loop Phis.
          delayed_phi_uses.emplace_back(op.input(1), op_index);
          continue;
        }
      }

      // Add uses.
      for (OpIndex input_index : op.inputs()) {
        AddUse(&graph, input_index, op_index);
      }
    }
  }

  for (auto [input_index, op_index] : delayed_phi_uses) {
    AddUse(&graph, input_index, op_index);
  }
}

base::Vector<const OpIndex> UseMap::uses(OpIndex index) const {
  DCHECK(index.valid());
  int32_t offset = table_[index].offset;
  uint32_t count = table_[index].count;
  DCHECK_NE(offset, 0);
  if (V8_LIKELY(offset > 0)) {
    return base::Vector<const OpIndex>(uses_.data() + offset, count);
  } else {
    DCHECK_EQ(count, saturated_uses_[-offset - 1].size());
    return base::Vector<const OpIndex>(saturated_uses_[-offset - 1].data(),
                                       count);
  }
}

void UseMap::AddUse(const Graph* graph, OpIndex node, OpIndex use) {
  int32_t input_offset = table_[node].offset;
  uint32_t& input_count = table_[node].count;
  DCHECK_NE(input_offset, 0);
  if (V8_LIKELY(input_offset > 0)) {
    DCHECK_LT(input_count, graph->Get(node).saturated_use_count.Get());
    DCHECK(!uses_[input_offset + input_count].valid());
    uses_[input_offset + input_count] = use;
  } else {
    ZoneVector<OpIndex>& uses = saturated_uses_[-input_offset - 1];
    DCHECK_EQ(uses.size(), input_count);
    uses.emplace_back(use);
  }
  ++input_count;
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/use-map.h                                               0000664 0000000 0000000 00000004103 14746647661 0022345 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_USE_MAP_H_
#define V8_COMPILER_TURBOSHAFT_USE_MAP_H_

#include "src/compiler/turboshaft/sidetable.h"

namespace v8::internal::compiler::turboshaft {

typedef bool (*FunctionType)(const Operation& op, Zone* zone);

// UseMap computes uses of all operations of the given turboshaft graph. It
// provides a mapping from `OpIndex` to its `uses`.
class UseMap {
  struct PerOperationUses {
    // We encode offsets as follows:
    // offset < 0: -offset-1 indexes into {saturated_uses_}.
    // offset = 0: definition not visited yet.
    // offset > 0: offset indexes into {uses_}.
    int32_t offset = 0;
    uint32_t count = 0;
  };

 public:
  UseMap(const Graph& graph, Zone* zone, FunctionType filter);

  UseMap(const Graph& graph, Zone* zone)
      : UseMap(graph, zone,
               [](const Operation& op, Zone* zone) { return false; }) {}

  base::Vector<const OpIndex> uses(OpIndex index) const;

 private:
  void AddUse(const Graph* graph, OpIndex node, OpIndex use);

  FixedOpIndexSidetable<PerOperationUses> table_;
  ZoneVector<OpIndex> uses_;
  ZoneVector<ZoneVector<OpIndex>> saturated_uses_;
};

// SimdUseMap computes uses of SIMD operations of the given turboshaft graph and
// skip other operations.
class SimdUseMap : public UseMap, public NON_EXPORTED_BASE(ZoneObject) {
 public:
  SimdUseMap(const Graph& graph, Zone* zone)
      : UseMap(graph, zone, [](const Operation& op, Zone* zone) {
          if (op.outputs_rep().size() == 1 &&
              op.outputs_rep()[0] == RegisterRepresentation::Simd128()) {
            return false;
          }

          ZoneVector<MaybeRegisterRepresentation> storage(zone);
          for (auto rep : op.inputs_rep(storage)) {
            if (rep == MaybeRegisterRepresentation::Simd128()) return false;
          }
          return true;
        }) {}
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_USE_MAP_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/utils.cc                                                0000664 0000000 0000000 00000001304 14746647661 0022274 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/utils.h"

#include "src/base/platform/platform.h"
#include "src/flags/flags.h"

namespace v8::internal::compiler::turboshaft {

#ifdef DEBUG
bool ShouldSkipOptimizationStep() {
  static std::atomic<uint64_t> counter{0};
  uint64_t current = counter++;
  if (current == v8_flags.turboshaft_opt_bisect_break) {
    base::OS::DebugBreak();
  }
  if (current >= v8_flags.turboshaft_opt_bisect_limit) {
    return true;
  }
  return false;
}
#endif  // DEBUG

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/utils.h                                                 0000664 0000000 0000000 00000013243 14746647661 0022143 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_UTILS_H_
#define V8_COMPILER_TURBOSHAFT_UTILS_H_

#include <iostream>
#include <limits>
#include <tuple>

#include "src/base/logging.h"
#include "src/base/macros.h"

namespace v8::internal::compiler::turboshaft {

template <class... Ts>
struct any_of : std::tuple<const Ts&...> {
  explicit any_of(const Ts&... args) : std::tuple<const Ts&...>(args...) {}

  template <class T, size_t... indices>
  bool Contains(const T& value, std::index_sequence<indices...>) {
    return ((value == std::get<indices>(*this)) || ...);
  }

  template <size_t... indices>
  std::ostream& PrintTo(std::ostream& os, std::index_sequence<indices...>) {
    bool first = true;
    os << "any_of(";
    (((first ? (first = false, os) : os << ", "),
      os << base::PrintCheckOperand(std::get<indices>(*this))),
     ...);
    return os << ")";
  }
};
template <class... Args>
any_of(const Args&...) -> any_of<Args...>;

template <class T, class... Ts>
bool operator==(const T& value, any_of<Ts...> options) {
  return options.Contains(value, std::index_sequence_for<Ts...>{});
}

template <class... Ts>
std::ostream& operator<<(std::ostream& os, any_of<Ts...> any) {
  return any.PrintTo(os, std::index_sequence_for<Ts...>{});
}

template <class... Ts>
struct all_of : std::tuple<const Ts&...> {
  explicit all_of(const Ts&... args) : std::tuple<const Ts&...>(args...) {}

  template <class T, size_t... indices>
  bool AllEqualTo(const T& value, std::index_sequence<indices...>) {
    return ((value == std::get<indices>(*this)) && ...);
  }

  template <size_t... indices>
  std::ostream& PrintTo(std::ostream& os, std::index_sequence<indices...>) {
    bool first = true;
    os << "all_of(";
    (((first ? (first = false, os) : os << ", "),
      os << base::PrintCheckOperand(std::get<indices>(*this))),
     ...);
    return os << ")";
  }
};
template <class... Args>
all_of(const Args&...) -> all_of<Args...>;

template <class T, class... Ts>
bool operator==(all_of<Ts...> values, const T& target) {
  return values.AllEqualTo(target, std::index_sequence_for<Ts...>{});
}

template <class... Ts>
std::ostream& operator<<(std::ostream& os, all_of<Ts...> all) {
  return all.PrintTo(os, std::index_sequence_for<Ts...>{});
}

#ifdef DEBUG
V8_EXPORT_PRIVATE bool ShouldSkipOptimizationStep();
#else
V8_EXPORT_PRIVATE inline bool ShouldSkipOptimizationStep() { return false; }
#endif

// Set `*ptr` to `new_value` while the scope is active, reset to the previous
// value upon destruction.
template <class T>
class ScopedModification {
 public:
  ScopedModification(T* ptr, T new_value)
      : ptr_(ptr), old_value_(std::move(*ptr)) {
    *ptr = std::move(new_value);
  }

  ~ScopedModification() { *ptr_ = std::move(old_value_); }

  const T& old_value() const { return old_value_; }

 private:
  T* ptr_;
  T old_value_;
};

// The `multi`-switch mechanism helps to switch on multiple values at the same
// time. Example:
//
//   switch (multi(change.from, change.to)) {
//     case multi(Word32(), Float32()): ...
//     case multi(Word32(), Float64()): ...
//     case multi(Word64(), Float32()): ...
//     case multi(Word64(), Float64()): ...
//     ...
//   }
//
// This works for an arbitrary number of dimensions and arbitrary types as long
// as they can be encoded into an integral value and their combination fits into
// a uint64_t. For types to be used, they need to provide a specialization of
// MultiSwitch<T> with this signature:
//
//   template<>
//   struct MultiSwitch<T> {
//     static constexpr uint64_t max_value = ...
//     static constexpr uint64_t encode(T value) { ... }
//   };
//
// For `max_value` choose a value that is larger than all encoded values. Choose
// this as small as possible to make jump tables more dense. If a type's value
// count is somewhat close to a multiple of two, consider using this, as this
// might lead to slightly faster encoding. The encoding follows this formula:
//
//   multi(v1, v2, v3) =
//     let t1 = MultiSwitch<T3>::encode(v3) in
//     let t2 = (t1 * MultiSwitch<T2>::max_value)
//              + MultiSwitch<T2>::encode(v2) in
//     (t2 * MultiSwitch<T1>::max_value) + MultiSwitch<T1>::encode(v1)
//
// For integral types (like enums), use
//
//   DEFINE_MULTI_SWITCH_INTEGRAL(MyType, MaxValue)
//
template <typename T, typename Enable = void>
struct MultiSwitch;

template <typename T, uint64_t MaxValue>
struct MultiSwitchIntegral {
  static constexpr uint64_t max_value = MaxValue;
  static constexpr uint64_t encode(T value) {
    const uint64_t v = static_cast<uint64_t>(value);
    DCHECK_LT(v, max_value);
    return v;
  }
};

#define DEFINE_MULTI_SWITCH_INTEGRAL(name, max_value) \
  template <>                                         \
  struct MultiSwitch<name> : MultiSwitchIntegral<name, max_value> {};

namespace detail {
template <typename T>
constexpr uint64_t multi_encode(const T& value) {
  return MultiSwitch<T>::encode(value);
}

template <typename Head, typename Next, typename... Rest>
constexpr uint64_t multi_encode(const Head& head, const Next& next,
                                const Rest&... rest) {
  uint64_t v = multi_encode(next, rest...);
  DCHECK_LT(
      v, std::numeric_limits<uint64_t>::max() / MultiSwitch<Head>::max_value);
  return (v * MultiSwitch<Head>::max_value) + MultiSwitch<Head>::encode(head);
}
}  // namespace detail

template <typename... Ts>
inline constexpr uint64_t multi(const Ts&... values) {
  return detail::multi_encode(values...);
}

DEFINE_MULTI_SWITCH_INTEGRAL(bool, 2)

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_UTILS_H_
                                                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/value-numbering-reducer.h                               0000664 0000000 0000000 00000031223 14746647661 0025530 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_VALUE_NUMBERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_VALUE_NUMBERING_REDUCER_H_

#include "src/base/logging.h"
#include "src/base/vector.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/fast-hash.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/reducer-traits.h"
#include "src/utils/utils.h"
#include "src/zone/zone-containers.h"

namespace v8 {
namespace internal {
namespace compiler {
namespace turboshaft {

// Value numbering removes redundant nodes from the graph. A simple example
// could be:
//
//   x = a + b
//   y = a + b
//   z = x * y
//
// Is simplified to
//
//   x = a + b
//   z = x * x
//
// It works by storing previously seen nodes in a hashmap, and when visiting a
// new node, we check to see if it's already in the hashmap. If yes, then we
// return the old node. If not, then we keep the new one (and add it into the
// hashmap). A high-level pseudo-code would be:
//
//   def VisitOp(op):
//     if op in hashmap:
//        return hashmap.get(op)
//     else:
//        hashmap.add(op)
//        return op
//
// We implemented our own hashmap (to have more control, it should become
// clearer why by the end of this explanation). When there is a collision, we
// look at the next index (and the next one if there is yet another collision,
// etc). While not the fastest approach, it has the advantage of not requiring
// any dynamic memory allocation (besides the initial table, and the resizing).
//
// For the approach describe above (the pseudocode and the paragraph before it)
// to be correct, a node should only be replaced by a node defined in blocks
// that dominate the current block. Thus, this reducer relies on the fact that
// OptimizationPhases that iterate the graph dominator order. Then, when going
// down the dominator tree, we add nodes to the hashmap, and when going back up
// the dominator tree, we remove nodes from the hashmap.
//
// In order to efficiently remove all the nodes of a given block from the
// hashmap, we maintain a linked-list of hashmap entries per block (this way, we
// don't have to iterate the wole hashmap). Note that, in practice, we think in
// terms of "depth" rather than "block", and we thus have one linked-list per
// depth of the dominator tree. The heads of those linked lists are stored in
// the vector {depths_heads_}. The linked lists are then implemented in-place in
// the hashtable entries, thanks to the `depth_neighboring_entry` field of the
// `Entry` structure.
// To remove all of the entries from a given linked list, we iterate the entries
// in the linked list, setting all of their `hash` field to 0 (we prevent hashes
// from being equal to 0, in order to detect empty entries: their hash is 0).

template <class Next>
class TypeInferenceReducer;

class ScopeCounter {
 public:
  void enter() { scopes_++; }
  void leave() { scopes_--; }
  bool is_active() { return scopes_ > 0; }

 private:
  int scopes_{0};
};

// In rare cases of intentional duplication of instructions, we need to disable
// value numbering. This scope manages that.
class DisableValueNumbering {
 public:
  template <class Reducer>
  explicit DisableValueNumbering(Reducer* reducer) {
    if constexpr (reducer_list_contains<typename Reducer::ReducerList,
                                        ValueNumberingReducer>::value) {
      scopes_ = reducer->gvn_disabled_scope();
      scopes_->enter();
    }
  }

  ~DisableValueNumbering() {
    if (scopes_ != nullptr) scopes_->leave();
  }

 private:
  ScopeCounter* scopes_{nullptr};
};

template <class Next>
class ValueNumberingReducer : public Next {
#if defined(__clang__)
  static_assert(next_is_bottom_of_assembler_stack<Next>::value ||
                next_reducer_is<Next, TypeInferenceReducer>::value);
#endif

 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(ValueNumbering)

  template <typename Op>
  static constexpr bool CanBeGVNed() {
    constexpr Opcode opcode = operation_to_opcode_v<Op>;
    /* Throwing operations have a non-trivial lowering, so they don't work
     * with value numbering. */
    if constexpr (MayThrow(opcode)) return false;
    if constexpr (opcode == Opcode::kCatchBlockBegin) {
      /* CatchBlockBegin are never interesting to GVN, but additionally
       * split-edge can transform CatchBlockBeginOp into PhiOp, which means
       * that there is no guarantee here than {result} is indeed a
       * CatchBlockBegin. */
      return false;
    }
    if constexpr (opcode == Opcode::kComment) {
      /* We don't want to GVN comments. */
      return false;
    }
    return true;
  }

#define EMIT_OP(Name)                                                 \
  template <class... Args>                                            \
  OpIndex Reduce##Name(Args... args) {                                \
    OpIndex next_index = Asm().output_graph().next_operation_index(); \
    USE(next_index);                                                  \
    OpIndex result = Next::Reduce##Name(args...);                     \
    if (ShouldSkipOptimizationStep()) return result;                  \
    if constexpr (!CanBeGVNed<Name##Op>()) return result;             \
    DCHECK_EQ(next_index, result);                                    \
    return AddOrFind<Name##Op>(result);                               \
  }
  TURBOSHAFT_OPERATION_LIST(EMIT_OP)
#undef EMIT_OP

  void Bind(Block* block) {
    Next::Bind(block);
    ResetToBlock(block);
    dominator_path_.push_back(block);
    depths_heads_.push_back(nullptr);
  }

  // Resets {table_} up to the first dominator of {block} that it contains.
  void ResetToBlock(Block* block) {
    Block* target = block->GetDominator();
    while (!dominator_path_.empty() && target != nullptr &&
           dominator_path_.back() != target) {
      if (dominator_path_.back()->Depth() > target->Depth()) {
        ClearCurrentDepthEntries();
      } else if (dominator_path_.back()->Depth() < target->Depth()) {
        target = target->GetDominator();
      } else {
        // {target} and {dominator_path.back} have the same depth but are not
        // equal, so we go one level up for both.
        ClearCurrentDepthEntries();
        target = target->GetDominator();
      }
    }
  }

  template <class Op>
  bool WillGVNOp(const Op& op) {
    Entry* entry = Find(op);
    return !entry->IsEmpty();
  }

  ScopeCounter* gvn_disabled_scope() { return &disabled_scope_; }

 private:
  // TODO(dmercadier): Once the mapping from Operations to Blocks has been added
  // to turboshaft, remove the `block` field from the `Entry` structure.
  struct Entry {
    OpIndex value;
    BlockIndex block;
    size_t hash = 0;
    Entry* depth_neighboring_entry = nullptr;

    bool IsEmpty() const { return hash == 0; }
  };

  template <class Op>
  OpIndex AddOrFind(OpIndex op_idx) {
    if (is_disabled()) return op_idx;

    const Op& op = Asm().output_graph().Get(op_idx).template Cast<Op>();
    if (std::is_same_v<Op, PendingLoopPhiOp> || op.IsBlockTerminator() ||
        (!op.Effects().repetition_is_eliminatable() &&
         !std::is_same_v<Op, DeoptimizeIfOp>)) {
      // GVNing DeoptimizeIf is safe, despite its lack of
      // repetition_is_eliminatable.
      return op_idx;
    }
    RehashIfNeeded();

    size_t hash;
    Entry* entry = Find(op, &hash);
    if (entry->IsEmpty()) {
      // {op} is not present in the state, inserting it.
      *entry = Entry{op_idx, Asm().current_block()->index(), hash,
                     depths_heads_.back()};
      depths_heads_.back() = entry;
      ++entry_count_;
      return op_idx;
    } else {
      // {op} is already present, removing it from the graph and returning the
      // previous one.
      Next::RemoveLast(op_idx);
      return entry->value;
    }
  }

  template <class Op>
  Entry* Find(const Op& op, size_t* hash_ret = nullptr) {
    constexpr bool same_block_only = std::is_same<Op, PhiOp>::value;
    size_t hash = ComputeHash<same_block_only>(op);
    size_t start_index = hash & mask_;
    for (size_t i = start_index;; i = NextEntryIndex(i)) {
      Entry& entry = table_[i];
      if (entry.IsEmpty()) {
        // We didn't find {op} in {table_}. Returning where it could be
        // inserted.
        if (hash_ret) *hash_ret = hash;
        return &entry;
      }
      if (entry.hash == hash) {
        const Operation& entry_op = Asm().output_graph().Get(entry.value);
        if (entry_op.Is<Op>() &&
            (!same_block_only ||
             entry.block == Asm().current_block()->index()) &&
            entry_op.Cast<Op>().EqualsForGVN(op)) {
          return &entry;
        }
      }
      // Making sure that we don't have an infinite loop.
      DCHECK_NE(start_index, NextEntryIndex(i));
    }
  }

  // Remove all of the Entries of the current depth.
  void ClearCurrentDepthEntries() {
    for (Entry* entry = depths_heads_.back(); entry != nullptr;) {
      entry->hash = 0;
      Entry* next_entry = entry->depth_neighboring_entry;
      entry->depth_neighboring_entry = nullptr;
      entry = next_entry;
      --entry_count_;
    }
    depths_heads_.pop_back();
    dominator_path_.pop_back();
  }

  // If the table is too full, double its size and re-insert the old entries.
  void RehashIfNeeded() {
    if (V8_LIKELY(table_.size() - (table_.size() / 4) > entry_count_)) return;
    base::Vector<Entry> new_table = table_ =
        Asm().phase_zone()->template NewVector<Entry>(table_.size() * 2);
    size_t mask = mask_ = table_.size() - 1;

    for (size_t depth_idx = 0; depth_idx < depths_heads_.size(); depth_idx++) {
      // It's important to fill the new hash by inserting data in increasing
      // depth order, in order to avoid holes when later calling
      // ClearCurrentDepthEntries. Consider for instance:
      //
      //  ---+------+------+------+----
      //     |  a1  |  a2  |  a3  |
      //  ---+------+------+------+----
      //
      // Where a1, a2 and a3 have the same hash. By construction, we know that
      // depth(a1) <= depth(a2) <= depth(a3). If, when re-hashing, we were to
      // insert them in another order, say:
      //
      //  ---+------+------+------+----
      //     |  a3  |  a1  |  a2  |
      //  ---+------+------+------+----
      //
      // Then, when we'll call ClearCurrentDepthEntries to remove entries from
      // a3's depth, we'll get this:
      //
      //  ---+------+------+------+----
      //     | null |  a1  |  a2  |
      //  ---+------+------+------+----
      //
      // And, when looking if a1 is in the hash, we'd find a "null" where we
      // expect it, and assume that it's not present. If, instead, we always
      // conserve the increasing depth order, then when removing a3, we'd get:
      //
      //  ---+------+------+------+----
      //     |  a1  |  a2  | null |
      //  ---+------+------+------+----
      //
      // Where we can still find a1 and a2.
      Entry* entry = depths_heads_[depth_idx];
      depths_heads_[depth_idx] = nullptr;

      while (entry != nullptr) {
        for (size_t i = entry->hash & mask;; i = NextEntryIndex(i)) {
          if (new_table[i].hash == 0) {
            new_table[i] = *entry;
            Entry* next_entry = entry->depth_neighboring_entry;
            new_table[i].depth_neighboring_entry = depths_heads_[depth_idx];
            depths_heads_[depth_idx] = &new_table[i];
            entry = next_entry;
            break;
          }
        }
      }
    }
  }

  template <bool same_block_only, class Op>
  size_t ComputeHash(const Op& op) {
    size_t hash = op.hash_value();
    if (same_block_only) {
      hash = fast_hash_combine(Asm().current_block()->index(), hash);
    }
    if (V8_UNLIKELY(hash == 0)) return 1;
    return hash;
  }

  size_t NextEntryIndex(size_t index) { return (index + 1) & mask_; }
  Entry* NextEntry(Entry* entry) {
    return V8_LIKELY(entry + 1 < table_.end()) ? entry + 1 : &table_[0];
  }
  Entry* PrevEntry(Entry* entry) {
    return V8_LIKELY(entry > table_.begin()) ? entry - 1 : table_.end() - 1;
  }

  bool is_disabled() { return disabled_scope_.is_active(); }

  ZoneVector<Block*> dominator_path_{Asm().phase_zone()};
  base::Vector<Entry> table_ = Asm().phase_zone()->template NewVector<Entry>(
      base::bits::RoundUpToPowerOfTwo(
          std::max<size_t>(128, Asm().input_graph().op_id_capacity() / 2)));
  size_t mask_ = table_.size() - 1;
  size_t entry_count_ = 0;
  ZoneVector<Entry*> depths_heads_{Asm().phase_zone()};
  ScopeCounter disabled_scope_;
};

}  // namespace turboshaft
}  // namespace compiler
}  // namespace internal
}  // namespace v8

#endif  // V8_COMPILER_TURBOSHAFT_VALUE_NUMBERING_REDUCER_H_
                                                                                                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/variable-reducer.h                                      0000664 0000000 0000000 00000031711 14746647661 0024217 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2022 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#ifndef V8_COMPILER_TURBOSHAFT_VARIABLE_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_VARIABLE_REDUCER_H_

#include <algorithm>
#include <optional>

#include "src/base/logging.h"
#include "src/codegen/machine-type.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/representations.h"
#include "src/compiler/turboshaft/required-optimization-reducer.h"
#include "src/compiler/turboshaft/snapshot-table.h"
#include "src/zone/zone-containers.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

// When cloning a Block or duplicating an Operation, we end up with some
// Operations of the old graph mapping to multiple Operations in the new graph.
// When using those Operations in subsequent Operations, we need to know which
// of the new-Operation to use, and, in particular, if a Block has 2
// predecessors that have a mapping for the same old-Operation, we need to
// merge them in a Phi node. All of this is handled by the VariableAssembler.
//
// The typical workflow when working with the VariableAssembler would be:
//    - At some point, you need to introduce a Variable (for instance
//      because you cloned a block or an Operation) and call NewVariable or
//      NewLoopInvariantVariable to get a fresh Variable. A loop invariant
//      variable must not need loop phis, that is, not change its value
//      depending on loop iteration while being visible across loop iterations.
//    - You can then Set the new-OpIndex associated with this Variable in the
//      current Block with the Set method.
//    - If you later need to set an OpIndex for this Variable in another Block,
//      call Set again.
//    - At any time, you can call Get to get the new-Operation associated to
//      this Variable. Get will return:
//         * if the current block is dominated by a block who did a Set on the
//           Variable, then the Operation that was Set then.
//         * otherwise, the current block must be dominated by a Merge whose
//           predecessors have all Set this Variable. In that case, the
//           VariableAssembler introduced a Phi in this merge, and will return
//           this Phi.
//
// Note that the VariableAssembler does not do "old-OpIndex => Variable"
// book-keeping: the users of the Variable should do that themselves (which
// is what CopyingPhase does for instance).

// VariableReducer always adds a RequiredOptimizationReducer, because phis
// with constant inputs introduced by `VariableReducer` need to be eliminated.
template <class AfterNext>
class VariableReducer : public RequiredOptimizationReducer<AfterNext> {
  using Next = RequiredOptimizationReducer<AfterNext>;
  using Snapshot = SnapshotTable<OpIndex, VariableData>::Snapshot;

  struct GetActiveLoopVariablesIndex {
    IntrusiveSetIndex& operator()(Variable var) const {
      return var.data().active_loop_variables_index;
    }
  };

  struct VariableTable
      : ChangeTrackingSnapshotTable<VariableTable, OpIndex, VariableData> {
    explicit VariableTable(Zone* zone)
        : ChangeTrackingSnapshotTable<VariableTable, OpIndex, VariableData>(
              zone),
          active_loop_variables(zone) {}

    ZoneIntrusiveSet<Variable, GetActiveLoopVariablesIndex>
        active_loop_variables;

    void OnNewKey(Variable var, OpIndex value) { DCHECK(!value.valid()); }
    void OnValueChange(Variable var, OpIndex old_value, OpIndex new_value) {
      if (var.data().loop_invariant) {
        return;
      }
      if (old_value.valid() && !new_value.valid()) {
        active_loop_variables.Remove(var);
      } else if (!old_value.valid() && new_value.valid()) {
        active_loop_variables.Add(var);
      }
    }
  };

 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(VariableReducer)

  void Bind(Block* new_block) {
    Next::Bind(new_block);

    SealAndSaveVariableSnapshot();

    predecessors_.clear();
    for (const Block* pred : new_block->PredecessorsIterable()) {
      std::optional<Snapshot> pred_snapshot =
          block_to_snapshot_mapping_[pred->index()];
      DCHECK(pred_snapshot.has_value());
      predecessors_.push_back(pred_snapshot.value());
    }
    std::reverse(predecessors_.begin(), predecessors_.end());

    auto merge_variables =
        [&](Variable var, base::Vector<const OpIndex> predecessors) -> OpIndex {
      for (OpIndex idx : predecessors) {
        if (!idx.valid()) {
          // If any of the predecessors' value is Invalid, then we shouldn't
          // merge {var}.
          return OpIndex::Invalid();
        } else if (__ output_graph()
                       .Get(idx)
                       .template Is<LoadRootRegisterOp>()) {
          // Variables that once contain the root register never contain another
          // value.
          return __ LoadRootRegister();
        }
      }
      return MergeOpIndices(predecessors, var.data().rep);
    };

    table_.StartNewSnapshot(base::VectorOf(predecessors_), merge_variables);
    current_block_ = new_block;
    if (new_block->IsLoop()) {
      // When starting a loop, we need to create a PendingLoopPhi for each
      // currently active variable (except those that are marked as
      // loop-invariant).
      auto active_loop_variables_begin = table_.active_loop_variables.begin();
      auto active_loop_variables_end = table_.active_loop_variables.end();
      if (active_loop_variables_begin != active_loop_variables_end) {
        ZoneVector<std::pair<Variable, OpIndex>> pending_phis(__ phase_zone());
        for (Variable var : table_.active_loop_variables) {
          MaybeRegisterRepresentation rep = var.data().rep;
          DCHECK_NE(rep, MaybeRegisterRepresentation::None());
          V<Any> pending_loop_phi =
              __ PendingLoopPhi(table_.Get(var), RegisterRepresentation(rep));
          SetVariable(var, pending_loop_phi);
          pending_phis.push_back({var, pending_loop_phi});
        }
        loop_pending_phis_[new_block->index()].emplace(pending_phis);
      }
    }
  }

  void RestoreTemporaryVariableSnapshotAfter(const Block* block) {
    DCHECK(table_.IsSealed());
    DCHECK(block_to_snapshot_mapping_[block->index()].has_value());
    table_.StartNewSnapshot(*block_to_snapshot_mapping_[block->index()]);
    is_temporary_ = true;
  }
  void CloseTemporaryVariableSnapshot() {
    DCHECK(is_temporary_);
    table_.Seal();
    is_temporary_ = false;
  }

  V<None> REDUCE(Goto)(Block* destination, bool is_backedge) {
    V<None> result = Next::ReduceGoto(destination, is_backedge);
    if (!destination->IsBound()) {
      return result;
    }

    // For loops, we have to "fix" the PendingLoopPhis (= replace them with
    // regular loop phis).
    DCHECK(destination->IsLoop());
    DCHECK_EQ(destination->PredecessorCount(), 2);

    if (loop_pending_phis_.contains(destination->index())) {
      for (auto [var, pending_phi_idx] :
           loop_pending_phis_[destination->index()].value()) {
        const PendingLoopPhiOp& pending_phi =
            __ Get(pending_phi_idx).template Cast<PendingLoopPhiOp>();
        __ output_graph().template Replace<PhiOp>(
            pending_phi_idx,
            base::VectorOf({pending_phi.first(), GetVariable(var)}),
            pending_phi.rep);
      }
    }

    return result;
  }

  OpIndex GetVariable(Variable var) { return table_.Get(var); }

  OpIndex GetPredecessorValue(Variable var, int predecessor_index) {
    return table_.GetPredecessorValue(var, predecessor_index);
  }

  void SetVariable(Variable var, OpIndex new_index) {
    DCHECK(!is_temporary_);
    if (V8_UNLIKELY(__ generating_unreachable_operations())) return;
    table_.Set(var, new_index);
  }
  template <typename Rep>
  void Set(Variable var, V<Rep> value) {
    DCHECK(!is_temporary_);
    if (V8_UNLIKELY(__ generating_unreachable_operations())) return;
    DCHECK(Rep::allows_representation(RegisterRepresentation(var.data().rep)));
    table_.Set(var, value);
  }

  Variable NewLoopInvariantVariable(MaybeRegisterRepresentation rep) {
    DCHECK(!is_temporary_);
    return table_.NewKey(VariableData{rep, true}, OpIndex::Invalid());
  }
  Variable NewVariable(MaybeRegisterRepresentation rep) {
    DCHECK(!is_temporary_);
    return table_.NewKey(VariableData{rep, false}, OpIndex::Invalid());
  }

  // SealAndSaveVariableSnapshot seals the current snapshot, and stores it in
  // {block_to_snapshot_mapping_}, so that it can be used for later merging.
  void SealAndSaveVariableSnapshot() {
    if (table_.IsSealed()) {
      DCHECK_EQ(current_block_, nullptr);
      return;
    }

    DCHECK_NOT_NULL(current_block_);
    block_to_snapshot_mapping_[current_block_->index()] = table_.Seal();
    current_block_ = nullptr;
  }

 private:
  OpIndex MergeOpIndices(base::Vector<const OpIndex> inputs,
                         MaybeRegisterRepresentation maybe_rep) {
    if (maybe_rep != MaybeRegisterRepresentation::None()) {
      // Every Operation that has a RegisterRepresentation can be merged with a
      // simple Phi.
      return __ Phi(base::VectorOf(inputs), RegisterRepresentation(maybe_rep));
    } else if (__ output_graph().Get(inputs[0]).template Is<FrameStateOp>()) {
      // Frame states need be be merged recursively, because they represent
      // multiple scalar values that will lead to multiple phi nodes.
      return MergeFrameState(inputs);
    } else {
      return OpIndex::Invalid();
    }
  }

  OpIndex MergeFrameState(base::Vector<const OpIndex> frame_states_indices) {
    base::SmallVector<const FrameStateOp*, 32> frame_states;
    for (OpIndex idx : frame_states_indices) {
      frame_states.push_back(
          &__ output_graph().Get(idx).template Cast<FrameStateOp>());
    }
    const FrameStateOp* first_frame = frame_states[0];

#if DEBUG
    // Making sure that all frame states have the same number of inputs, the
    // same "inlined" field, and the same data.
    for (auto frame_state : frame_states) {
      DCHECK_EQ(first_frame->input_count, frame_state->input_count);
      DCHECK_EQ(first_frame->inlined, frame_state->inlined);
      DCHECK_EQ(first_frame->data, frame_state->data);
    }
#endif

    base::SmallVector<OpIndex, 32> new_inputs;

    // Merging the parent frame states.
    if (first_frame->inlined) {
      ZoneVector<OpIndex> indices_to_merge(__ phase_zone());
      bool all_parent_frame_states_are_the_same = true;
      for (auto frame_state : frame_states) {
        indices_to_merge.push_back(frame_state->parent_frame_state());
        all_parent_frame_states_are_the_same =
            all_parent_frame_states_are_the_same &&
            first_frame->parent_frame_state() ==
                frame_state->parent_frame_state();
      }
      if (all_parent_frame_states_are_the_same) {
        new_inputs.push_back(first_frame->parent_frame_state());
      } else {
        OpIndex merged_parent_frame_state =
            MergeFrameState(base::VectorOf(indices_to_merge));
        new_inputs.push_back(merged_parent_frame_state);
      }
    }

    // Merging the state values.
    for (int i = 0; i < first_frame->state_values_count(); i++) {
      ZoneVector<OpIndex> indices_to_merge(__ phase_zone());
      bool all_inputs_are_the_same = true;
      for (auto frame_state : frame_states) {
        indices_to_merge.push_back(frame_state->state_value(i));
        all_inputs_are_the_same =
            all_inputs_are_the_same &&
            first_frame->state_value(i) == frame_state->state_value(i);
      }
      if (all_inputs_are_the_same) {
        // This input does not need to be merged, since its identical for all of
        // the frame states.
        new_inputs.push_back(first_frame->state_value(i));
      } else {
        RegisterRepresentation rep = first_frame->state_value_rep(i);
        OpIndex new_input =
            MergeOpIndices(base::VectorOf(indices_to_merge), rep);
        new_inputs.push_back(new_input);
      }
    }

    return __ FrameState(base::VectorOf(new_inputs), first_frame->inlined,
                         first_frame->data);
  }

  VariableTable table_{__ phase_zone()};
  const Block* current_block_ = nullptr;
  GrowingBlockSidetable<std::optional<Snapshot>> block_to_snapshot_mapping_{
      __ input_graph().block_count(), std::nullopt, __ phase_zone()};
  bool is_temporary_ = false;

  // {predecessors_} is used during merging, but we use an instance variable for
  // it, in order to save memory and not reallocate it for each merge.
  ZoneVector<Snapshot> predecessors_{__ phase_zone()};

  // Map from loop headers to the pending loop phis in these headers which have
  // to be patched on backedges.
  ZoneAbslFlatHashMap<BlockIndex,
                      std::optional<ZoneVector<std::pair<Variable, OpIndex>>>>
      loop_pending_phis_{__ phase_zone()};
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_VARIABLE_REDUCER_H_
                                                       node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-assembler-helpers.h                                0000664 0000000 0000000 00000005016 14746647661 0025364 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_ASSEMBLER_HELPERS_H_
#define V8_COMPILER_TURBOSHAFT_WASM_ASSEMBLER_HELPERS_H_

#include "src/compiler/turboshaft/operations.h"
#include "src/roots/roots.h"

namespace v8::internal::compiler::turboshaft {

struct RootTypes {
#define DEFINE_TYPE(type, name, CamelName) using k##CamelName##Type = type;
  ROOT_LIST(DEFINE_TYPE)
#undef DEFINE_TYPE
};

template <typename AssemblerT>
OpIndex LoadRootHelper(AssemblerT&& assembler, RootIndex index) {
  if (RootsTable::IsImmortalImmovable(index)) {
    // Note that we skip the bit cast here as the value does not need to be
    // tagged as the object will never be collected / moved.
    return assembler.Load(
        assembler.LoadRootRegister(), LoadOp::Kind::RawAligned().Immutable(),
        MemoryRepresentation::UintPtr(), IsolateData::root_slot_offset(index));
  } else {
    return assembler.BitcastWordPtrToTagged(assembler.Load(
        assembler.LoadRootRegister(), LoadOp::Kind::RawAligned(),
        MemoryRepresentation::UintPtr(), IsolateData::root_slot_offset(index)));
  }
}

#define LOAD_INSTANCE_FIELD(instance, name, representation)     \
  __ Load(instance, LoadOp::Kind::TaggedBase(), representation, \
          WasmTrustedInstanceData::k##name##Offset)

#define LOAD_PROTECTED_INSTANCE_FIELD(instance, name, type) \
  V<type>::Cast(__ LoadProtectedPointerField(               \
      instance, LoadOp::Kind::TaggedBase(),                 \
      WasmTrustedInstanceData::kProtected##name##Offset))

#define LOAD_IMMUTABLE_PROTECTED_INSTANCE_FIELD(instance, name, type) \
  V<type>::Cast(__ LoadProtectedPointerField(                         \
      instance, LoadOp::Kind::TaggedBase().Immutable(),               \
      WasmTrustedInstanceData::kProtected##name##Offset))

#define LOAD_IMMUTABLE_INSTANCE_FIELD(instance, name, representation)       \
  __ Load(instance, LoadOp::Kind::TaggedBase().Immutable(), representation, \
          WasmTrustedInstanceData::k##name##Offset)

#define LOAD_ROOT(name)                                    \
  V<compiler::turboshaft::RootTypes::k##name##Type>::Cast( \
      LoadRootHelper(Asm(), RootIndex::k##name))

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_ASSEMBLER_HELPERS_H_
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-dead-code-elimination-phase.cc                     0000664 0000000 0000000 00000003412 14746647661 0027314 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/wasm-dead-code-elimination-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/dead-code-elimination-reducer.h"
#include "src/compiler/turboshaft/duplication-optimization-reducer.h"
#include "src/compiler/turboshaft/instruction-selection-normalization-reducer.h"
#include "src/compiler/turboshaft/load-store-simplification-reducer.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/stack-check-lowering-reducer.h"
#include "src/compiler/turboshaft/value-numbering-reducer.h"

namespace v8::internal::compiler::turboshaft {

void WasmDeadCodeEliminationPhase::Run(PipelineData* data, Zone* temp_zone) {
  UnparkedScopeIfNeeded scope(data->broker(), DEBUG_BOOL);

  // The value numbering ensures that load with similar patterns in the complex
  // loads can share those calculations.
  CopyingPhase<DeadCodeEliminationReducer, StackCheckLoweringReducer,
               LoadStoreSimplificationReducer,
               // We make sure that DuplicationOptimizationReducer runs after
               // LoadStoreSimplificationReducer, so that it can optimize
               // Loads/Stores produced by LoadStoreSimplificationReducer
               // (which, for simplificy, doesn't use the Assembler helper
               // methods, but only calls Next::ReduceLoad/Store).
               DuplicationOptimizationReducer,
               InstructionSelectionNormalizationReducer,
               ValueNumberingReducer>::Run(data, temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                      node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-dead-code-elimination-phase.h                      0000664 0000000 0000000 00000001440 14746647661 0027155 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_DEAD_CODE_ELIMINATION_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_WASM_DEAD_CODE_ELIMINATION_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct WasmDeadCodeEliminationPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(WasmDeadCodeElimination)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_DEAD_CODE_ELIMINATION_PHASE_H_
                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-gc-optimize-phase.cc                               0000664 0000000 0000000 00000001576 14746647661 0025441 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/wasm-gc-optimize-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/wasm-gc-typed-optimization-reducer.h"
#include "src/compiler/turboshaft/wasm-load-elimination-reducer.h"

namespace v8::internal::compiler::turboshaft {

void WasmGCOptimizePhase::Run(PipelineData* data, Zone* temp_zone) {
  UnparkedScopeIfNeeded scope(data->broker(),
                              v8_flags.turboshaft_trace_reduction);
  CopyingPhase<WasmLoadEliminationReducer, WasmGCTypedOptimizationReducer>::Run(
      data, temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                  node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-gc-optimize-phase.h                                0000664 0000000 0000000 00000001360 14746647661 0025272 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_GC_OPTIMIZE_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_WASM_GC_OPTIMIZE_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct WasmGCOptimizePhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(WasmGCOptimize)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_GC_OPTIMIZE_PHASE_H_
                                                                                                                                                                                                                                                                                node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-gc-typed-optimization-reducer.cc                   0000664 0000000 0000000 00000046575 14746647661 0030013 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/wasm-gc-typed-optimization-reducer.h"

#include "src/compiler/turboshaft/analyzer-iterator.h"
#include "src/compiler/turboshaft/loop-finder.h"

namespace v8::internal::compiler::turboshaft {

void WasmGCTypeAnalyzer::Run() {
  LoopFinder loop_finder(phase_zone_, &graph_);
  AnalyzerIterator iterator(phase_zone_, graph_, loop_finder);
  while (iterator.HasNext()) {
    const Block& block = *iterator.Next();
    ProcessBlock(block);

    // Finish snapshot.
    Snapshot snapshot = types_table_.Seal();
    block_to_snapshot_[block.index()] = MaybeSnapshot(snapshot);

    // Consider re-processing for loops.
    if (const GotoOp* last = block.LastOperation(graph_).TryCast<GotoOp>()) {
      if (IsReachable(block) && last->destination->IsLoop() &&
          last->destination->LastPredecessor() == &block) {
        const Block& loop_header = *last->destination;
        // Create a merged snapshot state for the forward- and backedge and
        // process all operations inside the loop header.
        ProcessBlock(loop_header);
        Snapshot old_snapshot = block_to_snapshot_[loop_header.index()].value();
        Snapshot snapshot = types_table_.Seal();
        // TODO(14108): The snapshot isn't needed at all, we only care about the
        // information if two snapshots are equivalent. Unfortunately, currently
        // this can only be answered by creating a merge snapshot.
        bool needs_revisit =
            CreateMergeSnapshot(base::VectorOf({old_snapshot, snapshot}),
                                base::VectorOf({true, true}));
        types_table_.Seal();  // Discard the snapshot.

        // TODO(14108): This currently encodes a fixed point analysis where the
        // analysis is finished once the backedge doesn't provide updated type
        // information any more compared to the previous iteration. This could
        // be stopped in cases where the backedge only refines types (i.e. only
        // defines more precise types than the previous iteration).
        if (needs_revisit) {
          block_to_snapshot_[loop_header.index()] = MaybeSnapshot(snapshot);
          // This will push the successors of the loop header to the iterator
          // stack, so the loop body will be visited in the next iteration.
          iterator.MarkLoopForRevisitSkipHeader();
        }
      }
    }
  }
}

void WasmGCTypeAnalyzer::ProcessBlock(const Block& block) {
  DCHECK_NULL(current_block_);
  current_block_ = &block;
  StartNewSnapshotFor(block);
  ProcessOperations(block);
  current_block_ = nullptr;
}

void WasmGCTypeAnalyzer::StartNewSnapshotFor(const Block& block) {
  is_first_loop_header_evaluation_ = false;
  // Reset reachability information. This can be outdated in case of loop
  // revisits. Below the reachability is calculated again and potentially
  // re-added.
  bool block_was_previously_reachable = IsReachable(block);
  block_is_unreachable_.Remove(block.index().id());
  // Start new snapshot based on predecessor information.
  if (block.HasPredecessors() == 0) {
    // The first block just starts with an empty snapshot.
    DCHECK_EQ(block.index().id(), 0);
    types_table_.StartNewSnapshot();
  } else if (block.IsLoop()) {
    const Block& forward_predecessor =
        *block.LastPredecessor()->NeighboringPredecessor();
    if (!IsReachable(forward_predecessor)) {
      // If a loop isn't reachable through its forward edge, it can't possibly
      // become reachable via the backedge.
      block_is_unreachable_.Add(block.index().id());
    }
    MaybeSnapshot back_edge_snap =
        block_to_snapshot_[block.LastPredecessor()->index()];
    if (back_edge_snap.has_value() && block_was_previously_reachable) {
      // The loop was already visited at least once. In this case use the
      // available information from the backedge.
      // Note that we only do this if the loop wasn't marked as unreachable
      // before. This solves an issue where a single block loop would think the
      // backedge is reachable as we just removed the unreachable information
      // above. Once the analyzer hits the backedge, it will re-evaluate if the
      // backedge changes any analysis results and then potentially revisit
      // this loop with forward edge and backedge.
      CreateMergeSnapshot(block);
    } else {
      // The loop wasn't visited yet. There isn't any type information available
      // for the backedge.
      is_first_loop_header_evaluation_ = true;
      Snapshot forward_edge_snap =
          block_to_snapshot_[forward_predecessor.index()].value();
      types_table_.StartNewSnapshot(forward_edge_snap);
    }
  } else if (block.IsBranchTarget()) {
    DCHECK_EQ(block.PredecessorCount(), 1);
    const Block& predecessor = *block.LastPredecessor();
    types_table_.StartNewSnapshot(
        block_to_snapshot_[predecessor.index()].value());
    if (IsReachable(predecessor)) {
      const BranchOp* branch =
          block.Predecessors()[0]->LastOperation(graph_).TryCast<BranchOp>();
      if (branch != nullptr) {
        ProcessBranchOnTarget(*branch, block);
      }
    } else {
      block_is_unreachable_.Add(block.index().id());
    }
  } else {
    DCHECK_EQ(block.kind(), Block::Kind::kMerge);
    CreateMergeSnapshot(block);
  }
}

void WasmGCTypeAnalyzer::ProcessOperations(const Block& block) {
  for (OpIndex op_idx : graph_.OperationIndices(block)) {
    Operation& op = graph_.Get(op_idx);
    switch (op.opcode) {
      case Opcode::kWasmTypeCast:
        ProcessTypeCast(op.Cast<WasmTypeCastOp>());
        break;
      case Opcode::kWasmTypeCheck:
        ProcessTypeCheck(op.Cast<WasmTypeCheckOp>());
        break;
      case Opcode::kAssertNotNull:
        ProcessAssertNotNull(op.Cast<AssertNotNullOp>());
        break;
      case Opcode::kNull:
        ProcessNull(op.Cast<NullOp>());
        break;
      case Opcode::kIsNull:
        ProcessIsNull(op.Cast<IsNullOp>());
        break;
      case Opcode::kParameter:
        ProcessParameter(op.Cast<ParameterOp>());
        break;
      case Opcode::kStructGet:
        ProcessStructGet(op.Cast<StructGetOp>());
        break;
      case Opcode::kStructSet:
        ProcessStructSet(op.Cast<StructSetOp>());
        break;
      case Opcode::kArrayGet:
        ProcessArrayGet(op.Cast<ArrayGetOp>());
        break;
      case Opcode::kArrayLength:
        ProcessArrayLength(op.Cast<ArrayLengthOp>());
        break;
      case Opcode::kGlobalGet:
        ProcessGlobalGet(op.Cast<GlobalGetOp>());
        break;
      case Opcode::kWasmRefFunc:
        ProcessRefFunc(op.Cast<WasmRefFuncOp>());
        break;
      case Opcode::kWasmAllocateArray:
        ProcessAllocateArray(op.Cast<WasmAllocateArrayOp>());
        break;
      case Opcode::kWasmAllocateStruct:
        ProcessAllocateStruct(op.Cast<WasmAllocateStructOp>());
        break;
      case Opcode::kPhi:
        ProcessPhi(op.Cast<PhiOp>());
        break;
      case Opcode::kWasmTypeAnnotation:
        ProcessTypeAnnotation(op.Cast<WasmTypeAnnotationOp>());
        break;
      case Opcode::kBranch:
        // Handling branch conditions implying special values is handled on the
        // beginning of the successor block.
      default:
        break;
    }
  }
}

void WasmGCTypeAnalyzer::ProcessTypeCast(const WasmTypeCastOp& type_cast) {
  V<Object> object = type_cast.object();
  wasm::ValueType target_type = type_cast.config.to;
  wasm::ValueType known_input_type = RefineTypeKnowledge(object, target_type);
  input_type_map_[graph_.Index(type_cast)] = known_input_type;
}

void WasmGCTypeAnalyzer::ProcessTypeCheck(const WasmTypeCheckOp& type_check) {
  wasm::ValueType type = GetResolvedType(type_check.object());
  input_type_map_[graph_.Index(type_check)] = type;
}

void WasmGCTypeAnalyzer::ProcessAssertNotNull(
    const AssertNotNullOp& assert_not_null) {
  V<Object> object = assert_not_null.object();
  wasm::ValueType new_type = assert_not_null.type.AsNonNull();
  wasm::ValueType known_input_type = RefineTypeKnowledge(object, new_type);
  input_type_map_[graph_.Index(assert_not_null)] = known_input_type;
}

void WasmGCTypeAnalyzer::ProcessIsNull(const IsNullOp& is_null) {
  input_type_map_[graph_.Index(is_null)] = GetResolvedType(is_null.object());
}

void WasmGCTypeAnalyzer::ProcessParameter(const ParameterOp& parameter) {
  if (parameter.parameter_index != wasm::kWasmInstanceParameterIndex) {
    RefineTypeKnowledge(graph_.Index(parameter),
                        signature_->GetParam(parameter.parameter_index - 1));
  }
}

void WasmGCTypeAnalyzer::ProcessStructGet(const StructGetOp& struct_get) {
  // struct.get performs a null check.
  wasm::ValueType type = RefineTypeKnowledgeNotNull(struct_get.object());
  input_type_map_[graph_.Index(struct_get)] = type;
  RefineTypeKnowledge(
      graph_.Index(struct_get),
      struct_get.type->field(struct_get.field_index).Unpacked());
}

void WasmGCTypeAnalyzer::ProcessStructSet(const StructSetOp& struct_set) {
  // struct.set performs a null check.
  wasm::ValueType type = RefineTypeKnowledgeNotNull(struct_set.object());
  input_type_map_[graph_.Index(struct_set)] = type;
}

void WasmGCTypeAnalyzer::ProcessArrayGet(const ArrayGetOp& array_get) {
  // array.get traps on null. (Typically already on the array length access
  // needed for the bounds check.)
  RefineTypeKnowledgeNotNull(array_get.array());
  // The result type is at least the static array element type.
  RefineTypeKnowledge(graph_.Index(array_get),
                      array_get.array_type->element_type().Unpacked());
}

void WasmGCTypeAnalyzer::ProcessArrayLength(const ArrayLengthOp& array_length) {
  // array.len performs a null check.
  wasm::ValueType type = RefineTypeKnowledgeNotNull(array_length.array());
  input_type_map_[graph_.Index(array_length)] = type;
}

void WasmGCTypeAnalyzer::ProcessGlobalGet(const GlobalGetOp& global_get) {
  RefineTypeKnowledge(graph_.Index(global_get), global_get.global->type);
}

void WasmGCTypeAnalyzer::ProcessRefFunc(const WasmRefFuncOp& ref_func) {
  uint32_t sig_index = module_->functions[ref_func.function_index].sig_index;
  RefineTypeKnowledge(graph_.Index(ref_func), wasm::ValueType::Ref(sig_index));
}

void WasmGCTypeAnalyzer::ProcessAllocateArray(
    const WasmAllocateArrayOp& allocate_array) {
  uint32_t type_index =
      graph_.Get(allocate_array.rtt()).Cast<RttCanonOp>().type_index;
  RefineTypeKnowledge(graph_.Index(allocate_array),
                      wasm::ValueType::Ref(type_index));
}

void WasmGCTypeAnalyzer::ProcessAllocateStruct(
    const WasmAllocateStructOp& allocate_struct) {
  uint32_t type_index =
      graph_.Get(allocate_struct.rtt()).Cast<RttCanonOp>().type_index;
  RefineTypeKnowledge(graph_.Index(allocate_struct),
                      wasm::ValueType::Ref(type_index));
}

void WasmGCTypeAnalyzer::ProcessPhi(const PhiOp& phi) {
  // The result type of a phi is the union of all its input types.
  // If any of the inputs is the default value ValueType(), there isn't any type
  // knowledge inferrable.
  DCHECK_GT(phi.input_count, 0);
  if (is_first_loop_header_evaluation_) {
    // We don't know anything about the backedge yet, so we only use the
    // forward edge. We will revisit the loop header again once the block with
    // the back edge is evaluated.
    RefineTypeKnowledge(graph_.Index(phi), GetResolvedType((phi.input(0))));
    return;
  }
  wasm::ValueType union_type =
      types_table_.GetPredecessorValue(ResolveAliases(phi.input(0)), 0);
  if (union_type == wasm::ValueType()) return;
  for (int i = 1; i < phi.input_count; ++i) {
    wasm::ValueType input_type =
        types_table_.GetPredecessorValue(ResolveAliases(phi.input(i)), i);
    if (input_type == wasm::ValueType()) return;
    // <bottom> types have to be skipped as an unreachable predecessor doesn't
    // change our type knowledge.
    // TODO(mliedtke): Ideally, we'd skip unreachable predecessors here
    // completely, as we might loosen the known type due to an unreachable
    // predecessor.
    if (input_type.is_uninhabited()) continue;
    if (union_type.is_uninhabited()) {
      union_type = input_type;
    } else {
      union_type = wasm::Union(union_type, input_type, module_, module_).type;
    }
  }
  RefineTypeKnowledge(graph_.Index(phi), union_type);
}

void WasmGCTypeAnalyzer::ProcessTypeAnnotation(
    const WasmTypeAnnotationOp& type_annotation) {
  RefineTypeKnowledge(type_annotation.value(), type_annotation.type);
}

void WasmGCTypeAnalyzer::ProcessBranchOnTarget(const BranchOp& branch,
                                               const Block& target) {
  DCHECK_EQ(current_block_, &target);
  const Operation& condition = graph_.Get(branch.condition());
  switch (condition.opcode) {
    case Opcode::kWasmTypeCheck: {
      const WasmTypeCheckOp& check = condition.Cast<WasmTypeCheckOp>();
      if (branch.if_true == &target) {
        // It is known from now on that the type is at least the checked one.
        RefineTypeKnowledge(check.object(), check.config.to);
      } else {
        DCHECK_EQ(branch.if_false, &target);
        if (wasm::IsSubtypeOf(GetResolvedType(check.object()), check.config.to,
                              module_)) {
          // The type check always succeeds, the target is impossible to be
          // reached.
          DCHECK_EQ(target.PredecessorCount(), 1);
          block_is_unreachable_.Add(target.index().id());
        }
      }
    } break;
    case Opcode::kIsNull: {
      const IsNullOp& is_null = condition.Cast<IsNullOp>();
      if (branch.if_true == &target) {
        if (GetResolvedType(is_null.object()).is_non_nullable()) {
          // The target is impossible to be reached.
          DCHECK_EQ(target.PredecessorCount(), 1);
          block_is_unreachable_.Add(target.index().id());
          return;
        }
        RefineTypeKnowledge(is_null.object(),
                            wasm::ToNullSentinel({is_null.type, module_}));
      } else {
        DCHECK_EQ(branch.if_false, &target);
        RefineTypeKnowledge(is_null.object(), is_null.type.AsNonNull());
      }
    } break;
    default:
      break;
  }
}

void WasmGCTypeAnalyzer::ProcessNull(const NullOp& null) {
  wasm::ValueType null_type = wasm::ToNullSentinel({null.type, module_});
  RefineTypeKnowledge(graph_.Index(null), null_type);
}

void WasmGCTypeAnalyzer::CreateMergeSnapshot(const Block& block) {
  base::SmallVector<Snapshot, 8> snapshots;
  // Unreachable predecessors should be ignored when merging but we can't remove
  // them from the predecessors as that would mess up the phi inputs. Therefore
  // the reachability of the predecessors is passed as a separate list.
  base::SmallVector<bool, 8> reachable;
  bool all_predecessors_unreachable = true;
  for (const Block* predecessor : block.PredecessorsIterable()) {
    snapshots.push_back(block_to_snapshot_[predecessor->index()].value());
    bool predecessor_reachable = IsReachable(*predecessor);
    reachable.push_back(predecessor_reachable);
    all_predecessors_unreachable &= !predecessor_reachable;
  }
  if (all_predecessors_unreachable) {
    block_is_unreachable_.Add(block.index().id());
  }
  // The predecessor snapshots need to be reversed to restore the "original"
  // order of predecessors. (This is used to map phi inputs to their
  // corresponding predecessor.)
  std::reverse(snapshots.begin(), snapshots.end());
  std::reverse(reachable.begin(), reachable.end());
  CreateMergeSnapshot(base::VectorOf(snapshots), base::VectorOf(reachable));
}

bool WasmGCTypeAnalyzer::CreateMergeSnapshot(
    base::Vector<const Snapshot> predecessors,
    base::Vector<const bool> reachable) {
  DCHECK_EQ(predecessors.size(), reachable.size());
  // The merging logic is also used to evaluate if two snapshots are
  // "identical", i.e. the known types for all operations are the same.
  bool types_are_equivalent = true;
  types_table_.StartNewSnapshot(
      predecessors, [this, &types_are_equivalent, reachable](
                        TypeSnapshotTable::Key,
                        base::Vector<const wasm::ValueType> predecessors) {
        DCHECK_GT(predecessors.size(), 1);
        size_t i = 0;
        // Initialize the type based on the first reachable predecessor.
        wasm::ValueType first = wasm::kWasmBottom;
        for (; i < reachable.size(); ++i) {
          // Uninhabitated types can only occur in unreachable code e.g. as a
          // result of an always failing cast. Still reachability tracking might
          // in some cases miss that a block becomes unreachable, so we still
          // check for uninhabited in the if below.
          DCHECK_IMPLIES(reachable[i], !predecessors[i].is_uninhabited());
          if (reachable[i] && !predecessors[i].is_uninhabited()) {
            first = predecessors[i];
            ++i;
            break;
          }
        }

        wasm::ValueType res = first;
        for (; i < reachable.size(); ++i) {
          if (!reachable[i]) continue;  // Skip unreachable predecessors.
          wasm::ValueType type = predecessors[i];
          // Uninhabitated types can only occur in unreachable code e.g. as a
          // result of an always failing cast. Still reachability tracking might
          // in some cases miss that a block becomes unreachable, so we still
          // check for uninhabited in the if below.
          DCHECK(!type.is_uninhabited());
          if (type.is_uninhabited()) continue;
          types_are_equivalent &= first == type;
          if (res == wasm::ValueType() || type == wasm::ValueType()) {
            res = wasm::ValueType();
          } else {
            res = wasm::Union(res, type, module_, module_).type;
          }
        }
        return res;
      });
  return !types_are_equivalent;
}

wasm::ValueType WasmGCTypeAnalyzer::RefineTypeKnowledge(
    OpIndex object, wasm::ValueType new_type) {
  DCHECK_NOT_NULL(current_block_);
  object = ResolveAliases(object);
  wasm::ValueType previous_value = types_table_.Get(object);
  wasm::ValueType intersection_type =
      previous_value == wasm::ValueType()
          ? new_type
          : wasm::Intersection(previous_value, new_type, module_, module_).type;
  if (intersection_type.is_uninhabited()) {
    block_is_unreachable_.Add(current_block_->index().id());
  }
  types_table_.Set(object, intersection_type);
  return previous_value;
}

wasm::ValueType WasmGCTypeAnalyzer::RefineTypeKnowledgeNotNull(OpIndex object) {
  object = ResolveAliases(object);
  wasm::ValueType previous_value = types_table_.Get(object);
  wasm::ValueType not_null_type = previous_value.AsNonNull();
  if (not_null_type.is_uninhabited()) {
    block_is_unreachable_.Add(current_block_->index().id());
  }
  types_table_.Set(object, not_null_type);
  return previous_value;
}

OpIndex WasmGCTypeAnalyzer::ResolveAliases(OpIndex object) const {
  while (true) {
    const Operation* op = &graph_.Get(object);
    switch (op->opcode) {
      case Opcode::kWasmTypeCast:
        object = op->Cast<WasmTypeCastOp>().object();
        break;
      case Opcode::kAssertNotNull:
        object = op->Cast<AssertNotNullOp>().object();
        break;
      case Opcode::kWasmTypeAnnotation:
        object = op->Cast<WasmTypeAnnotationOp>().value();
        break;
      default:
        return object;
    }
  }
}

bool WasmGCTypeAnalyzer::IsReachable(const Block& block) const {
  return !block_is_unreachable_.Contains(block.index().id());
}

wasm::ValueType WasmGCTypeAnalyzer::GetResolvedType(OpIndex object) const {
  return types_table_.Get(ResolveAliases(object));
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                   node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-gc-typed-optimization-reducer.h                    0000664 0000000 0000000 00000035214 14746647661 0027641 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_GC_TYPED_OPTIMIZATION_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_WASM_GC_TYPED_OPTIMIZATION_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/snapshot-table-opindex.h"
#include "src/compiler/wasm-graph-assembler.h"
#include "src/wasm/wasm-subtyping.h"

namespace v8::internal::compiler::turboshaft {

// The WasmGCTypedOptimizationReducer infers type information based on the input
// graph and reduces type checks and casts based on that information.
//
// This is done in two steps:
// 1) The WasmGCTypeAnalyzer infers the types based on the input graph, e.g.:
//    func (param anyref) (result i32)
//      local.get 0
//      ref.test $MyType
//      if                     // local 0 is known to be (a subtype of) $MyType
//        local.get 0
//        ref.cast $MyType     // the input of this cast is a subtype of $MyType
//                             // it can be removed during reduction
//        struct.get $MyType 0
//        return
//      end                    // local 0 is still anyref
//        i32.const 0
//
// 2) The WasmGCTypedOptimizationReducer reduces the graph to a new graph
//    potentially removing, simplifying (e.g. replacing a cast with a null
//    check) or refining (setting the from type to a more specific type) type
//    operations.

class WasmGCTypeAnalyzer {
 public:
  WasmGCTypeAnalyzer(PipelineData* data, Graph& graph, Zone* zone)
      : data_(data), graph_(graph), phase_zone_(zone) {}

  void Run();

  wasm::ValueType GetInputType(OpIndex op) const {
    auto iter = input_type_map_.find(op);
    DCHECK_NE(iter, input_type_map_.end());
    return iter->second;
  }

 private:
  using TypeSnapshotTable = SparseOpIndexSnapshotTable<wasm::ValueType>;
  using Snapshot = TypeSnapshotTable::Snapshot;
  using MaybeSnapshot = TypeSnapshotTable::MaybeSnapshot;

  void StartNewSnapshotFor(const Block& block);
  void ProcessOperations(const Block& block);
  void ProcessBlock(const Block& block);
  void ProcessBranchOnTarget(const BranchOp& branch, const Block& target);

  void ProcessTypeCast(const WasmTypeCastOp& type_cast);
  void ProcessTypeCheck(const WasmTypeCheckOp& type_check);
  void ProcessAssertNotNull(const AssertNotNullOp& type_cast);
  void ProcessNull(const NullOp& null);
  void ProcessIsNull(const IsNullOp& is_null);
  void ProcessParameter(const ParameterOp& parameter);
  void ProcessStructGet(const StructGetOp& struct_get);
  void ProcessStructSet(const StructSetOp& struct_set);
  void ProcessArrayGet(const ArrayGetOp& array_get);
  void ProcessArrayLength(const ArrayLengthOp& array_length);
  void ProcessGlobalGet(const GlobalGetOp& global_get);
  void ProcessRefFunc(const WasmRefFuncOp& ref_func);
  void ProcessAllocateArray(const WasmAllocateArrayOp& allocate_array);
  void ProcessAllocateStruct(const WasmAllocateStructOp& allocate_struct);
  void ProcessPhi(const PhiOp& phi);
  void ProcessTypeAnnotation(const WasmTypeAnnotationOp& type_annotation);

  void CreateMergeSnapshot(const Block& block);
  bool CreateMergeSnapshot(base::Vector<const Snapshot> predecessors,
                           base::Vector<const bool> reachable);

  // Updates the knowledge in the side table about the type of {object},
  // returning the previous known type.
  wasm::ValueType RefineTypeKnowledge(OpIndex object, wasm::ValueType new_type);
  // Updates the knowledge in the side table to be a non-nullable type for
  // {object}, returning the previous known type.
  wasm::ValueType RefineTypeKnowledgeNotNull(OpIndex object);

  OpIndex ResolveAliases(OpIndex object) const;
  wasm::ValueType GetResolvedType(OpIndex object) const;

  bool IsReachable(const Block& block) const;

  PipelineData* data_;
  Graph& graph_;
  Zone* phase_zone_;
  const wasm::WasmModule* module_ = data_->wasm_module();
  const wasm::FunctionSig* signature_ = data_->wasm_sig();
  // Contains the snapshots for all blocks in the CFG.
  TypeSnapshotTable types_table_{phase_zone_};
  // Maps the block id to a snapshot in the table defining the type knowledge
  // at the end of the block.
  FixedBlockSidetable<MaybeSnapshot> block_to_snapshot_{graph_.block_count(),
                                                        phase_zone_};
  BitVector block_is_unreachable_{static_cast<int>(graph_.block_count()),
                                  phase_zone_};
  const Block* current_block_ = nullptr;
  // For any operation that could potentially refined, this map stores an entry
  // to the inferred input type based on the analysis.
  ZoneUnorderedMap<OpIndex, wasm::ValueType> input_type_map_{phase_zone_};
  // Marker wheteher it is the first time visiting a loop header. In that case,
  // loop phis can only use type information based on the forward edge of the
  // loop. The value is false outside of loop headers.
  bool is_first_loop_header_evaluation_ = false;
};

#include "src/compiler/turboshaft/define-assembler-macros.inc"

template <class Next>
class WasmGCTypedOptimizationReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(WasmGCTypedOptimization)

  void Analyze() {
    analyzer_.Run();
    Next::Analyze();
  }

  V<Object> REDUCE_INPUT_GRAPH(WasmTypeCast)(V<Object> op_idx,
                                             const WasmTypeCastOp& cast_op) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphWasmTypeCast(op_idx, cast_op);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    wasm::ValueType type = analyzer_.GetInputType(op_idx);
    if (type != wasm::ValueType() && !type.is_uninhabited()) {
      DCHECK(wasm::IsSameTypeHierarchy(type.heap_type(),
                                       cast_op.config.to.heap_type(), module_));
      bool to_nullable = cast_op.config.to.is_nullable();
      if (wasm::IsHeapSubtypeOf(type.heap_type(), cast_op.config.to.heap_type(),
                                module_, module_)) {
        if (to_nullable || type.is_non_nullable()) {
          // The inferred type is already as specific as the cast target, the
          // cast is guaranteed to always succeed and can therefore be removed.
          return __ MapToNewGraph(cast_op.object());
        } else {
          // The inferred heap type is already as specific as the cast target,
          // but the source can be nullable and the target cannot be, so a null
          // check is still required.
          return __ AssertNotNull(__ MapToNewGraph(cast_op.object()), type,
                                  TrapId::kTrapIllegalCast);
        }
      }
      if (wasm::HeapTypesUnrelated(type.heap_type(),
                                   cast_op.config.to.heap_type(), module_,
                                   module_)) {
        // A cast between unrelated types can only succeed if the argument is
        // null. Otherwise, it always fails.
        V<Word32> non_trapping_condition =
            type.is_nullable() && to_nullable ? __ IsNull(__ MapToNewGraph(
                                                              cast_op.object()),
                                                          type)
                                              : __ Word32Constant(0);
        __ TrapIfNot(non_trapping_condition, TrapId::kTrapIllegalCast);
        if (!to_nullable) {
          __ Unreachable();
        }
        return __ MapToNewGraph(cast_op.object());
      }
      // The cast cannot be replaced. Still, we can refine the source type, so
      // that the lowering could potentially skip null or smi checks.
      wasm::ValueType from_type =
          wasm::Intersection(type, cast_op.config.from, module_, module_).type;
      DCHECK(!from_type.is_uninhabited());
      WasmTypeCheckConfig config{from_type, cast_op.config.to};
      return __ WasmTypeCast(__ MapToNewGraph(cast_op.object()),
                             __ MapToNewGraph(cast_op.rtt()), config);
    }
    goto no_change;
  }

  V<Word32> REDUCE_INPUT_GRAPH(WasmTypeCheck)(
      V<Word32> op_idx, const WasmTypeCheckOp& type_check) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphWasmTypeCheck(op_idx, type_check);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    wasm::ValueType type = analyzer_.GetInputType(op_idx);
    if (type != wasm::ValueType() && !type.is_uninhabited()) {
      DCHECK(wasm::IsSameTypeHierarchy(
          type.heap_type(), type_check.config.to.heap_type(), module_));
      bool to_nullable = type_check.config.to.is_nullable();
      if (wasm::IsHeapSubtypeOf(type.heap_type(),
                                type_check.config.to.heap_type(), module_,
                                module_)) {
        if (to_nullable || type.is_non_nullable()) {
          // The inferred type is guaranteed to be a subtype of the checked
          // type.
          return __ Word32Constant(1);
        } else {
          // The inferred type is guaranteed to be a subtype of the checked
          // type if it is not null.
          return __ Word32Equal(
              __ IsNull(__ MapToNewGraph(type_check.object()), type), 0);
        }
      }
      if (wasm::HeapTypesUnrelated(type.heap_type(),
                                   type_check.config.to.heap_type(), module_,
                                   module_)) {
        if (to_nullable && type.is_nullable()) {
          return __ IsNull(__ MapToNewGraph(type_check.object()), type);
        } else {
          return __ Word32Constant(0);
        }
      }
      // The check cannot be replaced. Still, we can refine the source type, so
      // that the lowering could potentially skip null or smi checks.
      wasm::ValueType from_type =
          wasm::Intersection(type, type_check.config.from, module_, module_)
              .type;
      DCHECK(!from_type.is_uninhabited());
      WasmTypeCheckConfig config{from_type, type_check.config.to};
      return __ WasmTypeCheck(__ MapToNewGraph(type_check.object()),
                              __ MapToNewGraph(type_check.rtt()), config);
    }
    goto no_change;
  }

  V<Object> REDUCE_INPUT_GRAPH(AssertNotNull)(
      V<Object> op_idx, const AssertNotNullOp& assert_not_null) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphAssertNotNull(op_idx, assert_not_null);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    wasm::ValueType type = analyzer_.GetInputType(op_idx);
    if (type.is_non_nullable()) {
      return __ MapToNewGraph(assert_not_null.object());
    }
    goto no_change;
  }

  V<Word32> REDUCE_INPUT_GRAPH(IsNull)(V<Word32> op_idx,
                                       const IsNullOp& is_null) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphIsNull(op_idx, is_null);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    const wasm::ValueType type = analyzer_.GetInputType(op_idx);
    if (type.is_non_nullable()) {
      return __ Word32Constant(0);
    }
    if (type != wasm::ValueType() && type != wasm::kWasmBottom &&
        wasm::ToNullSentinel({type, module_}) == type) {
      return __ Word32Constant(1);
    }
    goto no_change;
  }

  V<Object> REDUCE_INPUT_GRAPH(WasmTypeAnnotation)(
      V<Object> op_idx, const WasmTypeAnnotationOp& type_annotation) {
    // Remove type annotation operations as they are not needed any more.
    return __ MapToNewGraph(type_annotation.value());
  }

  V<Any> REDUCE_INPUT_GRAPH(StructGet)(V<Any> op_idx,
                                       const StructGetOp& struct_get) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphStructGet(op_idx, struct_get);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    const wasm::ValueType type = analyzer_.GetInputType(op_idx);
    // Remove the null check if it is known to be not null.
    if (struct_get.null_check == kWithNullCheck && type.is_non_nullable()) {
      return __ StructGet(__ MapToNewGraph(struct_get.object()),
                          struct_get.type, struct_get.type_index,
                          struct_get.field_index, struct_get.is_signed,
                          kWithoutNullCheck);
    }
    goto no_change;
  }

  V<None> REDUCE_INPUT_GRAPH(StructSet)(V<None> op_idx,
                                        const StructSetOp& struct_set) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphStructSet(op_idx, struct_set);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    const wasm::ValueType type = analyzer_.GetInputType(op_idx);
    // Remove the null check if it is known to be not null.
    if (struct_set.null_check == kWithNullCheck && type.is_non_nullable()) {
      __ StructSet(__ MapToNewGraph(struct_set.object()),
                   __ MapToNewGraph(struct_set.value()), struct_set.type,
                   struct_set.type_index, struct_set.field_index,
                   kWithoutNullCheck);
      return OpIndex::Invalid();
    }
    goto no_change;
  }

  V<Word32> REDUCE_INPUT_GRAPH(ArrayLength)(V<Word32> op_idx,
                                            const ArrayLengthOp& array_length) {
    LABEL_BLOCK(no_change) {
      return Next::ReduceInputGraphArrayLength(op_idx, array_length);
    }
    if (ShouldSkipOptimizationStep()) goto no_change;

    const wasm::ValueType type = analyzer_.GetInputType(op_idx);
    // Remove the null check if it is known to be not null.
    if (array_length.null_check == kWithNullCheck && type.is_non_nullable()) {
      return __ ArrayLength(__ MapToNewGraph(array_length.array()),
                            kWithoutNullCheck);
    }
    goto no_change;
  }

  // TODO(14108): This isn't a type optimization and doesn't fit well into this
  // reducer.
  V<Object> REDUCE(AnyConvertExtern)(V<Object> object) {
    LABEL_BLOCK(no_change) { return Next::ReduceAnyConvertExtern(object); }
    if (ShouldSkipOptimizationStep()) goto no_change;

    if (object.valid()) {
      const ExternConvertAnyOp* externalize =
          __ output_graph().Get(object).template TryCast<ExternConvertAnyOp>();
      if (externalize != nullptr) {
        // Directly return the object as
        // any.convert_extern(extern.convert_any(x)) == x.
        return externalize->object();
      }
    }
    goto no_change;
  }

 private:
  Graph& graph_ = __ modifiable_input_graph();
  const wasm::WasmModule* module_ = __ data() -> wasm_module();
  WasmGCTypeAnalyzer analyzer_{__ data(), graph_, __ phase_zone()};
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_GC_TYPED_OPTIMIZATION_REDUCER_H_
                                                                                                                                                                                                                                                                                                                                                                                    node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-js-lowering-reducer.h                              0000664 0000000 0000000 00000007346 14746647661 0025646 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_JS_LOWERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_WASM_JS_LOWERING_REDUCER_H_

#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/wasm-graph-assembler.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

// This reducer is part of the JavaScript pipeline and contains lowering of
// wasm nodes (from inlined wasm functions).
//
// The reducer replaces all TrapIf nodes with a conditional goto to deferred
// code containing a call to the trap builtin.
template <class Next>
class WasmJSLoweringReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(WasmJSLowering)

  V<None> REDUCE(TrapIf)(V<Word32> condition, OptionalV<FrameState> frame_state,
                         bool negated, TrapId trap_id) {
    // All TrapIf nodes in JS need to have a FrameState.
    DCHECK(frame_state.valid());
    Builtin trap = static_cast<Builtin>(trap_id);
    // The call is not marked as Operator::kNoDeopt. While it cannot actually
    // deopt, deopt info based on the provided FrameState is required for stack
    // trace creation of the wasm trap.
    const bool needs_frame_state = true;
    const CallDescriptor* tf_descriptor = GetBuiltinCallDescriptor(
        trap, Asm().graph_zone(), StubCallMode::kCallBuiltinPointer,
        needs_frame_state, Operator::kNoProperties);
    const TSCallDescriptor* ts_descriptor =
        TSCallDescriptor::Create(tf_descriptor, CanThrow::kYes,
                                 LazyDeoptOnThrow::kNo, Asm().graph_zone());

    V<FrameState> new_frame_state =
        CreateFrameStateWithUpdatedBailoutId(frame_state.value());
    V<Word32> should_trap = negated ? __ Word32Equal(condition, 0) : condition;
    IF (UNLIKELY(should_trap)) {
      OpIndex call_target = __ NumberConstant(static_cast<int>(trap));
      __ Call(call_target, new_frame_state, {}, ts_descriptor);
      __ Unreachable();  // The trap builtin never returns.
    }

    return V<None>::Invalid();
  }

 private:
  OpIndex CreateFrameStateWithUpdatedBailoutId(OpIndex frame_state) {
    // Create new FrameState with the correct source position (the position of
    // the trap location).
    const FrameStateOp& frame_state_op =
        Asm().output_graph().Get(frame_state).template Cast<FrameStateOp>();
    const FrameStateData* data = frame_state_op.data;
    const FrameStateInfo& info = data->frame_state_info;

    V<AnyOrNone> origin = Asm().current_operation_origin();
    DCHECK(origin.valid());
    int offset = __ input_graph().source_positions()[origin].ScriptOffset();

    const FrameStateInfo* new_info =
        Asm().graph_zone()->template New<FrameStateInfo>(
            BytecodeOffset(offset), info.state_combine(), info.function_info());
    FrameStateData* new_data = Asm().graph_zone()->template New<FrameStateData>(
        FrameStateData{*new_info, data->instructions, data->machine_types,
                       data->int_operands});
    return __ FrameState(frame_state_op.inputs(), frame_state_op.inlined,
                         new_data);
  }

  Isolate* isolate_ = __ data() -> isolate();
  SourcePositionTable* source_positions_ = __ data() -> source_positions();
};

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_JS_LOWERING_REDUCER_H_
                                                                                                                                                                                                                                                                                          node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-load-elimination-reducer.h                         0000664 0000000 0000000 00000120464 14746647661 0026630 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include <optional>

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_LOAD_ELIMINATION_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_WASM_LOAD_ELIMINATION_REDUCER_H_

#include "src/base/doubly-threaded-list.h"
#include "src/compiler/turboshaft/analyzer-iterator.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/graph.h"
#include "src/compiler/turboshaft/loop-finder.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/snapshot-table-opindex.h"
#include "src/compiler/turboshaft/utils.h"
#include "src/wasm/wasm-subtyping.h"
#include "src/zone/zone.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

// WLE is short for Wasm Load Elimination.
// We need the namespace because we reuse class names below that also exist
// in the LateLoadEliminationReducer, and in the same namespace that'd be
// an ODR violation, i.e. Undefined Behavior.
// TODO(jkummerow): Refactor the two Load Elimination implementations to
// reuse commonalities.
namespace wle {

// We model array length and string canonicalization as fields at negative
// indices.
static constexpr int kArrayLengthFieldIndex = -1;
static constexpr int kStringPrepareForGetCodeunitIndex = -2;
static constexpr int kStringAsWtf16Index = -3;
static constexpr int kAnyConvertExternIndex = -4;
static constexpr int kAssertNotNullIndex = -5;

// All "load-like" special cases use the same fake size and type. The specific
// values we use don't matter; for accurate alias analysis, the type should
// be "unrelated" to any struct type.
static constexpr uint32_t kLoadLikeType = wasm::HeapType::kExtern;
static constexpr int kLoadLikeSize = 4;  // Chosen by fair dice roll.

struct WasmMemoryAddress {
  OpIndex base;
  int32_t offset;
  uint32_t type_index;
  uint8_t size;
  bool mutability;

  bool operator==(const WasmMemoryAddress& other) const {
    return base == other.base && offset == other.offset &&
           type_index == other.type_index && size == other.size &&
           mutability == other.mutability;
  }
};

inline size_t hash_value(WasmMemoryAddress const& mem) {
  return fast_hash_combine(mem.base, mem.offset, mem.type_index, mem.size,
                           mem.mutability);
}

struct KeyData {
  using Key = SnapshotTableKey<OpIndex, KeyData>;
  WasmMemoryAddress mem = {};
  // Pointers to the previous and the next Keys at the same base.
  Key* prev_same_base = nullptr;
  Key next_same_base = {};
  // Pointers to either the next/previous Keys at the same offset.
  Key* prev_same_offset = nullptr;
  Key next_same_offset = {};
};

struct OffsetListTraits {
  using T = SnapshotTable<OpIndex, KeyData>::Key;
  static T** prev(T t) { return &(t.data().prev_same_offset); }
  static T* next(T t) { return &(t.data().next_same_offset); }
  static bool non_empty(T t) { return t.valid(); }
};

struct BaseListTraits {
  using T = SnapshotTable<OpIndex, KeyData>::Key;
  static T** prev(T t) { return &(t.data().prev_same_base); }
  static T* next(T t) { return &(t.data().next_same_base); }
  static bool non_empty(T t) { return t.valid(); }
};

struct BaseData {
  using Key = SnapshotTable<OpIndex, KeyData>::Key;
  // List of every value at this base that has an offset rather than an index.
  v8::base::DoublyThreadedList<Key, BaseListTraits> with_offsets;
};

class WasmMemoryContentTable
    : public ChangeTrackingSnapshotTable<WasmMemoryContentTable, OpIndex,
                                         KeyData> {
 public:
  using MemoryAddress = WasmMemoryAddress;

  explicit WasmMemoryContentTable(
      PipelineData* data, Zone* zone,
      SparseOpIndexSnapshotTable<bool>& non_aliasing_objects,
      FixedOpIndexSidetable<OpIndex>& replacements, Graph& graph)
      : ChangeTrackingSnapshotTable(zone),
        non_aliasing_objects_(non_aliasing_objects),
        replacements_(replacements),
        data_(data),
        graph_(graph),
        all_keys_(zone),
        base_keys_(zone),
        offset_keys_(zone) {}

  void OnNewKey(Key key, OpIndex value) {
    if (value.valid()) {
      AddKeyInBaseOffsetMaps(key);
    }
  }

  void OnValueChange(Key key, OpIndex old_value, OpIndex new_value) {
    DCHECK_NE(old_value, new_value);
    if (old_value.valid() && !new_value.valid()) {
      RemoveKeyFromBaseOffsetMaps(key);
    } else if (new_value.valid() && !old_value.valid()) {
      AddKeyInBaseOffsetMaps(key);
    } else {
      DCHECK_EQ(new_value.valid(), old_value.valid());
    }
  }

  bool TypesUnrelated(uint32_t type1, uint32_t type2) {
    return wasm::TypesUnrelated(wasm::ValueType::Ref(type1),
                                wasm::ValueType::Ref(type2), module_, module_);
  }

  void Invalidate(const StructSetOp& set) {
    // This is like LateLoadElimination's {InvalidateAtOffset}, but based
    // on Wasm types instead of tracked JS maps.
    int offset = field_offset(set.type, set.field_index);
    auto offset_keys = offset_keys_.find(offset);
    if (offset_keys == offset_keys_.end()) return;
    for (auto it = offset_keys->second.begin();
         it != offset_keys->second.end();) {
      Key key = *it;
      DCHECK_EQ(offset, key.data().mem.offset);
      OpIndex base = key.data().mem.base;

      // If the base is guaranteed non-aliasing, we don't need to clear any
      // other entries. Any existing entry for this base will be overwritten
      // by {Insert(set)}.
      if (non_aliasing_objects_.Get(base)) {
        ++it;
        continue;
      }

      if (TypesUnrelated(set.type_index, key.data().mem.type_index)) {
        ++it;
        continue;
      }

      it = offset_keys->second.RemoveAt(it);
      Set(key, OpIndex::Invalid());
    }
  }

  // Invalidates all Keys that are not known as non-aliasing.
  enum class EntriesWithOffsets { kInvalidate, kKeep };
  template <EntriesWithOffsets offsets = EntriesWithOffsets::kInvalidate>
  void InvalidateMaybeAliasing() {
    // We find current active keys through {base_keys_} so that we can bail out
    // for whole buckets non-aliasing buckets (if we had gone through
    // {offset_keys_} instead, then for each key we would've had to check
    // whether it was non-aliasing or not).
    for (auto& base_keys : base_keys_) {
      OpIndex base = base_keys.first;
      if (non_aliasing_objects_.Get(base)) continue;
      if constexpr (offsets == EntriesWithOffsets::kInvalidate) {
        for (auto it = base_keys.second.with_offsets.begin();
             it != base_keys.second.with_offsets.end();) {
          Key key = *it;
          if (key.data().mem.mutability == false) {
            ++it;
            continue;
          }
          // It's important to remove with RemoveAt before Setting the key to
          // invalid, otherwise OnKeyChange will remove {key} from {base_keys},
          // which will invalidate {it}.
          it = base_keys.second.with_offsets.RemoveAt(it);
          Set(key, OpIndex::Invalid());
        }
      }
    }
  }

  // TODO(jkummerow): Move this to the WasmStruct class?
  int field_offset(const wasm::StructType* type, int field_index) {
    return WasmStruct::kHeaderSize + type->field_offset(field_index);
  }

  OpIndex Find(const StructGetOp& get) {
    int32_t offset = field_offset(get.type, get.field_index);
    uint8_t size = get.type->field(get.field_index).value_kind_size();
    bool mutability = get.type->mutability(get.field_index);
    return FindImpl(ResolveBase(get.object()), offset, get.type_index, size,
                    mutability);
  }

  bool HasValueWithIncorrectMutability(const StructSetOp& set) {
    int32_t offset = field_offset(set.type, set.field_index);
    uint8_t size = set.type->field(set.field_index).value_kind_size();
    bool mutability = set.type->mutability(set.field_index);
    WasmMemoryAddress mem{ResolveBase(set.object()), offset, set.type_index,
                          size, !mutability};
    return all_keys_.find(mem) != all_keys_.end();
  }

  OpIndex FindLoadLike(OpIndex op_idx, int offset_sentinel) {
    static constexpr bool mutability = false;
    return FindImpl(ResolveBase(op_idx), offset_sentinel, kLoadLikeType,
                    kLoadLikeSize, mutability);
  }

  OpIndex FindImpl(OpIndex object, int offset, uint32_t type_index,
                   uint8_t size, bool mutability,
                   OptionalOpIndex index = OptionalOpIndex::Nullopt()) {
    WasmMemoryAddress mem{object, offset, type_index, size, mutability};
    auto key = all_keys_.find(mem);
    if (key == all_keys_.end()) return OpIndex::Invalid();
    return Get(key->second);
  }

  void Insert(const StructSetOp& set) {
    OpIndex base = ResolveBase(set.object());
    int32_t offset = field_offset(set.type, set.field_index);
    uint8_t size = set.type->field(set.field_index).value_kind_size();
    bool mutability = set.type->mutability(set.field_index);
    Insert(base, offset, set.type_index, size, mutability, set.value());
  }

  void Insert(const StructGetOp& get, OpIndex get_idx) {
    OpIndex base = ResolveBase(get.object());
    int32_t offset = field_offset(get.type, get.field_index);
    uint8_t size = get.type->field(get.field_index).value_kind_size();
    bool mutability = get.type->mutability(get.field_index);
    Insert(base, offset, get.type_index, size, mutability, get_idx);
  }

  void InsertLoadLike(OpIndex base_idx, int offset_sentinel,
                      OpIndex value_idx) {
    OpIndex base = ResolveBase(base_idx);
    static constexpr bool mutability = false;
    Insert(base, offset_sentinel, kLoadLikeType, kLoadLikeSize, mutability,
           value_idx);
  }

#ifdef DEBUG
  void Print() {
    std::cout << "WasmMemoryContentTable:\n";
    for (const auto& base_keys : base_keys_) {
      for (Key key : base_keys.second.with_offsets) {
        std::cout << "  * " << key.data().mem.base << " - "
                  << key.data().mem.offset << " ==> " << Get(key) << "\n";
      }
    }
  }
#endif  // DEBUG

 private:
  void Insert(OpIndex base, int32_t offset, uint32_t type_index, uint8_t size,
              bool mutability, OpIndex value) {
    DCHECK_EQ(base, ResolveBase(base));

    WasmMemoryAddress mem{base, offset, type_index, size, mutability};
    auto existing_key = all_keys_.find(mem);
    if (existing_key != all_keys_.end()) {
      if (mutability) {
        Set(existing_key->second, value);
      } else {
        SetNoNotify(existing_key->second, value);
      }
      return;
    }

    // Creating a new key.
    Key key = NewKey({mem});
    all_keys_.insert({mem, key});
    if (mutability) {
      Set(key, value);
    } else {
      // Call `SetNoNotify` to avoid calls to `OnNewKey` and `OnValueChanged`.
      SetNoNotify(key, value);
    }
  }

 public:
  OpIndex ResolveBase(OpIndex base) {
    while (true) {
      if (replacements_[base] != OpIndex::Invalid()) {
        base = replacements_[base];
        continue;
      }
      Operation& op = graph_.Get(base);
      if (AssertNotNullOp* check = op.TryCast<AssertNotNullOp>()) {
        base = check->object();
        continue;
      }
      if (WasmTypeCastOp* cast = op.TryCast<WasmTypeCastOp>()) {
        base = cast->object();
        continue;
      }
      break;  // Terminate if nothing happened.
    }
    return base;
  }

  void AddKeyInBaseOffsetMaps(Key key) {
    // Inserting in {base_keys_}.
    OpIndex base = key.data().mem.base;
    auto base_keys = base_keys_.find(base);
    if (base_keys != base_keys_.end()) {
      base_keys->second.with_offsets.PushFront(key);
    } else {
      BaseData data;
      data.with_offsets.PushFront(key);
      base_keys_.insert({base, std::move(data)});
    }

    // Inserting in {offset_keys_}.
    int offset = key.data().mem.offset;
    auto offset_keys = offset_keys_.find(offset);
    if (offset_keys != offset_keys_.end()) {
      offset_keys->second.PushFront(key);
    } else {
      v8::base::DoublyThreadedList<Key, OffsetListTraits> list;
      list.PushFront(key);
      offset_keys_.insert({offset, std::move(list)});
    }
  }

  void RemoveKeyFromBaseOffsetMaps(Key key) {
    // Removing from {base_keys_}.
    v8::base::DoublyThreadedList<Key, BaseListTraits>::Remove(key);
    v8::base::DoublyThreadedList<Key, OffsetListTraits>::Remove(key);
  }

  SparseOpIndexSnapshotTable<bool>& non_aliasing_objects_;
  FixedOpIndexSidetable<OpIndex>& replacements_;

  PipelineData* data_;
  Graph& graph_;

  const wasm::WasmModule* module_ = data_->wasm_module();

  // TODO(dmercadier): consider using a faster datastructure than
  // ZoneUnorderedMap for {all_keys_}, {base_keys_} and {offset_keys_}.

  // A map containing all of the keys, for fast lookup of a specific
  // MemoryAddress.
  ZoneUnorderedMap<WasmMemoryAddress, Key> all_keys_;
  // Map from base OpIndex to keys associated with this base.
  ZoneUnorderedMap<OpIndex, BaseData> base_keys_;
  // Map from offsets to keys associated with this offset.
  ZoneUnorderedMap<int, v8::base::DoublyThreadedList<Key, OffsetListTraits>>
      offset_keys_;
};

}  // namespace wle

class WasmLoadEliminationAnalyzer {
 public:
  using AliasTable = SparseOpIndexSnapshotTable<bool>;
  using AliasKey = AliasTable::Key;
  using AliasSnapshot = AliasTable::Snapshot;

  using MemoryKey = wle::WasmMemoryContentTable::Key;
  using MemorySnapshot = wle::WasmMemoryContentTable::Snapshot;

  WasmLoadEliminationAnalyzer(PipelineData* data, Graph& graph,
                              Zone* phase_zone)
      : graph_(graph),
        phase_zone_(phase_zone),
        replacements_(graph.op_id_count(), phase_zone, &graph),
        non_aliasing_objects_(phase_zone),
        memory_(data, phase_zone, non_aliasing_objects_, replacements_, graph_),
        block_to_snapshot_mapping_(graph.block_count(), phase_zone),
        predecessor_alias_snapshots_(phase_zone),
        predecessor_memory_snapshots_(phase_zone) {}

  void Run() {
    LoopFinder loop_finder(phase_zone_, &graph_);
    AnalyzerIterator iterator(phase_zone_, graph_, loop_finder);

    bool compute_start_snapshot = true;
    while (iterator.HasNext()) {
      const Block* block = iterator.Next();

      ProcessBlock(*block, compute_start_snapshot);
      compute_start_snapshot = true;

      // Consider re-processing for loops.
      if (const GotoOp* last = block->LastOperation(graph_).TryCast<GotoOp>()) {
        if (last->destination->IsLoop() &&
            last->destination->LastPredecessor() == block) {
          const Block* loop_header = last->destination;
          // {block} is the backedge of a loop. We recompute the loop header's
          // initial snapshots, and if they differ from its original snapshot,
          // then we revisit the loop.
          if (BeginBlock<true>(loop_header)) {
            // We set the snapshot of the loop's 1st predecessor to the newly
            // computed snapshot. It's not quite correct, but this predecessor
            // is guaranteed to end with a Goto, and we are now visiting the
            // loop, which means that we don't really care about this
            // predecessor anymore.
            // The reason for saving this snapshot is to prevent infinite
            // looping, since the next time we reach this point, the backedge
            // snapshot could still invalidate things from the forward edge
            // snapshot. By restricting the forward edge snapshot, we prevent
            // this.
            const Block* loop_1st_pred =
                loop_header->LastPredecessor()->NeighboringPredecessor();
            FinishBlock(loop_1st_pred);
            // And we start a new fresh snapshot from this predecessor.
            auto pred_snapshots =
                block_to_snapshot_mapping_[loop_1st_pred->index()];
            non_aliasing_objects_.StartNewSnapshot(
                pred_snapshots->alias_snapshot);
            memory_.StartNewSnapshot(pred_snapshots->memory_snapshot);

            iterator.MarkLoopForRevisit();
            compute_start_snapshot = false;
          } else {
            SealAndDiscard();
          }
        }
      }
    }
  }

  OpIndex Replacement(OpIndex index) { return replacements_[index]; }

 private:
  void ProcessBlock(const Block& block, bool compute_start_snapshot);
  void ProcessStructGet(OpIndex op_idx, const StructGetOp& op);
  void ProcessStructSet(OpIndex op_idx, const StructSetOp& op);
  void ProcessArrayLength(OpIndex op_idx, const ArrayLengthOp& op);
  void ProcessWasmAllocateArray(OpIndex op_idx, const WasmAllocateArrayOp& op);
  void ProcessStringAsWtf16(OpIndex op_idx, const StringAsWtf16Op& op);
  void ProcessStringPrepareForGetCodeUnit(
      OpIndex op_idx, const StringPrepareForGetCodeUnitOp& op);
  void ProcessAnyConvertExtern(OpIndex op_idx, const AnyConvertExternOp& op);
  void ProcessAssertNotNull(OpIndex op_idx, const AssertNotNullOp& op);
  void ProcessAllocate(OpIndex op_idx, const AllocateOp& op);
  void ProcessCall(OpIndex op_idx, const CallOp& op);
  void ProcessPhi(OpIndex op_idx, const PhiOp& op);

  void DcheckWordBinop(OpIndex op_idx, const WordBinopOp& binop);

  // BeginBlock initializes the various SnapshotTables for {block}, and returns
  // true if {block} is a loop that should be revisited.
  template <bool for_loop_revisit = false>
  bool BeginBlock(const Block* block);
  void FinishBlock(const Block* block);
  // Seals the current snapshot, but discards it. This is used when considering
  // whether a loop should be revisited or not: we recompute the loop header's
  // snapshots, and then revisit the loop if the snapshots contain
  // modifications. If the snapshots are unchanged, we discard them and don't
  // revisit the loop.
  void SealAndDiscard();
  void StoreLoopSnapshotInForwardPredecessor(const Block& loop_header);

  // Returns true if the loop's backedge already has snapshot data (meaning that
  // it was already visited).
  bool BackedgeHasSnapshot(const Block& loop_header) const;

  void InvalidateAllNonAliasingInputs(const Operation& op);
  void InvalidateIfAlias(OpIndex op_idx);

  Graph& graph_;
  Zone* phase_zone_;

  FixedOpIndexSidetable<OpIndex> replacements_;

  AliasTable non_aliasing_objects_;
  wle::WasmMemoryContentTable memory_;

  struct Snapshot {
    AliasSnapshot alias_snapshot;
    MemorySnapshot memory_snapshot;
  };
  FixedBlockSidetable<std::optional<Snapshot>> block_to_snapshot_mapping_;

  // {predecessor_alias_napshots_}, {predecessor_maps_snapshots_} and
  // {predecessor_memory_snapshots_} are used as temporary vectors when starting
  // to process a block. We store them as members to avoid reallocation.
  ZoneVector<AliasSnapshot> predecessor_alias_snapshots_;
  ZoneVector<MemorySnapshot> predecessor_memory_snapshots_;
};

template <class Next>
class WasmLoadEliminationReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(WasmLoadElimination)

  void Analyze() {
    if (v8_flags.turboshaft_wasm_load_elimination) {
      DCHECK(AllowHandleDereference::IsAllowed());
      analyzer_.Run();
    }
    Next::Analyze();
  }

#define EMIT_OP(Name)                                                          \
  OpIndex REDUCE_INPUT_GRAPH(Name)(OpIndex ig_index, const Name##Op& op) {     \
    if (v8_flags.turboshaft_wasm_load_elimination) {                           \
      OpIndex ig_replacement_index = analyzer_.Replacement(ig_index);          \
      if (ig_replacement_index.valid()) {                                      \
        OpIndex replacement = Asm().MapToNewGraph(ig_replacement_index);       \
        return replacement;                                                    \
      }                                                                        \
    }                                                                          \
    return Next::ReduceInputGraph##Name(ig_index, op);                         \
  }

  EMIT_OP(StructGet)
  EMIT_OP(ArrayLength)
  EMIT_OP(StringAsWtf16)
  EMIT_OP(StringPrepareForGetCodeUnit)
  EMIT_OP(AnyConvertExtern)

  OpIndex REDUCE_INPUT_GRAPH(StructSet)(OpIndex ig_index,
                                        const StructSetOp& op) {
    if (v8_flags.turboshaft_wasm_load_elimination) {
      OpIndex ig_replacement_index = analyzer_.Replacement(ig_index);
      if (ig_replacement_index.valid()) {
        // For struct.set, "replace with itself" is a sentinel for
        // "unreachable", and those are the only replacements we schedule for
        // this operation.
        DCHECK_EQ(ig_replacement_index, ig_index);
        __ Unreachable();
        return OpIndex::Invalid();
      }
    }
    return Next::ReduceInputGraphStructSet(ig_index, op);
  }

 private:
  WasmLoadEliminationAnalyzer analyzer_{
      Asm().data(), Asm().modifiable_input_graph(), Asm().phase_zone()};
};

void WasmLoadEliminationAnalyzer::ProcessBlock(const Block& block,
                                               bool compute_start_snapshot) {
  if (compute_start_snapshot) {
    BeginBlock(&block);
  }
  if (block.IsLoop() && BackedgeHasSnapshot(block)) {
    // Update the associated snapshot for the forward edge with the merged
    // snapshot information from the forward- and backward edge.
    // This will make sure that when evaluating whether a loop needs to be
    // revisited, the inner loop compares the merged state with the backedge
    // preventing us from exponential revisits for loops where the backedge
    // invalidates loads which are eliminatable on the forward edge.
    StoreLoopSnapshotInForwardPredecessor(block);
  }

  for (OpIndex op_idx : graph_.OperationIndices(block)) {
    Operation& op = graph_.Get(op_idx);
    if (ShouldSkipOptimizationStep()) continue;
    if (ShouldSkipOperation(op)) continue;
    switch (op.opcode) {
      case Opcode::kStructGet:
        ProcessStructGet(op_idx, op.Cast<StructGetOp>());
        break;
      case Opcode::kStructSet:
        ProcessStructSet(op_idx, op.Cast<StructSetOp>());
        break;
      case Opcode::kArrayLength:
        ProcessArrayLength(op_idx, op.Cast<ArrayLengthOp>());
        break;
      case Opcode::kWasmAllocateArray:
        ProcessWasmAllocateArray(op_idx, op.Cast<WasmAllocateArrayOp>());
        break;
      case Opcode::kStringAsWtf16:
        ProcessStringAsWtf16(op_idx, op.Cast<StringAsWtf16Op>());
        break;
      case Opcode::kStringPrepareForGetCodeUnit:
        ProcessStringPrepareForGetCodeUnit(
            op_idx, op.Cast<StringPrepareForGetCodeUnitOp>());
        break;
      case Opcode::kAnyConvertExtern:
        ProcessAnyConvertExtern(op_idx, op.Cast<AnyConvertExternOp>());
        break;
      case Opcode::kAssertNotNull:
        // TODO(14108): We'll probably want to handle WasmTypeCast as
        // a "load-like" instruction too, to eliminate repeated casts.
        ProcessAssertNotNull(op_idx, op.Cast<AssertNotNullOp>());
        break;
      case Opcode::kArraySet:
        break;
      case Opcode::kAllocate:
        // Create new non-alias.
        ProcessAllocate(op_idx, op.Cast<AllocateOp>());
        break;
      case Opcode::kCall:
        // Invalidate state (+ maybe invalidate aliases).
        ProcessCall(op_idx, op.Cast<CallOp>());
        break;
      case Opcode::kPhi:
        // Invalidate aliases.
        ProcessPhi(op_idx, op.Cast<PhiOp>());
        break;
      case Opcode::kLoad:
        // Atomic loads have the "can_write" bit set, because they make
        // writes on other threads visible. At any rate, we have to
        // explicitly skip them here.
      case Opcode::kStore:
        // We rely on having no raw "Store" operations operating on Wasm
        // objects at this point in the pipeline.
        // TODO(jkummerow): Is there any way to DCHECK that?
      case Opcode::kAssumeMap:
      case Opcode::kCatchBlockBegin:
      case Opcode::kRetain:
      case Opcode::kDidntThrow:
      case Opcode::kCheckException:
      case Opcode::kAtomicRMW:
      case Opcode::kAtomicWord32Pair:
      case Opcode::kMemoryBarrier:
      case Opcode::kJSStackCheck:
      case Opcode::kWasmStackCheck:
      case Opcode::kSimd128LaneMemory:
      case Opcode::kGlobalSet:
      case Opcode::kParameter:
        // We explicitly break for those operations that have can_write effects
        // but don't actually write, or cannot interfere with load elimination.
        break;

      case Opcode::kWordBinop:
        // A WordBinop should never invalidate aliases (since the only time when
        // it should take a non-aliasing object as input is for Smi checks).
        DcheckWordBinop(op_idx, op.Cast<WordBinopOp>());
        break;

      case Opcode::kFrameState:
      case Opcode::kDeoptimizeIf:
      case Opcode::kComparison:
      case Opcode::kTrapIf:
        // We explicitly break for these opcodes so that we don't call
        // InvalidateAllNonAliasingInputs on their inputs, since they don't
        // really create aliases. (and also, they don't write so it's
        // fine to break)
        DCHECK(!op.Effects().can_write());
        break;

      case Opcode::kDeoptimize:
      case Opcode::kReturn:
        // We explicitly break for these opcodes so that we don't call
        // InvalidateAllNonAliasingInputs on their inputs, since they are block
        // terminators without successors, meaning that it's not useful for the
        // rest of the analysis to invalidate anything here.
        DCHECK(op.IsBlockTerminator() && SuccessorBlocks(op).empty());
        break;

      default:
        // Operations that `can_write` should invalidate the state. All such
        // operations should be already handled above, which means that we don't
        // need a `if (can_write) { Invalidate(); }` here.
        CHECK(!op.Effects().can_write());

        // Even if the operation doesn't write, it could create an alias to its
        // input by returning it. This happens for instance in Phis and in
        // Change (although ChangeOp is already handled earlier by calling
        // ProcessChange). We are conservative here by calling
        // InvalidateAllNonAliasingInputs for all operations even though only
        // few can actually create aliases to fresh allocations, the reason
        // being that missing such a case would be a security issue, and it
        // should be rare for fresh allocations to be used outside of
        // Call/Store/Load/Change anyways.
        InvalidateAllNonAliasingInputs(op);

        break;
    }
  }

  FinishBlock(&block);
}

// Returns true if replacing a load with a RegisterRepresentation
// {expected_reg_rep} and size {in_memory_size} with an
// operation with RegisterRepresentation {actual} is valid. For instance,
// replacing an operation that returns a Float64 by one that returns a Word64 is
// not valid. Similarly, replacing a Tagged with an untagged value is probably
// not valid because of the GC.

bool RepIsCompatible(RegisterRepresentation actual,
                     RegisterRepresentation expected_reg_repr,
                     uint8_t in_memory_size) {
  if (in_memory_size !=
      MemoryRepresentation::FromRegisterRepresentation(actual, true)
          .SizeInBytes()) {
    // The replacement was truncated when being stored or should be truncated
    // (or sign-extended) during the load. Since we don't have enough
    // truncations operators in Turboshaft (eg, we don't have Int32 to Int8
    // truncation), we just prevent load elimination in this case.

    // TODO(jkummerow): support eliminating repeated loads of the same i8/i16
    // field.
    return false;
  }

  return expected_reg_repr == actual;
}

void WasmLoadEliminationAnalyzer::ProcessStructGet(OpIndex op_idx,
                                                   const StructGetOp& get) {
  OpIndex existing = memory_.Find(get);
  if (existing.valid()) {
    const Operation& replacement = graph_.Get(existing);
    DCHECK_EQ(replacement.outputs_rep().size(), 1);
    DCHECK_EQ(get.outputs_rep().size(), 1);
    uint8_t size = get.type->field(get.field_index).value_kind_size();
    if (RepIsCompatible(replacement.outputs_rep()[0], get.outputs_rep()[0],
                        size)) {
      replacements_[op_idx] = existing;
      return;
    }
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.Insert(get, op_idx);
}

void WasmLoadEliminationAnalyzer::ProcessStructSet(OpIndex op_idx,
                                                   const StructSetOp& set) {
  if (memory_.HasValueWithIncorrectMutability(set)) {
    // This struct.set is unreachable. We don't have a good way to annotate
    // it as such, so we use "replace with itself" as a sentinel.
    // TODO(jkummerow): Check how often this case is triggered in practice.
    replacements_[op_idx] = op_idx;
    return;
  }

  memory_.Invalidate(set);
  memory_.Insert(set);

  // Updating aliases if the value stored was known as non-aliasing.
  OpIndex value = set.value();
  if (non_aliasing_objects_.HasKeyFor(value)) {
    non_aliasing_objects_.Set(value, false);
  }
}

void WasmLoadEliminationAnalyzer::ProcessArrayLength(
    OpIndex op_idx, const ArrayLengthOp& length) {
  static constexpr int offset = wle::kArrayLengthFieldIndex;
  OpIndex existing = memory_.FindLoadLike(length.array(), offset);
  if (existing.valid()) {
#if DEBUG
    const Operation& replacement = graph_.Get(existing);
    DCHECK_EQ(replacement.outputs_rep().size(), 1);
    DCHECK_EQ(length.outputs_rep().size(), 1);
    DCHECK_EQ(replacement.outputs_rep()[0], length.outputs_rep()[0]);
#endif
    replacements_[op_idx] = existing;
    return;
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.InsertLoadLike(length.array(), offset, op_idx);
}

void WasmLoadEliminationAnalyzer::ProcessWasmAllocateArray(
    OpIndex op_idx, const WasmAllocateArrayOp& alloc) {
  non_aliasing_objects_.Set(op_idx, true);
  static constexpr int offset = wle::kArrayLengthFieldIndex;
  memory_.InsertLoadLike(op_idx, offset, alloc.length());
}

void WasmLoadEliminationAnalyzer::ProcessStringAsWtf16(
    OpIndex op_idx, const StringAsWtf16Op& op) {
  static constexpr int offset = wle::kStringAsWtf16Index;
  OpIndex existing = memory_.FindLoadLike(op.string(), offset);
  if (existing.valid()) {
    DCHECK_EQ(Opcode::kStringAsWtf16, graph_.Get(existing).opcode);
    replacements_[op_idx] = existing;
    return;
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.InsertLoadLike(op.string(), offset, op_idx);
}

void WasmLoadEliminationAnalyzer::ProcessStringPrepareForGetCodeUnit(
    OpIndex op_idx, const StringPrepareForGetCodeUnitOp& prep) {
  static constexpr int offset = wle::kStringPrepareForGetCodeunitIndex;
  OpIndex existing = memory_.FindLoadLike(prep.string(), offset);
  if (existing.valid()) {
    DCHECK_EQ(Opcode::kStringPrepareForGetCodeUnit,
              graph_.Get(existing).opcode);
    replacements_[op_idx] = existing;
    return;
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.InsertLoadLike(prep.string(), offset, op_idx);
}

void WasmLoadEliminationAnalyzer::ProcessAnyConvertExtern(
    OpIndex op_idx, const AnyConvertExternOp& convert) {
  static constexpr int offset = wle::kAnyConvertExternIndex;
  OpIndex existing = memory_.FindLoadLike(convert.object(), offset);
  if (existing.valid()) {
    DCHECK_EQ(Opcode::kAnyConvertExtern, graph_.Get(existing).opcode);
    replacements_[op_idx] = existing;
    return;
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.InsertLoadLike(convert.object(), offset, op_idx);
}

void WasmLoadEliminationAnalyzer::ProcessAssertNotNull(
    OpIndex op_idx, const AssertNotNullOp& assert) {
  static constexpr int offset = wle::kAssertNotNullIndex;
  OpIndex existing = memory_.FindLoadLike(assert.object(), offset);
  if (existing.valid()) {
    DCHECK_EQ(Opcode::kAssertNotNull, graph_.Get(existing).opcode);
    replacements_[op_idx] = existing;
    return;
  }
  replacements_[op_idx] = OpIndex::Invalid();
  memory_.InsertLoadLike(assert.object(), offset, op_idx);
}

// Since we only loosely keep track of what can or can't alias, we assume that
// anything that was guaranteed to not alias with anything (because it's in
// {non_aliasing_objects_}) can alias with anything when coming back from the
// call if it was an argument of the call.
void WasmLoadEliminationAnalyzer::ProcessCall(OpIndex op_idx,
                                              const CallOp& op) {
  // Some builtins do not create aliases and do not invalidate existing
  // memory, and some even return fresh objects. For such cases, we don't
  // invalidate the state, and record the non-alias if any.
  if (!op.Effects().can_write()) {
    return;
  }
  // TODO(jkummerow): Add special handling to builtins that are known not to
  // have relevant side effects. Alternatively, specify their effects to not
  // include `CanWriteMemory()`.
#if 0
  if (auto builtin_id = TryGetBuiltinId(
          graph_.Get(op.callee()).TryCast<ConstantOp>(), broker_)) {
    switch (*builtin_id) {
      case Builtin::kExample:
        // This builtin touches no Wasm objects, and calls no other functions.
        return;
      default:
        break;
    }
  }
#endif
  // Not a builtin call, or not a builtin that we know doesn't invalidate
  // memory.

  InvalidateAllNonAliasingInputs(op);

  // The call could modify arbitrary memory, so we invalidate every
  // potentially-aliasing object.
  memory_.InvalidateMaybeAliasing();
}

void WasmLoadEliminationAnalyzer::InvalidateAllNonAliasingInputs(
    const Operation& op) {
  for (OpIndex input : op.inputs()) {
    InvalidateIfAlias(input);
  }
}

void WasmLoadEliminationAnalyzer::InvalidateIfAlias(OpIndex op_idx) {
  if (auto key = non_aliasing_objects_.TryGetKeyFor(op_idx);
      key.has_value() && non_aliasing_objects_.Get(*key)) {
    // An known non-aliasing object was passed as input to the Call; the Call
    // could create aliases, so we have to consider going forward that this
    // object could actually have aliases.
    non_aliasing_objects_.Set(*key, false);
  }
}

// The only time an Allocate should flow into a WordBinop is for Smi checks
// (which, by the way, should be removed by MachineOptimizationReducer (since
// Allocate never returns a Smi), but there is no guarantee that this happens
// before load elimination). So, there is no need to invalidate non-aliases, and
// we just DCHECK in this function that indeed, nothing else than a Smi check
// happens on non-aliasing objects.
void WasmLoadEliminationAnalyzer::DcheckWordBinop(OpIndex op_idx,
                                                  const WordBinopOp& binop) {
#ifdef DEBUG
  auto check = [&](V<Word> left, V<Word> right) {
    if (auto key = non_aliasing_objects_.TryGetKeyFor(left);
        key.has_value() && non_aliasing_objects_.Get(*key)) {
      int64_t cst;
      DCHECK_EQ(binop.kind, WordBinopOp::Kind::kBitwiseAnd);
      DCHECK(OperationMatcher(graph_).MatchSignedIntegralConstant(right, &cst));
      DCHECK_EQ(cst, kSmiTagMask);
    }
  };
  check(binop.left(), binop.right());
  check(binop.right(), binop.left());
#endif
}

void WasmLoadEliminationAnalyzer::ProcessAllocate(OpIndex op_idx,
                                                  const AllocateOp&) {
  // In particular, this handles {struct.new}.
  non_aliasing_objects_.Set(op_idx, true);
}

void WasmLoadEliminationAnalyzer::ProcessPhi(OpIndex op_idx, const PhiOp& phi) {
  InvalidateAllNonAliasingInputs(phi);

  base::Vector<const OpIndex> inputs = phi.inputs();
  // This copies some of the functionality of {RequiredOptimizationReducer}:
  // Phis whose inputs are all the same value can be replaced by that value.
  // We need to have this logic here because interleaving it with other cases
  // of load elimination can unlock further optimizations: simplifying Phis
  // can allow elimination of more loads, which can then allow simplification
  // of even more Phis.
  if (inputs.size() > 0) {
    bool same_inputs = true;
    OpIndex first = memory_.ResolveBase(inputs.first());
    for (const OpIndex& input : inputs.SubVectorFrom(1)) {
      if (memory_.ResolveBase(input) != first) {
        same_inputs = false;
        break;
      }
    }
    if (same_inputs) {
      replacements_[op_idx] = first;
    }
  }
}

void WasmLoadEliminationAnalyzer::FinishBlock(const Block* block) {
  block_to_snapshot_mapping_[block->index()] =
      Snapshot{non_aliasing_objects_.Seal(), memory_.Seal()};
}

void WasmLoadEliminationAnalyzer::SealAndDiscard() {
  non_aliasing_objects_.Seal();
  memory_.Seal();
}

void WasmLoadEliminationAnalyzer::StoreLoopSnapshotInForwardPredecessor(
    const Block& loop_header) {
  auto non_aliasing_snapshot = non_aliasing_objects_.Seal();
  auto memory_snapshot = memory_.Seal();

  block_to_snapshot_mapping_
      [loop_header.LastPredecessor()->NeighboringPredecessor()->index()] =
          Snapshot{non_aliasing_snapshot, memory_snapshot};

  non_aliasing_objects_.StartNewSnapshot(non_aliasing_snapshot);
  memory_.StartNewSnapshot(memory_snapshot);
}

bool WasmLoadEliminationAnalyzer::BackedgeHasSnapshot(
    const Block& loop_header) const {
  DCHECK(loop_header.IsLoop());
  return block_to_snapshot_mapping_[loop_header.LastPredecessor()->index()]
      .has_value();
}

template <bool for_loop_revisit>
bool WasmLoadEliminationAnalyzer::BeginBlock(const Block* block) {
  DCHECK_IMPLIES(
      for_loop_revisit,
      block->IsLoop() &&
          block_to_snapshot_mapping_[block->LastPredecessor()->index()]
              .has_value());

  // Collect the snapshots of all predecessors.
  {
    predecessor_alias_snapshots_.clear();
    predecessor_memory_snapshots_.clear();
    for (const Block* p : block->PredecessorsIterable()) {
      auto pred_snapshots = block_to_snapshot_mapping_[p->index()];
      // When we visit the loop for the first time, the loop header hasn't
      // been visited yet, so we ignore it.
      DCHECK_IMPLIES(!pred_snapshots.has_value(),
                     block->IsLoop() && block->LastPredecessor() == p);
      if (!pred_snapshots.has_value()) {
        DCHECK(!for_loop_revisit);
        continue;
      }
      // Note that the backedge snapshot of an inner loop in kFirstVisit will
      // also be taken into account if we are in the kSecondVisit of an outer
      // loop. The data in the backedge snapshot could be out-dated, but if it
      // is, then it's fine: if the backedge of the outer-loop was more
      // restrictive than its forward incoming edge, then the forward incoming
      // edge of the inner loop should reflect this restriction.
      predecessor_alias_snapshots_.push_back(pred_snapshots->alias_snapshot);
      predecessor_memory_snapshots_.push_back(pred_snapshots->memory_snapshot);
    }
  }

  // Note that predecessors are in reverse order, which means that the backedge
  // is at offset 0.
  constexpr int kBackedgeOffset = 0;
  constexpr int kForwardEdgeOffset = 1;

  bool loop_needs_revisit = false;
  // Start a new snapshot for this block by merging information from
  // predecessors.
  auto merge_aliases = [&](AliasKey key,
                           base::Vector<const bool> predecessors) -> bool {
    if (for_loop_revisit && predecessors[kForwardEdgeOffset] &&
        !predecessors[kBackedgeOffset]) {
      // The backedge doesn't think that {key} is no-alias, but the loop
      // header previously thought it was --> need to revisit.
      loop_needs_revisit = true;
    }
    return base::all_of(predecessors);
  };
  non_aliasing_objects_.StartNewSnapshot(
      base::VectorOf(predecessor_alias_snapshots_), merge_aliases);

  // Merging for {memory_} means setting values to Invalid unless all
  // predecessors have the same value.
  // TODO(dmercadier): we could insert of Phis during the pass to merge existing
  // information. This is a bit hard, because we are currently in an analyzer
  // rather than a reducer. Still, we could "prepare" the insertion now and then
  // really insert them during the Reduce phase of the CopyingPhase.
  auto merge_memory = [&](MemoryKey key,
                          base::Vector<const OpIndex> predecessors) -> OpIndex {
    if (for_loop_revisit && predecessors[kForwardEdgeOffset].valid() &&
        predecessors[kBackedgeOffset] != predecessors[kForwardEdgeOffset]) {
      // {key} had a value in the loop header, but the backedge and the forward
      // edge don't agree on its value, which means that the loop invalidated
      // some memory data, and thus needs to be revisited.
      loop_needs_revisit = true;
    }
    return base::all_equal(predecessors) ? predecessors[0] : OpIndex::Invalid();
  };
  memory_.StartNewSnapshot(base::VectorOf(predecessor_memory_snapshots_),
                           merge_memory);

  if (block->IsLoop()) return loop_needs_revisit;
  return false;
}

#include "src/compiler/turboshaft/undef-assembler-macros.inc"

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_LOAD_ELIMINATION_REDUCER_H_
                                                                                                                                                                                                            node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-lowering-phase.cc                                  0000664 0000000 0000000 00000002311 14746647661 0025024 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/wasm-lowering-phase.h"

#include "src/compiler/js-heap-broker.h"
#include "src/compiler/turboshaft/copying-phase.h"
#include "src/compiler/turboshaft/machine-optimization-reducer.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/required-optimization-reducer.h"
#include "src/compiler/turboshaft/variable-reducer.h"
#include "src/compiler/turboshaft/wasm-lowering-reducer.h"
#include "src/numbers/conversions-inl.h"

namespace v8::internal::compiler::turboshaft {

void WasmLoweringPhase::Run(PipelineData* data, Zone* temp_zone) {
  UnparkedScopeIfNeeded scope(data->broker(),
                              v8_flags.turboshaft_trace_reduction);
  // Also run the MachineOptimizationReducer as it can help the late load
  // elimination that follows this phase eliminate more loads.
  CopyingPhase<WasmLoweringReducer, MachineOptimizationReducer>::Run(data,
                                                                     temp_zone);
}

}  // namespace v8::internal::compiler::turboshaft
                                                                                                                                                                                                                                                                                                                       node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-lowering-phase.h                                   0000664 0000000 0000000 00000001343 14746647661 0024672 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_LOWERING_PHASE_H_
#define V8_COMPILER_TURBOSHAFT_WASM_LOWERING_PHASE_H_

#include "src/compiler/turboshaft/phase.h"

namespace v8::internal::compiler::turboshaft {

struct WasmLoweringPhase {
  DECL_TURBOSHAFT_PHASE_CONSTANTS(WasmLowering)

  void Run(PipelineData* data, Zone* temp_zone);
};

}  // namespace v8::internal::compiler::turboshaft

#endif  // V8_COMPILER_TURBOSHAFT_WASM_LOWERING_PHASE_H_
                                                                                                                                                                                                                                                                                             node-23.7.0/deps/v8/src/compiler/turboshaft/wasm-lowering-reducer.h                                 0000664 0000000 0000000 00000121461 14746647661 0025227 0                                                                                                    ustar 00root                            root                            0000000 0000000                                                                                                                                                                        // Copyright 2023 the V8 project authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#include "src/compiler/turboshaft/builtin-call-descriptors.h"
#if !V8_ENABLE_WEBASSEMBLY
#error This header should only be included if WebAssembly is enabled.
#endif  // !V8_ENABLE_WEBASSEMBLY

#ifndef V8_COMPILER_TURBOSHAFT_WASM_LOWERING_REDUCER_H_
#define V8_COMPILER_TURBOSHAFT_WASM_LOWERING_REDUCER_H_

#include "src/compiler/globals.h"
#include "src/compiler/turboshaft/assembler.h"
#include "src/compiler/turboshaft/index.h"
#include "src/compiler/turboshaft/operations.h"
#include "src/compiler/turboshaft/phase.h"
#include "src/compiler/turboshaft/wasm-assembler-helpers.h"
#include "src/wasm/wasm-engine.h"
#include "src/wasm/wasm-module.h"
#include "src/wasm/wasm-objects.h"
#include "src/wasm/wasm-subtyping.h"

namespace v8::internal::compiler::turboshaft {

#include "src/compiler/turboshaft/define-assembler-macros.inc"

template <class Next>
class WasmLoweringReducer : public Next {
 public:
  TURBOSHAFT_REDUCER_BOILERPLATE(WasmLowering)

  V<Any> REDUCE(GlobalGet)(V<WasmTrustedInstanceData> instance,
                           const wasm::WasmGlobal* global) {
    return LowerGlobalSetOrGet(instance, OpIndex::Invalid(), global,
                               GlobalMode::kLoad);
  }

  OpIndex REDUCE(GlobalSet)(V<WasmTrustedInstanceData> instance, V<Any> value,
                            const wasm::WasmGlobal* global) {
    return LowerGlobalSetOrGet(instance, value, global, GlobalMode::kStore);
  }

  OpIndex REDUCE(Null)(wasm::ValueType type) { return Null(type); }

  V<Word32> REDUCE(IsNull)(OpIndex object, wasm::ValueType type) {
#if V8_STATIC_ROOTS_BOOL
    // TODO(14616): Extend this for shared types.
    const bool is_wasm_null =
        !wasm::IsSubtypeOf(type, wasm::kWasmExternRef, module_) &&
        !wasm::IsSubtypeOf(type, wasm::kWasmExnRef, module_);
    V<Object> null_value = V<Object>::Cast(
        __ UintPtrConstant(is_wasm_null ? StaticReadOnlyRoot::kWasmNull
                                        : StaticReadOnlyRoot::kNullValue));
#else
    OpIndex null_value = Null(type);
#endif
    return __ TaggedEqual(object, null_value);
  }

  V<Object> REDUCE(AssertNotNull)(V<Object> object, wasm::ValueType type,
                                  TrapId trap_id) {
    if (trap_id == TrapId::kTrapNullDereference) {
      // Skip the check altogether if null checks are turned off.
      if (!v8_flags.experimental_wasm_skip_null_checks) {
        // Use an explicit null check if
        // (1) we cannot use trap handler or
        // (2) the object might be a Smi or
        // (3) the object might be a JS object.
        if (null_check_strategy_ == NullCheckStrategy::kExplicit ||
            wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), type, module_) ||
            wasm::IsSubtypeOf(type, wasm::kWasmExternRef, module_) ||
            wasm::IsSubtypeOf(type, wasm::kWasmExnRef, module_)) {
          __ TrapIf(__ IsNull(object, type), trap_id);
        } else {
          // Otherwise, load the word after the map word.
          static_assert(WasmStruct::kHeaderSize > kTaggedSize);
          static_assert(WasmArray::kHeaderSize > kTaggedSize);
          static_assert(WasmInternalFunction::kHeaderSize > kTaggedSize);
          __ Load(object, LoadOp::Kind::TrapOnNull().Immutable(),
                  MemoryRepresentation::Int32(), kTaggedSize);
        }
      }
    } else {
      __ TrapIf(__ IsNull(object, type), trap_id);
    }
    return object;
  }

  V<Map> REDUCE(RttCanon)(V<FixedArray> rtts, uint32_t type_index) {
    int map_offset = FixedArray::kHeaderSize + type_index * kTaggedSize;
    return __ Load(rtts, LoadOp::Kind::TaggedBase().Immutable(),
                   MemoryRepresentation::AnyTagged(), map_offset);
  }

  V<Word32> REDUCE(WasmTypeCheck)(V<Object> object, OptionalV<Map> rtt,
                                  WasmTypeCheckConfig config) {
    if (rtt.has_value()) {
      return ReduceWasmTypeCheckRtt(object, rtt, config);
    } else {
      return ReduceWasmTypeCheckAbstract(object, config);
    }
  }

  V<Object> REDUCE(WasmTypeCast)(V<Object> object, OptionalV<Map> rtt,
                                 WasmTypeCheckConfig config) {
    if (rtt.has_value()) {
      return ReduceWasmTypeCastRtt(object, rtt, config);
    } else {
      return ReduceWasmTypeCastAbstract(object, config);
    }
  }

  V<Object> REDUCE(AnyConvertExtern)(V<Object> object) {
    Label<Object> end_label(&Asm());
    Label<> null_label(&Asm());
    Label<> smi_label(&Asm());
    Label<> int_to_smi_label(&Asm());
    Label<> heap_number_label(&Asm());

    constexpr int32_t kInt31MaxValue = 0x3fffffff;
    constexpr int32_t kInt31MinValue = -kInt31MaxValue - 1;

    GOTO_IF(__ IsNull(object, wasm::kWasmExternRef), null_label);
    GOTO_IF(__ IsSmi(object), smi_label);
    GOTO_IF(__ HasInstanceType(object, HEAP_NUMBER_TYPE), heap_number_label);
    // For anything else, just pass through the value.
    GOTO(end_label, object);

    BIND(null_label);
    GOTO(end_label, Null(wasm::kWasmAnyRef));

    // Canonicalize SMI.
    BIND(smi_label);
    if constexpr (SmiValuesAre31Bits()) {
      GOTO(end_label, object);
    } else {
      Label<> convert_to_heap_number_label(&Asm());
      V<Word32> int_value = __ UntagSmi(V<Smi>::Cast(object));

      // Convert to heap number if the int32 does not fit into an i31ref.
      GOTO_IF(__ Int32LessThan(__ Word32Constant(kInt31MaxValue), int_value),
              convert_to_heap_number_label);
      GOTO_IF(__ Int32LessThan(int_value, __ Word32Constant(kInt31MinValue)),
              convert_to_heap_number_label);
      GOTO(end_label, object);

      BIND(convert_to_heap_number_label);
      V<Object> heap_number = __ template WasmCallBuiltinThroughJumptable<
          BuiltinCallDescriptor::WasmInt32ToHeapNumber>({int_value});
      GOTO(end_label, heap_number);
    }

    // Convert HeapNumber to SMI if possible.
    BIND(heap_number_label);
    V<Float64> float_value =
        __ LoadHeapNumberValue(V<HeapNumber>::Cast(object));
    // Check range of float value.
    GOTO_IF(__ Float64LessThan(float_value, __ Float64Constant(kInt31MinValue)),
            end_label, object);
    GOTO_IF(__ Float64LessThan(__ Float64Constant(kInt31MaxValue), float_value),
            end_label, object);
    // Check if value is -0.
    V<Word32> is_minus_zero;
    if constexpr (Is64()) {
      V<Word64> minus_zero = __ Word64Constant(kMinusZeroBits);
      V<Word64> float_bits = __ BitcastFloat64ToWord64(float_value);
      is_minus_zero = __ Word64Equal(float_bits, minus_zero);
    } else {
      Label<Word32> done(&Asm());

      V<Word32> value_lo = __ Float64ExtractLowWord32(float_value);
      GOTO_IF_NOT(__ Word32Equal(value_lo, __ Word32Constant(kMinusZeroLoBits)),
                  done, __ Word32Constant(0));
      V<Word32> value_hi = __ Float64ExtractHighWord32(float_value);
      GOTO(done, __ Word32Equal(value_hi, __ Word32Constant(kMinusZeroHiBits)));
      BIND(done, phi_is_minus_zero);
      is_minus_zero = phi_is_minus_zero;
    }
    GOTO_IF(is_minus_zero, end_label, object);
    // Check if value is integral.
    V<Word32> int_value =
        __ TruncateFloat64ToInt32OverflowUndefined(float_value);
    GOTO_IF(__ Float64Equal(float_value, __ ChangeInt32ToFloat64(int_value)),
            int_to_smi_label);
    GOTO(end_label, object);

    BIND(int_to_smi_label);
    GOTO(end_label, __ TagSmi(int_value));

    BIND(end_label, result);
    return result;
  }

  V<Object> REDUCE(ExternConvertAny)(V<Object> object) {
    Label<Object> end(&Asm());
    GOTO_IF_NOT(__ IsNull(object, wasm::kWasmAnyRef), end, object);
    GOTO(end, Null(wasm::kWasmExternRef));
    BIND(end, result);
    return result;
  }

  V<Object> REDUCE(WasmTypeAnnotation)(V<Object> value, wasm::ValueType type) {
    // Remove type annotation operations as they are not needed any more.
    return value;
  }

  V<Any> REDUCE(StructGet)(V<WasmStructNullable> object,
                           const wasm::StructType* type, uint32_t type_index,
                           int field_index, bool is_signed,
                           CheckForNull null_check) {
    auto [explicit_null_check, implicit_null_check] =
        null_checks_for_struct_op(null_check, field_index);

    if (explicit_null_check) {
      __ TrapIf(__ IsNull(object, wasm::kWasmAnyRef),
                TrapId::kTrapNullDereference);
    }

    LoadOp::Kind load_kind = implicit_null_check ? LoadOp::Kind::TrapOnNull()
                                                 : LoadOp::Kind::TaggedBase();
    if (!type->mutability(field_index)) {
      load_kind = load_kind.Immutable();
    }
    MemoryRepresentation repr =
        RepresentationFor(type->field(field_index), is_signed);

    return __ Load(object, load_kind, repr, field_offset(type, field_index));
  }

  V<None> REDUCE(StructSet)(V<WasmStructNullable> object, V<Any> value,
                            const wasm::StructType* type, uint32_t type_index,
                            int field_index, CheckForNull null_check) {
    auto [explicit_null_check, implicit_null_check] =
        null_checks_for_struct_op(null_check, field_index);

    if (explicit_null_check) {
      __ TrapIf(__ IsNull(object, wasm::kWasmAnyRef),
                TrapId::kTrapNullDereference);
    }

    StoreOp::Kind store_kind = implicit_null_check
                                   ? StoreOp::Kind::TrapOnNull()
                                   : StoreOp::Kind::TaggedBase();
    MemoryRepresentation repr =
        RepresentationFor(type->field(field_index), true);

    __ Store(object, value, store_kind, repr,
             type->field(field_index).is_reference() ? kFullWriteBarrier
                                                     : kNoWriteBarrier,
             field_offset(type, field_index));

    return OpIndex::Invalid();
  }

  V<Any> REDUCE(ArrayGet)(V<WasmArrayNullable> array, V<Word32> index,
                          const wasm::ArrayType* array_type, bool is_signed) {
    bool is_mutable = array_type->mutability();
    LoadOp::Kind load_kind = is_mutable
                                 ? LoadOp::Kind::TaggedBase()
                                 : LoadOp::Kind::TaggedBase().Immutable();
    return __ Load(array, __ ChangeInt32ToIntPtr(index), load_kind,
                   RepresentationFor(array_type->element_type(), is_signed),
                   WasmArray::kHeaderSize,
                   array_type->element_type().value_kind_size_log2());
  }

  V<None> REDUCE(ArraySet)(V<WasmArrayNullable> array, V<Word32> index,
                           V<Any> value, wasm::ValueType element_type) {
    __ Store(array, __ ChangeInt32ToIntPtr(index), value,
             LoadOp::Kind::TaggedBase(), RepresentationFor(element_type, true),
             element_type.is_reference() ? kFullWriteBarrier : kNoWriteBarrier,
             WasmArray::kHeaderSize, element_type.value_kind_size_log2());
    return {};
  }

  V<Word32> REDUCE(ArrayLength)(V<WasmArrayNullable> array,
                                CheckForNull null_check) {
    bool explicit_null_check =
        null_check == kWithNullCheck &&
        null_check_strategy_ == NullCheckStrategy::kExplicit;
    bool implicit_null_check =
        null_check == kWithNullCheck &&
        null_check_strategy_ == NullCheckStrategy::kTrapHandler;

    if (explicit_null_check) {
      __ TrapIf(__ IsNull(array, wasm::kWasmAnyRef),
                TrapId::kTrapNullDereference);
    }

    LoadOp::Kind load_kind = implicit_null_check
                                 ? LoadOp::Kind::TrapOnNull().Immutable()
                                 : LoadOp::Kind::TaggedBase().Immutable();

    return __ Load(array, load_kind, RepresentationFor(wasm::kWasmI32, true),
                   WasmArray::kLengthOffset);
  }

  V<WasmArray> REDUCE(WasmAllocateArray)(V<Map> rtt, V<Word32> length,
                                         const wasm::ArrayType* array_type) {
    __ TrapIfNot(
        __ Uint32LessThanOrEqual(
            length, __ Word32Constant(WasmArray::MaxLength(array_type))),
        TrapId::kTrapArrayTooLarge);
    wasm::ValueType element_type = array_type->element_type();

    // RoundUp(length * value_size, kObjectAlignment) =
    //   RoundDown(length * value_size + kObjectAlignment - 1,
    //             kObjectAlignment);
    V<Word32> padded_length = __ Word32BitwiseAnd(
        __ Word32Add(__ Word32Mul(length, __ Word32Constant(
                                              element_type.value_kind_size())),
                     __ Word32Constant(int32_t{kObjectAlignment - 1})),
        __ Word32Constant(int32_t{-kObjectAlignment}));
    Uninitialized<WasmArray> a = __ template Allocate<WasmArray>(
        __ ChangeUint32ToUintPtr(__ Word32Add(
            padded_length, __ Word32Constant(WasmArray::kHeaderSize))),
        AllocationType::kYoung);

    // TODO(14108): The map and empty fixed array initialization should be an
    // immutable store.
    __ InitializeField(a, AccessBuilder::ForMap(compiler::kNoWriteBarrier),
                       rtt);
    __ InitializeField(a, AccessBuilder::ForJSObjectPropertiesOrHash(),
                       LOAD_ROOT(EmptyFixedArray));
    __ InitializeField(a, AccessBuilder::ForWasmArrayLength(), length);

    // Note: Only the array header initialization is finished here, the elements
    // still need to be initialized by other code.
    V<WasmArray> array = __ FinishInitialization(std::move(a));
    return array;
  }

  V<WasmStruct> REDUCE(WasmAllocateStruct)(
      V<Map> rtt, const wasm::StructType* struct_type) {
    int size = WasmStruct::Size(struct_type);
    Uninitialized<WasmStruct> s =
        __ template Allocate<WasmStruct>(size, AllocationType::kYoung);
    __ InitializeField(s, AccessBuilder::ForMap(compiler::kNoWriteBarrier),
                       rtt);
    __ InitializeField(s, AccessBuilder::ForJSObjectPropertiesOrHash(),
                       LOAD_ROOT(EmptyFixedArray));
    // Note: Struct initialization isn't finished here, the user defined fields
    // still need to be initialized by other operations.
    V<WasmStruct> struct_value = __ FinishInitialization(std::move(s));
    return struct_value;
  }

  V<WasmFuncRef> REDUCE(WasmRefFunc)(V<WasmTrustedInstanceData> wasm_instance,
                                     uint32_t function_index) {
    V<FixedArray> func_refs = LOAD_IMMUTABLE_INSTANCE_FIELD(
        wasm_instance, FuncRefs, MemoryRepresentation::TaggedPointer());
    V<Object> maybe_func_ref =
        __ LoadFixedArrayElement(func_refs, function_index);

    Label<WasmFuncRef> done(&Asm());
    IF (UNLIKELY(__ IsSmi(maybe_func_ref))) {
      bool extract_shared_data =
          !shared_ && module_->function_is_shared(function_index);

      V<WasmFuncRef> from_builtin = __ template WasmCallBuiltinThroughJumptable<
          BuiltinCallDescriptor::WasmRefFunc>(
          {__ Word32Constant(function_index),
           __ Word32Constant(extract_shared_data ? 1 : 0)});

      GOTO(done, from_builtin);
    } ELSE {
      GOTO(done, V<WasmFuncRef>::Cast(maybe_func_ref));
    }

    BIND(done, result_value);
    return result_value;
  }

  V<String> REDUCE(StringAsWtf16)(V<String> string) {
    Label<String> done(&Asm());
    V<Word32> instance_type = __ LoadInstanceTypeField(__ LoadMapField(string));
    V<Word32> string_representation = __ Word32BitwiseAnd(
        instance_type, __ Word32Constant(kStringRepresentationMask));
    GOTO_IF(__ Word32Equal(string_representation, kSeqStringTag), done, string);

    GOTO(done, __ template WasmCallBuiltinThroughJumptable<
                   BuiltinCallDescriptor::WasmStringAsWtf16>({string}));
    BIND(done, result);
    return result;
  }

  OpIndex REDUCE(StringPrepareForGetCodeUnit)(V<Object> original_string) {
    LoopLabel<Object /*string*/, Word32 /*instance type*/, Word32 /*offset*/>
        dispatch(&Asm());
    Label<Object /*string*/, Word32 /*instance type*/, Word32 /*offset*/>
        direct_string(&Asm());

    // These values will be used to replace the original node's projections.
    // The first, "string", is either a SeqString or Tagged<Smi>(0) (in case of
    // external string). Notably this makes it GC-safe: if that string moves,
    // this pointer will be updated accordingly. The second, "offset", has full
    // register width so that it can be used to store external pointers: for
    // external strings, we add up the character backing store's base address
    // and any slice offset. The third, "character width", is a shift width,
    // i.e. it is 0 for one-byte strings, 1 for two-byte strings,
    // kCharWidthBailoutSentinel for uncached external strings (for which
    // "string"/"offset" are invalid and unusable).
    Label<Object /*string*/, WordPtr /*offset*/, Word32 /*character width*/>
        done(&Asm());

    V<Word32> original_type =
        __ LoadInstanceTypeField(__ LoadMapField(original_string));
    GOTO(dispatch, original_string, original_type, __ Word32Constant(0));

    BIND_LOOP(dispatch, string, instance_type, offset) {
      Label<> thin_string(&Asm());
      Label<> cons_string(&Asm());

      static_assert(kIsIndirectStringTag == 1);
      static constexpr int kIsDirectStringTag = 0;
      GOTO_IF(__ Word32Equal(
                  __ Word32BitwiseAnd(instance_type, kIsIndirectStringMask),
                  kIsDirectStringTag),
              direct_string, string, instance_type, offset);

      // Handle indirect strings.
      V<Word32> string_representation =
          __ Word32BitwiseAnd(instance_type, kStringRepresentationMask);
      GOTO_IF(__ Word32Equal(string_representation, kThinStringTag),
              thin_string);
      GOTO_IF(__ Word32Equal(string_representation, kConsStringTag),
              cons_string);

      // Sliced string.
      V<Word32> new_offset = __ Word32Add(
          offset, __ UntagSmi(__ template LoadField<Smi>(
                      string, AccessBuilder::ForSlicedStringOffset())));
      V<Object> parent = __ template LoadField<Object>(
          string, AccessBuilder::ForSlicedStringParent());
      V<Word32> parent_type = __ LoadInstanceTypeField(__ LoadMapField(parent));
      GOTO(dispatch, parent, parent_type, new_offset);

      // Thin string.
      BIND(thin_string);
      V<Object> actual = __ template LoadField<Object>(
          string, AccessBuilder::ForThinStringActual());
      V<Word32> actual_type = __ LoadInstanceTypeField(__ LoadMapField(actual));
      // ThinStrings always reference (internalized) direct strings.
      GOTO(direct_string, actual, actual_type, offset);

      // Flat cons string. (Non-flat cons strings are ruled out by
      // string.as_wtf16.)
      BIND(cons_string);
      V<Object> first = __ template LoadField<Object>(
          string, AccessBuilder::ForConsStringFirst());
      V<Word32> first_type = __ LoadInstanceTypeField(__ LoadMapField(first));
      GOTO(dispatch, first, first_type, offset);
    }
    {
      BIND(direct_string, string, instance_type, offset);

      V<Word32> is_onebyte =
          __ Word32BitwiseAnd(instance_type, kStringEncodingMask);
      // Char width shift is 1 - (is_onebyte).
      static_assert(kStringEncodingMask == 1 << 3);
      V<Word32> charwidth_shift =
          __ Word32Sub(1, __ Word32ShiftRightLogical(is_onebyte, 3));

      Label<> external(&Asm());
      V<Word32> string_representation =
          __ Word32BitwiseAnd(instance_type, kStringRepresentationMask);
      GOTO_IF(__ Word32Equal(string_representation, kExternalStringTag),
              external);

      // Sequential string.
      DCHECK_EQ(AccessBuilder::ForSeqOneByteStringCharacter().header_size,
                AccessBuilder::ForSeqTwoByteStringCharacter().header_size);
      const int chars_start_offset =
          AccessBuilder::ForSeqOneByteStringCharacter().header_size;
      V<Word32> final_offset =
          __ Word32Add(chars_start_offset - kHeapObjectTag,
                       __ Word32ShiftLeft(offset, charwidth_shift));
      GOTO(done, string, __ ChangeInt32ToIntPtr(final_offset), charwidth_shift);

      // External string.
      BIND(external);
      GOTO_IF(__ Word32BitwiseAnd(instance_type, kUncachedExternalStringMask),
              done, string, /*offset*/ 0, kCharWidthBailoutSentinel);
      V<WordPtr> resource = BuildLoadExternalPointerFromObject(
          string, AccessBuilder::ForExternalStringResourceData());
      V<Word32> shifted_offset = __ Word32ShiftLeft(offset, charwidth_shift);
      V<WordPtr> final_offset_external =
          __ WordPtrAdd(resource, __ ChangeInt32ToIntPtr(shifted_offset));
      GOTO(done, __ SmiConstant(Smi::FromInt(0)), final_offset_external,
           charwidth_shift);
    }
    {
      BIND(done, base, final_offset, charwidth_shift);
      return __ Tuple({base, final_offset, charwidth_shift});
    }
  }

 private:
  enum class GlobalMode { kLoad, kStore };

  static constexpr MemoryRepresentation kMaybeSandboxedPointer =
      V8_ENABLE_SANDBOX_BOOL ? MemoryRepresentation::SandboxedPointer()
                             : MemoryRepresentation::UintPtr();

  MemoryRepresentation RepresentationFor(wasm::ValueType type, bool is_signed) {
    switch (type.kind()) {
      case wasm::kI8:
        return is_signed ? MemoryRepresentation::Int8()
                         : MemoryRepresentation::Uint8();
      case wasm::kI16:
        return is_signed ? MemoryRepresentation::Int16()
                         : MemoryRepresentation::Uint16();
      case wasm::kI32:
        return is_signed ? MemoryRepresentation::Int32()
                         : MemoryRepresentation::Uint32();
      case wasm::kI64:
        return is_signed ? MemoryRepresentation::Int64()
                         : MemoryRepresentation::Uint64();
      case wasm::kF16:
        return MemoryRepresentation::Float16();
      case wasm::kF32:
        return MemoryRepresentation::Float32();
      case wasm::kF64:
        return MemoryRepresentation::Float64();
      case wasm::kS128:
        return MemoryRepresentation::Simd128();
      case wasm::kRtt:
      case wasm::kRef:
      case wasm::kRefNull:
        return MemoryRepresentation::AnyTagged();
      case wasm::kVoid:
      case wasm::kBottom:
        UNREACHABLE();
    }
  }

  V<WordPtr> BuildLoadExternalPointerFromObject(V<Object> object,
                                                FieldAccess access) {
#ifdef V8_ENABLE_SANDBOX
    DCHECK_NE(access.external_pointer_tag, kExternalPointerNullTag);
    V<Word32> handle = __ Load(object, LoadOp::Kind::TaggedBase(),
                               MemoryRepresentation::Uint32(), access.offset);
    return __ DecodeExternalPointer(handle, access.external_pointer_tag);
#else
    return __ Load(object, LoadOp::Kind::TaggedBase(),
                   MemoryRepresentation::UintPtr(), access.offset);
#endif  // V8_ENABLE_SANDBOX
  }

  V<Word32> ReduceWasmTypeCheckAbstract(V<Object> object,
                                        WasmTypeCheckConfig config) {
    const bool object_can_be_null = config.from.is_nullable();
    const bool null_succeeds = config.to.is_nullable();
    const bool object_can_be_i31 =
        wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from,
                          module_) ||
        config.from.heap_representation() == wasm::HeapType::kExtern;

    V<Word32> result;
    Label<Word32> end_label(&Asm());

    wasm::HeapType::Representation to_rep = config.to.heap_representation();
    do {
      // The none-types only perform a null check. They need no control flow.
      if (to_rep == wasm::HeapType::kNone ||
          to_rep == wasm::HeapType::kNoExtern ||
          to_rep == wasm::HeapType::kNoFunc ||
          to_rep == wasm::HeapType::kNoExn) {
        result = __ IsNull(object, config.from);
        break;
      }
      // Null checks performed by any other type check need control flow. We can
      // skip the null check if null fails, because it's covered by the Smi
      // check or instance type check we'll do later.
      if (object_can_be_null && null_succeeds) {
        const int kResult = 1;
        GOTO_IF(UNLIKELY(__ IsNull(object, wasm::kWasmAnyRef)), end_label,
                __ Word32Constant(kResult));
      }
      // i31 is special in that the Smi check is the last thing to do.
      if (to_rep == wasm::HeapType::kI31) {
        // If earlier optimization passes reached the limit of possible graph
        // transformations, we could DCHECK(object_can_be_i31) here.
        result = object_can_be_i31 ? __ IsSmi(object) : __ Word32Constant(0);
        break;
      }
      if (to_rep == wasm::HeapType::kEq) {
        if (object_can_be_i31) {
          GOTO_IF(UNLIKELY(__ IsSmi(object)), end_label, __ Word32Constant(1));
        }
        result = IsDataRefMap(__ LoadMapField(object));
        break;
      }
      // array, struct, string: i31 fails.
      if (object_can_be_i31) {
        GOTO_IF(UNLIKELY(__ IsSmi(object)), end_label, __ Word32Constant(0));
      }
      if (to_rep == wasm::HeapType::kArray) {
        result = __ HasInstanceType(object, WASM_ARRAY_TYPE);
        break;
      }
      if (to_rep == wasm::HeapType::kStruct) {
        result = __ HasInstanceType(object, WASM_STRUCT_TYPE);
        break;
      }
      if (to_rep == wasm::HeapType::kString ||
          to_rep == wasm::HeapType::kExternString) {
        V<Word32> instance_type =
            __ LoadInstanceTypeField(__ LoadMapField(object));
        result = __ Uint32LessThan(instance_type,
                                   __ Word32Constant(FIRST_NONSTRING_TYPE));
        break;
      }
      UNREACHABLE();
    } while (false);

    DCHECK(__ generating_unreachable_operations() || result.valid());
    GOTO(end_label, result);
    BIND(end_label, final_result);
    return final_result;
  }

  V<Object> ReduceWasmTypeCastAbstract(V<Object> object,
                                       WasmTypeCheckConfig config) {
    const bool object_can_be_null = config.from.is_nullable();
    const bool null_succeeds = config.to.is_nullable();
    const bool object_can_be_i31 =
        wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from,
                          module_) ||
        config.from.heap_representation() == wasm::HeapType::kExtern;

    Label<> end_label(&Asm());

    wasm::HeapType::Representation to_rep = config.to.heap_representation();

    do {
      // The none-types only perform a nu